{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b34d606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "93cb8651",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialRegression:\n",
    "    def __init__(self, degrees, n_iters=1000, lr=0.01):\n",
    "        self.degrees = degrees\n",
    "        self.n_iters = n_iters\n",
    "        self.lr = lr\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.losses = None\n",
    "        \n",
    "    #Calculate the R2 score    \n",
    "    def r2_score(self, y, y_pred):\n",
    "        mean_y = np.mean(y)\n",
    "        ss_total = np.sum((y - mean_y)**2)\n",
    "        ss_residual = np.sum((y - y_pred)**2)\n",
    "        r2 = 1 - (ss_residual / ss_total)\n",
    "        return r2\n",
    "\n",
    "    #Calculate the loss function    \n",
    "    def loss(self, y, y_pred):\n",
    "        return np.mean((y_pred - y)**2)\n",
    "    \n",
    "    #Calculate Gradients (2* is adjusted in the learning rate parameter)\n",
    "    def gradients(self, X, y, y_pred):\n",
    "        n_samples, n_features = X.shape\n",
    "        dw = (1/n_samples)*np.dot(X.T, (y_pred - y))\n",
    "        db = (1/n_samples)*np.sum(y_pred - y)\n",
    "        return dw, db\n",
    "    \n",
    "    #Copy the values from X to t and as per the degree provided in the list find it exponent\n",
    "    #example: degree=[3] then X -> X^2 -> X^3\n",
    "    def x_transform(self, X):\n",
    "        t = X.copy()\n",
    "        for i in self.degrees:\n",
    "            X = np.append(X, t**i, axis=1)\n",
    "        return X\n",
    "    \n",
    "    \n",
    "    #Fit the Model\n",
    "    def fit(self, X, y):\n",
    "        x = self.x_transform(X)\n",
    "        n_samples, n_features = x.shape\n",
    "        self.weights = np.zeros((n_features, 1))\n",
    "        self.bias = 0\n",
    "        y = y.reshape(n_samples, 1)\n",
    "        self.losses = []\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            y_pred = np.dot(x, self.weights) + self.bias\n",
    "            \n",
    "            dw = (1/n_samples)*np.dot(x.T, (y_pred - y))\n",
    "            db = (1/n_samples)*np.sum(y_pred - y)\n",
    "            \n",
    "            #Update the weights and bias\n",
    "            self.weights = self.weights - self.lr * dw\n",
    "            self.bias = self.bias - self.lr * db\n",
    "            \n",
    "            #calculate and Store the loss in a empty list\n",
    "            l = self.loss(y, y_pred)\n",
    "            self.losses.append(l)\n",
    "            print('Iteration',_,'Loss:',l)\n",
    "\n",
    "    def predict(self, X):\n",
    "        x1 = self.x_transform(X)\n",
    "        y_pred = np.dot(x1, self.weights) + self.bias\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b79214e",
   "metadata": {},
   "source": [
    "### Initialize the Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "21a490b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "X = np.random.rand(100,1)\n",
    "y = 2 * (4 ** X) + np.random.rand(100,1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X[:,0], y, color=\"blue\", s = 10, label='Data Points')\n",
    "plt.xlabel('Feature X')\n",
    "plt.ylabel('Target y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1cfa43",
   "metadata": {},
   "source": [
    "### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "544016c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 Loss: 25.70638552207037\n",
      "Iteration 1 Loss: 25.012642372009775\n",
      "Iteration 2 Loss: 24.337987594586007\n",
      "Iteration 3 Loss: 23.681895244238806\n",
      "Iteration 4 Loss: 23.043853868563463\n",
      "Iteration 5 Loss: 22.423366108928093\n",
      "Iteration 6 Loss: 21.819948312096535\n",
      "Iteration 7 Loss: 21.233130152553617\n",
      "Iteration 8 Loss: 20.662454265237898\n",
      "Iteration 9 Loss: 20.10747588839506\n",
      "Iteration 10 Loss: 19.567762516273095\n",
      "Iteration 11 Loss: 19.042893561388006\n",
      "Iteration 12 Loss: 18.532460026096388\n",
      "Iteration 13 Loss: 18.036064183218365\n",
      "Iteration 14 Loss: 17.55331926546146\n",
      "Iteration 15 Loss: 17.083849163402977\n",
      "Iteration 16 Loss: 16.62728813179488\n",
      "Iteration 17 Loss: 16.183280503962052\n",
      "Iteration 18 Loss: 15.75148041407068\n",
      "Iteration 19 Loss: 15.33155152705007\n",
      "Iteration 20 Loss: 14.923166775956863\n",
      "Iteration 21 Loss: 14.526008106576683\n",
      "Iteration 22 Loss: 14.139766229063621\n",
      "Iteration 23 Loss: 13.764140376423825\n",
      "Iteration 24 Loss: 13.39883806965434\n",
      "Iteration 25 Loss: 13.043574889354076\n",
      "Iteration 26 Loss: 12.698074253628302\n",
      "Iteration 27 Loss: 12.362067202113433\n",
      "Iteration 28 Loss: 12.035292185953328\n",
      "Iteration 29 Loss: 11.717494863563152\n",
      "Iteration 30 Loss: 11.408427902021328\n",
      "Iteration 31 Loss: 11.10785078393442\n",
      "Iteration 32 Loss: 10.815529619624215\n",
      "Iteration 33 Loss: 10.531236964490294\n",
      "Iteration 34 Loss: 10.254751641405463\n",
      "Iteration 35 Loss: 9.985858568005394\n",
      "Iteration 36 Loss: 9.724348588737591\n",
      "Iteration 37 Loss: 9.470018311538496\n",
      "Iteration 38 Loss: 9.222669949011268\n",
      "Iteration 39 Loss: 8.982111163980154\n",
      "Iteration 40 Loss: 8.748154919300848\n",
      "Iteration 41 Loss: 8.520619331809629\n",
      "Iteration 42 Loss: 8.299327530297104\n",
      "Iteration 43 Loss: 8.084107517395783\n",
      "Iteration 44 Loss: 7.874792035273536\n",
      "Iteration 45 Loss: 7.671218435028095\n",
      "Iteration 46 Loss: 7.473228549680629\n",
      "Iteration 47 Loss: 7.280668570669155\n",
      "Iteration 48 Loss: 7.09338892774542\n",
      "Iteration 49 Loss: 6.911244172181396\n",
      "Iteration 50 Loss: 6.734092863194216\n",
      "Iteration 51 Loss: 6.561797457500885\n",
      "Iteration 52 Loss: 6.394224201916452\n",
      "Iteration 53 Loss: 6.231243028911868\n",
      "Iteration 54 Loss: 6.072727455049875\n",
      "Iteration 55 Loss: 5.918554482219697\n",
      "Iteration 56 Loss: 5.768604501593352\n",
      "Iteration 57 Loss: 5.622761200228616\n",
      "Iteration 58 Loss: 5.480911470245701\n",
      "Iteration 59 Loss: 5.3429453205067166\n",
      "Iteration 60 Loss: 5.208755790728942\n",
      "Iteration 61 Loss: 5.07823886796487\n",
      "Iteration 62 Loss: 4.951293405383764\n",
      "Iteration 63 Loss: 4.827821043291329\n",
      "Iteration 64 Loss: 4.707726132325827\n",
      "Iteration 65 Loss: 4.5909156587706335\n",
      "Iteration 66 Loss: 4.477299171924941\n",
      "Iteration 67 Loss: 4.36678871347589\n",
      "Iteration 68 Loss: 4.259298748816946\n",
      "Iteration 69 Loss: 4.154746100258941\n",
      "Iteration 70 Loss: 4.053049882081573\n",
      "Iteration 71 Loss: 3.9541314373746785\n",
      "Iteration 72 Loss: 3.8579142766199523\n",
      "Iteration 73 Loss: 3.7643240179651363\n",
      "Iteration 74 Loss: 3.6732883291440714\n",
      "Iteration 75 Loss: 3.5847368709972214\n",
      "Iteration 76 Loss: 3.498601242548591\n",
      "Iteration 77 Loss: 3.4148149275961424\n",
      "Iteration 78 Loss: 3.333313242773989\n",
      "Iteration 79 Loss: 3.2540332870458406\n",
      "Iteration 80 Loss: 3.176913892590226\n",
      "Iteration 81 Loss: 3.101895577039154\n",
      "Iteration 82 Loss: 3.0289204970329346\n",
      "Iteration 83 Loss: 2.957932403054844\n",
      "Iteration 84 Loss: 2.888876595510429\n",
      "Iteration 85 Loss: 2.821699882017091\n",
      "Iteration 86 Loss: 2.7563505358706437\n",
      "Iteration 87 Loss: 2.6927782556563846\n",
      "Iteration 88 Loss: 2.6309341259731553\n",
      "Iteration 89 Loss: 2.5707705792397113\n",
      "Iteration 90 Loss: 2.512241358553582\n",
      "Iteration 91 Loss: 2.455301481573417\n",
      "Iteration 92 Loss: 2.3999072053966146\n",
      "Iteration 93 Loss: 2.346015992404799\n",
      "Iteration 94 Loss: 2.2935864770504937\n",
      "Iteration 95 Loss: 2.242578433559029\n",
      "Iteration 96 Loss: 2.19295274452049\n",
      "Iteration 97 Loss: 2.1446713703471554\n",
      "Iteration 98 Loss: 2.097697319572588\n",
      "Iteration 99 Loss: 2.0519946199691934\n",
      "Iteration 100 Loss: 2.0075282904616643\n",
      "Iteration 101 Loss: 1.9642643138144116\n",
      "Iteration 102 Loss: 1.922169610071632\n",
      "Iteration 103 Loss: 1.8812120107292742\n",
      "Iteration 104 Loss: 1.841360233618748\n",
      "Iteration 105 Loss: 1.8025838584827483\n",
      "Iteration 106 Loss: 1.7648533032241283\n",
      "Iteration 107 Loss: 1.7281398008092714\n",
      "Iteration 108 Loss: 1.6924153768079377\n",
      "Iteration 109 Loss: 1.6576528275520275\n",
      "Iteration 110 Loss: 1.6238256988962163\n",
      "Iteration 111 Loss: 1.5909082655638838\n",
      "Iteration 112 Loss: 1.5588755110621915\n",
      "Iteration 113 Loss: 1.5277031081506351\n",
      "Iteration 114 Loss: 1.4973673998478199\n",
      "Iteration 115 Loss: 1.4678453809616236\n",
      "Iteration 116 Loss: 1.439114680128328\n",
      "Iteration 117 Loss: 1.4111535423466892\n",
      "Iteration 118 Loss: 1.383940811993311\n",
      "Iteration 119 Loss: 1.3574559163060584\n",
      "Iteration 120 Loss: 1.3316788493226106\n",
      "Iteration 121 Loss: 1.3065901562616165\n",
      "Iteration 122 Loss: 1.2821709183342496\n",
      "Iteration 123 Loss: 1.2584027379743057\n",
      "Iteration 124 Loss: 1.2352677244753139\n",
      "Iteration 125 Loss: 1.2127484800234314\n",
      "Iteration 126 Loss: 1.1908280861152363\n",
      "Iteration 127 Loss: 1.1694900903497885\n",
      "Iteration 128 Loss: 1.1487184935846642\n",
      "Iteration 129 Loss: 1.1284977374459257\n",
      "Iteration 130 Loss: 1.1088126921822723\n",
      "Iteration 131 Loss: 1.0896486448538885\n",
      "Iteration 132 Loss: 1.0709912878467756\n",
      "Iteration 133 Loss: 1.0528267077035764\n",
      "Iteration 134 Loss: 1.035141374262198\n",
      "Iteration 135 Loss: 1.0179221300937271\n",
      "Iteration 136 Loss: 1.0011561802314073\n",
      "Iteration 137 Loss: 0.9848310821826468\n",
      "Iteration 138 Loss: 0.9689347362162636\n",
      "Iteration 139 Loss: 0.9534553759173807\n",
      "Iteration 140 Loss: 0.9383815590025961\n",
      "Iteration 141 Loss: 0.9237021583882559\n",
      "Iteration 142 Loss: 0.9094063535048544\n",
      "Iteration 143 Loss: 0.8954836218507826\n",
      "Iteration 144 Loss: 0.881923730778821\n",
      "Iteration 145 Loss: 0.868716729508978\n",
      "Iteration 146 Loss: 0.8558529413614167\n",
      "Iteration 147 Loss: 0.8433229562034237\n",
      "Iteration 148 Loss: 0.8311176231045104\n",
      "Iteration 149 Loss: 0.8192280431939138\n",
      "Iteration 150 Loss: 0.8076455627149232\n",
      "Iteration 151 Loss: 0.7963617662706042\n",
      "Iteration 152 Loss: 0.7853684702556459\n",
      "Iteration 153 Loss: 0.7746577164692081\n",
      "Iteration 154 Loss: 0.7642217659037719\n",
      "Iteration 155 Loss: 0.7540530927051519\n",
      "Iteration 156 Loss: 0.7441443782989479\n",
      "Iteration 157 Loss: 0.7344885056788524\n",
      "Iteration 158 Loss: 0.7250785538523516\n",
      "Iteration 159 Loss: 0.7159077924394855\n",
      "Iteration 160 Loss: 0.7069696764204453\n",
      "Iteration 161 Loss: 0.6982578410279124\n",
      "Iteration 162 Loss: 0.6897660967801433\n",
      "Iteration 163 Loss: 0.6814884246509266\n",
      "Iteration 164 Loss: 0.6734189713726433\n",
      "Iteration 165 Loss: 0.6655520448687503\n",
      "Iteration 166 Loss: 0.657882109812138\n",
      "Iteration 167 Loss: 0.6504037833058773\n",
      "Iteration 168 Loss: 0.6431118306829949\n",
      "Iteration 169 Loss: 0.6360011614219889\n",
      "Iteration 170 Loss: 0.6290668251749004\n",
      "Iteration 171 Loss: 0.6223040079048374\n",
      "Iteration 172 Loss: 0.6157080281299355\n",
      "Iteration 173 Loss: 0.6092743332708208\n",
      "Iteration 174 Loss: 0.6029984960987257\n",
      "Iteration 175 Loss: 0.5968762112814796\n",
      "Iteration 176 Loss: 0.5909032920246837\n",
      "Iteration 177 Loss: 0.58507566680544\n",
      "Iteration 178 Loss: 0.5793893761960893\n",
      "Iteration 179 Loss: 0.5738405697754752\n",
      "Iteration 180 Loss: 0.5684255031253223\n",
      "Iteration 181 Loss: 0.5631405349093846\n",
      "Iteration 182 Loss: 0.5579821240330824\n",
      "Iteration 183 Loss: 0.5529468268814082\n",
      "Iteration 184 Loss: 0.5480312946329475\n",
      "Iteration 185 Loss: 0.5432322706479158\n",
      "Iteration 186 Loss: 0.5385465879281712\n",
      "Iteration 187 Loss: 0.5339711666472198\n",
      "Iteration 188 Loss: 0.5295030117482875\n",
      "Iteration 189 Loss: 0.525139210608578\n",
      "Iteration 190 Loss: 0.5208769307678968\n",
      "Iteration 191 Loss: 0.5167134177198649\n",
      "Iteration 192 Loss: 0.5126459927640015\n",
      "Iteration 193 Loss: 0.5086720509169933\n",
      "Iteration 194 Loss: 0.5047890588815218\n",
      "Iteration 195 Loss: 0.5009945530710637\n",
      "Iteration 196 Loss: 0.49728613768911956\n",
      "Iteration 197 Loss: 0.4936614828613732\n",
      "Iteration 198 Loss: 0.4901183228193199\n",
      "Iteration 199 Loss: 0.48665445413394953\n",
      "Iteration 200 Loss: 0.4832677339981027\n",
      "Iteration 201 Loss: 0.4799560785561569\n",
      "Iteration 202 Loss: 0.47671746127974324\n",
      "Iteration 203 Loss: 0.47354991138821934\n",
      "Iteration 204 Loss: 0.47045151231267235\n",
      "Iteration 205 Loss: 0.4674204002022437\n",
      "Iteration 206 Loss: 0.4644547624716183\n",
      "Iteration 207 Loss: 0.46155283638853595\n",
      "Iteration 208 Loss: 0.45871290770022916\n",
      "Iteration 209 Loss: 0.45593330929770826\n",
      "Iteration 210 Loss: 0.4532124199168564\n",
      "Iteration 211 Loss: 0.4505486628753146\n",
      "Iteration 212 Loss: 0.4479405048441773\n",
      "Iteration 213 Loss: 0.44538645465353144\n",
      "Iteration 214 Loss: 0.44288506213091383\n",
      "Iteration 215 Loss: 0.440434916971774\n",
      "Iteration 216 Loss: 0.4380346476410636\n",
      "Iteration 217 Loss: 0.4356829203050937\n",
      "Iteration 218 Loss: 0.433378437792827\n",
      "Iteration 219 Loss: 0.43111993858579095\n",
      "Iteration 220 Loss: 0.4289061958358264\n",
      "Iteration 221 Loss: 0.4267360164099038\n",
      "Iteration 222 Loss: 0.4246082399612588\n",
      "Iteration 223 Loss: 0.42252173802612536\n",
      "Iteration 224 Loss: 0.42047541314535886\n",
      "Iteration 225 Loss: 0.4184681980102633\n",
      "Iteration 226 Loss: 0.41649905463195774\n",
      "Iteration 227 Loss: 0.41456697353362937\n",
      "Iteration 228 Loss: 0.4126709729650478\n",
      "Iteration 229 Loss: 0.4108100981387208\n",
      "Iteration 230 Loss: 0.40898342048710246\n",
      "Iteration 231 Loss: 0.40719003694026623\n",
      "Iteration 232 Loss: 0.40542906922348526\n",
      "Iteration 233 Loss: 0.4036996631741651\n",
      "Iteration 234 Loss: 0.4020009880776042\n",
      "Iteration 235 Loss: 0.400332236021052\n",
      "Iteration 236 Loss: 0.39869262126557187\n",
      "Iteration 237 Loss: 0.3970813796352096\n",
      "Iteration 238 Loss: 0.3954977679229956\n",
      "Iteration 239 Loss: 0.393941063313314\n",
      "Iteration 240 Loss: 0.3924105628201905\n",
      "Iteration 241 Loss: 0.39090558274105647\n",
      "Iteration 242 Loss: 0.3894254581255673\n",
      "Iteration 243 Loss: 0.387969542259056\n",
      "Iteration 244 Loss: 0.3865372061602221\n",
      "Iteration 245 Loss: 0.3851278380926611\n",
      "Iteration 246 Loss: 0.38374084308985423\n",
      "Iteration 247 Loss: 0.38237564249324835\n",
      "Iteration 248 Loss: 0.3810316735030626\n",
      "Iteration 249 Loss: 0.37970838874147483\n",
      "Iteration 250 Loss: 0.3784052558278436\n",
      "Iteration 251 Loss: 0.3771217569656349\n",
      "Iteration 252 Loss: 0.375857388540732\n",
      "Iteration 253 Loss: 0.37461166073081464\n",
      "Iteration 254 Loss: 0.37338409712549997\n",
      "Iteration 255 Loss: 0.37217423435695307\n",
      "Iteration 256 Loss: 0.3709816217406739\n",
      "Iteration 257 Loss: 0.3698058209261846\n",
      "Iteration 258 Loss: 0.3686464055573413\n",
      "Iteration 259 Loss: 0.367502960942005\n",
      "Iteration 260 Loss: 0.3663750837308171\n",
      "Iteration 261 Loss: 0.3652623816048245\n",
      "Iteration 262 Loss: 0.3641644729717132\n",
      "Iteration 263 Loss: 0.3630809866704118\n",
      "Iteration 264 Loss: 0.362011561683835\n",
      "Iteration 265 Loss: 0.3609558468595419\n",
      "Iteration 266 Loss: 0.35991350063809296\n",
      "Iteration 267 Loss: 0.35888419078889006\n",
      "Iteration 268 Loss: 0.3578675941532972\n",
      "Iteration 269 Loss: 0.35686339639483905\n",
      "Iteration 270 Loss: 0.35587129175628185\n",
      "Iteration 271 Loss: 0.3548909828234093\n",
      "Iteration 272 Loss: 0.3539221802953071\n",
      "Iteration 273 Loss: 0.35296460276097597\n",
      "Iteration 274 Loss: 0.3520179764821021\n",
      "Iteration 275 Loss: 0.3510820351818119\n",
      "Iteration 276 Loss: 0.3501565198392488\n",
      "Iteration 277 Loss: 0.3492411784898075\n",
      "Iteration 278 Loss: 0.34833576603087724\n",
      "Iteration 279 Loss: 0.34744004403293316\n",
      "Iteration 280 Loss: 0.3465537805558359\n",
      "Iteration 281 Loss: 0.34567674997019165\n",
      "Iteration 282 Loss: 0.34480873278363505\n",
      "Iteration 283 Loss: 0.3439495154718985\n",
      "Iteration 284 Loss: 0.34309889031453594\n",
      "Iteration 285 Loss: 0.3422566552351732\n",
      "Iteration 286 Loss: 0.3414226136461612\n",
      "Iteration 287 Loss: 0.3405965742975075\n",
      "Iteration 288 Loss: 0.33977835112997234\n",
      "Iteration 289 Loss: 0.3389677631322111\n",
      "Iteration 290 Loss: 0.3381646342018535\n",
      "Iteration 291 Loss: 0.33736879301041167\n",
      "Iteration 292 Loss: 0.3365800728719074\n",
      "Iteration 293 Loss: 0.3357983116151232\n",
      "Iteration 294 Loss: 0.33502335145937\n",
      "Iteration 295 Loss: 0.3342550388936797\n",
      "Iteration 296 Loss: 0.33349322455932556\n",
      "Iteration 297 Loss: 0.33273776313558\n",
      "Iteration 298 Loss: 0.331988513228619\n",
      "Iteration 299 Loss: 0.33124533726348987\n",
      "Iteration 300 Loss: 0.33050810137905284\n",
      "Iteration 301 Loss: 0.3297766753258191\n",
      "Iteration 302 Loss: 0.3290509323666032\n",
      "Iteration 303 Loss: 0.32833074917991256\n",
      "Iteration 304 Loss: 0.32761600576599864\n",
      "Iteration 305 Loss: 0.3269065853554981\n",
      "Iteration 306 Loss: 0.32620237432058846\n",
      "Iteration 307 Loss: 0.325503262088594\n",
      "Iteration 308 Loss: 0.32480914105797104\n",
      "Iteration 309 Loss: 0.3241199065166076\n",
      "Iteration 310 Loss: 0.323435456562375\n",
      "Iteration 311 Loss: 0.3227556920258675\n",
      "Iteration 312 Loss: 0.32208051639527113\n",
      "Iteration 313 Loss: 0.32140983574330206\n",
      "Iteration 314 Loss: 0.3207435586561585\n",
      "Iteration 315 Loss: 0.3200815961644296\n",
      "Iteration 316 Loss: 0.31942386167590786\n",
      "Iteration 317 Loss: 0.31877027091025295\n",
      "Iteration 318 Loss: 0.31812074183545463\n",
      "Iteration 319 Loss: 0.31747519460604795\n",
      "Iteration 320 Loss: 0.3168335515030291\n",
      "Iteration 321 Loss: 0.31619573687542707\n",
      "Iteration 322 Loss: 0.3155616770834847\n",
      "Iteration 323 Loss: 0.31493130044340545\n",
      "Iteration 324 Loss: 0.31430453717362306\n",
      "Iteration 325 Loss: 0.3136813193425498\n",
      "Iteration 326 Loss: 0.3130615808177658\n",
      "Iteration 327 Loss: 0.31244525721660626\n",
      "Iteration 328 Loss: 0.3118322858581132\n",
      "Iteration 329 Loss: 0.3112226057163066\n",
      "Iteration 330 Loss: 0.3106161573747467\n",
      "Iteration 331 Loss: 0.3100128829823455\n",
      "Iteration 332 Loss: 0.30941272621039706\n",
      "Iteration 333 Loss: 0.30881563221079034\n",
      "Iteration 334 Loss: 0.30822154757537423\n",
      "Iteration 335 Loss: 0.3076304202964414\n",
      "Iteration 336 Loss: 0.30704219972830027\n",
      "Iteration 337 Loss: 0.3064568365499071\n",
      "Iteration 338 Loss: 0.3058742827285256\n",
      "Iteration 339 Loss: 0.3052944914843886\n",
      "Iteration 340 Loss: 0.304717417256333\n",
      "Iteration 341 Loss: 0.30414301566838164\n",
      "Iteration 342 Loss: 0.30357124349724457\n",
      "Iteration 343 Loss: 0.303002058640717\n",
      "Iteration 344 Loss: 0.3024354200869464\n",
      "Iteration 345 Loss: 0.3018712878845474\n",
      "Iteration 346 Loss: 0.3013096231135386\n",
      "Iteration 347 Loss: 0.3007503878570808\n",
      "Iteration 348 Loss: 0.3001935451739929\n",
      "Iteration 349 Loss: 0.299639059072026\n",
      "Iteration 350 Loss: 0.2990868944818721\n",
      "Iteration 351 Loss: 0.29853701723188997\n",
      "Iteration 352 Loss: 0.29798939402352614\n",
      "Iteration 353 Loss: 0.2974439924074133\n",
      "Iteration 354 Loss: 0.2969007807601275\n",
      "Iteration 355 Loss: 0.29635972826158463\n",
      "Iteration 356 Loss: 0.2958208048730592\n",
      "Iteration 357 Loss: 0.29528398131580963\n",
      "Iteration 358 Loss: 0.294749229050292\n",
      "Iteration 359 Loss: 0.2942165202559445\n",
      "Iteration 360 Loss: 0.2936858278115313\n",
      "Iteration 361 Loss: 0.29315712527602533\n",
      "Iteration 362 Loss: 0.29263038687001897\n",
      "Iteration 363 Loss: 0.29210558745764664\n",
      "Iteration 364 Loss: 0.2915827025290034\n",
      "Iteration 365 Loss: 0.2910617081830509\n",
      "Iteration 366 Loss: 0.2905425811109903\n",
      "Iteration 367 Loss: 0.290025298580096\n",
      "Iteration 368 Loss: 0.28950983841799327\n",
      "Iteration 369 Loss: 0.2889961789973688\n",
      "Iteration 370 Loss: 0.28848429922110175\n",
      "Iteration 371 Loss: 0.28797417850780593\n",
      "Iteration 372 Loss: 0.28746579677776807\n",
      "Iteration 373 Loss: 0.2869591344392741\n",
      "Iteration 374 Loss: 0.2864541723753119\n",
      "Iteration 375 Loss: 0.2859508919306403\n",
      "Iteration 376 Loss: 0.28544927489921373\n",
      "Iteration 377 Loss: 0.2849493035119531\n",
      "Iteration 378 Loss: 0.284450960424853\n",
      "Iteration 379 Loss: 0.2839542287074177\n",
      "Iteration 380 Loss: 0.2834590918314134\n",
      "Iteration 381 Loss: 0.2829655336599319\n",
      "Iteration 382 Loss: 0.2824735384367538\n",
      "Iteration 383 Loss: 0.2819830907760064\n",
      "Iteration 384 Loss: 0.28149417565210316\n",
      "Iteration 385 Loss: 0.28100677838996424\n",
      "Iteration 386 Loss: 0.2805208846555024\n",
      "Iteration 387 Loss: 0.2800364804463725\n",
      "Iteration 388 Loss: 0.27955355208297555\n",
      "Iteration 389 Loss: 0.27907208619971047\n",
      "Iteration 390 Loss: 0.2785920697364655\n",
      "Iteration 391 Loss: 0.2781134899303447\n",
      "Iteration 392 Loss: 0.27763633430762225\n",
      "Iteration 393 Loss: 0.2771605906759175\n",
      "Iteration 394 Loss: 0.27668624711658535\n",
      "Iteration 395 Loss: 0.2762132919773168\n",
      "Iteration 396 Loss: 0.27574171386494206\n",
      "Iteration 397 Loss: 0.27527150163843206\n",
      "Iteration 398 Loss: 0.2748026444020928\n",
      "Iteration 399 Loss: 0.2743351314989467\n",
      "Iteration 400 Loss: 0.2738689525042954\n",
      "Iteration 401 Loss: 0.2734040972194608\n",
      "Iteration 402 Loss: 0.2729405556656975\n",
      "Iteration 403 Loss: 0.2724783180782721\n",
      "Iteration 404 Loss: 0.27201737490070715\n",
      "Iteration 405 Loss: 0.2715577167791815\n",
      "Iteration 406 Loss: 0.2710993345570854\n",
      "Iteration 407 Loss: 0.2706422192697258\n",
      "Iteration 408 Loss: 0.2701863621391764\n",
      "Iteration 409 Loss: 0.2697317545692698\n",
      "Iteration 410 Loss: 0.2692783881407267\n",
      "Iteration 411 Loss: 0.26882625460642046\n",
      "Iteration 412 Loss: 0.26837534588677014\n",
      "Iteration 413 Loss: 0.26792565406526114\n",
      "Iteration 414 Loss: 0.2674771713840882\n",
      "Iteration 415 Loss: 0.2670298902399197\n",
      "Iteration 416 Loss: 0.2665838031797765\n",
      "Iteration 417 Loss: 0.26613890289702524\n",
      "Iteration 418 Loss: 0.2656951822274811\n",
      "Iteration 419 Loss: 0.2652526341456181\n",
      "Iteration 420 Loss: 0.2648112517608834\n",
      "Iteration 421 Loss: 0.26437102831411285\n",
      "Iteration 422 Loss: 0.2639319571740443\n",
      "Iteration 423 Loss: 0.26349403183392817\n",
      "Iteration 424 Loss: 0.26305724590822954\n",
      "Iteration 425 Loss: 0.26262159312942224\n",
      "Iteration 426 Loss: 0.26218706734486985\n",
      "Iteration 427 Loss: 0.26175366251379256\n",
      "Iteration 428 Loss: 0.261321372704318\n",
      "Iteration 429 Loss: 0.26089019209061226\n",
      "Iteration 430 Loss: 0.26046011495008986\n",
      "Iteration 431 Loss: 0.26003113566070074\n",
      "Iteration 432 Loss: 0.2596032486982911\n",
      "Iteration 433 Loss: 0.25917644863403677\n",
      "Iteration 434 Loss: 0.2587507301319476\n",
      "Iteration 435 Loss: 0.2583260879464395\n",
      "Iteration 436 Loss: 0.2579025169199737\n",
      "Iteration 437 Loss: 0.25748001198076037\n",
      "Iteration 438 Loss: 0.25705856814052525\n",
      "Iteration 439 Loss: 0.25663818049233805\n",
      "Iteration 440 Loss: 0.25621884420849966\n",
      "Iteration 441 Loss: 0.25580055453848793\n",
      "Iteration 442 Loss: 0.255383306806959\n",
      "Iteration 443 Loss: 0.2549670964118044\n",
      "Iteration 444 Loss: 0.25455191882226025\n",
      "Iteration 445 Loss: 0.25413776957706957\n",
      "Iteration 446 Loss: 0.25372464428269353\n",
      "Iteration 447 Loss: 0.2533125386115732\n",
      "Iteration 448 Loss: 0.25290144830043737\n",
      "Iteration 449 Loss: 0.2524913691486578\n",
      "Iteration 450 Loss: 0.25208229701664936\n",
      "Iteration 451 Loss: 0.2516742278243136\n",
      "Iteration 452 Loss: 0.2512671575495248\n",
      "Iteration 453 Loss: 0.25086108222665804\n",
      "Iteration 454 Loss: 0.2504559979451574\n",
      "Iteration 455 Loss: 0.2500519008481429\n",
      "Iteration 456 Loss: 0.24964878713105582\n",
      "Iteration 457 Loss: 0.24924665304034113\n",
      "Iteration 458 Loss: 0.24884549487216603\n",
      "Iteration 459 Loss: 0.2484453089711732\n",
      "Iteration 460 Loss: 0.248046091729268\n",
      "Iteration 461 Loss: 0.2476478395844397\n",
      "Iteration 462 Loss: 0.2472505490196138\n",
      "Iteration 463 Loss: 0.24685421656153644\n",
      "Iteration 464 Loss: 0.2464588387796888\n",
      "Iteration 465 Loss: 0.2460644122852318\n",
      "Iteration 466 Loss: 0.24567093372997856\n",
      "Iteration 467 Loss: 0.24527839980539573\n",
      "Iteration 468 Loss: 0.24488680724163236\n",
      "Iteration 469 Loss: 0.24449615280657366\n",
      "Iteration 470 Loss: 0.24410643330492254\n",
      "Iteration 471 Loss: 0.24371764557730485\n",
      "Iteration 472 Loss: 0.24332978649939935\n",
      "Iteration 473 Loss: 0.2429428529810917\n",
      "Iteration 474 Loss: 0.2425568419656508\n",
      "Iteration 475 Loss: 0.2421717504289286\n",
      "Iteration 476 Loss: 0.2417875753785804\n",
      "Iteration 477 Loss: 0.24140431385330752\n",
      "Iteration 478 Loss: 0.2410219629221196\n",
      "Iteration 479 Loss: 0.24064051968361805\n",
      "Iteration 480 Loss: 0.24025998126529774\n",
      "Iteration 481 Loss: 0.23988034482286888\n",
      "Iteration 482 Loss: 0.2395016075395958\n",
      "Iteration 483 Loss: 0.23912376662565568\n",
      "Iteration 484 Loss: 0.23874681931751293\n",
      "Iteration 485 Loss: 0.23837076287731113\n",
      "Iteration 486 Loss: 0.2379955945922819\n",
      "Iteration 487 Loss: 0.23762131177416937\n",
      "Iteration 488 Loss: 0.23724791175866997\n",
      "Iteration 489 Loss: 0.2368753919048882\n",
      "Iteration 490 Loss: 0.23650374959480627\n",
      "Iteration 491 Loss: 0.23613298223276874\n",
      "Iteration 492 Loss: 0.23576308724498013\n",
      "Iteration 493 Loss: 0.23539406207901825\n",
      "Iteration 494 Loss: 0.2350259042033577\n",
      "Iteration 495 Loss: 0.23465861110690875\n",
      "Iteration 496 Loss: 0.2342921802985673\n",
      "Iteration 497 Loss: 0.23392660930677742\n",
      "Iteration 498 Loss: 0.23356189567910562\n",
      "Iteration 499 Loss: 0.2331980369818269\n",
      "Iteration 500 Loss: 0.23283503079952106\n",
      "Iteration 501 Loss: 0.23247287473468115\n",
      "Iteration 502 Loss: 0.23211156640733116\n",
      "Iteration 503 Loss: 0.2317511034546551\n",
      "Iteration 504 Loss: 0.2313914835306352\n",
      "Iteration 505 Loss: 0.23103270430570041\n",
      "Iteration 506 Loss: 0.23067476346638402\n",
      "Iteration 507 Loss: 0.2303176587149903\n",
      "Iteration 508 Loss: 0.22996138776927055\n",
      "Iteration 509 Loss: 0.2296059483621077\n",
      "Iteration 510 Loss: 0.22925133824120883\n",
      "Iteration 511 Loss: 0.2288975551688067\n",
      "Iteration 512 Loss: 0.22854459692136847\n",
      "Iteration 513 Loss: 0.22819246128931275\n",
      "Iteration 514 Loss: 0.22784114607673436\n",
      "Iteration 515 Loss: 0.22749064910113542\n",
      "Iteration 516 Loss: 0.22714096819316493\n",
      "Iteration 517 Loss: 0.22679210119636412\n",
      "Iteration 518 Loss: 0.2264440459669191\n",
      "Iteration 519 Loss: 0.22609680037342067\n",
      "Iteration 520 Loss: 0.22575036229662918\n",
      "Iteration 521 Loss: 0.2254047296292463\n",
      "Iteration 522 Loss: 0.2250599002756936\n",
      "Iteration 523 Loss: 0.2247158721518953\n",
      "Iteration 524 Loss: 0.22437264318506847\n",
      "Iteration 525 Loss: 0.22403021131351725\n",
      "Iteration 526 Loss: 0.22368857448643403\n",
      "Iteration 527 Loss: 0.22334773066370453\n",
      "Iteration 528 Loss: 0.22300767781571862\n",
      "Iteration 529 Loss: 0.22266841392318643\n",
      "Iteration 530 Loss: 0.2223299369769583\n",
      "Iteration 531 Loss: 0.22199224497785036\n",
      "Iteration 532 Loss: 0.22165533593647474\n",
      "Iteration 533 Loss: 0.2213192078730728\n",
      "Iteration 534 Loss: 0.2209838588173553\n",
      "Iteration 535 Loss: 0.22064928680834356\n",
      "Iteration 536 Loss: 0.22031548989421737\n",
      "Iteration 537 Loss: 0.21998246613216602\n",
      "Iteration 538 Loss: 0.21965021358824202\n",
      "Iteration 539 Loss: 0.21931873033722074\n",
      "Iteration 540 Loss: 0.21898801446246258\n",
      "Iteration 541 Loss: 0.2186580640557773\n",
      "Iteration 542 Loss: 0.21832887721729494\n",
      "Iteration 543 Loss: 0.21800045205533758\n",
      "Iteration 544 Loss: 0.21767278668629536\n",
      "Iteration 545 Loss: 0.21734587923450538\n",
      "Iteration 546 Loss: 0.2170197278321341\n",
      "Iteration 547 Loss: 0.21669433061906213\n",
      "Iteration 548 Loss: 0.21636968574277313\n",
      "Iteration 549 Loss: 0.21604579135824348\n",
      "Iteration 550 Loss: 0.21572264562783722\n",
      "Iteration 551 Loss: 0.2154002467212018\n",
      "Iteration 552 Loss: 0.21507859281516728\n",
      "Iteration 553 Loss: 0.21475768209364793\n",
      "Iteration 554 Loss: 0.21443751274754647\n",
      "Iteration 555 Loss: 0.21411808297466042\n",
      "Iteration 556 Loss: 0.21379939097959055\n",
      "Iteration 557 Loss: 0.21348143497365285\n",
      "Iteration 558 Loss: 0.21316421317479134\n",
      "Iteration 559 Loss: 0.21284772380749337\n",
      "Iteration 560 Loss: 0.21253196510270794\n",
      "Iteration 561 Loss: 0.2122169352977646\n",
      "Iteration 562 Loss: 0.2119026326362957\n",
      "Iteration 563 Loss: 0.21158905536815958\n",
      "Iteration 564 Loss: 0.21127620174936626\n",
      "Iteration 565 Loss: 0.21096407004200457\n",
      "Iteration 566 Loss: 0.21065265851417134\n",
      "Iteration 567 Loss: 0.21034196543990227\n",
      "Iteration 568 Loss: 0.21003198909910412\n",
      "Iteration 569 Loss: 0.20972272777748913\n",
      "Iteration 570 Loss: 0.20941417976651092\n",
      "Iteration 571 Loss: 0.20910634336330108\n",
      "Iteration 572 Loss: 0.20879921687060862\n",
      "Iteration 573 Loss: 0.20849279859674014\n",
      "Iteration 574 Loss: 0.20818708685550097\n",
      "Iteration 575 Loss: 0.207882079966139\n",
      "Iteration 576 Loss: 0.20757777625328852\n",
      "Iteration 577 Loss: 0.20727417404691645\n",
      "Iteration 578 Loss: 0.20697127168226875\n",
      "Iteration 579 Loss: 0.206669067499819\n",
      "Iteration 580 Loss: 0.2063675598452182\n",
      "Iteration 581 Loss: 0.2060667470692449\n",
      "Iteration 582 Loss: 0.20576662752775698\n",
      "Iteration 583 Loss: 0.20546719958164522\n",
      "Iteration 584 Loss: 0.20516846159678642\n",
      "Iteration 585 Loss: 0.2048704119439991\n",
      "Iteration 586 Loss: 0.20457304899899914\n",
      "Iteration 587 Loss: 0.20427637114235714\n",
      "Iteration 588 Loss: 0.2039803767594564\n",
      "Iteration 589 Loss: 0.2036850642404516\n",
      "Iteration 590 Loss: 0.20339043198022927\n",
      "Iteration 591 Loss: 0.20309647837836825\n",
      "Iteration 592 Loss: 0.20280320183910092\n",
      "Iteration 593 Loss: 0.20251060077127692\n",
      "Iteration 594 Loss: 0.20221867358832527\n",
      "Iteration 595 Loss: 0.201927418708219\n",
      "Iteration 596 Loss: 0.20163683455344042\n",
      "Iteration 597 Loss: 0.20134691955094608\n",
      "Iteration 598 Loss: 0.20105767213213363\n",
      "Iteration 599 Loss: 0.20076909073280902\n",
      "Iteration 600 Loss: 0.20048117379315422\n",
      "Iteration 601 Loss: 0.20019391975769557\n",
      "Iteration 602 Loss: 0.19990732707527337\n",
      "Iteration 603 Loss: 0.19962139419901112\n",
      "Iteration 604 Loss: 0.19933611958628675\n",
      "Iteration 605 Loss: 0.19905150169870278\n",
      "Iteration 606 Loss: 0.19876753900205885\n",
      "Iteration 607 Loss: 0.19848422996632337\n",
      "Iteration 608 Loss: 0.19820157306560643\n",
      "Iteration 609 Loss: 0.19791956677813333\n",
      "Iteration 610 Loss: 0.19763820958621842\n",
      "Iteration 611 Loss: 0.19735749997623936\n",
      "Iteration 612 Loss: 0.19707743643861197\n",
      "Iteration 613 Loss: 0.19679801746776607\n",
      "Iteration 614 Loss: 0.19651924156212072\n",
      "Iteration 615 Loss: 0.19624110722406116\n",
      "Iteration 616 Loss: 0.19596361295991513\n",
      "Iteration 617 Loss: 0.1956867572799303\n",
      "Iteration 618 Loss: 0.19541053869825226\n",
      "Iteration 619 Loss: 0.19513495573290185\n",
      "Iteration 620 Loss: 0.1948600069057543\n",
      "Iteration 621 Loss: 0.19458569074251797\n",
      "Iteration 622 Loss: 0.19431200577271335\n",
      "Iteration 623 Loss: 0.19403895052965317\n",
      "Iteration 624 Loss: 0.19376652355042195\n",
      "Iteration 625 Loss: 0.19349472337585683\n",
      "Iteration 626 Loss: 0.19322354855052765\n",
      "Iteration 627 Loss: 0.19295299762271872\n",
      "Iteration 628 Loss: 0.1926830691444099\n",
      "Iteration 629 Loss: 0.19241376167125818\n",
      "Iteration 630 Loss: 0.19214507376257978\n",
      "Iteration 631 Loss: 0.19187700398133253\n",
      "Iteration 632 Loss: 0.19160955089409842\n",
      "Iteration 633 Loss: 0.1913427130710666\n",
      "Iteration 634 Loss: 0.19107648908601663\n",
      "Iteration 635 Loss: 0.19081087751630146\n",
      "Iteration 636 Loss: 0.19054587694283193\n",
      "Iteration 637 Loss: 0.19028148595006011\n",
      "Iteration 638 Loss: 0.19001770312596378\n",
      "Iteration 639 Loss: 0.18975452706203086\n",
      "Iteration 640 Loss: 0.18949195635324423\n",
      "Iteration 641 Loss: 0.18922998959806672\n",
      "Iteration 642 Loss: 0.18896862539842596\n",
      "Iteration 643 Loss: 0.18870786235970027\n",
      "Iteration 644 Loss: 0.18844769909070394\n",
      "Iteration 645 Loss: 0.18818813420367309\n",
      "Iteration 646 Loss: 0.187929166314252\n",
      "Iteration 647 Loss: 0.1876707940414789\n",
      "Iteration 648 Loss: 0.18741301600777263\n",
      "Iteration 649 Loss: 0.18715583083891957\n",
      "Iteration 650 Loss: 0.18689923716405965\n",
      "Iteration 651 Loss: 0.18664323361567417\n",
      "Iteration 652 Loss: 0.18638781882957267\n",
      "Iteration 653 Loss: 0.18613299144488002\n",
      "Iteration 654 Loss: 0.18587875010402405\n",
      "Iteration 655 Loss: 0.18562509345272368\n",
      "Iteration 656 Loss: 0.185372020139976\n",
      "Iteration 657 Loss: 0.18511952881804455\n",
      "Iteration 658 Loss: 0.18486761814244773\n",
      "Iteration 659 Loss: 0.18461628677194675\n",
      "Iteration 660 Loss: 0.1843655333685342\n",
      "Iteration 661 Loss: 0.1841153565974224\n",
      "Iteration 662 Loss: 0.18386575512703232\n",
      "Iteration 663 Loss: 0.1836167276289824\n",
      "Iteration 664 Loss: 0.18336827277807752\n",
      "Iteration 665 Loss: 0.18312038925229798\n",
      "Iteration 666 Loss: 0.18287307573278858\n",
      "Iteration 667 Loss: 0.18262633090384833\n",
      "Iteration 668 Loss: 0.1823801534529196\n",
      "Iteration 669 Loss: 0.18213454207057783\n",
      "Iteration 670 Loss: 0.1818894954505212\n",
      "Iteration 671 Loss: 0.18164501228955995\n",
      "Iteration 672 Loss: 0.18140109128760756\n",
      "Iteration 673 Loss: 0.18115773114766917\n",
      "Iteration 674 Loss: 0.18091493057583272\n",
      "Iteration 675 Loss: 0.18067268828125863\n",
      "Iteration 676 Loss: 0.1804310029761705\n",
      "Iteration 677 Loss: 0.1801898733758452\n",
      "Iteration 678 Loss: 0.17994929819860334\n",
      "Iteration 679 Loss: 0.17970927616580026\n",
      "Iteration 680 Loss: 0.1794698060018161\n",
      "Iteration 681 Loss: 0.17923088643404705\n",
      "Iteration 682 Loss: 0.17899251619289594\n",
      "Iteration 683 Loss: 0.17875469401176278\n",
      "Iteration 684 Loss: 0.17851741862703693\n",
      "Iteration 685 Loss: 0.17828068877808678\n",
      "Iteration 686 Loss: 0.17804450320725188\n",
      "Iteration 687 Loss: 0.17780886065983376\n",
      "Iteration 688 Loss: 0.1775737598840871\n",
      "Iteration 689 Loss: 0.17733919963121175\n",
      "Iteration 690 Loss: 0.1771051786553431\n",
      "Iteration 691 Loss: 0.17687169571354477\n",
      "Iteration 692 Loss: 0.17663874956579945\n",
      "Iteration 693 Loss: 0.17640633897500063\n",
      "Iteration 694 Loss: 0.17617446270694453\n",
      "Iteration 695 Loss: 0.1759431195303218\n",
      "Iteration 696 Loss: 0.1757123082167092\n",
      "Iteration 697 Loss: 0.17548202754056158\n",
      "Iteration 698 Loss: 0.17525227627920398\n",
      "Iteration 699 Loss: 0.1750230532128235\n",
      "Iteration 700 Loss: 0.17479435712446142\n",
      "Iteration 701 Loss: 0.17456618680000513\n",
      "Iteration 702 Loss: 0.17433854102818078\n",
      "Iteration 703 Loss: 0.17411141860054494\n",
      "Iteration 704 Loss: 0.17388481831147704\n",
      "Iteration 705 Loss: 0.17365873895817227\n",
      "Iteration 706 Loss: 0.17343317934063301\n",
      "Iteration 707 Loss: 0.17320813826166215\n",
      "Iteration 708 Loss: 0.17298361452685498\n",
      "Iteration 709 Loss: 0.1727596069445918\n",
      "Iteration 710 Loss: 0.1725361143260308\n",
      "Iteration 711 Loss: 0.17231313548510052\n",
      "Iteration 712 Loss: 0.1720906692384924\n",
      "Iteration 713 Loss: 0.17186871440565327\n",
      "Iteration 714 Loss: 0.1716472698087791\n",
      "Iteration 715 Loss: 0.17142633427280635\n",
      "Iteration 716 Loss: 0.17120590662540613\n",
      "Iteration 717 Loss: 0.17098598569697615\n",
      "Iteration 718 Loss: 0.1707665703206338\n",
      "Iteration 719 Loss: 0.17054765933220964\n",
      "Iteration 720 Loss: 0.17032925157023984\n",
      "Iteration 721 Loss: 0.17011134587595933\n",
      "Iteration 722 Loss: 0.16989394109329498\n",
      "Iteration 723 Loss: 0.16967703606885878\n",
      "Iteration 724 Loss: 0.16946062965194045\n",
      "Iteration 725 Loss: 0.16924472069450117\n",
      "Iteration 726 Loss: 0.1690293080511667\n",
      "Iteration 727 Loss: 0.16881439057922037\n",
      "Iteration 728 Loss: 0.16859996713859635\n",
      "Iteration 729 Loss: 0.1683860365918736\n",
      "Iteration 730 Loss: 0.16817259780426813\n",
      "Iteration 731 Loss: 0.16795964964362703\n",
      "Iteration 732 Loss: 0.16774719098042218\n",
      "Iteration 733 Loss: 0.16753522068774274\n",
      "Iteration 734 Loss: 0.16732373764128938\n",
      "Iteration 735 Loss: 0.1671127407193674\n",
      "Iteration 736 Loss: 0.16690222880288058\n",
      "Iteration 737 Loss: 0.16669220077532398\n",
      "Iteration 738 Loss: 0.16648265552277883\n",
      "Iteration 739 Loss: 0.16627359193390456\n",
      "Iteration 740 Loss: 0.16606500889993367\n",
      "Iteration 741 Loss: 0.16585690531466474\n",
      "Iteration 742 Loss: 0.16564928007445615\n",
      "Iteration 743 Loss: 0.16544213207822006\n",
      "Iteration 744 Loss: 0.16523546022741603\n",
      "Iteration 745 Loss: 0.16502926342604463\n",
      "Iteration 746 Loss: 0.16482354058064144\n",
      "Iteration 747 Loss: 0.16461829060027056\n",
      "Iteration 748 Loss: 0.16441351239651883\n",
      "Iteration 749 Loss: 0.16420920488348925\n",
      "Iteration 750 Loss: 0.16400536697779533\n",
      "Iteration 751 Loss: 0.16380199759855443\n",
      "Iteration 752 Loss: 0.1635990956673824\n",
      "Iteration 753 Loss: 0.16339666010838674\n",
      "Iteration 754 Loss: 0.16319468984816124\n",
      "Iteration 755 Loss: 0.16299318381577935\n",
      "Iteration 756 Loss: 0.1627921409427889\n",
      "Iteration 757 Loss: 0.16259156016320558\n",
      "Iteration 758 Loss: 0.16239144041350717\n",
      "Iteration 759 Loss: 0.16219178063262746\n",
      "Iteration 760 Loss: 0.16199257976195075\n",
      "Iteration 761 Loss: 0.16179383674530573\n",
      "Iteration 762 Loss: 0.16159555052895938\n",
      "Iteration 763 Loss: 0.16139772006161154\n",
      "Iteration 764 Loss: 0.16120034429438893\n",
      "Iteration 765 Loss: 0.16100342218083907\n",
      "Iteration 766 Loss: 0.16080695267692507\n",
      "Iteration 767 Loss: 0.16061093474101942\n",
      "Iteration 768 Loss: 0.16041536733389816\n",
      "Iteration 769 Loss: 0.16022024941873575\n",
      "Iteration 770 Loss: 0.16002557996109873\n",
      "Iteration 771 Loss: 0.15983135792894015\n",
      "Iteration 772 Loss: 0.1596375822925944\n",
      "Iteration 773 Loss: 0.15944425202477086\n",
      "Iteration 774 Loss: 0.1592513661005487\n",
      "Iteration 775 Loss: 0.1590589234973712\n",
      "Iteration 776 Loss: 0.15886692319503998\n",
      "Iteration 777 Loss: 0.15867536417570957\n",
      "Iteration 778 Loss: 0.15848424542388195\n",
      "Iteration 779 Loss: 0.1582935659264006\n",
      "Iteration 780 Loss: 0.1581033246724453\n",
      "Iteration 781 Loss: 0.15791352065352685\n",
      "Iteration 782 Loss: 0.15772415286348068\n",
      "Iteration 783 Loss: 0.15753522029846243\n",
      "Iteration 784 Loss: 0.15734672195694208\n",
      "Iteration 785 Loss: 0.15715865683969776\n",
      "Iteration 786 Loss: 0.15697102394981183\n",
      "Iteration 787 Loss: 0.1567838222926639\n",
      "Iteration 788 Loss: 0.15659705087592649\n",
      "Iteration 789 Loss: 0.15641070870955937\n",
      "Iteration 790 Loss: 0.1562247948058038\n",
      "Iteration 791 Loss: 0.15603930817917772\n",
      "Iteration 792 Loss: 0.1558542478464701\n",
      "Iteration 793 Loss: 0.15566961282673572\n",
      "Iteration 794 Loss: 0.15548540214128978\n",
      "Iteration 795 Loss: 0.1553016148137027\n",
      "Iteration 796 Loss: 0.15511824986979478\n",
      "Iteration 797 Loss: 0.15493530633763083\n",
      "Iteration 798 Loss: 0.1547527832475152\n",
      "Iteration 799 Loss: 0.15457067963198629\n",
      "Iteration 800 Loss: 0.1543889945258114\n",
      "Iteration 801 Loss: 0.15420772696598167\n",
      "Iteration 802 Loss: 0.15402687599170653\n",
      "Iteration 803 Loss: 0.15384644064440886\n",
      "Iteration 804 Loss: 0.15366641996771974\n",
      "Iteration 805 Loss: 0.15348681300747313\n",
      "Iteration 806 Loss: 0.15330761881170102\n",
      "Iteration 807 Loss: 0.15312883643062797\n",
      "Iteration 808 Loss: 0.1529504649166663\n",
      "Iteration 809 Loss: 0.1527725033244109\n",
      "Iteration 810 Loss: 0.152594950710634\n",
      "Iteration 811 Loss: 0.1524178061342803\n",
      "Iteration 812 Loss: 0.15224106865646184\n",
      "Iteration 813 Loss: 0.15206473734045284\n",
      "Iteration 814 Loss: 0.15188881125168482\n",
      "Iteration 815 Loss: 0.1517132894577416\n",
      "Iteration 816 Loss: 0.15153817102835423\n",
      "Iteration 817 Loss: 0.1513634550353958\n",
      "Iteration 818 Loss: 0.15118914055287677\n",
      "Iteration 819 Loss: 0.15101522665694006\n",
      "Iteration 820 Loss: 0.1508417124258558\n",
      "Iteration 821 Loss: 0.15066859694001644\n",
      "Iteration 822 Loss: 0.15049587928193192\n",
      "Iteration 823 Loss: 0.150323558536225\n",
      "Iteration 824 Loss: 0.15015163378962573\n",
      "Iteration 825 Loss: 0.14998010413096702\n",
      "Iteration 826 Loss: 0.14980896865117982\n",
      "Iteration 827 Loss: 0.1496382264432879\n",
      "Iteration 828 Loss: 0.14946787660240365\n",
      "Iteration 829 Loss: 0.14929791822572197\n",
      "Iteration 830 Loss: 0.14912835041251693\n",
      "Iteration 831 Loss: 0.148959172264136\n",
      "Iteration 832 Loss: 0.14879038288399565\n",
      "Iteration 833 Loss: 0.14862198137757635\n",
      "Iteration 834 Loss: 0.1484539668524178\n",
      "Iteration 835 Loss: 0.14828633841811448\n",
      "Iteration 836 Loss: 0.14811909518631042\n",
      "Iteration 837 Loss: 0.14795223627069448\n",
      "Iteration 838 Loss: 0.1477857607869964\n",
      "Iteration 839 Loss: 0.14761966785298117\n",
      "Iteration 840 Loss: 0.14745395658844432\n",
      "Iteration 841 Loss: 0.14728862611520815\n",
      "Iteration 842 Loss: 0.147123675557116\n",
      "Iteration 843 Loss: 0.14695910404002815\n",
      "Iteration 844 Loss: 0.14679491069181708\n",
      "Iteration 845 Loss: 0.1466310946423625\n",
      "Iteration 846 Loss: 0.1464676550235474\n",
      "Iteration 847 Loss: 0.14630459096925263\n",
      "Iteration 848 Loss: 0.14614190161535254\n",
      "Iteration 849 Loss: 0.14597958609971096\n",
      "Iteration 850 Loss: 0.14581764356217547\n",
      "Iteration 851 Loss: 0.14565607314457388\n",
      "Iteration 852 Loss: 0.1454948739907092\n",
      "Iteration 853 Loss: 0.14533404524635488\n",
      "Iteration 854 Loss: 0.14517358605925063\n",
      "Iteration 855 Loss: 0.14501349557909776\n",
      "Iteration 856 Loss: 0.1448537729575546\n",
      "Iteration 857 Loss: 0.1446944173482319\n",
      "Iteration 858 Loss: 0.14453542790668836\n",
      "Iteration 859 Loss: 0.1443768037904265\n",
      "Iteration 860 Loss: 0.14421854415888757\n",
      "Iteration 861 Loss: 0.1440606481734475\n",
      "Iteration 862 Loss: 0.1439031149974123\n",
      "Iteration 863 Loss: 0.1437459437960135\n",
      "Iteration 864 Loss: 0.14358913373640392\n",
      "Iteration 865 Loss: 0.14343268398765302\n",
      "Iteration 866 Loss: 0.14327659372074283\n",
      "Iteration 867 Loss: 0.14312086210856304\n",
      "Iteration 868 Loss: 0.14296548832590683\n",
      "Iteration 869 Loss: 0.14281047154946666\n",
      "Iteration 870 Loss: 0.1426558109578298\n",
      "Iteration 871 Loss: 0.14250150573147358\n",
      "Iteration 872 Loss: 0.14234755505276164\n",
      "Iteration 873 Loss: 0.14219395810593907\n",
      "Iteration 874 Loss: 0.14204071407712818\n",
      "Iteration 875 Loss: 0.14188782215432477\n",
      "Iteration 876 Loss: 0.14173528152739273\n",
      "Iteration 877 Loss: 0.1415830913880607\n",
      "Iteration 878 Loss: 0.14143125092991707\n",
      "Iteration 879 Loss: 0.14127975934840625\n",
      "Iteration 880 Loss: 0.14112861584082403\n",
      "Iteration 881 Loss: 0.14097781960631334\n",
      "Iteration 882 Loss: 0.14082736984586036\n",
      "Iteration 883 Loss: 0.1406772657622897\n",
      "Iteration 884 Loss: 0.1405275065602606\n",
      "Iteration 885 Loss: 0.14037809144626237\n",
      "Iteration 886 Loss: 0.14022901962861056\n",
      "Iteration 887 Loss: 0.14008029031744235\n",
      "Iteration 888 Loss: 0.13993190272471273\n",
      "Iteration 889 Loss: 0.1397838560641901\n",
      "Iteration 890 Loss: 0.13963614955145193\n",
      "Iteration 891 Loss: 0.1394887824038809\n",
      "Iteration 892 Loss: 0.13934175384066075\n",
      "Iteration 893 Loss: 0.13919506308277166\n",
      "Iteration 894 Loss: 0.1390487093529868\n",
      "Iteration 895 Loss: 0.13890269187586754\n",
      "Iteration 896 Loss: 0.13875700987775985\n",
      "Iteration 897 Loss: 0.13861166258678992\n",
      "Iteration 898 Loss: 0.13846664923286003\n",
      "Iteration 899 Loss: 0.1383219690476447\n",
      "Iteration 900 Loss: 0.13817762126458627\n",
      "Iteration 901 Loss: 0.13803360511889112\n",
      "Iteration 902 Loss: 0.13788991984752547\n",
      "Iteration 903 Loss: 0.13774656468921115\n",
      "Iteration 904 Loss: 0.13760353888442212\n",
      "Iteration 905 Loss: 0.13746084167537992\n",
      "Iteration 906 Loss: 0.13731847230604965\n",
      "Iteration 907 Loss: 0.13717643002213611\n",
      "Iteration 908 Loss: 0.1370347140710801\n",
      "Iteration 909 Loss: 0.13689332370205365\n",
      "Iteration 910 Loss: 0.13675225816595687\n",
      "Iteration 911 Loss: 0.1366115167154131\n",
      "Iteration 912 Loss: 0.13647109860476606\n",
      "Iteration 913 Loss: 0.13633100309007484\n",
      "Iteration 914 Loss: 0.13619122942911047\n",
      "Iteration 915 Loss: 0.13605177688135178\n",
      "Iteration 916 Loss: 0.13591264470798164\n",
      "Iteration 917 Loss: 0.13577383217188305\n",
      "Iteration 918 Loss: 0.13563533853763504\n",
      "Iteration 919 Loss: 0.1354971630715087\n",
      "Iteration 920 Loss: 0.13535930504146373\n",
      "Iteration 921 Loss: 0.13522176371714384\n",
      "Iteration 922 Loss: 0.13508453836987389\n",
      "Iteration 923 Loss: 0.13494762827265497\n",
      "Iteration 924 Loss: 0.1348110327001611\n",
      "Iteration 925 Loss: 0.13467475092873538\n",
      "Iteration 926 Loss: 0.13453878223638568\n",
      "Iteration 927 Loss: 0.13440312590278178\n",
      "Iteration 928 Loss: 0.1342677812092504\n",
      "Iteration 929 Loss: 0.13413274743877196\n",
      "Iteration 930 Loss: 0.1339980238759772\n",
      "Iteration 931 Loss: 0.13386360980714226\n",
      "Iteration 932 Loss: 0.13372950452018612\n",
      "Iteration 933 Loss: 0.13359570730466575\n",
      "Iteration 934 Loss: 0.13346221745177325\n",
      "Iteration 935 Loss: 0.13332903425433115\n",
      "Iteration 936 Loss: 0.1331961570067898\n",
      "Iteration 937 Loss: 0.1330635850052225\n",
      "Iteration 938 Loss: 0.1329313175473225\n",
      "Iteration 939 Loss: 0.13279935393239886\n",
      "Iteration 940 Loss: 0.13266769346137314\n",
      "Iteration 941 Loss: 0.13253633543677512\n",
      "Iteration 942 Loss: 0.13240527916273975\n",
      "Iteration 943 Loss: 0.132274523945003\n",
      "Iteration 944 Loss: 0.13214406909089815\n",
      "Iteration 945 Loss: 0.13201391390935258\n",
      "Iteration 946 Loss: 0.13188405771088338\n",
      "Iteration 947 Loss: 0.13175449980759468\n",
      "Iteration 948 Loss: 0.1316252395131728\n",
      "Iteration 949 Loss: 0.13149627614288362\n",
      "Iteration 950 Loss: 0.13136760901356836\n",
      "Iteration 951 Loss: 0.13123923744364033\n",
      "Iteration 952 Loss: 0.13111116075308082\n",
      "Iteration 953 Loss: 0.13098337826343615\n",
      "Iteration 954 Loss: 0.13085588929781358\n",
      "Iteration 955 Loss: 0.1307286931808776\n",
      "Iteration 956 Loss: 0.13060178923884702\n",
      "Iteration 957 Loss: 0.13047517679949067\n",
      "Iteration 958 Loss: 0.1303488551921243\n",
      "Iteration 959 Loss: 0.13022282374760685\n",
      "Iteration 960 Loss: 0.1300970817983366\n",
      "Iteration 961 Loss: 0.12997162867824835\n",
      "Iteration 962 Loss: 0.12984646372280934\n",
      "Iteration 963 Loss: 0.1297215862690156\n",
      "Iteration 964 Loss: 0.12959699565538899\n",
      "Iteration 965 Loss: 0.12947269122197316\n",
      "Iteration 966 Loss: 0.1293486723103304\n",
      "Iteration 967 Loss: 0.1292249382635379\n",
      "Iteration 968 Loss: 0.1291014884261846\n",
      "Iteration 969 Loss: 0.12897832214436708\n",
      "Iteration 970 Loss: 0.12885543876568692\n",
      "Iteration 971 Loss: 0.12873283763924653\n",
      "Iteration 972 Loss: 0.12861051811564592\n",
      "Iteration 973 Loss: 0.12848847954697978\n",
      "Iteration 974 Loss: 0.12836672128683296\n",
      "Iteration 975 Loss: 0.1282452426902781\n",
      "Iteration 976 Loss: 0.12812404311387143\n",
      "Iteration 977 Loss: 0.12800312191564997\n",
      "Iteration 978 Loss: 0.12788247845512754\n",
      "Iteration 979 Loss: 0.12776211209329208\n",
      "Iteration 980 Loss: 0.1276420221926013\n",
      "Iteration 981 Loss: 0.12752220811698023\n",
      "Iteration 982 Loss: 0.12740266923181737\n",
      "Iteration 983 Loss: 0.12728340490396134\n",
      "Iteration 984 Loss: 0.1271644145017175\n",
      "Iteration 985 Loss: 0.12704569739484484\n",
      "Iteration 986 Loss: 0.12692725295455234\n",
      "Iteration 987 Loss: 0.12680908055349585\n",
      "Iteration 988 Loss: 0.1266911795657746\n",
      "Iteration 989 Loss: 0.126573549366928\n",
      "Iteration 990 Loss: 0.12645618933393227\n",
      "Iteration 991 Loss: 0.12633909884519712\n",
      "Iteration 992 Loss: 0.1262222772805625\n",
      "Iteration 993 Loss: 0.1261057240212955\n",
      "Iteration 994 Loss: 0.1259894384500862\n",
      "Iteration 995 Loss: 0.12587341995104578\n",
      "Iteration 996 Loss: 0.12575766790970205\n",
      "Iteration 997 Loss: 0.1256421817129969\n",
      "Iteration 998 Loss: 0.12552696074928268\n",
      "Iteration 999 Loss: 0.12541200440831907\n",
      "Iteration 1000 Loss: 0.1252973120812699\n",
      "Iteration 1001 Loss: 0.12518288316069975\n",
      "Iteration 1002 Loss: 0.12506871704057104\n",
      "Iteration 1003 Loss: 0.12495481311624054\n",
      "Iteration 1004 Loss: 0.12484117078445603\n",
      "Iteration 1005 Loss: 0.12472778944335365\n",
      "Iteration 1006 Loss: 0.12461466849245402\n",
      "Iteration 1007 Loss: 0.12450180733265966\n",
      "Iteration 1008 Loss: 0.12438920536625142\n",
      "Iteration 1009 Loss: 0.12427686199688544\n",
      "Iteration 1010 Loss: 0.12416477662959009\n",
      "Iteration 1011 Loss: 0.12405294867076253\n",
      "Iteration 1012 Loss: 0.12394137752816588\n",
      "Iteration 1013 Loss: 0.1238300626109258\n",
      "Iteration 1014 Loss: 0.12371900332952752\n",
      "Iteration 1015 Loss: 0.12360819909581275\n",
      "Iteration 1016 Loss: 0.12349764932297642\n",
      "Iteration 1017 Loss: 0.12338735342556362\n",
      "Iteration 1018 Loss: 0.12327731081946658\n",
      "Iteration 1019 Loss: 0.12316752092192136\n",
      "Iteration 1020 Loss: 0.12305798315150507\n",
      "Iteration 1021 Loss: 0.12294869692813246\n",
      "Iteration 1022 Loss: 0.12283966167305302\n",
      "Iteration 1023 Loss: 0.12273087680884784\n",
      "Iteration 1024 Loss: 0.12262234175942677\n",
      "Iteration 1025 Loss: 0.12251405595002496\n",
      "Iteration 1026 Loss: 0.12240601880720012\n",
      "Iteration 1027 Loss: 0.12229822975882931\n",
      "Iteration 1028 Loss: 0.12219068823410606\n",
      "Iteration 1029 Loss: 0.1220833936635373\n",
      "Iteration 1030 Loss: 0.12197634547894012\n",
      "Iteration 1031 Loss: 0.12186954311343912\n",
      "Iteration 1032 Loss: 0.12176298600146307\n",
      "Iteration 1033 Loss: 0.12165667357874219\n",
      "Iteration 1034 Loss: 0.12155060528230477\n",
      "Iteration 1035 Loss: 0.12144478055047477\n",
      "Iteration 1036 Loss: 0.12133919882286821\n",
      "Iteration 1037 Loss: 0.12123385954039065\n",
      "Iteration 1038 Loss: 0.12112876214523398\n",
      "Iteration 1039 Loss: 0.1210239060808737\n",
      "Iteration 1040 Loss: 0.1209192907920654\n",
      "Iteration 1041 Loss: 0.12081491572484271\n",
      "Iteration 1042 Loss: 0.12071078032651354\n",
      "Iteration 1043 Loss: 0.1206068840456577\n",
      "Iteration 1044 Loss: 0.12050322633212354\n",
      "Iteration 1045 Loss: 0.12039980663702524\n",
      "Iteration 1046 Loss: 0.12029662441274018\n",
      "Iteration 1047 Loss: 0.1201936791129055\n",
      "Iteration 1048 Loss: 0.12009097019241555\n",
      "Iteration 1049 Loss: 0.11998849710741885\n",
      "Iteration 1050 Loss: 0.11988625931531523\n",
      "Iteration 1051 Loss: 0.11978425627475305\n",
      "Iteration 1052 Loss: 0.11968248744562629\n",
      "Iteration 1053 Loss: 0.11958095228907155\n",
      "Iteration 1054 Loss: 0.11947965026746528\n",
      "Iteration 1055 Loss: 0.1193785808444209\n",
      "Iteration 1056 Loss: 0.11927774348478634\n",
      "Iteration 1057 Loss: 0.1191771376546403\n",
      "Iteration 1058 Loss: 0.1190767628212904\n",
      "Iteration 1059 Loss: 0.11897661845326968\n",
      "Iteration 1060 Loss: 0.11887670402033416\n",
      "Iteration 1061 Loss: 0.11877701899345972\n",
      "Iteration 1062 Loss: 0.11867756284483968\n",
      "Iteration 1063 Loss: 0.11857833504788151\n",
      "Iteration 1064 Loss: 0.11847933507720461\n",
      "Iteration 1065 Loss: 0.11838056240863697\n",
      "Iteration 1066 Loss: 0.1182820165192127\n",
      "Iteration 1067 Loss: 0.11818369688716937\n",
      "Iteration 1068 Loss: 0.11808560299194468\n",
      "Iteration 1069 Loss: 0.11798773431417446\n",
      "Iteration 1070 Loss: 0.11789009033568934\n",
      "Iteration 1071 Loss: 0.11779267053951208\n",
      "Iteration 1072 Loss: 0.11769547440985526\n",
      "Iteration 1073 Loss: 0.11759850143211799\n",
      "Iteration 1074 Loss: 0.1175017510928835\n",
      "Iteration 1075 Loss: 0.11740522287991617\n",
      "Iteration 1076 Loss: 0.1173089162821593\n",
      "Iteration 1077 Loss: 0.11721283078973163\n",
      "Iteration 1078 Loss: 0.11711696589392546\n",
      "Iteration 1079 Loss: 0.11702132108720331\n",
      "Iteration 1080 Loss: 0.11692589586319566\n",
      "Iteration 1081 Loss: 0.11683068971669805\n",
      "Iteration 1082 Loss: 0.11673570214366813\n",
      "Iteration 1083 Loss: 0.1166409326412238\n",
      "Iteration 1084 Loss: 0.11654638070763963\n",
      "Iteration 1085 Loss: 0.11645204584234474\n",
      "Iteration 1086 Loss: 0.11635792754592007\n",
      "Iteration 1087 Loss: 0.11626402532009535\n",
      "Iteration 1088 Loss: 0.11617033866774731\n",
      "Iteration 1089 Loss: 0.11607686709289604\n",
      "Iteration 1090 Loss: 0.11598361010070293\n",
      "Iteration 1091 Loss: 0.11589056719746797\n",
      "Iteration 1092 Loss: 0.11579773789062711\n",
      "Iteration 1093 Loss: 0.11570512168874958\n",
      "Iteration 1094 Loss: 0.11561271810153544\n",
      "Iteration 1095 Loss: 0.11552052663981269\n",
      "Iteration 1096 Loss: 0.11542854681553497\n",
      "Iteration 1097 Loss: 0.11533677814177881\n",
      "Iteration 1098 Loss: 0.11524522013274105\n",
      "Iteration 1099 Loss: 0.11515387230373642\n",
      "Iteration 1100 Loss: 0.11506273417119481\n",
      "Iteration 1101 Loss: 0.11497180525265875\n",
      "Iteration 1102 Loss: 0.1148810850667807\n",
      "Iteration 1103 Loss: 0.11479057313332071\n",
      "Iteration 1104 Loss: 0.11470026897314392\n",
      "Iteration 1105 Loss: 0.11461017210821786\n",
      "Iteration 1106 Loss: 0.11452028206160991\n",
      "Iteration 1107 Loss: 0.11443059835748466\n",
      "Iteration 1108 Loss: 0.11434112052110182\n",
      "Iteration 1109 Loss: 0.1142518480788132\n",
      "Iteration 1110 Loss: 0.11416278055806059\n",
      "Iteration 1111 Loss: 0.11407391748737299\n",
      "Iteration 1112 Loss: 0.11398525839636406\n",
      "Iteration 1113 Loss: 0.11389680281573003\n",
      "Iteration 1114 Loss: 0.11380855027724661\n",
      "Iteration 1115 Loss: 0.11372050031376721\n",
      "Iteration 1116 Loss: 0.11363265245921986\n",
      "Iteration 1117 Loss: 0.11354500624860486\n",
      "Iteration 1118 Loss: 0.11345756121799275\n",
      "Iteration 1119 Loss: 0.11337031690452107\n",
      "Iteration 1120 Loss: 0.1132832728463928\n",
      "Iteration 1121 Loss: 0.11319642858287318\n",
      "Iteration 1122 Loss: 0.11310978365428762\n",
      "Iteration 1123 Loss: 0.11302333760201903\n",
      "Iteration 1124 Loss: 0.1129370899685059\n",
      "Iteration 1125 Loss: 0.11285104029723905\n",
      "Iteration 1126 Loss: 0.1127651881327602\n",
      "Iteration 1127 Loss: 0.11267953302065847\n",
      "Iteration 1128 Loss: 0.11259407450756889\n",
      "Iteration 1129 Loss: 0.11250881214116952\n",
      "Iteration 1130 Loss: 0.11242374547017911\n",
      "Iteration 1131 Loss: 0.11233887404435479\n",
      "Iteration 1132 Loss: 0.11225419741448968\n",
      "Iteration 1133 Loss: 0.11216971513241034\n",
      "Iteration 1134 Loss: 0.11208542675097477\n",
      "Iteration 1135 Loss: 0.11200133182406953\n",
      "Iteration 1136 Loss: 0.11191742990660765\n",
      "Iteration 1137 Loss: 0.11183372055452642\n",
      "Iteration 1138 Loss: 0.11175020332478476\n",
      "Iteration 1139 Loss: 0.11166687777536086\n",
      "Iteration 1140 Loss: 0.11158374346525024\n",
      "Iteration 1141 Loss: 0.11150079995446263\n",
      "Iteration 1142 Loss: 0.11141804680402076\n",
      "Iteration 1143 Loss: 0.11133548357595695\n",
      "Iteration 1144 Loss: 0.1112531098333112\n",
      "Iteration 1145 Loss: 0.11117092514012923\n",
      "Iteration 1146 Loss: 0.11108892906145944\n",
      "Iteration 1147 Loss: 0.11100712116335136\n",
      "Iteration 1148 Loss: 0.11092550101285263\n",
      "Iteration 1149 Loss: 0.11084406817800743\n",
      "Iteration 1150 Loss: 0.11076282222785357\n",
      "Iteration 1151 Loss: 0.1106817627324205\n",
      "Iteration 1152 Loss: 0.11060088926272696\n",
      "Iteration 1153 Loss: 0.11052020139077863\n",
      "Iteration 1154 Loss: 0.11043969868956635\n",
      "Iteration 1155 Loss: 0.11035938073306299\n",
      "Iteration 1156 Loss: 0.1102792470962218\n",
      "Iteration 1157 Loss: 0.1101992973549741\n",
      "Iteration 1158 Loss: 0.11011953108622677\n",
      "Iteration 1159 Loss: 0.11003994786786045\n",
      "Iteration 1160 Loss: 0.10996054727872676\n",
      "Iteration 1161 Loss: 0.10988132889864637\n",
      "Iteration 1162 Loss: 0.1098022923084069\n",
      "Iteration 1163 Loss: 0.1097234370897604\n",
      "Iteration 1164 Loss: 0.10964476282542109\n",
      "Iteration 1165 Loss: 0.10956626909906364\n",
      "Iteration 1166 Loss: 0.10948795549532035\n",
      "Iteration 1167 Loss: 0.1094098215997795\n",
      "Iteration 1168 Loss: 0.10933186699898252\n",
      "Iteration 1169 Loss: 0.10925409128042254\n",
      "Iteration 1170 Loss: 0.10917649403254144\n",
      "Iteration 1171 Loss: 0.1090990748447284\n",
      "Iteration 1172 Loss: 0.1090218333073171\n",
      "Iteration 1173 Loss: 0.10894476901158394\n",
      "Iteration 1174 Loss: 0.10886788154974578\n",
      "Iteration 1175 Loss: 0.10879117051495753\n",
      "Iteration 1176 Loss: 0.10871463550131044\n",
      "Iteration 1177 Loss: 0.10863827610382955\n",
      "Iteration 1178 Loss: 0.10856209191847169\n",
      "Iteration 1179 Loss: 0.10848608254212337\n",
      "Iteration 1180 Loss: 0.10841024757259872\n",
      "Iteration 1181 Loss: 0.10833458660863719\n",
      "Iteration 1182 Loss: 0.10825909924990136\n",
      "Iteration 1183 Loss: 0.10818378509697513\n",
      "Iteration 1184 Loss: 0.10810864375136112\n",
      "Iteration 1185 Loss: 0.10803367481547924\n",
      "Iteration 1186 Loss: 0.10795887789266374\n",
      "Iteration 1187 Loss: 0.1078842525871619\n",
      "Iteration 1188 Loss: 0.10780979850413137\n",
      "Iteration 1189 Loss: 0.10773551524963838\n",
      "Iteration 1190 Loss: 0.10766140243065556\n",
      "Iteration 1191 Loss: 0.10758745965505985\n",
      "Iteration 1192 Loss: 0.10751368653163022\n",
      "Iteration 1193 Loss: 0.10744008267004608\n",
      "Iteration 1194 Loss: 0.1073666476808847\n",
      "Iteration 1195 Loss: 0.10729338117561946\n",
      "Iteration 1196 Loss: 0.10722028276661769\n",
      "Iteration 1197 Loss: 0.10714735206713855\n",
      "Iteration 1198 Loss: 0.10707458869133121\n",
      "Iteration 1199 Loss: 0.10700199225423224\n",
      "Iteration 1200 Loss: 0.10692956237176447\n",
      "Iteration 1201 Loss: 0.10685729866073412\n",
      "Iteration 1202 Loss: 0.10678520073882913\n",
      "Iteration 1203 Loss: 0.10671326822461716\n",
      "Iteration 1204 Loss: 0.10664150073754355\n",
      "Iteration 1205 Loss: 0.10656989789792903\n",
      "Iteration 1206 Loss: 0.10649845932696808\n",
      "Iteration 1207 Loss: 0.10642718464672678\n",
      "Iteration 1208 Loss: 0.10635607348014073\n",
      "Iteration 1209 Loss: 0.10628512545101314\n",
      "Iteration 1210 Loss: 0.1062143401840127\n",
      "Iteration 1211 Loss: 0.10614371730467184\n",
      "Iteration 1212 Loss: 0.10607325643938448\n",
      "Iteration 1213 Loss: 0.10600295721540429\n",
      "Iteration 1214 Loss: 0.10593281926084228\n",
      "Iteration 1215 Loss: 0.10586284220466562\n",
      "Iteration 1216 Loss: 0.10579302567669471\n",
      "Iteration 1217 Loss: 0.10572336930760193\n",
      "Iteration 1218 Loss: 0.10565387272890929\n",
      "Iteration 1219 Loss: 0.10558453557298683\n",
      "Iteration 1220 Loss: 0.10551535747305023\n",
      "Iteration 1221 Loss: 0.10544633806315935\n",
      "Iteration 1222 Loss: 0.10537747697821573\n",
      "Iteration 1223 Loss: 0.10530877385396123\n",
      "Iteration 1224 Loss: 0.10524022832697548\n",
      "Iteration 1225 Loss: 0.10517184003467475\n",
      "Iteration 1226 Loss: 0.10510360861530914\n",
      "Iteration 1227 Loss: 0.10503553370796152\n",
      "Iteration 1228 Loss: 0.10496761495254459\n",
      "Iteration 1229 Loss: 0.10489985198980023\n",
      "Iteration 1230 Loss: 0.10483224446129642\n",
      "Iteration 1231 Loss: 0.10476479200942608\n",
      "Iteration 1232 Loss: 0.10469749427740496\n",
      "Iteration 1233 Loss: 0.10463035090926964\n",
      "Iteration 1234 Loss: 0.10456336154987564\n",
      "Iteration 1235 Loss: 0.10449652584489591\n",
      "Iteration 1236 Loss: 0.10442984344081821\n",
      "Iteration 1237 Loss: 0.10436331398494417\n",
      "Iteration 1238 Loss: 0.1042969371253866\n",
      "Iteration 1239 Loss: 0.10423071251106801\n",
      "Iteration 1240 Loss: 0.10416463979171893\n",
      "Iteration 1241 Loss: 0.10409871861787556\n",
      "Iteration 1242 Loss: 0.1040329486408782\n",
      "Iteration 1243 Loss: 0.10396732951286955\n",
      "Iteration 1244 Loss: 0.1039018608867924\n",
      "Iteration 1245 Loss: 0.10383654241638832\n",
      "Iteration 1246 Loss: 0.10377137375619538\n",
      "Iteration 1247 Loss: 0.10370635456154668\n",
      "Iteration 1248 Loss: 0.10364148448856843\n",
      "Iteration 1249 Loss: 0.10357676319417772\n",
      "Iteration 1250 Loss: 0.10351219033608121\n",
      "Iteration 1251 Loss: 0.10344776557277324\n",
      "Iteration 1252 Loss: 0.10338348856353381\n",
      "Iteration 1253 Loss: 0.1033193589684267\n",
      "Iteration 1254 Loss: 0.10325537644829821\n",
      "Iteration 1255 Loss: 0.10319154066477472\n",
      "Iteration 1256 Loss: 0.10312785128026128\n",
      "Iteration 1257 Loss: 0.1030643079579396\n",
      "Iteration 1258 Loss: 0.1030009103617664\n",
      "Iteration 1259 Loss: 0.1029376581564716\n",
      "Iteration 1260 Loss: 0.10287455100755663\n",
      "Iteration 1261 Loss: 0.1028115885812924\n",
      "Iteration 1262 Loss: 0.10274877054471783\n",
      "Iteration 1263 Loss: 0.10268609656563783\n",
      "Iteration 1264 Loss: 0.10262356631262164\n",
      "Iteration 1265 Loss: 0.1025611794550012\n",
      "Iteration 1266 Loss: 0.10249893566286916\n",
      "Iteration 1267 Loss: 0.10243683460707728\n",
      "Iteration 1268 Loss: 0.10237487595923471\n",
      "Iteration 1269 Loss: 0.10231305939170607\n",
      "Iteration 1270 Loss: 0.10225138457760993\n",
      "Iteration 1271 Loss: 0.10218985119081683\n",
      "Iteration 1272 Loss: 0.1021284589059479\n",
      "Iteration 1273 Loss: 0.10206720739837272\n",
      "Iteration 1274 Loss: 0.1020060963442079\n",
      "Iteration 1275 Loss: 0.10194512542031534\n",
      "Iteration 1276 Loss: 0.10188429430430028\n",
      "Iteration 1277 Loss: 0.10182360267450981\n",
      "Iteration 1278 Loss: 0.10176305021003111\n",
      "Iteration 1279 Loss: 0.10170263659068987\n",
      "Iteration 1280 Loss: 0.10164236149704813\n",
      "Iteration 1281 Loss: 0.10158222461040327\n",
      "Iteration 1282 Loss: 0.10152222561278579\n",
      "Iteration 1283 Loss: 0.10146236418695773\n",
      "Iteration 1284 Loss: 0.10140264001641124\n",
      "Iteration 1285 Loss: 0.10134305278536651\n",
      "Iteration 1286 Loss: 0.10128360217877061\n",
      "Iteration 1287 Loss: 0.10122428788229498\n",
      "Iteration 1288 Loss: 0.10116510958233489\n",
      "Iteration 1289 Loss: 0.1011060669660067\n",
      "Iteration 1290 Loss: 0.10104715972114704\n",
      "Iteration 1291 Loss: 0.10098838753631038\n",
      "Iteration 1292 Loss: 0.10092975010076813\n",
      "Iteration 1293 Loss: 0.1008712471045066\n",
      "Iteration 1294 Loss: 0.10081287823822534\n",
      "Iteration 1295 Loss: 0.10075464319333535\n",
      "Iteration 1296 Loss: 0.10069654166195804\n",
      "Iteration 1297 Loss: 0.10063857333692305\n",
      "Iteration 1298 Loss: 0.10058073791176661\n",
      "Iteration 1299 Loss: 0.10052303508073031\n",
      "Iteration 1300 Loss: 0.10046546453875918\n",
      "Iteration 1301 Loss: 0.10040802598150007\n",
      "Iteration 1302 Loss: 0.10035071910530018\n",
      "Iteration 1303 Loss: 0.10029354360720547\n",
      "Iteration 1304 Loss: 0.10023649918495865\n",
      "Iteration 1305 Loss: 0.10017958553699817\n",
      "Iteration 1306 Loss: 0.10012280236245638\n",
      "Iteration 1307 Loss: 0.10006614936115757\n",
      "Iteration 1308 Loss: 0.10000962623361694\n",
      "Iteration 1309 Loss: 0.09995323268103874\n",
      "Iteration 1310 Loss: 0.09989696840531459\n",
      "Iteration 1311 Loss: 0.09984083310902218\n",
      "Iteration 1312 Loss: 0.09978482649542339\n",
      "Iteration 1313 Loss: 0.09972894826846304\n",
      "Iteration 1314 Loss: 0.09967319813276684\n",
      "Iteration 1315 Loss: 0.09961757579364039\n",
      "Iteration 1316 Loss: 0.0995620809570672\n",
      "Iteration 1317 Loss: 0.09950671332970734\n",
      "Iteration 1318 Loss: 0.09945147261889589\n",
      "Iteration 1319 Loss: 0.09939635853264106\n",
      "Iteration 1320 Loss: 0.09934137077962311\n",
      "Iteration 1321 Loss: 0.09928650906919256\n",
      "Iteration 1322 Loss: 0.09923177311136863\n",
      "Iteration 1323 Loss: 0.0991771626168377\n",
      "Iteration 1324 Loss: 0.09912267729695194\n",
      "Iteration 1325 Loss: 0.09906831686372762\n",
      "Iteration 1326 Loss: 0.09901408102984358\n",
      "Iteration 1327 Loss: 0.09895996950863976\n",
      "Iteration 1328 Loss: 0.0989059820141158\n",
      "Iteration 1329 Loss: 0.09885211826092914\n",
      "Iteration 1330 Loss: 0.09879837796439406\n",
      "Iteration 1331 Loss: 0.09874476084047948\n",
      "Iteration 1332 Loss: 0.09869126660580813\n",
      "Iteration 1333 Loss: 0.09863789497765481\n",
      "Iteration 1334 Loss: 0.09858464567394451\n",
      "Iteration 1335 Loss: 0.09853151841325158\n",
      "Iteration 1336 Loss: 0.0984785129147977\n",
      "Iteration 1337 Loss: 0.0984256288984505\n",
      "Iteration 1338 Loss: 0.09837286608472247\n",
      "Iteration 1339 Loss: 0.09832022419476896\n",
      "Iteration 1340 Loss: 0.09826770295038678\n",
      "Iteration 1341 Loss: 0.09821530207401308\n",
      "Iteration 1342 Loss: 0.09816302128872356\n",
      "Iteration 1343 Loss: 0.098110860318231\n",
      "Iteration 1344 Loss: 0.09805881888688418\n",
      "Iteration 1345 Loss: 0.09800689671966571\n",
      "Iteration 1346 Loss: 0.09795509354219126\n",
      "Iteration 1347 Loss: 0.09790340908070767\n",
      "Iteration 1348 Loss: 0.0978518430620918\n",
      "Iteration 1349 Loss: 0.09780039521384874\n",
      "Iteration 1350 Loss: 0.09774906526411073\n",
      "Iteration 1351 Loss: 0.0976978529416355\n",
      "Iteration 1352 Loss: 0.09764675797580476\n",
      "Iteration 1353 Loss: 0.09759578009662304\n",
      "Iteration 1354 Loss: 0.09754491903471593\n",
      "Iteration 1355 Loss: 0.09749417452132898\n",
      "Iteration 1356 Loss: 0.09744354628832592\n",
      "Iteration 1357 Loss: 0.09739303406818775\n",
      "Iteration 1358 Loss: 0.0973426375940106\n",
      "Iteration 1359 Loss: 0.09729235659950493\n",
      "Iteration 1360 Loss: 0.09724219081899388\n",
      "Iteration 1361 Loss: 0.0971921399874119\n",
      "Iteration 1362 Loss: 0.0971422038403032\n",
      "Iteration 1363 Loss: 0.09709238211382058\n",
      "Iteration 1364 Loss: 0.09704267454472386\n",
      "Iteration 1365 Loss: 0.09699308087037856\n",
      "Iteration 1366 Loss: 0.09694360082875451\n",
      "Iteration 1367 Loss: 0.09689423415842455\n",
      "Iteration 1368 Loss: 0.09684498059856261\n",
      "Iteration 1369 Loss: 0.09679583988894333\n",
      "Iteration 1370 Loss: 0.09674681176993964\n",
      "Iteration 1371 Loss: 0.09669789598252204\n",
      "Iteration 1372 Loss: 0.09664909226825694\n",
      "Iteration 1373 Loss: 0.09660040036930548\n",
      "Iteration 1374 Loss: 0.09655182002842191\n",
      "Iteration 1375 Loss: 0.0965033509889525\n",
      "Iteration 1376 Loss: 0.09645499299483402\n",
      "Iteration 1377 Loss: 0.09640674579059233\n",
      "Iteration 1378 Loss: 0.09635860912134109\n",
      "Iteration 1379 Loss: 0.09631058273278058\n",
      "Iteration 1380 Loss: 0.09626266637119607\n",
      "Iteration 1381 Loss: 0.09621485978345663\n",
      "Iteration 1382 Loss: 0.09616716271701366\n",
      "Iteration 1383 Loss: 0.09611957491989978\n",
      "Iteration 1384 Loss: 0.09607209614072734\n",
      "Iteration 1385 Loss: 0.09602472612868687\n",
      "Iteration 1386 Loss: 0.09597746463354635\n",
      "Iteration 1387 Loss: 0.09593031140564919\n",
      "Iteration 1388 Loss: 0.09588326619591345\n",
      "Iteration 1389 Loss: 0.09583632875583022\n",
      "Iteration 1390 Loss: 0.09578949883746225\n",
      "Iteration 1391 Loss: 0.09574277619344299\n",
      "Iteration 1392 Loss: 0.09569616057697489\n",
      "Iteration 1393 Loss: 0.09564965174182812\n",
      "Iteration 1394 Loss: 0.09560324944233979\n",
      "Iteration 1395 Loss: 0.09555695343341195\n",
      "Iteration 1396 Loss: 0.0955107634705105\n",
      "Iteration 1397 Loss: 0.09546467930966444\n",
      "Iteration 1398 Loss: 0.09541870070746361\n",
      "Iteration 1399 Loss: 0.09537282742105808\n",
      "Iteration 1400 Loss: 0.09532705920815679\n",
      "Iteration 1401 Loss: 0.09528139582702606\n",
      "Iteration 1402 Loss: 0.09523583703648844\n",
      "Iteration 1403 Loss: 0.09519038259592134\n",
      "Iteration 1404 Loss: 0.09514503226525581\n",
      "Iteration 1405 Loss: 0.09509978580497554\n",
      "Iteration 1406 Loss: 0.09505464297611489\n",
      "Iteration 1407 Loss: 0.09500960354025825\n",
      "Iteration 1408 Loss: 0.09496466725953859\n",
      "Iteration 1409 Loss: 0.0949198338966363\n",
      "Iteration 1410 Loss: 0.09487510321477745\n",
      "Iteration 1411 Loss: 0.09483047497773323\n",
      "Iteration 1412 Loss: 0.0947859489498183\n",
      "Iteration 1413 Loss: 0.0947415248958895\n",
      "Iteration 1414 Loss: 0.09469720258134472\n",
      "Iteration 1415 Loss: 0.09465298177212167\n",
      "Iteration 1416 Loss: 0.09460886223469658\n",
      "Iteration 1417 Loss: 0.09456484373608313\n",
      "Iteration 1418 Loss: 0.09452092604383086\n",
      "Iteration 1419 Loss: 0.09447710892602428\n",
      "Iteration 1420 Loss: 0.09443339215128146\n",
      "Iteration 1421 Loss: 0.09438977548875287\n",
      "Iteration 1422 Loss: 0.09434625870812016\n",
      "Iteration 1423 Loss: 0.09430284157959479\n",
      "Iteration 1424 Loss: 0.09425952387391703\n",
      "Iteration 1425 Loss: 0.09421630536235474\n",
      "Iteration 1426 Loss: 0.09417318581670178\n",
      "Iteration 1427 Loss: 0.09413016500927741\n",
      "Iteration 1428 Loss: 0.09408724271292453\n",
      "Iteration 1429 Loss: 0.09404441870100891\n",
      "Iteration 1430 Loss: 0.09400169274741732\n",
      "Iteration 1431 Loss: 0.09395906462655736\n",
      "Iteration 1432 Loss: 0.09391653411335543\n",
      "Iteration 1433 Loss: 0.0938741009832557\n",
      "Iteration 1434 Loss: 0.09383176501221921\n",
      "Iteration 1435 Loss: 0.09378952597672233\n",
      "Iteration 1436 Loss: 0.0937473836537557\n",
      "Iteration 1437 Loss: 0.09370533782082324\n",
      "Iteration 1438 Loss: 0.0936633882559407\n",
      "Iteration 1439 Loss: 0.09362153473763465\n",
      "Iteration 1440 Loss: 0.09357977704494111\n",
      "Iteration 1441 Loss: 0.09353811495740472\n",
      "Iteration 1442 Loss: 0.09349654825507703\n",
      "Iteration 1443 Loss: 0.09345507671851613\n",
      "Iteration 1444 Loss: 0.09341370012878462\n",
      "Iteration 1445 Loss: 0.09337241826744891\n",
      "Iteration 1446 Loss: 0.09333123091657816\n",
      "Iteration 1447 Loss: 0.09329013785874281\n",
      "Iteration 1448 Loss: 0.09324913887701365\n",
      "Iteration 1449 Loss: 0.09320823375496072\n",
      "Iteration 1450 Loss: 0.09316742227665165\n",
      "Iteration 1451 Loss: 0.09312670422665134\n",
      "Iteration 1452 Loss: 0.09308607939002005\n",
      "Iteration 1453 Loss: 0.09304554755231279\n",
      "Iteration 1454 Loss: 0.09300510849957797\n",
      "Iteration 1455 Loss: 0.09296476201835609\n",
      "Iteration 1456 Loss: 0.09292450789567894\n",
      "Iteration 1457 Loss: 0.09288434591906833\n",
      "Iteration 1458 Loss: 0.09284427587653503\n",
      "Iteration 1459 Loss: 0.09280429755657724\n",
      "Iteration 1460 Loss: 0.09276441074818019\n",
      "Iteration 1461 Loss: 0.09272461524081446\n",
      "Iteration 1462 Loss: 0.09268491082443489\n",
      "Iteration 1463 Loss: 0.09264529728947986\n",
      "Iteration 1464 Loss: 0.09260577442686978\n",
      "Iteration 1465 Loss: 0.0925663420280061\n",
      "Iteration 1466 Loss: 0.0925269998847703\n",
      "Iteration 1467 Loss: 0.09248774778952276\n",
      "Iteration 1468 Loss: 0.09244858553510146\n",
      "Iteration 1469 Loss: 0.09240951291482118\n",
      "Iteration 1470 Loss: 0.09237052972247214\n",
      "Iteration 1471 Loss: 0.09233163575231909\n",
      "Iteration 1472 Loss: 0.09229283079910011\n",
      "Iteration 1473 Loss: 0.0922541146580256\n",
      "Iteration 1474 Loss: 0.09221548712477709\n",
      "Iteration 1475 Loss: 0.0921769479955064\n",
      "Iteration 1476 Loss: 0.09213849706683408\n",
      "Iteration 1477 Loss: 0.09210013413584887\n",
      "Iteration 1478 Loss: 0.09206185900010627\n",
      "Iteration 1479 Loss: 0.09202367145762765\n",
      "Iteration 1480 Loss: 0.09198557130689908\n",
      "Iteration 1481 Loss: 0.0919475583468702\n",
      "Iteration 1482 Loss: 0.09190963237695328\n",
      "Iteration 1483 Loss: 0.09187179319702235\n",
      "Iteration 1484 Loss: 0.09183404060741147\n",
      "Iteration 1485 Loss: 0.09179637440891453\n",
      "Iteration 1486 Loss: 0.09175879440278337\n",
      "Iteration 1487 Loss: 0.09172130039072748\n",
      "Iteration 1488 Loss: 0.09168389217491238\n",
      "Iteration 1489 Loss: 0.09164656955795886\n",
      "Iteration 1490 Loss: 0.09160933234294173\n",
      "Iteration 1491 Loss: 0.09157218033338924\n",
      "Iteration 1492 Loss: 0.09153511333328117\n",
      "Iteration 1493 Loss: 0.09149813114704883\n",
      "Iteration 1494 Loss: 0.09146123357957317\n",
      "Iteration 1495 Loss: 0.09142442043618408\n",
      "Iteration 1496 Loss: 0.09138769152265967\n",
      "Iteration 1497 Loss: 0.09135104664522462\n",
      "Iteration 1498 Loss: 0.09131448561054967\n",
      "Iteration 1499 Loss: 0.09127800822575018\n",
      "Iteration 1500 Loss: 0.09124161429838541\n",
      "Iteration 1501 Loss: 0.09120530363645737\n",
      "Iteration 1502 Loss: 0.09116907604841008\n",
      "Iteration 1503 Loss: 0.09113293134312786\n",
      "Iteration 1504 Loss: 0.09109686932993519\n",
      "Iteration 1505 Loss: 0.09106088981859492\n",
      "Iteration 1506 Loss: 0.09102499261930781\n",
      "Iteration 1507 Loss: 0.09098917754271138\n",
      "Iteration 1508 Loss: 0.09095344439987856\n",
      "Iteration 1509 Loss: 0.09091779300231724\n",
      "Iteration 1510 Loss: 0.09088222316196896\n",
      "Iteration 1511 Loss: 0.0908467346912079\n",
      "Iteration 1512 Loss: 0.09081132740284\n",
      "Iteration 1513 Loss: 0.09077600111010178\n",
      "Iteration 1514 Loss: 0.0907407556266597\n",
      "Iteration 1515 Loss: 0.09070559076660877\n",
      "Iteration 1516 Loss: 0.0906705063444717\n",
      "Iteration 1517 Loss: 0.09063550217519814\n",
      "Iteration 1518 Loss: 0.09060057807416345\n",
      "Iteration 1519 Loss: 0.09056573385716768\n",
      "Iteration 1520 Loss: 0.09053096934043485\n",
      "Iteration 1521 Loss: 0.09049628434061172\n",
      "Iteration 1522 Loss: 0.09046167867476698\n",
      "Iteration 1523 Loss: 0.09042715216039018\n",
      "Iteration 1524 Loss: 0.09039270461539091\n",
      "Iteration 1525 Loss: 0.09035833585809745\n",
      "Iteration 1526 Loss: 0.09032404570725643\n",
      "Iteration 1527 Loss: 0.09028983398203125\n",
      "Iteration 1528 Loss: 0.09025570050200167\n",
      "Iteration 1529 Loss: 0.0902216450871621\n",
      "Iteration 1530 Loss: 0.09018766755792179\n",
      "Iteration 1531 Loss: 0.09015376773510253\n",
      "Iteration 1532 Loss: 0.09011994543993888\n",
      "Iteration 1533 Loss: 0.09008620049407626\n",
      "Iteration 1534 Loss: 0.09005253271957106\n",
      "Iteration 1535 Loss: 0.09001894193888847\n",
      "Iteration 1536 Loss: 0.08998542797490268\n",
      "Iteration 1537 Loss: 0.08995199065089497\n",
      "Iteration 1538 Loss: 0.08991862979055351\n",
      "Iteration 1539 Loss: 0.08988534521797212\n",
      "Iteration 1540 Loss: 0.08985213675764925\n",
      "Iteration 1541 Loss: 0.08981900423448727\n",
      "Iteration 1542 Loss: 0.08978594747379126\n",
      "Iteration 1543 Loss: 0.08975296630126851\n",
      "Iteration 1544 Loss: 0.08972006054302714\n",
      "Iteration 1545 Loss: 0.08968723002557535\n",
      "Iteration 1546 Loss: 0.08965447457582057\n",
      "Iteration 1547 Loss: 0.08962179402106865\n",
      "Iteration 1548 Loss: 0.08958918818902248\n",
      "Iteration 1549 Loss: 0.08955665690778167\n",
      "Iteration 1550 Loss: 0.08952420000584116\n",
      "Iteration 1551 Loss: 0.08949181731209062\n",
      "Iteration 1552 Loss: 0.08945950865581334\n",
      "Iteration 1553 Loss: 0.08942727386668536\n",
      "Iteration 1554 Loss: 0.08939511277477484\n",
      "Iteration 1555 Loss: 0.08936302521054071\n",
      "Iteration 1556 Loss: 0.08933101100483196\n",
      "Iteration 1557 Loss: 0.08929906998888697\n",
      "Iteration 1558 Loss: 0.08926720199433232\n",
      "Iteration 1559 Loss: 0.08923540685318199\n",
      "Iteration 1560 Loss: 0.08920368439783652\n",
      "Iteration 1561 Loss: 0.08917203446108195\n",
      "Iteration 1562 Loss: 0.08914045687608918\n",
      "Iteration 1563 Loss: 0.08910895147641289\n",
      "Iteration 1564 Loss: 0.08907751809599083\n",
      "Iteration 1565 Loss: 0.08904615656914265\n",
      "Iteration 1566 Loss: 0.08901486673056946\n",
      "Iteration 1567 Loss: 0.08898364841535242\n",
      "Iteration 1568 Loss: 0.08895250145895253\n",
      "Iteration 1569 Loss: 0.08892142569720878\n",
      "Iteration 1570 Loss: 0.08889042096633851\n",
      "Iteration 1571 Loss: 0.0888594871029355\n",
      "Iteration 1572 Loss: 0.08882862394396958\n",
      "Iteration 1573 Loss: 0.0887978313267859\n",
      "Iteration 1574 Loss: 0.08876710908910354\n",
      "Iteration 1575 Loss: 0.08873645706901509\n",
      "Iteration 1576 Loss: 0.0887058751049858\n",
      "Iteration 1577 Loss: 0.0886753630358524\n",
      "Iteration 1578 Loss: 0.08864492070082255\n",
      "Iteration 1579 Loss: 0.08861454793947392\n",
      "Iteration 1580 Loss: 0.0885842445917531\n",
      "Iteration 1581 Loss: 0.08855401049797511\n",
      "Iteration 1582 Loss: 0.08852384549882222\n",
      "Iteration 1583 Loss: 0.08849374943534354\n",
      "Iteration 1584 Loss: 0.08846372214895357\n",
      "Iteration 1585 Loss: 0.08843376348143198\n",
      "Iteration 1586 Loss: 0.0884038732749223\n",
      "Iteration 1587 Loss: 0.0883740513719313\n",
      "Iteration 1588 Loss: 0.08834429761532825\n",
      "Iteration 1589 Loss: 0.08831461184834385\n",
      "Iteration 1590 Loss: 0.08828499391456943\n",
      "Iteration 1591 Loss: 0.0882554436579563\n",
      "Iteration 1592 Loss: 0.0882259609228148\n",
      "Iteration 1593 Loss: 0.08819654555381348\n",
      "Iteration 1594 Loss: 0.08816719739597836\n",
      "Iteration 1595 Loss: 0.08813791629469192\n",
      "Iteration 1596 Loss: 0.08810870209569238\n",
      "Iteration 1597 Loss: 0.08807955464507299\n",
      "Iteration 1598 Loss: 0.08805047378928117\n",
      "Iteration 1599 Loss: 0.0880214593751175\n",
      "Iteration 1600 Loss: 0.08799251124973509\n",
      "Iteration 1601 Loss: 0.08796362926063868\n",
      "Iteration 1602 Loss: 0.08793481325568404\n",
      "Iteration 1603 Loss: 0.08790606308307689\n",
      "Iteration 1604 Loss: 0.08787737859137235\n",
      "Iteration 1605 Loss: 0.0878487596294737\n",
      "Iteration 1606 Loss: 0.08782020604663206\n",
      "Iteration 1607 Loss: 0.0877917176924455\n",
      "Iteration 1608 Loss: 0.08776329441685793\n",
      "Iteration 1609 Loss: 0.08773493607015867\n",
      "Iteration 1610 Loss: 0.08770664250298157\n",
      "Iteration 1611 Loss: 0.08767841356630376\n",
      "Iteration 1612 Loss: 0.08765024911144578\n",
      "Iteration 1613 Loss: 0.08762214899006993\n",
      "Iteration 1614 Loss: 0.08759411305417983\n",
      "Iteration 1615 Loss: 0.08756614115611984\n",
      "Iteration 1616 Loss: 0.08753823314857374\n",
      "Iteration 1617 Loss: 0.0875103888845645\n",
      "Iteration 1618 Loss: 0.08748260821745321\n",
      "Iteration 1619 Loss: 0.08745489100093841\n",
      "Iteration 1620 Loss: 0.08742723708905495\n",
      "Iteration 1621 Loss: 0.08739964633617395\n",
      "Iteration 1622 Loss: 0.08737211859700136\n",
      "Iteration 1623 Loss: 0.08734465372657742\n",
      "Iteration 1624 Loss: 0.08731725158027612\n",
      "Iteration 1625 Loss: 0.08728991201380394\n",
      "Iteration 1626 Loss: 0.08726263488319956\n",
      "Iteration 1627 Loss: 0.08723542004483274\n",
      "Iteration 1628 Loss: 0.08720826735540387\n",
      "Iteration 1629 Loss: 0.08718117667194296\n",
      "Iteration 1630 Loss: 0.08715414785180894\n",
      "Iteration 1631 Loss: 0.08712718075268902\n",
      "Iteration 1632 Loss: 0.08710027523259777\n",
      "Iteration 1633 Loss: 0.08707343114987662\n",
      "Iteration 1634 Loss: 0.08704664836319273\n",
      "Iteration 1635 Loss: 0.0870199267315385\n",
      "Iteration 1636 Loss: 0.0869932661142309\n",
      "Iteration 1637 Loss: 0.08696666637091037\n",
      "Iteration 1638 Loss: 0.08694012736154053\n",
      "Iteration 1639 Loss: 0.08691364894640709\n",
      "Iteration 1640 Loss: 0.08688723098611721\n",
      "Iteration 1641 Loss: 0.08686087334159873\n",
      "Iteration 1642 Loss: 0.08683457587409964\n",
      "Iteration 1643 Loss: 0.08680833844518696\n",
      "Iteration 1644 Loss: 0.08678216091674654\n",
      "Iteration 1645 Loss: 0.08675604315098162\n",
      "Iteration 1646 Loss: 0.08672998501041287\n",
      "Iteration 1647 Loss: 0.08670398635787688\n",
      "Iteration 1648 Loss: 0.08667804705652622\n",
      "Iteration 1649 Loss: 0.08665216696982807\n",
      "Iteration 1650 Loss: 0.08662634596156396\n",
      "Iteration 1651 Loss: 0.08660058389582867\n",
      "Iteration 1652 Loss: 0.08657488063702967\n",
      "Iteration 1653 Loss: 0.08654923604988657\n",
      "Iteration 1654 Loss: 0.08652364999943007\n",
      "Iteration 1655 Loss: 0.08649812235100147\n",
      "Iteration 1656 Loss: 0.08647265297025206\n",
      "Iteration 1657 Loss: 0.086447241723142\n",
      "Iteration 1658 Loss: 0.08642188847594\n",
      "Iteration 1659 Loss: 0.08639659309522245\n",
      "Iteration 1660 Loss: 0.08637135544787264\n",
      "Iteration 1661 Loss: 0.08634617540108033\n",
      "Iteration 1662 Loss: 0.08632105282234066\n",
      "Iteration 1663 Loss: 0.08629598757945381\n",
      "Iteration 1664 Loss: 0.08627097954052396\n",
      "Iteration 1665 Loss: 0.08624602857395888\n",
      "Iteration 1666 Loss: 0.08622113454846918\n",
      "Iteration 1667 Loss: 0.08619629733306737\n",
      "Iteration 1668 Loss: 0.08617151679706747\n",
      "Iteration 1669 Loss: 0.08614679281008417\n",
      "Iteration 1670 Loss: 0.08612212524203217\n",
      "Iteration 1671 Loss: 0.08609751396312536\n",
      "Iteration 1672 Loss: 0.08607295884387647\n",
      "Iteration 1673 Loss: 0.08604845975509606\n",
      "Iteration 1674 Loss: 0.08602401656789176\n",
      "Iteration 1675 Loss: 0.08599962915366802\n",
      "Iteration 1676 Loss: 0.08597529738412506\n",
      "Iteration 1677 Loss: 0.08595102113125826\n",
      "Iteration 1678 Loss: 0.08592680026735762\n",
      "Iteration 1679 Loss: 0.08590263466500694\n",
      "Iteration 1680 Loss: 0.08587852419708322\n",
      "Iteration 1681 Loss: 0.08585446873675577\n",
      "Iteration 1682 Loss: 0.08583046815748598\n",
      "Iteration 1683 Loss: 0.08580652233302624\n",
      "Iteration 1684 Loss: 0.08578263113741944\n",
      "Iteration 1685 Loss: 0.08575879444499834\n",
      "Iteration 1686 Loss: 0.08573501213038477\n",
      "Iteration 1687 Loss: 0.08571128406848903\n",
      "Iteration 1688 Loss: 0.08568761013450929\n",
      "Iteration 1689 Loss: 0.08566399020393085\n",
      "Iteration 1690 Loss: 0.08564042415252546\n",
      "Iteration 1691 Loss: 0.08561691185635081\n",
      "Iteration 1692 Loss: 0.08559345319174963\n",
      "Iteration 1693 Loss: 0.0855700480353493\n",
      "Iteration 1694 Loss: 0.08554669626406093\n",
      "Iteration 1695 Loss: 0.08552339775507886\n",
      "Iteration 1696 Loss: 0.0855001523858802\n",
      "Iteration 1697 Loss: 0.08547696003422367\n",
      "Iteration 1698 Loss: 0.08545382057814949\n",
      "Iteration 1699 Loss: 0.08543073389597824\n",
      "Iteration 1700 Loss: 0.08540769986631074\n",
      "Iteration 1701 Loss: 0.0853847183680269\n",
      "Iteration 1702 Loss: 0.08536178928028557\n",
      "Iteration 1703 Loss: 0.08533891248252337\n",
      "Iteration 1704 Loss: 0.08531608785445444\n",
      "Iteration 1705 Loss: 0.08529331527606969\n",
      "Iteration 1706 Loss: 0.08527059462763618\n",
      "Iteration 1707 Loss: 0.08524792578969641\n",
      "Iteration 1708 Loss: 0.08522530864306763\n",
      "Iteration 1709 Loss: 0.08520274306884157\n",
      "Iteration 1710 Loss: 0.08518022894838347\n",
      "Iteration 1711 Loss: 0.08515776616333122\n",
      "Iteration 1712 Loss: 0.08513535459559554\n",
      "Iteration 1713 Loss: 0.08511299412735848\n",
      "Iteration 1714 Loss: 0.08509068464107336\n",
      "Iteration 1715 Loss: 0.08506842601946392\n",
      "Iteration 1716 Loss: 0.08504621814552367\n",
      "Iteration 1717 Loss: 0.08502406090251535\n",
      "Iteration 1718 Loss: 0.08500195417397047\n",
      "Iteration 1719 Loss: 0.08497989784368827\n",
      "Iteration 1720 Loss: 0.0849578917957355\n",
      "Iteration 1721 Loss: 0.08493593591444562\n",
      "Iteration 1722 Loss: 0.0849140300844183\n",
      "Iteration 1723 Loss: 0.08489217419051846\n",
      "Iteration 1724 Loss: 0.0848703681178763\n",
      "Iteration 1725 Loss: 0.0848486117518861\n",
      "Iteration 1726 Loss: 0.08482690497820575\n",
      "Iteration 1727 Loss: 0.08480524768275652\n",
      "Iteration 1728 Loss: 0.08478363975172194\n",
      "Iteration 1729 Loss: 0.08476208107154737\n",
      "Iteration 1730 Loss: 0.08474057152893968\n",
      "Iteration 1731 Loss: 0.08471911101086618\n",
      "Iteration 1732 Loss: 0.08469769940455432\n",
      "Iteration 1733 Loss: 0.08467633659749114\n",
      "Iteration 1734 Loss: 0.0846550224774224\n",
      "Iteration 1735 Loss: 0.0846337569323522\n",
      "Iteration 1736 Loss: 0.08461253985054232\n",
      "Iteration 1737 Loss: 0.08459137112051171\n",
      "Iteration 1738 Loss: 0.08457025063103574\n",
      "Iteration 1739 Loss: 0.08454917827114562\n",
      "Iteration 1740 Loss: 0.0845281539301281\n",
      "Iteration 1741 Loss: 0.08450717749752448\n",
      "Iteration 1742 Loss: 0.08448624886313019\n",
      "Iteration 1743 Loss: 0.08446536791699441\n",
      "Iteration 1744 Loss: 0.08444453454941925\n",
      "Iteration 1745 Loss: 0.08442374865095899\n",
      "Iteration 1746 Loss: 0.08440301011242\n",
      "Iteration 1747 Loss: 0.08438231882485989\n",
      "Iteration 1748 Loss: 0.0843616746795867\n",
      "Iteration 1749 Loss: 0.0843410775681587\n",
      "Iteration 1750 Loss: 0.08432052738238369\n",
      "Iteration 1751 Loss: 0.0843000240143184\n",
      "Iteration 1752 Loss: 0.08427956735626782\n",
      "Iteration 1753 Loss: 0.08425915730078493\n",
      "Iteration 1754 Loss: 0.08423879374066967\n",
      "Iteration 1755 Loss: 0.08421847656896889\n",
      "Iteration 1756 Loss: 0.08419820567897532\n",
      "Iteration 1757 Loss: 0.08417798096422738\n",
      "Iteration 1758 Loss: 0.08415780231850825\n",
      "Iteration 1759 Loss: 0.08413766963584567\n",
      "Iteration 1760 Loss: 0.08411758281051125\n",
      "Iteration 1761 Loss: 0.08409754173701951\n",
      "Iteration 1762 Loss: 0.08407754631012809\n",
      "Iteration 1763 Loss: 0.08405759642483658\n",
      "Iteration 1764 Loss: 0.08403769197638626\n",
      "Iteration 1765 Loss: 0.08401783286025934\n",
      "Iteration 1766 Loss: 0.08399801897217851\n",
      "Iteration 1767 Loss: 0.08397825020810648\n",
      "Iteration 1768 Loss: 0.08395852646424526\n",
      "Iteration 1769 Loss: 0.08393884763703577\n",
      "Iteration 1770 Loss: 0.08391921362315709\n",
      "Iteration 1771 Loss: 0.08389962431952605\n",
      "Iteration 1772 Loss: 0.08388007962329674\n",
      "Iteration 1773 Loss: 0.08386057943185984\n",
      "Iteration 1774 Loss: 0.08384112364284213\n",
      "Iteration 1775 Loss: 0.08382171215410589\n",
      "Iteration 1776 Loss: 0.08380234486374842\n",
      "Iteration 1777 Loss: 0.08378302167010152\n",
      "Iteration 1778 Loss: 0.08376374247173095\n",
      "Iteration 1779 Loss: 0.08374450716743569\n",
      "Iteration 1780 Loss: 0.08372531565624775\n",
      "Iteration 1781 Loss: 0.08370616783743141\n",
      "Iteration 1782 Loss: 0.08368706361048263\n",
      "Iteration 1783 Loss: 0.08366800287512875\n",
      "Iteration 1784 Loss: 0.08364898553132774\n",
      "Iteration 1785 Loss: 0.08363001147926771\n",
      "Iteration 1786 Loss: 0.08361108061936666\n",
      "Iteration 1787 Loss: 0.08359219285227137\n",
      "Iteration 1788 Loss: 0.08357334807885743\n",
      "Iteration 1789 Loss: 0.08355454620022845\n",
      "Iteration 1790 Loss: 0.08353578711771564\n",
      "Iteration 1791 Loss: 0.08351707073287726\n",
      "Iteration 1792 Loss: 0.0834983969474978\n",
      "Iteration 1793 Loss: 0.08347976566358804\n",
      "Iteration 1794 Loss: 0.08346117678338413\n",
      "Iteration 1795 Loss: 0.08344263020934709\n",
      "Iteration 1796 Loss: 0.08342412584416253\n",
      "Iteration 1797 Loss: 0.08340566359073968\n",
      "Iteration 1798 Loss: 0.08338724335221137\n",
      "Iteration 1799 Loss: 0.08336886503193344\n",
      "Iteration 1800 Loss: 0.0833505285334838\n",
      "Iteration 1801 Loss: 0.08333223376066226\n",
      "Iteration 1802 Loss: 0.08331398061749015\n",
      "Iteration 1803 Loss: 0.08329576900820951\n",
      "Iteration 1804 Loss: 0.08327759883728277\n",
      "Iteration 1805 Loss: 0.08325947000939202\n",
      "Iteration 1806 Loss: 0.08324138242943896\n",
      "Iteration 1807 Loss: 0.08322333600254377\n",
      "Iteration 1808 Loss: 0.08320533063404519\n",
      "Iteration 1809 Loss: 0.08318736622949946\n",
      "Iteration 1810 Loss: 0.08316944269468063\n",
      "Iteration 1811 Loss: 0.08315155993557896\n",
      "Iteration 1812 Loss: 0.0831337178584015\n",
      "Iteration 1813 Loss: 0.08311591636957073\n",
      "Iteration 1814 Loss: 0.08309815537572467\n",
      "Iteration 1815 Loss: 0.08308043478371616\n",
      "Iteration 1816 Loss: 0.08306275450061215\n",
      "Iteration 1817 Loss: 0.08304511443369363\n",
      "Iteration 1818 Loss: 0.08302751449045483\n",
      "Iteration 1819 Loss: 0.08300995457860282\n",
      "Iteration 1820 Loss: 0.08299243460605707\n",
      "Iteration 1821 Loss: 0.08297495448094891\n",
      "Iteration 1822 Loss: 0.08295751411162093\n",
      "Iteration 1823 Loss: 0.0829401134066268\n",
      "Iteration 1824 Loss: 0.08292275227473045\n",
      "Iteration 1825 Loss: 0.08290543062490578\n",
      "Iteration 1826 Loss: 0.08288814836633611\n",
      "Iteration 1827 Loss: 0.08287090540841376\n",
      "Iteration 1828 Loss: 0.08285370166073953\n",
      "Iteration 1829 Loss: 0.0828365370331221\n",
      "Iteration 1830 Loss: 0.08281941143557779\n",
      "Iteration 1831 Loss: 0.08280232477832995\n",
      "Iteration 1832 Loss: 0.0827852769718084\n",
      "Iteration 1833 Loss: 0.08276826792664918\n",
      "Iteration 1834 Loss: 0.08275129755369384\n",
      "Iteration 1835 Loss: 0.08273436576398914\n",
      "Iteration 1836 Loss: 0.08271747246878638\n",
      "Iteration 1837 Loss: 0.0827006175795413\n",
      "Iteration 1838 Loss: 0.08268380100791307\n",
      "Iteration 1839 Loss: 0.0826670226657644\n",
      "Iteration 1840 Loss: 0.08265028246516055\n",
      "Iteration 1841 Loss: 0.08263358031836927\n",
      "Iteration 1842 Loss: 0.08261691613786018\n",
      "Iteration 1843 Loss: 0.08260028983630413\n",
      "Iteration 1844 Loss: 0.08258370132657308\n",
      "Iteration 1845 Loss: 0.08256715052173932\n",
      "Iteration 1846 Loss: 0.08255063733507523\n",
      "Iteration 1847 Loss: 0.08253416168005275\n",
      "Iteration 1848 Loss: 0.08251772347034278\n",
      "Iteration 1849 Loss: 0.08250132261981509\n",
      "Iteration 1850 Loss: 0.08248495904253736\n",
      "Iteration 1851 Loss: 0.08246863265277522\n",
      "Iteration 1852 Loss: 0.08245234336499137\n",
      "Iteration 1853 Loss: 0.0824360910938454\n",
      "Iteration 1854 Loss: 0.08241987575419335\n",
      "Iteration 1855 Loss: 0.08240369726108698\n",
      "Iteration 1856 Loss: 0.08238755552977357\n",
      "Iteration 1857 Loss: 0.08237145047569552\n",
      "Iteration 1858 Loss: 0.08235538201448962\n",
      "Iteration 1859 Loss: 0.0823393500619869\n",
      "Iteration 1860 Loss: 0.08232335453421193\n",
      "Iteration 1861 Loss: 0.08230739534738266\n",
      "Iteration 1862 Loss: 0.08229147241790961\n",
      "Iteration 1863 Loss: 0.08227558566239572\n",
      "Iteration 1864 Loss: 0.08225973499763589\n",
      "Iteration 1865 Loss: 0.08224392034061634\n",
      "Iteration 1866 Loss: 0.08222814160851437\n",
      "Iteration 1867 Loss: 0.08221239871869782\n",
      "Iteration 1868 Loss: 0.0821966915887247\n",
      "Iteration 1869 Loss: 0.08218102013634253\n",
      "Iteration 1870 Loss: 0.08216538427948832\n",
      "Iteration 1871 Loss: 0.08214978393628772\n",
      "Iteration 1872 Loss: 0.08213421902505487\n",
      "Iteration 1873 Loss: 0.0821186894642918\n",
      "Iteration 1874 Loss: 0.08210319517268809\n",
      "Iteration 1875 Loss: 0.08208773606912041\n",
      "Iteration 1876 Loss: 0.08207231207265199\n",
      "Iteration 1877 Loss: 0.08205692310253236\n",
      "Iteration 1878 Loss: 0.08204156907819689\n",
      "Iteration 1879 Loss: 0.08202624991926617\n",
      "Iteration 1880 Loss: 0.08201096554554592\n",
      "Iteration 1881 Loss: 0.08199571587702625\n",
      "Iteration 1882 Loss: 0.08198050083388135\n",
      "Iteration 1883 Loss: 0.08196532033646922\n",
      "Iteration 1884 Loss: 0.08195017430533087\n",
      "Iteration 1885 Loss: 0.08193506266119037\n",
      "Iteration 1886 Loss: 0.08191998532495406\n",
      "Iteration 1887 Loss: 0.0819049422177103\n",
      "Iteration 1888 Loss: 0.08188993326072906\n",
      "Iteration 1889 Loss: 0.08187495837546122\n",
      "Iteration 1890 Loss: 0.08186001748353873\n",
      "Iteration 1891 Loss: 0.08184511050677365\n",
      "Iteration 1892 Loss: 0.08183023736715797\n",
      "Iteration 1893 Loss: 0.08181539798686326\n",
      "Iteration 1894 Loss: 0.08180059228824008\n",
      "Iteration 1895 Loss: 0.08178582019381753\n",
      "Iteration 1896 Loss: 0.08177108162630327\n",
      "Iteration 1897 Loss: 0.08175637650858239\n",
      "Iteration 1898 Loss: 0.08174170476371775\n",
      "Iteration 1899 Loss: 0.08172706631494916\n",
      "Iteration 1900 Loss: 0.08171246108569288\n",
      "Iteration 1901 Loss: 0.08169788899954156\n",
      "Iteration 1902 Loss: 0.08168334998026361\n",
      "Iteration 1903 Loss: 0.08166884395180278\n",
      "Iteration 1904 Loss: 0.08165437083827787\n",
      "Iteration 1905 Loss: 0.08163993056398222\n",
      "Iteration 1906 Loss: 0.08162552305338344\n",
      "Iteration 1907 Loss: 0.08161114823112281\n",
      "Iteration 1908 Loss: 0.08159680602201508\n",
      "Iteration 1909 Loss: 0.08158249635104806\n",
      "Iteration 1910 Loss: 0.08156821914338204\n",
      "Iteration 1911 Loss: 0.08155397432434956\n",
      "Iteration 1912 Loss: 0.08153976181945481\n",
      "Iteration 1913 Loss: 0.08152558155437353\n",
      "Iteration 1914 Loss: 0.0815114334549526\n",
      "Iteration 1915 Loss: 0.08149731744720926\n",
      "Iteration 1916 Loss: 0.08148323345733115\n",
      "Iteration 1917 Loss: 0.08146918141167551\n",
      "Iteration 1918 Loss: 0.08145516123676935\n",
      "Iteration 1919 Loss: 0.08144117285930849\n",
      "Iteration 1920 Loss: 0.08142721620615746\n",
      "Iteration 1921 Loss: 0.08141329120434918\n",
      "Iteration 1922 Loss: 0.08139939778108427\n",
      "Iteration 1923 Loss: 0.08138553586373098\n",
      "Iteration 1924 Loss: 0.08137170537982456\n",
      "Iteration 1925 Loss: 0.08135790625706726\n",
      "Iteration 1926 Loss: 0.0813441384233273\n",
      "Iteration 1927 Loss: 0.08133040180663917\n",
      "Iteration 1928 Loss: 0.08131669633520279\n",
      "Iteration 1929 Loss: 0.08130302193738324\n",
      "Iteration 1930 Loss: 0.08128937854171057\n",
      "Iteration 1931 Loss: 0.08127576607687928\n",
      "Iteration 1932 Loss: 0.08126218447174768\n",
      "Iteration 1933 Loss: 0.08124863365533808\n",
      "Iteration 1934 Loss: 0.08123511355683587\n",
      "Iteration 1935 Loss: 0.08122162410558945\n",
      "Iteration 1936 Loss: 0.08120816523110988\n",
      "Iteration 1937 Loss: 0.08119473686307027\n",
      "Iteration 1938 Loss: 0.08118133893130561\n",
      "Iteration 1939 Loss: 0.08116797136581222\n",
      "Iteration 1940 Loss: 0.08115463409674765\n",
      "Iteration 1941 Loss: 0.08114132705443011\n",
      "Iteration 1942 Loss: 0.08112805016933804\n",
      "Iteration 1943 Loss: 0.08111480337210994\n",
      "Iteration 1944 Loss: 0.08110158659354376\n",
      "Iteration 1945 Loss: 0.08108839976459682\n",
      "Iteration 1946 Loss: 0.08107524281638531\n",
      "Iteration 1947 Loss: 0.08106211568018383\n",
      "Iteration 1948 Loss: 0.0810490182874251\n",
      "Iteration 1949 Loss: 0.08103595056969962\n",
      "Iteration 1950 Loss: 0.08102291245875537\n",
      "Iteration 1951 Loss: 0.08100990388649726\n",
      "Iteration 1952 Loss: 0.08099692478498687\n",
      "Iteration 1953 Loss: 0.08098397508644213\n",
      "Iteration 1954 Loss: 0.08097105472323707\n",
      "Iteration 1955 Loss: 0.08095816362790106\n",
      "Iteration 1956 Loss: 0.08094530173311891\n",
      "Iteration 1957 Loss: 0.08093246897173027\n",
      "Iteration 1958 Loss: 0.08091966527672917\n",
      "Iteration 1959 Loss: 0.08090689058126396\n",
      "Iteration 1960 Loss: 0.08089414481863685\n",
      "Iteration 1961 Loss: 0.08088142792230335\n",
      "Iteration 1962 Loss: 0.08086873982587224\n",
      "Iteration 1963 Loss: 0.08085608046310482\n",
      "Iteration 1964 Loss: 0.08084344976791509\n",
      "Iteration 1965 Loss: 0.08083084767436899\n",
      "Iteration 1966 Loss: 0.08081827411668407\n",
      "Iteration 1967 Loss: 0.08080572902922932\n",
      "Iteration 1968 Loss: 0.08079321234652477\n",
      "Iteration 1969 Loss: 0.08078072400324098\n",
      "Iteration 1970 Loss: 0.08076826393419896\n",
      "Iteration 1971 Loss: 0.08075583207436972\n",
      "Iteration 1972 Loss: 0.08074342835887363\n",
      "Iteration 1973 Loss: 0.08073105272298058\n",
      "Iteration 1974 Loss: 0.08071870510210945\n",
      "Iteration 1975 Loss: 0.08070638543182744\n",
      "Iteration 1976 Loss: 0.08069409364785021\n",
      "Iteration 1977 Loss: 0.08068182968604129\n",
      "Iteration 1978 Loss: 0.08066959348241173\n",
      "Iteration 1979 Loss: 0.08065738497311985\n",
      "Iteration 1980 Loss: 0.08064520409447082\n",
      "Iteration 1981 Loss: 0.08063305078291647\n",
      "Iteration 1982 Loss: 0.08062092497505464\n",
      "Iteration 1983 Loss: 0.08060882660762929\n",
      "Iteration 1984 Loss: 0.08059675561752978\n",
      "Iteration 1985 Loss: 0.08058471194179066\n",
      "Iteration 1986 Loss: 0.08057269551759143\n",
      "Iteration 1987 Loss: 0.08056070628225606\n",
      "Iteration 1988 Loss: 0.08054874417325278\n",
      "Iteration 1989 Loss: 0.08053680912819379\n",
      "Iteration 1990 Loss: 0.08052490108483462\n",
      "Iteration 1991 Loss: 0.08051301998107416\n",
      "Iteration 1992 Loss: 0.08050116575495406\n",
      "Iteration 1993 Loss: 0.08048933834465885\n",
      "Iteration 1994 Loss: 0.08047753768851476\n",
      "Iteration 1995 Loss: 0.08046576372499041\n",
      "Iteration 1996 Loss: 0.08045401639269574\n",
      "Iteration 1997 Loss: 0.080442295630382\n",
      "Iteration 1998 Loss: 0.08043060137694141\n",
      "Iteration 1999 Loss: 0.08041893357140673\n",
      "Iteration 2000 Loss: 0.0804072921529509\n",
      "Iteration 2001 Loss: 0.08039567706088704\n",
      "Iteration 2002 Loss: 0.08038408823466778\n",
      "Iteration 2003 Loss: 0.08037252561388508\n",
      "Iteration 2004 Loss: 0.08036098913826979\n",
      "Iteration 2005 Loss: 0.08034947874769163\n",
      "Iteration 2006 Loss: 0.08033799438215845\n",
      "Iteration 2007 Loss: 0.08032653598181627\n",
      "Iteration 2008 Loss: 0.08031510348694881\n",
      "Iteration 2009 Loss: 0.08030369683797725\n",
      "Iteration 2010 Loss: 0.08029231597545965\n",
      "Iteration 2011 Loss: 0.08028096084009104\n",
      "Iteration 2012 Loss: 0.08026963137270274\n",
      "Iteration 2013 Loss: 0.0802583275142624\n",
      "Iteration 2014 Loss: 0.08024704920587336\n",
      "Iteration 2015 Loss: 0.08023579638877446\n",
      "Iteration 2016 Loss: 0.08022456900433975\n",
      "Iteration 2017 Loss: 0.08021336699407824\n",
      "Iteration 2018 Loss: 0.08020219029963352\n",
      "Iteration 2019 Loss: 0.08019103886278338\n",
      "Iteration 2020 Loss: 0.08017991262543953\n",
      "Iteration 2021 Loss: 0.08016881152964753\n",
      "Iteration 2022 Loss: 0.08015773551758604\n",
      "Iteration 2023 Loss: 0.08014668453156691\n",
      "Iteration 2024 Loss: 0.08013565851403467\n",
      "Iteration 2025 Loss: 0.08012465740756633\n",
      "Iteration 2026 Loss: 0.08011368115487097\n",
      "Iteration 2027 Loss: 0.08010272969878948\n",
      "Iteration 2028 Loss: 0.08009180298229432\n",
      "Iteration 2029 Loss: 0.080080900948489\n",
      "Iteration 2030 Loss: 0.08007002354060833\n",
      "Iteration 2031 Loss: 0.08005917070201725\n",
      "Iteration 2032 Loss: 0.08004834237621121\n",
      "Iteration 2033 Loss: 0.08003753850681579\n",
      "Iteration 2034 Loss: 0.08002675903758602\n",
      "Iteration 2035 Loss: 0.08001600391240674\n",
      "Iteration 2036 Loss: 0.08000527307529144\n",
      "Iteration 2037 Loss: 0.0799945664703827\n",
      "Iteration 2038 Loss: 0.07998388404195164\n",
      "Iteration 2039 Loss: 0.07997322573439737\n",
      "Iteration 2040 Loss: 0.07996259149224724\n",
      "Iteration 2041 Loss: 0.07995198126015593\n",
      "Iteration 2042 Loss: 0.07994139498290576\n",
      "Iteration 2043 Loss: 0.07993083260540582\n",
      "Iteration 2044 Loss: 0.07992029407269208\n",
      "Iteration 2045 Loss: 0.07990977932992707\n",
      "Iteration 2046 Loss: 0.07989928832239931\n",
      "Iteration 2047 Loss: 0.07988882099552337\n",
      "Iteration 2048 Loss: 0.0798783772948393\n",
      "Iteration 2049 Loss: 0.07986795716601243\n",
      "Iteration 2050 Loss: 0.07985756055483322\n",
      "Iteration 2051 Loss: 0.07984718740721677\n",
      "Iteration 2052 Loss: 0.07983683766920271\n",
      "Iteration 2053 Loss: 0.07982651128695467\n",
      "Iteration 2054 Loss: 0.0798162082067604\n",
      "Iteration 2055 Loss: 0.07980592837503088\n",
      "Iteration 2056 Loss: 0.07979567173830074\n",
      "Iteration 2057 Loss: 0.0797854382432274\n",
      "Iteration 2058 Loss: 0.07977522783659112\n",
      "Iteration 2059 Loss: 0.07976504046529455\n",
      "Iteration 2060 Loss: 0.0797548760763626\n",
      "Iteration 2061 Loss: 0.07974473461694187\n",
      "Iteration 2062 Loss: 0.07973461603430089\n",
      "Iteration 2063 Loss: 0.07972452027582926\n",
      "Iteration 2064 Loss: 0.07971444728903757\n",
      "Iteration 2065 Loss: 0.07970439702155742\n",
      "Iteration 2066 Loss: 0.07969436942114072\n",
      "Iteration 2067 Loss: 0.07968436443565982\n",
      "Iteration 2068 Loss: 0.07967438201310675\n",
      "Iteration 2069 Loss: 0.07966442210159323\n",
      "Iteration 2070 Loss: 0.07965448464935058\n",
      "Iteration 2071 Loss: 0.07964456960472895\n",
      "Iteration 2072 Loss: 0.07963467691619756\n",
      "Iteration 2073 Loss: 0.07962480653234416\n",
      "Iteration 2074 Loss: 0.0796149584018746\n",
      "Iteration 2075 Loss: 0.07960513247361298\n",
      "Iteration 2076 Loss: 0.07959532869650103\n",
      "Iteration 2077 Loss: 0.07958554701959793\n",
      "Iteration 2078 Loss: 0.07957578739208022\n",
      "Iteration 2079 Loss: 0.07956604976324119\n",
      "Iteration 2080 Loss: 0.07955633408249095\n",
      "Iteration 2081 Loss: 0.07954664029935588\n",
      "Iteration 2082 Loss: 0.0795369683634786\n",
      "Iteration 2083 Loss: 0.0795273182246175\n",
      "Iteration 2084 Loss: 0.07951768983264669\n",
      "Iteration 2085 Loss: 0.07950808313755545\n",
      "Iteration 2086 Loss: 0.07949849808944826\n",
      "Iteration 2087 Loss: 0.07948893463854449\n",
      "Iteration 2088 Loss: 0.07947939273517765\n",
      "Iteration 2089 Loss: 0.07946987232979597\n",
      "Iteration 2090 Loss: 0.0794603733729615\n",
      "Iteration 2091 Loss: 0.07945089581534999\n",
      "Iteration 2092 Loss: 0.07944143960775077\n",
      "Iteration 2093 Loss: 0.07943200470106637\n",
      "Iteration 2094 Loss: 0.07942259104631226\n",
      "Iteration 2095 Loss: 0.07941319859461667\n",
      "Iteration 2096 Loss: 0.07940382729722022\n",
      "Iteration 2097 Loss: 0.07939447710547572\n",
      "Iteration 2098 Loss: 0.07938514797084799\n",
      "Iteration 2099 Loss: 0.07937583984491334\n",
      "Iteration 2100 Loss: 0.07936655267935976\n",
      "Iteration 2101 Loss: 0.07935728642598615\n",
      "Iteration 2102 Loss: 0.07934804103670248\n",
      "Iteration 2103 Loss: 0.07933881646352928\n",
      "Iteration 2104 Loss: 0.07932961265859748\n",
      "Iteration 2105 Loss: 0.07932042957414817\n",
      "Iteration 2106 Loss: 0.07931126716253241\n",
      "Iteration 2107 Loss: 0.07930212537621067\n",
      "Iteration 2108 Loss: 0.07929300416775306\n",
      "Iteration 2109 Loss: 0.07928390348983858\n",
      "Iteration 2110 Loss: 0.07927482329525533\n",
      "Iteration 2111 Loss: 0.07926576353689985\n",
      "Iteration 2112 Loss: 0.07925672416777718\n",
      "Iteration 2113 Loss: 0.07924770514100046\n",
      "Iteration 2114 Loss: 0.07923870640979062\n",
      "Iteration 2115 Loss: 0.0792297279274764\n",
      "Iteration 2116 Loss: 0.07922076964749374\n",
      "Iteration 2117 Loss: 0.07921183152338587\n",
      "Iteration 2118 Loss: 0.07920291350880279\n",
      "Iteration 2119 Loss: 0.0791940155575012\n",
      "Iteration 2120 Loss: 0.07918513762334418\n",
      "Iteration 2121 Loss: 0.079176279660301\n",
      "Iteration 2122 Loss: 0.07916744162244675\n",
      "Iteration 2123 Loss: 0.07915862346396227\n",
      "Iteration 2124 Loss: 0.0791498251391339\n",
      "Iteration 2125 Loss: 0.07914104660235297\n",
      "Iteration 2126 Loss: 0.07913228780811579\n",
      "Iteration 2127 Loss: 0.07912354871102352\n",
      "Iteration 2128 Loss: 0.07911482926578159\n",
      "Iteration 2129 Loss: 0.07910612942719981\n",
      "Iteration 2130 Loss: 0.07909744915019187\n",
      "Iteration 2131 Loss: 0.0790887883897751\n",
      "Iteration 2132 Loss: 0.07908014710107057\n",
      "Iteration 2133 Loss: 0.07907152523930236\n",
      "Iteration 2134 Loss: 0.07906292275979784\n",
      "Iteration 2135 Loss: 0.07905433961798693\n",
      "Iteration 2136 Loss: 0.07904577576940211\n",
      "Iteration 2137 Loss: 0.07903723116967833\n",
      "Iteration 2138 Loss: 0.07902870577455248\n",
      "Iteration 2139 Loss: 0.07902019953986343\n",
      "Iteration 2140 Loss: 0.07901171242155151\n",
      "Iteration 2141 Loss: 0.07900324437565845\n",
      "Iteration 2142 Loss: 0.07899479535832722\n",
      "Iteration 2143 Loss: 0.07898636532580164\n",
      "Iteration 2144 Loss: 0.07897795423442613\n",
      "Iteration 2145 Loss: 0.07896956204064569\n",
      "Iteration 2146 Loss: 0.07896118870100546\n",
      "Iteration 2147 Loss: 0.07895283417215053\n",
      "Iteration 2148 Loss: 0.07894449841082586\n",
      "Iteration 2149 Loss: 0.07893618137387577\n",
      "Iteration 2150 Loss: 0.07892788301824404\n",
      "Iteration 2151 Loss: 0.07891960330097339\n",
      "Iteration 2152 Loss: 0.07891134217920547\n",
      "Iteration 2153 Loss: 0.07890309961018029\n",
      "Iteration 2154 Loss: 0.07889487555123675\n",
      "Iteration 2155 Loss: 0.07888666995981149\n",
      "Iteration 2156 Loss: 0.07887848279343913\n",
      "Iteration 2157 Loss: 0.07887031400975207\n",
      "Iteration 2158 Loss: 0.0788621635664803\n",
      "Iteration 2159 Loss: 0.07885403142145084\n",
      "Iteration 2160 Loss: 0.07884591753258785\n",
      "Iteration 2161 Loss: 0.07883782185791242\n",
      "Iteration 2162 Loss: 0.07882974435554191\n",
      "Iteration 2163 Loss: 0.07882168498369041\n",
      "Iteration 2164 Loss: 0.07881364370066797\n",
      "Iteration 2165 Loss: 0.07880562046488046\n",
      "Iteration 2166 Loss: 0.07879761523482973\n",
      "Iteration 2167 Loss: 0.07878962796911285\n",
      "Iteration 2168 Loss: 0.07878165862642231\n",
      "Iteration 2169 Loss: 0.07877370716554548\n",
      "Iteration 2170 Loss: 0.0787657735453648\n",
      "Iteration 2171 Loss: 0.07875785772485709\n",
      "Iteration 2172 Loss: 0.07874995966309366\n",
      "Iteration 2173 Loss: 0.07874207931923996\n",
      "Iteration 2174 Loss: 0.0787342166525554\n",
      "Iteration 2175 Loss: 0.07872637162239311\n",
      "Iteration 2176 Loss: 0.07871854418819982\n",
      "Iteration 2177 Loss: 0.0787107343095155\n",
      "Iteration 2178 Loss: 0.07870294194597317\n",
      "Iteration 2179 Loss: 0.07869516705729879\n",
      "Iteration 2180 Loss: 0.07868740960331104\n",
      "Iteration 2181 Loss: 0.07867966954392094\n",
      "Iteration 2182 Loss: 0.07867194683913185\n",
      "Iteration 2183 Loss: 0.07866424144903897\n",
      "Iteration 2184 Loss: 0.07865655333382951\n",
      "Iteration 2185 Loss: 0.07864888245378228\n",
      "Iteration 2186 Loss: 0.07864122876926735\n",
      "Iteration 2187 Loss: 0.07863359224074601\n",
      "Iteration 2188 Loss: 0.07862597282877062\n",
      "Iteration 2189 Loss: 0.07861837049398426\n",
      "Iteration 2190 Loss: 0.07861078519712039\n",
      "Iteration 2191 Loss: 0.07860321689900314\n",
      "Iteration 2192 Loss: 0.07859566556054647\n",
      "Iteration 2193 Loss: 0.0785881311427545\n",
      "Iteration 2194 Loss: 0.07858061360672085\n",
      "Iteration 2195 Loss: 0.07857311291362892\n",
      "Iteration 2196 Loss: 0.07856562902475121\n",
      "Iteration 2197 Loss: 0.07855816190144946\n",
      "Iteration 2198 Loss: 0.07855071150517415\n",
      "Iteration 2199 Loss: 0.07854327779746459\n",
      "Iteration 2200 Loss: 0.07853586073994857\n",
      "Iteration 2201 Loss: 0.07852846029434216\n",
      "Iteration 2202 Loss: 0.07852107642244946\n",
      "Iteration 2203 Loss: 0.07851370908616254\n",
      "Iteration 2204 Loss: 0.07850635824746108\n",
      "Iteration 2205 Loss: 0.07849902386841215\n",
      "Iteration 2206 Loss: 0.07849170591117036\n",
      "Iteration 2207 Loss: 0.07848440433797725\n",
      "Iteration 2208 Loss: 0.0784771191111611\n",
      "Iteration 2209 Loss: 0.07846985019313713\n",
      "Iteration 2210 Loss: 0.07846259754640683\n",
      "Iteration 2211 Loss: 0.07845536113355808\n",
      "Iteration 2212 Loss: 0.07844814091726471\n",
      "Iteration 2213 Loss: 0.07844093686028664\n",
      "Iteration 2214 Loss: 0.07843374892546921\n",
      "Iteration 2215 Loss: 0.07842657707574352\n",
      "Iteration 2216 Loss: 0.07841942127412588\n",
      "Iteration 2217 Loss: 0.07841228148371751\n",
      "Iteration 2218 Loss: 0.07840515766770464\n",
      "Iteration 2219 Loss: 0.07839804978935835\n",
      "Iteration 2220 Loss: 0.07839095781203409\n",
      "Iteration 2221 Loss: 0.07838388169917156\n",
      "Iteration 2222 Loss: 0.07837682141429461\n",
      "Iteration 2223 Loss: 0.0783697769210112\n",
      "Iteration 2224 Loss: 0.07836274818301278\n",
      "Iteration 2225 Loss: 0.07835573516407454\n",
      "Iteration 2226 Loss: 0.07834873782805477\n",
      "Iteration 2227 Loss: 0.07834175613889517\n",
      "Iteration 2228 Loss: 0.07833479006062026\n",
      "Iteration 2229 Loss: 0.07832783955733744\n",
      "Iteration 2230 Loss: 0.07832090459323654\n",
      "Iteration 2231 Loss: 0.07831398513258993\n",
      "Iteration 2232 Loss: 0.07830708113975203\n",
      "Iteration 2233 Loss: 0.07830019257915946\n",
      "Iteration 2234 Loss: 0.0782933194153305\n",
      "Iteration 2235 Loss: 0.07828646161286529\n",
      "Iteration 2236 Loss: 0.07827961913644506\n",
      "Iteration 2237 Loss: 0.07827279195083262\n",
      "Iteration 2238 Loss: 0.07826598002087176\n",
      "Iteration 2239 Loss: 0.0782591833114871\n",
      "Iteration 2240 Loss: 0.07825240178768403\n",
      "Iteration 2241 Loss: 0.07824563541454842\n",
      "Iteration 2242 Loss: 0.07823888415724645\n",
      "Iteration 2243 Loss: 0.07823214798102456\n",
      "Iteration 2244 Loss: 0.07822542685120908\n",
      "Iteration 2245 Loss: 0.078218720733206\n",
      "Iteration 2246 Loss: 0.07821202959250109\n",
      "Iteration 2247 Loss: 0.07820535339465946\n",
      "Iteration 2248 Loss: 0.07819869210532544\n",
      "Iteration 2249 Loss: 0.07819204569022245\n",
      "Iteration 2250 Loss: 0.0781854141151527\n",
      "Iteration 2251 Loss: 0.07817879734599709\n",
      "Iteration 2252 Loss: 0.07817219534871508\n",
      "Iteration 2253 Loss: 0.07816560808934445\n",
      "Iteration 2254 Loss: 0.07815903553400105\n",
      "Iteration 2255 Loss: 0.07815247764887882\n",
      "Iteration 2256 Loss: 0.07814593440024929\n",
      "Iteration 2257 Loss: 0.07813940575446184\n",
      "Iteration 2258 Loss: 0.07813289167794299\n",
      "Iteration 2259 Loss: 0.07812639213719681\n",
      "Iteration 2260 Loss: 0.07811990709880409\n",
      "Iteration 2261 Loss: 0.07811343652942285\n",
      "Iteration 2262 Loss: 0.07810698039578766\n",
      "Iteration 2263 Loss: 0.0781005386647097\n",
      "Iteration 2264 Loss: 0.07809411130307631\n",
      "Iteration 2265 Loss: 0.07808769827785127\n",
      "Iteration 2266 Loss: 0.0780812995560742\n",
      "Iteration 2267 Loss: 0.07807491510486075\n",
      "Iteration 2268 Loss: 0.07806854489140196\n",
      "Iteration 2269 Loss: 0.07806218888296462\n",
      "Iteration 2270 Loss: 0.07805584704689059\n",
      "Iteration 2271 Loss: 0.07804951935059713\n",
      "Iteration 2272 Loss: 0.07804320576157624\n",
      "Iteration 2273 Loss: 0.07803690624739491\n",
      "Iteration 2274 Loss: 0.07803062077569449\n",
      "Iteration 2275 Loss: 0.07802434931419108\n",
      "Iteration 2276 Loss: 0.07801809183067487\n",
      "Iteration 2277 Loss: 0.07801184829301021\n",
      "Iteration 2278 Loss: 0.07800561866913541\n",
      "Iteration 2279 Loss: 0.07799940292706253\n",
      "Iteration 2280 Loss: 0.07799320103487728\n",
      "Iteration 2281 Loss: 0.0779870129607386\n",
      "Iteration 2282 Loss: 0.07798083867287894\n",
      "Iteration 2283 Loss: 0.07797467813960376\n",
      "Iteration 2284 Loss: 0.07796853132929138\n",
      "Iteration 2285 Loss: 0.07796239821039297\n",
      "Iteration 2286 Loss: 0.07795627875143214\n",
      "Iteration 2287 Loss: 0.07795017292100508\n",
      "Iteration 2288 Loss: 0.07794408068778022\n",
      "Iteration 2289 Loss: 0.07793800202049797\n",
      "Iteration 2290 Loss: 0.07793193688797076\n",
      "Iteration 2291 Loss: 0.07792588525908273\n",
      "Iteration 2292 Loss: 0.07791984710278957\n",
      "Iteration 2293 Loss: 0.07791382238811856\n",
      "Iteration 2294 Loss: 0.07790781108416807\n",
      "Iteration 2295 Loss: 0.07790181316010761\n",
      "Iteration 2296 Loss: 0.07789582858517778\n",
      "Iteration 2297 Loss: 0.07788985732868965\n",
      "Iteration 2298 Loss: 0.07788389936002518\n",
      "Iteration 2299 Loss: 0.07787795464863667\n",
      "Iteration 2300 Loss: 0.07787202316404665\n",
      "Iteration 2301 Loss: 0.07786610487584787\n",
      "Iteration 2302 Loss: 0.07786019975370297\n",
      "Iteration 2303 Loss: 0.07785430776734446\n",
      "Iteration 2304 Loss: 0.07784842888657442\n",
      "Iteration 2305 Loss: 0.07784256308126444\n",
      "Iteration 2306 Loss: 0.07783671032135542\n",
      "Iteration 2307 Loss: 0.07783087057685752\n",
      "Iteration 2308 Loss: 0.07782504381784967\n",
      "Iteration 2309 Loss: 0.0778192300144799\n",
      "Iteration 2310 Loss: 0.07781342913696473\n",
      "Iteration 2311 Loss: 0.07780764115558934\n",
      "Iteration 2312 Loss: 0.07780186604070727\n",
      "Iteration 2313 Loss: 0.07779610376274013\n",
      "Iteration 2314 Loss: 0.07779035429217775\n",
      "Iteration 2315 Loss: 0.07778461759957771\n",
      "Iteration 2316 Loss: 0.07777889365556553\n",
      "Iteration 2317 Loss: 0.07777318243083405\n",
      "Iteration 2318 Loss: 0.07776748389614385\n",
      "Iteration 2319 Loss: 0.0777617980223225\n",
      "Iteration 2320 Loss: 0.07775612478026486\n",
      "Iteration 2321 Loss: 0.07775046414093267\n",
      "Iteration 2322 Loss: 0.07774481607535459\n",
      "Iteration 2323 Loss: 0.07773918055462584\n",
      "Iteration 2324 Loss: 0.07773355754990816\n",
      "Iteration 2325 Loss: 0.07772794703242962\n",
      "Iteration 2326 Loss: 0.07772234897348455\n",
      "Iteration 2327 Loss: 0.0777167633444333\n",
      "Iteration 2328 Loss: 0.0777111901167022\n",
      "Iteration 2329 Loss: 0.07770562926178311\n",
      "Iteration 2330 Loss: 0.07770008075123373\n",
      "Iteration 2331 Loss: 0.07769454455667694\n",
      "Iteration 2332 Loss: 0.07768902064980118\n",
      "Iteration 2333 Loss: 0.07768350900235985\n",
      "Iteration 2334 Loss: 0.07767800958617137\n",
      "Iteration 2335 Loss: 0.07767252237311903\n",
      "Iteration 2336 Loss: 0.07766704733515084\n",
      "Iteration 2337 Loss: 0.07766158444427929\n",
      "Iteration 2338 Loss: 0.07765613367258135\n",
      "Iteration 2339 Loss: 0.07765069499219804\n",
      "Iteration 2340 Loss: 0.07764526837533478\n",
      "Iteration 2341 Loss: 0.07763985379426065\n",
      "Iteration 2342 Loss: 0.07763445122130871\n",
      "Iteration 2343 Loss: 0.07762906062887567\n",
      "Iteration 2344 Loss: 0.0776236819894216\n",
      "Iteration 2345 Loss: 0.07761831527547011\n",
      "Iteration 2346 Loss: 0.07761296045960804\n",
      "Iteration 2347 Loss: 0.07760761751448501\n",
      "Iteration 2348 Loss: 0.077602286412814\n",
      "Iteration 2349 Loss: 0.07759696712737037\n",
      "Iteration 2350 Loss: 0.07759165963099243\n",
      "Iteration 2351 Loss: 0.07758636389658084\n",
      "Iteration 2352 Loss: 0.07758107989709857\n",
      "Iteration 2353 Loss: 0.07757580760557091\n",
      "Iteration 2354 Loss: 0.07757054699508513\n",
      "Iteration 2355 Loss: 0.07756529803879048\n",
      "Iteration 2356 Loss: 0.07756006070989788\n",
      "Iteration 2357 Loss: 0.07755483498168\n",
      "Iteration 2358 Loss: 0.07754962082747105\n",
      "Iteration 2359 Loss: 0.07754441822066642\n",
      "Iteration 2360 Loss: 0.07753922713472283\n",
      "Iteration 2361 Loss: 0.07753404754315811\n",
      "Iteration 2362 Loss: 0.07752887941955086\n",
      "Iteration 2363 Loss: 0.0775237227375406\n",
      "Iteration 2364 Loss: 0.07751857747082748\n",
      "Iteration 2365 Loss: 0.07751344359317221\n",
      "Iteration 2366 Loss: 0.07750832107839563\n",
      "Iteration 2367 Loss: 0.0775032099003792\n",
      "Iteration 2368 Loss: 0.07749811003306419\n",
      "Iteration 2369 Loss: 0.07749302145045185\n",
      "Iteration 2370 Loss: 0.07748794412660338\n",
      "Iteration 2371 Loss: 0.07748287803563955\n",
      "Iteration 2372 Loss: 0.07747782315174075\n",
      "Iteration 2373 Loss: 0.07747277944914663\n",
      "Iteration 2374 Loss: 0.07746774690215635\n",
      "Iteration 2375 Loss: 0.077462725485128\n",
      "Iteration 2376 Loss: 0.0774577151724787\n",
      "Iteration 2377 Loss: 0.07745271593868455\n",
      "Iteration 2378 Loss: 0.07744772775828018\n",
      "Iteration 2379 Loss: 0.077442750605859\n",
      "Iteration 2380 Loss: 0.07743778445607281\n",
      "Iteration 2381 Loss: 0.07743282928363171\n",
      "Iteration 2382 Loss: 0.07742788506330392\n",
      "Iteration 2383 Loss: 0.07742295176991588\n",
      "Iteration 2384 Loss: 0.07741802937835185\n",
      "Iteration 2385 Loss: 0.07741311786355384\n",
      "Iteration 2386 Loss: 0.07740821720052168\n",
      "Iteration 2387 Loss: 0.07740332736431244\n",
      "Iteration 2388 Loss: 0.07739844833004084\n",
      "Iteration 2389 Loss: 0.07739358007287875\n",
      "Iteration 2390 Loss: 0.0773887225680551\n",
      "Iteration 2391 Loss: 0.07738387579085601\n",
      "Iteration 2392 Loss: 0.07737903971662421\n",
      "Iteration 2393 Loss: 0.07737421432075942\n",
      "Iteration 2394 Loss: 0.07736939957871772\n",
      "Iteration 2395 Loss: 0.07736459546601179\n",
      "Iteration 2396 Loss: 0.07735980195821066\n",
      "Iteration 2397 Loss: 0.07735501903093962\n",
      "Iteration 2398 Loss: 0.07735024665987981\n",
      "Iteration 2399 Loss: 0.07734548482076861\n",
      "Iteration 2400 Loss: 0.07734073348939899\n",
      "Iteration 2401 Loss: 0.07733599264161975\n",
      "Iteration 2402 Loss: 0.07733126225333528\n",
      "Iteration 2403 Loss: 0.07732654230050531\n",
      "Iteration 2404 Loss: 0.07732183275914493\n",
      "Iteration 2405 Loss: 0.07731713360532444\n",
      "Iteration 2406 Loss: 0.0773124448151691\n",
      "Iteration 2407 Loss: 0.07730776636485921\n",
      "Iteration 2408 Loss: 0.07730309823062993\n",
      "Iteration 2409 Loss: 0.07729844038877083\n",
      "Iteration 2410 Loss: 0.07729379281562632\n",
      "Iteration 2411 Loss: 0.0772891554875951\n",
      "Iteration 2412 Loss: 0.0772845283811302\n",
      "Iteration 2413 Loss: 0.07727991147273879\n",
      "Iteration 2414 Loss: 0.07727530473898216\n",
      "Iteration 2415 Loss: 0.07727070815647549\n",
      "Iteration 2416 Loss: 0.07726612170188779\n",
      "Iteration 2417 Loss: 0.07726154535194166\n",
      "Iteration 2418 Loss: 0.07725697908341342\n",
      "Iteration 2419 Loss: 0.07725242287313265\n",
      "Iteration 2420 Loss: 0.07724787669798237\n",
      "Iteration 2421 Loss: 0.07724334053489869\n",
      "Iteration 2422 Loss: 0.07723881436087089\n",
      "Iteration 2423 Loss: 0.0772342981529411\n",
      "Iteration 2424 Loss: 0.07722979188820431\n",
      "Iteration 2425 Loss: 0.07722529554380819\n",
      "Iteration 2426 Loss: 0.07722080909695304\n",
      "Iteration 2427 Loss: 0.07721633252489161\n",
      "Iteration 2428 Loss: 0.07721186580492889\n",
      "Iteration 2429 Loss: 0.07720740891442221\n",
      "Iteration 2430 Loss: 0.07720296183078093\n",
      "Iteration 2431 Loss: 0.07719852453146639\n",
      "Iteration 2432 Loss: 0.07719409699399181\n",
      "Iteration 2433 Loss: 0.07718967919592216\n",
      "Iteration 2434 Loss: 0.07718527111487394\n",
      "Iteration 2435 Loss: 0.07718087272851534\n",
      "Iteration 2436 Loss: 0.07717648401456564\n",
      "Iteration 2437 Loss: 0.0771721049507957\n",
      "Iteration 2438 Loss: 0.07716773551502727\n",
      "Iteration 2439 Loss: 0.07716337568513326\n",
      "Iteration 2440 Loss: 0.07715902543903753\n",
      "Iteration 2441 Loss: 0.07715468475471457\n",
      "Iteration 2442 Loss: 0.07715035361018971\n",
      "Iteration 2443 Loss: 0.0771460319835388\n",
      "Iteration 2444 Loss: 0.07714171985288795\n",
      "Iteration 2445 Loss: 0.0771374171964139\n",
      "Iteration 2446 Loss: 0.07713312399234348\n",
      "Iteration 2447 Loss: 0.07712884021895346\n",
      "Iteration 2448 Loss: 0.07712456585457082\n",
      "Iteration 2449 Loss: 0.07712030087757224\n",
      "Iteration 2450 Loss: 0.07711604526638428\n",
      "Iteration 2451 Loss: 0.07711179899948305\n",
      "Iteration 2452 Loss: 0.07710756205539422\n",
      "Iteration 2453 Loss: 0.07710333441269275\n",
      "Iteration 2454 Loss: 0.07709911605000315\n",
      "Iteration 2455 Loss: 0.07709490694599887\n",
      "Iteration 2456 Loss: 0.07709070707940258\n",
      "Iteration 2457 Loss: 0.07708651642898574\n",
      "Iteration 2458 Loss: 0.07708233497356891\n",
      "Iteration 2459 Loss: 0.07707816269202108\n",
      "Iteration 2460 Loss: 0.07707399956326007\n",
      "Iteration 2461 Loss: 0.07706984556625218\n",
      "Iteration 2462 Loss: 0.07706570068001198\n",
      "Iteration 2463 Loss: 0.07706156488360245\n",
      "Iteration 2464 Loss: 0.07705743815613468\n",
      "Iteration 2465 Loss: 0.07705332047676791\n",
      "Iteration 2466 Loss: 0.07704921182470918\n",
      "Iteration 2467 Loss: 0.07704511217921353\n",
      "Iteration 2468 Loss: 0.07704102151958356\n",
      "Iteration 2469 Loss: 0.0770369398251696\n",
      "Iteration 2470 Loss: 0.0770328670753696\n",
      "Iteration 2471 Loss: 0.0770288032496286\n",
      "Iteration 2472 Loss: 0.0770247483274393\n",
      "Iteration 2473 Loss: 0.07702070228834121\n",
      "Iteration 2474 Loss: 0.07701666511192125\n",
      "Iteration 2475 Loss: 0.07701263677781316\n",
      "Iteration 2476 Loss: 0.07700861726569741\n",
      "Iteration 2477 Loss: 0.0770046065553015\n",
      "Iteration 2478 Loss: 0.07700060462639938\n",
      "Iteration 2479 Loss: 0.07699661145881151\n",
      "Iteration 2480 Loss: 0.07699262703240502\n",
      "Iteration 2481 Loss: 0.07698865132709301\n",
      "Iteration 2482 Loss: 0.07698468432283509\n",
      "Iteration 2483 Loss: 0.07698072599963682\n",
      "Iteration 2484 Loss: 0.07697677633754978\n",
      "Iteration 2485 Loss: 0.07697283531667162\n",
      "Iteration 2486 Loss: 0.0769689029171455\n",
      "Iteration 2487 Loss: 0.07696497911916048\n",
      "Iteration 2488 Loss: 0.07696106390295107\n",
      "Iteration 2489 Loss: 0.07695715724879738\n",
      "Iteration 2490 Loss: 0.07695325913702476\n",
      "Iteration 2491 Loss: 0.07694936954800394\n",
      "Iteration 2492 Loss: 0.07694548846215069\n",
      "Iteration 2493 Loss: 0.07694161585992604\n",
      "Iteration 2494 Loss: 0.07693775172183574\n",
      "Iteration 2495 Loss: 0.07693389602843062\n",
      "Iteration 2496 Loss: 0.07693004876030607\n",
      "Iteration 2497 Loss: 0.07692620989810213\n",
      "Iteration 2498 Loss: 0.07692237942250357\n",
      "Iteration 2499 Loss: 0.07691855731423938\n",
      "Iteration 2500 Loss: 0.07691474355408312\n",
      "Iteration 2501 Loss: 0.07691093812285237\n",
      "Iteration 2502 Loss: 0.07690714100140887\n",
      "Iteration 2503 Loss: 0.07690335217065858\n",
      "Iteration 2504 Loss: 0.07689957161155118\n",
      "Iteration 2505 Loss: 0.07689579930508036\n",
      "Iteration 2506 Loss: 0.07689203523228336\n",
      "Iteration 2507 Loss: 0.07688827937424111\n",
      "Iteration 2508 Loss: 0.07688453171207822\n",
      "Iteration 2509 Loss: 0.07688079222696255\n",
      "Iteration 2510 Loss: 0.07687706090010524\n",
      "Iteration 2511 Loss: 0.0768733377127609\n",
      "Iteration 2512 Loss: 0.07686962264622707\n",
      "Iteration 2513 Loss: 0.07686591568184438\n",
      "Iteration 2514 Loss: 0.07686221680099643\n",
      "Iteration 2515 Loss: 0.07685852598510952\n",
      "Iteration 2516 Loss: 0.07685484321565292\n",
      "Iteration 2517 Loss: 0.07685116847413824\n",
      "Iteration 2518 Loss: 0.07684750174211993\n",
      "Iteration 2519 Loss: 0.07684384300119465\n",
      "Iteration 2520 Loss: 0.07684019223300152\n",
      "Iteration 2521 Loss: 0.07683654941922183\n",
      "Iteration 2522 Loss: 0.07683291454157914\n",
      "Iteration 2523 Loss: 0.07682928758183889\n",
      "Iteration 2524 Loss: 0.07682566852180853\n",
      "Iteration 2525 Loss: 0.07682205734333755\n",
      "Iteration 2526 Loss: 0.07681845402831691\n",
      "Iteration 2527 Loss: 0.07681485855867945\n",
      "Iteration 2528 Loss: 0.07681127091639942\n",
      "Iteration 2529 Loss: 0.07680769108349272\n",
      "Iteration 2530 Loss: 0.07680411904201653\n",
      "Iteration 2531 Loss: 0.07680055477406926\n",
      "Iteration 2532 Loss: 0.0767969982617906\n",
      "Iteration 2533 Loss: 0.0767934494873613\n",
      "Iteration 2534 Loss: 0.07678990843300304\n",
      "Iteration 2535 Loss: 0.07678637508097859\n",
      "Iteration 2536 Loss: 0.07678284941359133\n",
      "Iteration 2537 Loss: 0.07677933141318544\n",
      "Iteration 2538 Loss: 0.07677582106214575\n",
      "Iteration 2539 Loss: 0.07677231834289755\n",
      "Iteration 2540 Loss: 0.07676882323790665\n",
      "Iteration 2541 Loss: 0.07676533572967911\n",
      "Iteration 2542 Loss: 0.07676185580076134\n",
      "Iteration 2543 Loss: 0.07675838343373981\n",
      "Iteration 2544 Loss: 0.07675491861124105\n",
      "Iteration 2545 Loss: 0.07675146131593172\n",
      "Iteration 2546 Loss: 0.0767480115305182\n",
      "Iteration 2547 Loss: 0.07674456923774665\n",
      "Iteration 2548 Loss: 0.07674113442040312\n",
      "Iteration 2549 Loss: 0.07673770706131304\n",
      "Iteration 2550 Loss: 0.07673428714334145\n",
      "Iteration 2551 Loss: 0.0767308746493928\n",
      "Iteration 2552 Loss: 0.07672746956241099\n",
      "Iteration 2553 Loss: 0.0767240718653789\n",
      "Iteration 2554 Loss: 0.07672068154131885\n",
      "Iteration 2555 Loss: 0.07671729857329203\n",
      "Iteration 2556 Loss: 0.0767139229443987\n",
      "Iteration 2557 Loss: 0.07671055463777791\n",
      "Iteration 2558 Loss: 0.07670719363660766\n",
      "Iteration 2559 Loss: 0.07670383992410444\n",
      "Iteration 2560 Loss: 0.07670049348352354\n",
      "Iteration 2561 Loss: 0.07669715429815872\n",
      "Iteration 2562 Loss: 0.0766938223513421\n",
      "Iteration 2563 Loss: 0.07669049762644427\n",
      "Iteration 2564 Loss: 0.07668718010687395\n",
      "Iteration 2565 Loss: 0.07668386977607822\n",
      "Iteration 2566 Loss: 0.07668056661754205\n",
      "Iteration 2567 Loss: 0.07667727061478855\n",
      "Iteration 2568 Loss: 0.07667398175137859\n",
      "Iteration 2569 Loss: 0.07667070001091102\n",
      "Iteration 2570 Loss: 0.07666742537702229\n",
      "Iteration 2571 Loss: 0.07666415783338668\n",
      "Iteration 2572 Loss: 0.07666089736371587\n",
      "Iteration 2573 Loss: 0.07665764395175897\n",
      "Iteration 2574 Loss: 0.0766543975813027\n",
      "Iteration 2575 Loss: 0.07665115823617089\n",
      "Iteration 2576 Loss: 0.07664792590022469\n",
      "Iteration 2577 Loss: 0.07664470055736236\n",
      "Iteration 2578 Loss: 0.07664148219151909\n",
      "Iteration 2579 Loss: 0.07663827078666727\n",
      "Iteration 2580 Loss: 0.07663506632681596\n",
      "Iteration 2581 Loss: 0.0766318687960111\n",
      "Iteration 2582 Loss: 0.07662867817833534\n",
      "Iteration 2583 Loss: 0.07662549445790783\n",
      "Iteration 2584 Loss: 0.07662231761888452\n",
      "Iteration 2585 Loss: 0.07661914764545755\n",
      "Iteration 2586 Loss: 0.07661598452185554\n",
      "Iteration 2587 Loss: 0.07661282823234342\n",
      "Iteration 2588 Loss: 0.07660967876122227\n",
      "Iteration 2589 Loss: 0.0766065360928293\n",
      "Iteration 2590 Loss: 0.07660340021153789\n",
      "Iteration 2591 Loss: 0.07660027110175713\n",
      "Iteration 2592 Loss: 0.07659714874793214\n",
      "Iteration 2593 Loss: 0.07659403313454374\n",
      "Iteration 2594 Loss: 0.07659092424610862\n",
      "Iteration 2595 Loss: 0.07658782206717896\n",
      "Iteration 2596 Loss: 0.07658472658234242\n",
      "Iteration 2597 Loss: 0.0765816377762222\n",
      "Iteration 2598 Loss: 0.07657855563347703\n",
      "Iteration 2599 Loss: 0.07657548013880074\n",
      "Iteration 2600 Loss: 0.07657241127692234\n",
      "Iteration 2601 Loss: 0.07656934903260608\n",
      "Iteration 2602 Loss: 0.07656629339065132\n",
      "Iteration 2603 Loss: 0.07656324433589227\n",
      "Iteration 2604 Loss: 0.07656020185319816\n",
      "Iteration 2605 Loss: 0.07655716592747283\n",
      "Iteration 2606 Loss: 0.07655413654365503\n",
      "Iteration 2607 Loss: 0.07655111368671817\n",
      "Iteration 2608 Loss: 0.07654809734167015\n",
      "Iteration 2609 Loss: 0.07654508749355331\n",
      "Iteration 2610 Loss: 0.07654208412744458\n",
      "Iteration 2611 Loss: 0.07653908722845505\n",
      "Iteration 2612 Loss: 0.07653609678173022\n",
      "Iteration 2613 Loss: 0.0765331127724496\n",
      "Iteration 2614 Loss: 0.07653013518582695\n",
      "Iteration 2615 Loss: 0.07652716400711002\n",
      "Iteration 2616 Loss: 0.07652419922158041\n",
      "Iteration 2617 Loss: 0.07652124081455361\n",
      "Iteration 2618 Loss: 0.07651828877137902\n",
      "Iteration 2619 Loss: 0.07651534307743954\n",
      "Iteration 2620 Loss: 0.07651240371815202\n",
      "Iteration 2621 Loss: 0.07650947067896643\n",
      "Iteration 2622 Loss: 0.07650654394536657\n",
      "Iteration 2623 Loss: 0.07650362350286952\n",
      "Iteration 2624 Loss: 0.07650070933702566\n",
      "Iteration 2625 Loss: 0.07649780143341856\n",
      "Iteration 2626 Loss: 0.07649489977766515\n",
      "Iteration 2627 Loss: 0.07649200435541528\n",
      "Iteration 2628 Loss: 0.07648911515235184\n",
      "Iteration 2629 Loss: 0.07648623215419072\n",
      "Iteration 2630 Loss: 0.07648335534668058\n",
      "Iteration 2631 Loss: 0.07648048471560309\n",
      "Iteration 2632 Loss: 0.07647762024677235\n",
      "Iteration 2633 Loss: 0.07647476192603524\n",
      "Iteration 2634 Loss: 0.0764719097392712\n",
      "Iteration 2635 Loss: 0.07646906367239222\n",
      "Iteration 2636 Loss: 0.0764662237113425\n",
      "Iteration 2637 Loss: 0.0764633898420988\n",
      "Iteration 2638 Loss: 0.07646056205066996\n",
      "Iteration 2639 Loss: 0.07645774032309728\n",
      "Iteration 2640 Loss: 0.07645492464545385\n",
      "Iteration 2641 Loss: 0.07645211500384493\n",
      "Iteration 2642 Loss: 0.0764493113844079\n",
      "Iteration 2643 Loss: 0.07644651377331182\n",
      "Iteration 2644 Loss: 0.07644372215675763\n",
      "Iteration 2645 Loss: 0.07644093652097814\n",
      "Iteration 2646 Loss: 0.07643815685223762\n",
      "Iteration 2647 Loss: 0.07643538313683222\n",
      "Iteration 2648 Loss: 0.07643261536108935\n",
      "Iteration 2649 Loss: 0.07642985351136804\n",
      "Iteration 2650 Loss: 0.07642709757405855\n",
      "Iteration 2651 Loss: 0.07642434753558272\n",
      "Iteration 2652 Loss: 0.07642160338239334\n",
      "Iteration 2653 Loss: 0.07641886510097459\n",
      "Iteration 2654 Loss: 0.0764161326778416\n",
      "Iteration 2655 Loss: 0.07641340609954057\n",
      "Iteration 2656 Loss: 0.07641068535264868\n",
      "Iteration 2657 Loss: 0.07640797042377398\n",
      "Iteration 2658 Loss: 0.07640526129955533\n",
      "Iteration 2659 Loss: 0.07640255796666236\n",
      "Iteration 2660 Loss: 0.07639986041179525\n",
      "Iteration 2661 Loss: 0.07639716862168491\n",
      "Iteration 2662 Loss: 0.07639448258309281\n",
      "Iteration 2663 Loss: 0.07639180228281073\n",
      "Iteration 2664 Loss: 0.0763891277076609\n",
      "Iteration 2665 Loss: 0.07638645884449588\n",
      "Iteration 2666 Loss: 0.07638379568019868\n",
      "Iteration 2667 Loss: 0.07638113820168199\n",
      "Iteration 2668 Loss: 0.07637848639588911\n",
      "Iteration 2669 Loss: 0.07637584024979313\n",
      "Iteration 2670 Loss: 0.07637319975039715\n",
      "Iteration 2671 Loss: 0.07637056488473419\n",
      "Iteration 2672 Loss: 0.07636793563986713\n",
      "Iteration 2673 Loss: 0.07636531200288857\n",
      "Iteration 2674 Loss: 0.0763626939609208\n",
      "Iteration 2675 Loss: 0.07636008150111581\n",
      "Iteration 2676 Loss: 0.07635747461065513\n",
      "Iteration 2677 Loss: 0.07635487327674964\n",
      "Iteration 2678 Loss: 0.07635227748663989\n",
      "Iteration 2679 Loss: 0.07634968722759553\n",
      "Iteration 2680 Loss: 0.07634710248691572\n",
      "Iteration 2681 Loss: 0.0763445232519287\n",
      "Iteration 2682 Loss: 0.07634194950999199\n",
      "Iteration 2683 Loss: 0.07633938124849196\n",
      "Iteration 2684 Loss: 0.07633681845484434\n",
      "Iteration 2685 Loss: 0.07633426111649355\n",
      "Iteration 2686 Loss: 0.076331709220913\n",
      "Iteration 2687 Loss: 0.07632916275560482\n",
      "Iteration 2688 Loss: 0.07632662170810008\n",
      "Iteration 2689 Loss: 0.07632408606595839\n",
      "Iteration 2690 Loss: 0.07632155581676807\n",
      "Iteration 2691 Loss: 0.07631903094814585\n",
      "Iteration 2692 Loss: 0.07631651144773718\n",
      "Iteration 2693 Loss: 0.07631399730321572\n",
      "Iteration 2694 Loss: 0.07631148850228363\n",
      "Iteration 2695 Loss: 0.07630898503267121\n",
      "Iteration 2696 Loss: 0.07630648688213733\n",
      "Iteration 2697 Loss: 0.07630399403846858\n",
      "Iteration 2698 Loss: 0.0763015064894798\n",
      "Iteration 2699 Loss: 0.07629902422301413\n",
      "Iteration 2700 Loss: 0.07629654722694233\n",
      "Iteration 2701 Loss: 0.07629407548916327\n",
      "Iteration 2702 Loss: 0.07629160899760357\n",
      "Iteration 2703 Loss: 0.07628914774021767\n",
      "Iteration 2704 Loss: 0.07628669170498772\n",
      "Iteration 2705 Loss: 0.07628424087992344\n",
      "Iteration 2706 Loss: 0.07628179525306233\n",
      "Iteration 2707 Loss: 0.07627935481246917\n",
      "Iteration 2708 Loss: 0.07627691954623643\n",
      "Iteration 2709 Loss: 0.07627448944248376\n",
      "Iteration 2710 Loss: 0.07627206448935835\n",
      "Iteration 2711 Loss: 0.07626964467503447\n",
      "Iteration 2712 Loss: 0.0762672299877138\n",
      "Iteration 2713 Loss: 0.07626482041562492\n",
      "Iteration 2714 Loss: 0.07626241594702372\n",
      "Iteration 2715 Loss: 0.07626001657019299\n",
      "Iteration 2716 Loss: 0.07625762227344257\n",
      "Iteration 2717 Loss: 0.07625523304510903\n",
      "Iteration 2718 Loss: 0.07625284887355595\n",
      "Iteration 2719 Loss: 0.07625046974717348\n",
      "Iteration 2720 Loss: 0.07624809565437876\n",
      "Iteration 2721 Loss: 0.07624572658361543\n",
      "Iteration 2722 Loss: 0.07624336252335366\n",
      "Iteration 2723 Loss: 0.07624100346209017\n",
      "Iteration 2724 Loss: 0.07623864938834826\n",
      "Iteration 2725 Loss: 0.07623630029067752\n",
      "Iteration 2726 Loss: 0.0762339561576539\n",
      "Iteration 2727 Loss: 0.07623161697787968\n",
      "Iteration 2728 Loss: 0.07622928273998328\n",
      "Iteration 2729 Loss: 0.07622695343261955\n",
      "Iteration 2730 Loss: 0.07622462904446899\n",
      "Iteration 2731 Loss: 0.07622230956423844\n",
      "Iteration 2732 Loss: 0.07621999498066075\n",
      "Iteration 2733 Loss: 0.0762176852824946\n",
      "Iteration 2734 Loss: 0.07621538045852443\n",
      "Iteration 2735 Loss: 0.07621308049756079\n",
      "Iteration 2736 Loss: 0.07621078538843962\n",
      "Iteration 2737 Loss: 0.07620849512002277\n",
      "Iteration 2738 Loss: 0.07620620968119768\n",
      "Iteration 2739 Loss: 0.07620392906087729\n",
      "Iteration 2740 Loss: 0.07620165324800014\n",
      "Iteration 2741 Loss: 0.07619938223153012\n",
      "Iteration 2742 Loss: 0.07619711600045653\n",
      "Iteration 2743 Loss: 0.07619485454379418\n",
      "Iteration 2744 Loss: 0.0761925978505829\n",
      "Iteration 2745 Loss: 0.07619034590988778\n",
      "Iteration 2746 Loss: 0.07618809871079932\n",
      "Iteration 2747 Loss: 0.07618585624243276\n",
      "Iteration 2748 Loss: 0.07618361849392868\n",
      "Iteration 2749 Loss: 0.07618138545445245\n",
      "Iteration 2750 Loss: 0.07617915711319445\n",
      "Iteration 2751 Loss: 0.07617693345936993\n",
      "Iteration 2752 Loss: 0.07617471448221895\n",
      "Iteration 2753 Loss: 0.07617250017100632\n",
      "Iteration 2754 Loss: 0.07617029051502153\n",
      "Iteration 2755 Loss: 0.07616808550357876\n",
      "Iteration 2756 Loss: 0.0761658851260167\n",
      "Iteration 2757 Loss: 0.07616368937169864\n",
      "Iteration 2758 Loss: 0.07616149823001227\n",
      "Iteration 2759 Loss: 0.07615931169036982\n",
      "Iteration 2760 Loss: 0.07615712974220779\n",
      "Iteration 2761 Loss: 0.0761549523749869\n",
      "Iteration 2762 Loss: 0.07615277957819233\n",
      "Iteration 2763 Loss: 0.07615061134133327\n",
      "Iteration 2764 Loss: 0.07614844765394314\n",
      "Iteration 2765 Loss: 0.07614628850557939\n",
      "Iteration 2766 Loss: 0.07614413388582356\n",
      "Iteration 2767 Loss: 0.07614198378428108\n",
      "Iteration 2768 Loss: 0.0761398381905813\n",
      "Iteration 2769 Loss: 0.07613769709437757\n",
      "Iteration 2770 Loss: 0.07613556048534682\n",
      "Iteration 2771 Loss: 0.07613342835318995\n",
      "Iteration 2772 Loss: 0.07613130068763135\n",
      "Iteration 2773 Loss: 0.07612917747841925\n",
      "Iteration 2774 Loss: 0.0761270587153254\n",
      "Iteration 2775 Loss: 0.07612494438814493\n",
      "Iteration 2776 Loss: 0.07612283448669661\n",
      "Iteration 2777 Loss: 0.07612072900082266\n",
      "Iteration 2778 Loss: 0.0761186279203886\n",
      "Iteration 2779 Loss: 0.07611653123528321\n",
      "Iteration 2780 Loss: 0.07611443893541865\n",
      "Iteration 2781 Loss: 0.07611235101073019\n",
      "Iteration 2782 Loss: 0.07611026745117641\n",
      "Iteration 2783 Loss: 0.07610818824673878\n",
      "Iteration 2784 Loss: 0.07610611338742194\n",
      "Iteration 2785 Loss: 0.07610404286325353\n",
      "Iteration 2786 Loss: 0.07610197666428417\n",
      "Iteration 2787 Loss: 0.07609991478058722\n",
      "Iteration 2788 Loss: 0.07609785720225908\n",
      "Iteration 2789 Loss: 0.0760958039194187\n",
      "Iteration 2790 Loss: 0.07609375492220809\n",
      "Iteration 2791 Loss: 0.07609171020079154\n",
      "Iteration 2792 Loss: 0.07608966974535629\n",
      "Iteration 2793 Loss: 0.07608763354611206\n",
      "Iteration 2794 Loss: 0.07608560159329095\n",
      "Iteration 2795 Loss: 0.07608357387714774\n",
      "Iteration 2796 Loss: 0.0760815503879596\n",
      "Iteration 2797 Loss: 0.07607953111602599\n",
      "Iteration 2798 Loss: 0.07607751605166868\n",
      "Iteration 2799 Loss: 0.07607550518523178\n",
      "Iteration 2800 Loss: 0.07607349850708149\n",
      "Iteration 2801 Loss: 0.07607149600760642\n",
      "Iteration 2802 Loss: 0.0760694976772171\n",
      "Iteration 2803 Loss: 0.07606750350634603\n",
      "Iteration 2804 Loss: 0.07606551348544803\n",
      "Iteration 2805 Loss: 0.07606352760499957\n",
      "Iteration 2806 Loss: 0.07606154585549926\n",
      "Iteration 2807 Loss: 0.0760595682274674\n",
      "Iteration 2808 Loss: 0.07605759471144621\n",
      "Iteration 2809 Loss: 0.07605562529799963\n",
      "Iteration 2810 Loss: 0.07605365997771339\n",
      "Iteration 2811 Loss: 0.07605169874119469\n",
      "Iteration 2812 Loss: 0.07604974157907247\n",
      "Iteration 2813 Loss: 0.07604778848199734\n",
      "Iteration 2814 Loss: 0.07604583944064111\n",
      "Iteration 2815 Loss: 0.0760438944456974\n",
      "Iteration 2816 Loss: 0.07604195348788093\n",
      "Iteration 2817 Loss: 0.07604001655792805\n",
      "Iteration 2818 Loss: 0.07603808364659626\n",
      "Iteration 2819 Loss: 0.07603615474466437\n",
      "Iteration 2820 Loss: 0.07603422984293244\n",
      "Iteration 2821 Loss: 0.07603230893222164\n",
      "Iteration 2822 Loss: 0.07603039200337423\n",
      "Iteration 2823 Loss: 0.07602847904725363\n",
      "Iteration 2824 Loss: 0.07602657005474421\n",
      "Iteration 2825 Loss: 0.07602466501675141\n",
      "Iteration 2826 Loss: 0.07602276392420144\n",
      "Iteration 2827 Loss: 0.07602086676804151\n",
      "Iteration 2828 Loss: 0.07601897353923967\n",
      "Iteration 2829 Loss: 0.07601708422878459\n",
      "Iteration 2830 Loss: 0.07601519882768595\n",
      "Iteration 2831 Loss: 0.07601331732697372\n",
      "Iteration 2832 Loss: 0.07601143971769893\n",
      "Iteration 2833 Loss: 0.07600956599093288\n",
      "Iteration 2834 Loss: 0.07600769613776755\n",
      "Iteration 2835 Loss: 0.07600583014931549\n",
      "Iteration 2836 Loss: 0.07600396801670947\n",
      "Iteration 2837 Loss: 0.07600210973110295\n",
      "Iteration 2838 Loss: 0.07600025528366947\n",
      "Iteration 2839 Loss: 0.07599840466560304\n",
      "Iteration 2840 Loss: 0.0759965578681179\n",
      "Iteration 2841 Loss: 0.0759947148824485\n",
      "Iteration 2842 Loss: 0.0759928756998494\n",
      "Iteration 2843 Loss: 0.0759910403115954\n",
      "Iteration 2844 Loss: 0.07598920870898122\n",
      "Iteration 2845 Loss: 0.07598738088332173\n",
      "Iteration 2846 Loss: 0.07598555682595177\n",
      "Iteration 2847 Loss: 0.07598373652822606\n",
      "Iteration 2848 Loss: 0.07598191998151929\n",
      "Iteration 2849 Loss: 0.0759801071772259\n",
      "Iteration 2850 Loss: 0.07597829810676011\n",
      "Iteration 2851 Loss: 0.07597649276155607\n",
      "Iteration 2852 Loss: 0.07597469113306747\n",
      "Iteration 2853 Loss: 0.07597289321276778\n",
      "Iteration 2854 Loss: 0.07597109899214992\n",
      "Iteration 2855 Loss: 0.07596930846272655\n",
      "Iteration 2856 Loss: 0.07596752161602974\n",
      "Iteration 2857 Loss: 0.07596573844361115\n",
      "Iteration 2858 Loss: 0.07596395893704178\n",
      "Iteration 2859 Loss: 0.07596218308791215\n",
      "Iteration 2860 Loss: 0.07596041088783197\n",
      "Iteration 2861 Loss: 0.07595864232843033\n",
      "Iteration 2862 Loss: 0.07595687740135552\n",
      "Iteration 2863 Loss: 0.07595511609827527\n",
      "Iteration 2864 Loss: 0.07595335841087619\n",
      "Iteration 2865 Loss: 0.07595160433086415\n",
      "Iteration 2866 Loss: 0.07594985384996404\n",
      "Iteration 2867 Loss: 0.07594810695991991\n",
      "Iteration 2868 Loss: 0.07594636365249471\n",
      "Iteration 2869 Loss: 0.07594462391947039\n",
      "Iteration 2870 Loss: 0.07594288775264771\n",
      "Iteration 2871 Loss: 0.07594115514384644\n",
      "Iteration 2872 Loss: 0.07593942608490505\n",
      "Iteration 2873 Loss: 0.07593770056768089\n",
      "Iteration 2874 Loss: 0.07593597858404988\n",
      "Iteration 2875 Loss: 0.0759342601259068\n",
      "Iteration 2876 Loss: 0.07593254518516504\n",
      "Iteration 2877 Loss: 0.07593083375375653\n",
      "Iteration 2878 Loss: 0.07592912582363179\n",
      "Iteration 2879 Loss: 0.07592742138675994\n",
      "Iteration 2880 Loss: 0.07592572043512841\n",
      "Iteration 2881 Loss: 0.07592402296074327\n",
      "Iteration 2882 Loss: 0.07592232895562873\n",
      "Iteration 2883 Loss: 0.0759206384118276\n",
      "Iteration 2884 Loss: 0.07591895132140089\n",
      "Iteration 2885 Loss: 0.07591726767642785\n",
      "Iteration 2886 Loss: 0.07591558746900602\n",
      "Iteration 2887 Loss: 0.07591391069125099\n",
      "Iteration 2888 Loss: 0.0759122373352966\n",
      "Iteration 2889 Loss: 0.07591056739329491\n",
      "Iteration 2890 Loss: 0.07590890085741568\n",
      "Iteration 2891 Loss: 0.07590723771984702\n",
      "Iteration 2892 Loss: 0.07590557797279489\n",
      "Iteration 2893 Loss: 0.07590392160848311\n",
      "Iteration 2894 Loss: 0.07590226861915354\n",
      "Iteration 2895 Loss: 0.0759006189970658\n",
      "Iteration 2896 Loss: 0.0758989727344973\n",
      "Iteration 2897 Loss: 0.07589732982374325\n",
      "Iteration 2898 Loss: 0.07589569025711661\n",
      "Iteration 2899 Loss: 0.07589405402694793\n",
      "Iteration 2900 Loss: 0.07589242112558561\n",
      "Iteration 2901 Loss: 0.07589079154539538\n",
      "Iteration 2902 Loss: 0.07588916527876077\n",
      "Iteration 2903 Loss: 0.07588754231808269\n",
      "Iteration 2904 Loss: 0.07588592265577954\n",
      "Iteration 2905 Loss: 0.07588430628428725\n",
      "Iteration 2906 Loss: 0.0758826931960591\n",
      "Iteration 2907 Loss: 0.07588108338356572\n",
      "Iteration 2908 Loss: 0.0758794768392951\n",
      "Iteration 2909 Loss: 0.0758778735557525\n",
      "Iteration 2910 Loss: 0.07587627352546035\n",
      "Iteration 2911 Loss: 0.07587467674095842\n",
      "Iteration 2912 Loss: 0.07587308319480358\n",
      "Iteration 2913 Loss: 0.07587149287956974\n",
      "Iteration 2914 Loss: 0.075869905787848\n",
      "Iteration 2915 Loss: 0.07586832191224649\n",
      "Iteration 2916 Loss: 0.07586674124539038\n",
      "Iteration 2917 Loss: 0.07586516377992167\n",
      "Iteration 2918 Loss: 0.0758635895084994\n",
      "Iteration 2919 Loss: 0.07586201842379955\n",
      "Iteration 2920 Loss: 0.07586045051851482\n",
      "Iteration 2921 Loss: 0.07585888578535474\n",
      "Iteration 2922 Loss: 0.07585732421704572\n",
      "Iteration 2923 Loss: 0.07585576580633083\n",
      "Iteration 2924 Loss: 0.07585421054596986\n",
      "Iteration 2925 Loss: 0.07585265842873914\n",
      "Iteration 2926 Loss: 0.07585110944743186\n",
      "Iteration 2927 Loss: 0.07584956359485746\n",
      "Iteration 2928 Loss: 0.07584802086384232\n",
      "Iteration 2929 Loss: 0.07584648124722901\n",
      "Iteration 2930 Loss: 0.07584494473787669\n",
      "Iteration 2931 Loss: 0.0758434113286609\n",
      "Iteration 2932 Loss: 0.07584188101247367\n",
      "Iteration 2933 Loss: 0.07584035378222327\n",
      "Iteration 2934 Loss: 0.07583882963083438\n",
      "Iteration 2935 Loss: 0.07583730855124782\n",
      "Iteration 2936 Loss: 0.07583579053642084\n",
      "Iteration 2937 Loss: 0.07583427557932676\n",
      "Iteration 2938 Loss: 0.0758327636729551\n",
      "Iteration 2939 Loss: 0.07583125481031146\n",
      "Iteration 2940 Loss: 0.07582974898441774\n",
      "Iteration 2941 Loss: 0.07582824618831144\n",
      "Iteration 2942 Loss: 0.07582674641504668\n",
      "Iteration 2943 Loss: 0.07582524965769313\n",
      "Iteration 2944 Loss: 0.07582375590933652\n",
      "Iteration 2945 Loss: 0.07582226516307844\n",
      "Iteration 2946 Loss: 0.07582077741203656\n",
      "Iteration 2947 Loss: 0.07581929264934406\n",
      "Iteration 2948 Loss: 0.07581781086815018\n",
      "Iteration 2949 Loss: 0.07581633206161989\n",
      "Iteration 2950 Loss: 0.07581485622293371\n",
      "Iteration 2951 Loss: 0.075813383345288\n",
      "Iteration 2952 Loss: 0.07581191342189476\n",
      "Iteration 2953 Loss: 0.07581044644598159\n",
      "Iteration 2954 Loss: 0.07580898241079162\n",
      "Iteration 2955 Loss: 0.07580752130958364\n",
      "Iteration 2956 Loss: 0.07580606313563182\n",
      "Iteration 2957 Loss: 0.07580460788222594\n",
      "Iteration 2958 Loss: 0.07580315554267103\n",
      "Iteration 2959 Loss: 0.07580170611028765\n",
      "Iteration 2960 Loss: 0.07580025957841181\n",
      "Iteration 2961 Loss: 0.0757988159403947\n",
      "Iteration 2962 Loss: 0.07579737518960285\n",
      "Iteration 2963 Loss: 0.0757959373194181\n",
      "Iteration 2964 Loss: 0.07579450232323744\n",
      "Iteration 2965 Loss: 0.07579307019447301\n",
      "Iteration 2966 Loss: 0.07579164092655233\n",
      "Iteration 2967 Loss: 0.07579021451291786\n",
      "Iteration 2968 Loss: 0.07578879094702715\n",
      "Iteration 2969 Loss: 0.07578737022235282\n",
      "Iteration 2970 Loss: 0.07578595233238257\n",
      "Iteration 2971 Loss: 0.07578453727061898\n",
      "Iteration 2972 Loss: 0.07578312503057968\n",
      "Iteration 2973 Loss: 0.07578171560579722\n",
      "Iteration 2974 Loss: 0.07578030898981884\n",
      "Iteration 2975 Loss: 0.07577890517620685\n",
      "Iteration 2976 Loss: 0.07577750415853832\n",
      "Iteration 2977 Loss: 0.07577610593040487\n",
      "Iteration 2978 Loss: 0.07577471048541329\n",
      "Iteration 2979 Loss: 0.0757733178171847\n",
      "Iteration 2980 Loss: 0.07577192791935508\n",
      "Iteration 2981 Loss: 0.07577054078557496\n",
      "Iteration 2982 Loss: 0.07576915640950965\n",
      "Iteration 2983 Loss: 0.07576777478483876\n",
      "Iteration 2984 Loss: 0.07576639590525673\n",
      "Iteration 2985 Loss: 0.07576501976447218\n",
      "Iteration 2986 Loss: 0.07576364635620865\n",
      "Iteration 2987 Loss: 0.07576227567420367\n",
      "Iteration 2988 Loss: 0.0757609077122094\n",
      "Iteration 2989 Loss: 0.07575954246399236\n",
      "Iteration 2990 Loss: 0.0757581799233334\n",
      "Iteration 2991 Loss: 0.0757568200840277\n",
      "Iteration 2992 Loss: 0.0757554629398847\n",
      "Iteration 2993 Loss: 0.07575410848472794\n",
      "Iteration 2994 Loss: 0.07575275671239542\n",
      "Iteration 2995 Loss: 0.07575140761673925\n",
      "Iteration 2996 Loss: 0.07575006119162546\n",
      "Iteration 2997 Loss: 0.07574871743093453\n",
      "Iteration 2998 Loss: 0.07574737632856071\n",
      "Iteration 2999 Loss: 0.07574603787841253\n",
      "Iteration 3000 Loss: 0.07574470207441239\n",
      "Iteration 3001 Loss: 0.07574336891049677\n",
      "Iteration 3002 Loss: 0.07574203838061606\n",
      "Iteration 3003 Loss: 0.07574071047873454\n",
      "Iteration 3004 Loss: 0.07573938519883039\n",
      "Iteration 3005 Loss: 0.07573806253489568\n",
      "Iteration 3006 Loss: 0.07573674248093623\n",
      "Iteration 3007 Loss: 0.07573542503097172\n",
      "Iteration 3008 Loss: 0.07573411017903567\n",
      "Iteration 3009 Loss: 0.07573279791917505\n",
      "Iteration 3010 Loss: 0.07573148824545076\n",
      "Iteration 3011 Loss: 0.07573018115193732\n",
      "Iteration 3012 Loss: 0.07572887663272287\n",
      "Iteration 3013 Loss: 0.07572757468190912\n",
      "Iteration 3014 Loss: 0.07572627529361138\n",
      "Iteration 3015 Loss: 0.0757249784619585\n",
      "Iteration 3016 Loss: 0.07572368418109283\n",
      "Iteration 3017 Loss: 0.07572239244517018\n",
      "Iteration 3018 Loss: 0.07572110324835984\n",
      "Iteration 3019 Loss: 0.07571981658484446\n",
      "Iteration 3020 Loss: 0.07571853244882021\n",
      "Iteration 3021 Loss: 0.0757172508344964\n",
      "Iteration 3022 Loss: 0.0757159717360958\n",
      "Iteration 3023 Loss: 0.07571469514785442\n",
      "Iteration 3024 Loss: 0.07571342106402165\n",
      "Iteration 3025 Loss: 0.07571214947885996\n",
      "Iteration 3026 Loss: 0.07571088038664506\n",
      "Iteration 3027 Loss: 0.07570961378166582\n",
      "Iteration 3028 Loss: 0.07570834965822437\n",
      "Iteration 3029 Loss: 0.07570708801063582\n",
      "Iteration 3030 Loss: 0.07570582883322832\n",
      "Iteration 3031 Loss: 0.0757045721203432\n",
      "Iteration 3032 Loss: 0.07570331786633477\n",
      "Iteration 3033 Loss: 0.07570206606557021\n",
      "Iteration 3034 Loss: 0.0757008167124299\n",
      "Iteration 3035 Loss: 0.07569956980130685\n",
      "Iteration 3036 Loss: 0.07569832532660727\n",
      "Iteration 3037 Loss: 0.07569708328275006\n",
      "Iteration 3038 Loss: 0.07569584366416691\n",
      "Iteration 3039 Loss: 0.07569460646530243\n",
      "Iteration 3040 Loss: 0.07569337168061405\n",
      "Iteration 3041 Loss: 0.07569213930457189\n",
      "Iteration 3042 Loss: 0.0756909093316587\n",
      "Iteration 3043 Loss: 0.07568968175637003\n",
      "Iteration 3044 Loss: 0.07568845657321419\n",
      "Iteration 3045 Loss: 0.0756872337767119\n",
      "Iteration 3046 Loss: 0.07568601336139659\n",
      "Iteration 3047 Loss: 0.07568479532181435\n",
      "Iteration 3048 Loss: 0.07568357965252359\n",
      "Iteration 3049 Loss: 0.0756823663480956\n",
      "Iteration 3050 Loss: 0.07568115540311385\n",
      "Iteration 3051 Loss: 0.07567994681217431\n",
      "Iteration 3052 Loss: 0.07567874056988545\n",
      "Iteration 3053 Loss: 0.07567753667086825\n",
      "Iteration 3054 Loss: 0.07567633510975588\n",
      "Iteration 3055 Loss: 0.0756751358811939\n",
      "Iteration 3056 Loss: 0.07567393897984023\n",
      "Iteration 3057 Loss: 0.07567274440036509\n",
      "Iteration 3058 Loss: 0.07567155213745091\n",
      "Iteration 3059 Loss: 0.0756703621857924\n",
      "Iteration 3060 Loss: 0.07566917454009643\n",
      "Iteration 3061 Loss: 0.07566798919508215\n",
      "Iteration 3062 Loss: 0.07566680614548063\n",
      "Iteration 3063 Loss: 0.07566562538603547\n",
      "Iteration 3064 Loss: 0.07566444691150184\n",
      "Iteration 3065 Loss: 0.07566327071664739\n",
      "Iteration 3066 Loss: 0.07566209679625162\n",
      "Iteration 3067 Loss: 0.07566092514510607\n",
      "Iteration 3068 Loss: 0.07565975575801431\n",
      "Iteration 3069 Loss: 0.07565858862979184\n",
      "Iteration 3070 Loss: 0.07565742375526603\n",
      "Iteration 3071 Loss: 0.07565626112927626\n",
      "Iteration 3072 Loss: 0.07565510074667361\n",
      "Iteration 3073 Loss: 0.0756539426023212\n",
      "Iteration 3074 Loss: 0.07565278669109386\n",
      "Iteration 3075 Loss: 0.07565163300787822\n",
      "Iteration 3076 Loss: 0.07565048154757278\n",
      "Iteration 3077 Loss: 0.07564933230508761\n",
      "Iteration 3078 Loss: 0.07564818527534453\n",
      "Iteration 3079 Loss: 0.07564704045327714\n",
      "Iteration 3080 Loss: 0.07564589783383056\n",
      "Iteration 3081 Loss: 0.07564475741196171\n",
      "Iteration 3082 Loss: 0.07564361918263891\n",
      "Iteration 3083 Loss: 0.07564248314084224\n",
      "Iteration 3084 Loss: 0.07564134928156323\n",
      "Iteration 3085 Loss: 0.07564021759980495\n",
      "Iteration 3086 Loss: 0.07563908809058192\n",
      "Iteration 3087 Loss: 0.07563796074892024\n",
      "Iteration 3088 Loss: 0.07563683556985734\n",
      "Iteration 3089 Loss: 0.07563571254844217\n",
      "Iteration 3090 Loss: 0.07563459167973503\n",
      "Iteration 3091 Loss: 0.07563347295880751\n",
      "Iteration 3092 Loss: 0.07563235638074256\n",
      "Iteration 3093 Loss: 0.0756312419406346\n",
      "Iteration 3094 Loss: 0.0756301296335891\n",
      "Iteration 3095 Loss: 0.07562901945472303\n",
      "Iteration 3096 Loss: 0.07562791139916432\n",
      "Iteration 3097 Loss: 0.07562680546205243\n",
      "Iteration 3098 Loss: 0.07562570163853768\n",
      "Iteration 3099 Loss: 0.07562459992378186\n",
      "Iteration 3100 Loss: 0.07562350031295753\n",
      "Iteration 3101 Loss: 0.0756224028012487\n",
      "Iteration 3102 Loss: 0.07562130738385027\n",
      "Iteration 3103 Loss: 0.07562021405596825\n",
      "Iteration 3104 Loss: 0.07561912281281966\n",
      "Iteration 3105 Loss: 0.07561803364963252\n",
      "Iteration 3106 Loss: 0.07561694656164586\n",
      "Iteration 3107 Loss: 0.07561586154410964\n",
      "Iteration 3108 Loss: 0.0756147785922848\n",
      "Iteration 3109 Loss: 0.07561369770144295\n",
      "Iteration 3110 Loss: 0.07561261886686707\n",
      "Iteration 3111 Loss: 0.07561154208385039\n",
      "Iteration 3112 Loss: 0.07561046734769741\n",
      "Iteration 3113 Loss: 0.07560939465372318\n",
      "Iteration 3114 Loss: 0.07560832399725381\n",
      "Iteration 3115 Loss: 0.07560725537362582\n",
      "Iteration 3116 Loss: 0.0756061887781867\n",
      "Iteration 3117 Loss: 0.07560512420629459\n",
      "Iteration 3118 Loss: 0.07560406165331819\n",
      "Iteration 3119 Loss: 0.07560300111463705\n",
      "Iteration 3120 Loss: 0.07560194258564118\n",
      "Iteration 3121 Loss: 0.07560088606173124\n",
      "Iteration 3122 Loss: 0.07559983153831862\n",
      "Iteration 3123 Loss: 0.07559877901082504\n",
      "Iteration 3124 Loss: 0.07559772847468291\n",
      "Iteration 3125 Loss: 0.07559667992533509\n",
      "Iteration 3126 Loss: 0.0755956333582349\n",
      "Iteration 3127 Loss: 0.0755945887688461\n",
      "Iteration 3128 Loss: 0.07559354615264306\n",
      "Iteration 3129 Loss: 0.0755925055051103\n",
      "Iteration 3130 Loss: 0.07559146682174298\n",
      "Iteration 3131 Loss: 0.07559043009804642\n",
      "Iteration 3132 Loss: 0.07558939532953642\n",
      "Iteration 3133 Loss: 0.07558836251173903\n",
      "Iteration 3134 Loss: 0.07558733164019057\n",
      "Iteration 3135 Loss: 0.07558630271043777\n",
      "Iteration 3136 Loss: 0.07558527571803733\n",
      "Iteration 3137 Loss: 0.07558425065855648\n",
      "Iteration 3138 Loss: 0.07558322752757247\n",
      "Iteration 3139 Loss: 0.07558220632067278\n",
      "Iteration 3140 Loss: 0.07558118703345501\n",
      "Iteration 3141 Loss: 0.07558016966152686\n",
      "Iteration 3142 Loss: 0.07557915420050625\n",
      "Iteration 3143 Loss: 0.07557814064602104\n",
      "Iteration 3144 Loss: 0.07557712899370929\n",
      "Iteration 3145 Loss: 0.075576119239219\n",
      "Iteration 3146 Loss: 0.07557511137820819\n",
      "Iteration 3147 Loss: 0.07557410540634489\n",
      "Iteration 3148 Loss: 0.0755731013193071\n",
      "Iteration 3149 Loss: 0.07557209911278266\n",
      "Iteration 3150 Loss: 0.07557109878246959\n",
      "Iteration 3151 Loss: 0.07557010032407552\n",
      "Iteration 3152 Loss: 0.07556910373331814\n",
      "Iteration 3153 Loss: 0.07556810900592485\n",
      "Iteration 3154 Loss: 0.07556711613763306\n",
      "Iteration 3155 Loss: 0.07556612512418977\n",
      "Iteration 3156 Loss: 0.07556513596135192\n",
      "Iteration 3157 Loss: 0.07556414864488617\n",
      "Iteration 3158 Loss: 0.07556316317056899\n",
      "Iteration 3159 Loss: 0.07556217953418636\n",
      "Iteration 3160 Loss: 0.07556119773153416\n",
      "Iteration 3161 Loss: 0.07556021775841786\n",
      "Iteration 3162 Loss: 0.07555923961065261\n",
      "Iteration 3163 Loss: 0.0755582632840631\n",
      "Iteration 3164 Loss: 0.07555728877448378\n",
      "Iteration 3165 Loss: 0.07555631607775856\n",
      "Iteration 3166 Loss: 0.07555534518974091\n",
      "Iteration 3167 Loss: 0.0755543761062939\n",
      "Iteration 3168 Loss: 0.07555340882329008\n",
      "Iteration 3169 Loss: 0.0755524433366116\n",
      "Iteration 3170 Loss: 0.07555147964214981\n",
      "Iteration 3171 Loss: 0.07555051773580579\n",
      "Iteration 3172 Loss: 0.07554955761349001\n",
      "Iteration 3173 Loss: 0.07554859927112223\n",
      "Iteration 3174 Loss: 0.07554764270463164\n",
      "Iteration 3175 Loss: 0.07554668790995685\n",
      "Iteration 3176 Loss: 0.07554573488304575\n",
      "Iteration 3177 Loss: 0.07554478361985562\n",
      "Iteration 3178 Loss: 0.07554383411635295\n",
      "Iteration 3179 Loss: 0.07554288636851356\n",
      "Iteration 3180 Loss: 0.07554194037232254\n",
      "Iteration 3181 Loss: 0.07554099612377418\n",
      "Iteration 3182 Loss: 0.07554005361887205\n",
      "Iteration 3183 Loss: 0.0755391128536289\n",
      "Iteration 3184 Loss: 0.07553817382406647\n",
      "Iteration 3185 Loss: 0.07553723652621591\n",
      "Iteration 3186 Loss: 0.07553630095611745\n",
      "Iteration 3187 Loss: 0.07553536710982031\n",
      "Iteration 3188 Loss: 0.07553443498338283\n",
      "Iteration 3189 Loss: 0.07553350457287249\n",
      "Iteration 3190 Loss: 0.0755325758743658\n",
      "Iteration 3191 Loss: 0.07553164888394821\n",
      "Iteration 3192 Loss: 0.07553072359771436\n",
      "Iteration 3193 Loss: 0.0755298000117676\n",
      "Iteration 3194 Loss: 0.07552887812222053\n",
      "Iteration 3195 Loss: 0.07552795792519446\n",
      "Iteration 3196 Loss: 0.0755270394168198\n",
      "Iteration 3197 Loss: 0.07552612259323574\n",
      "Iteration 3198 Loss: 0.07552520745059046\n",
      "Iteration 3199 Loss: 0.07552429398504086\n",
      "Iteration 3200 Loss: 0.07552338219275281\n",
      "Iteration 3201 Loss: 0.07552247206990091\n",
      "Iteration 3202 Loss: 0.07552156361266867\n",
      "Iteration 3203 Loss: 0.0755206568172482\n",
      "Iteration 3204 Loss: 0.07551975167984062\n",
      "Iteration 3205 Loss: 0.07551884819665555\n",
      "Iteration 3206 Loss: 0.07551794636391146\n",
      "Iteration 3207 Loss: 0.07551704617783554\n",
      "Iteration 3208 Loss: 0.07551614763466354\n",
      "Iteration 3209 Loss: 0.07551525073063994\n",
      "Iteration 3210 Loss: 0.07551435546201789\n",
      "Iteration 3211 Loss: 0.07551346182505911\n",
      "Iteration 3212 Loss: 0.0755125698160339\n",
      "Iteration 3213 Loss: 0.0755116794312213\n",
      "Iteration 3214 Loss: 0.07551079066690869\n",
      "Iteration 3215 Loss: 0.07550990351939209\n",
      "Iteration 3216 Loss: 0.07550901798497611\n",
      "Iteration 3217 Loss: 0.07550813405997367\n",
      "Iteration 3218 Loss: 0.07550725174070638\n",
      "Iteration 3219 Loss: 0.07550637102350427\n",
      "Iteration 3220 Loss: 0.07550549190470568\n",
      "Iteration 3221 Loss: 0.07550461438065748\n",
      "Iteration 3222 Loss: 0.0755037384477149\n",
      "Iteration 3223 Loss: 0.07550286410224163\n",
      "Iteration 3224 Loss: 0.0755019913406097\n",
      "Iteration 3225 Loss: 0.07550112015919944\n",
      "Iteration 3226 Loss: 0.07550025055439949\n",
      "Iteration 3227 Loss: 0.07549938252260684\n",
      "Iteration 3228 Loss: 0.0754985160602268\n",
      "Iteration 3229 Loss: 0.07549765116367291\n",
      "Iteration 3230 Loss: 0.07549678782936693\n",
      "Iteration 3231 Loss: 0.07549592605373888\n",
      "Iteration 3232 Loss: 0.0754950658332271\n",
      "Iteration 3233 Loss: 0.07549420716427797\n",
      "Iteration 3234 Loss: 0.07549335004334601\n",
      "Iteration 3235 Loss: 0.07549249446689413\n",
      "Iteration 3236 Loss: 0.07549164043139311\n",
      "Iteration 3237 Loss: 0.07549078793332205\n",
      "Iteration 3238 Loss: 0.07548993696916803\n",
      "Iteration 3239 Loss: 0.07548908753542628\n",
      "Iteration 3240 Loss: 0.07548823962860003\n",
      "Iteration 3241 Loss: 0.0754873932452006\n",
      "Iteration 3242 Loss: 0.07548654838174736\n",
      "Iteration 3243 Loss: 0.07548570503476759\n",
      "Iteration 3244 Loss: 0.07548486320079667\n",
      "Iteration 3245 Loss: 0.07548402287637787\n",
      "Iteration 3246 Loss: 0.07548318405806245\n",
      "Iteration 3247 Loss: 0.07548234674240964\n",
      "Iteration 3248 Loss: 0.07548151092598647\n",
      "Iteration 3249 Loss: 0.07548067660536795\n",
      "Iteration 3250 Loss: 0.07547984377713704\n",
      "Iteration 3251 Loss: 0.07547901243788434\n",
      "Iteration 3252 Loss: 0.0754781825842086\n",
      "Iteration 3253 Loss: 0.07547735421271606\n",
      "Iteration 3254 Loss: 0.075476527320021\n",
      "Iteration 3255 Loss: 0.07547570190274551\n",
      "Iteration 3256 Loss: 0.07547487795751918\n",
      "Iteration 3257 Loss: 0.07547405548097974\n",
      "Iteration 3258 Loss: 0.07547323446977232\n",
      "Iteration 3259 Loss: 0.07547241492054994\n",
      "Iteration 3260 Loss: 0.07547159682997326\n",
      "Iteration 3261 Loss: 0.07547078019471062\n",
      "Iteration 3262 Loss: 0.07546996501143811\n",
      "Iteration 3263 Loss: 0.0754691512768394\n",
      "Iteration 3264 Loss: 0.07546833898760566\n",
      "Iteration 3265 Loss: 0.07546752814043603\n",
      "Iteration 3266 Loss: 0.07546671873203678\n",
      "Iteration 3267 Loss: 0.07546591075912212\n",
      "Iteration 3268 Loss: 0.07546510421841371\n",
      "Iteration 3269 Loss: 0.07546429910664068\n",
      "Iteration 3270 Loss: 0.0754634954205397\n",
      "Iteration 3271 Loss: 0.07546269315685508\n",
      "Iteration 3272 Loss: 0.07546189231233849\n",
      "Iteration 3273 Loss: 0.07546109288374904\n",
      "Iteration 3274 Loss: 0.07546029486785348\n",
      "Iteration 3275 Loss: 0.0754594982614258\n",
      "Iteration 3276 Loss: 0.0754587030612475\n",
      "Iteration 3277 Loss: 0.07545790926410749\n",
      "Iteration 3278 Loss: 0.07545711686680205\n",
      "Iteration 3279 Loss: 0.07545632586613482\n",
      "Iteration 3280 Loss: 0.07545553625891685\n",
      "Iteration 3281 Loss: 0.07545474804196646\n",
      "Iteration 3282 Loss: 0.07545396121210923\n",
      "Iteration 3283 Loss: 0.07545317576617823\n",
      "Iteration 3284 Loss: 0.07545239170101367\n",
      "Iteration 3285 Loss: 0.07545160901346307\n",
      "Iteration 3286 Loss: 0.07545082770038118\n",
      "Iteration 3287 Loss: 0.07545004775863\n",
      "Iteration 3288 Loss: 0.07544926918507884\n",
      "Iteration 3289 Loss: 0.07544849197660401\n",
      "Iteration 3290 Loss: 0.07544771613008919\n",
      "Iteration 3291 Loss: 0.07544694164242513\n",
      "Iteration 3292 Loss: 0.07544616851050975\n",
      "Iteration 3293 Loss: 0.07544539673124816\n",
      "Iteration 3294 Loss: 0.0754446263015525\n",
      "Iteration 3295 Loss: 0.07544385721834215\n",
      "Iteration 3296 Loss: 0.0754430894785434\n",
      "Iteration 3297 Loss: 0.07544232307908978\n",
      "Iteration 3298 Loss: 0.07544155801692168\n",
      "Iteration 3299 Loss: 0.0754407942889867\n",
      "Iteration 3300 Loss: 0.07544003189223941\n",
      "Iteration 3301 Loss: 0.07543927082364142\n",
      "Iteration 3302 Loss: 0.07543851108016122\n",
      "Iteration 3303 Loss: 0.07543775265877438\n",
      "Iteration 3304 Loss: 0.07543699555646331\n",
      "Iteration 3305 Loss: 0.07543623977021754\n",
      "Iteration 3306 Loss: 0.07543548529703346\n",
      "Iteration 3307 Loss: 0.07543473213391425\n",
      "Iteration 3308 Loss: 0.07543398027787006\n",
      "Iteration 3309 Loss: 0.07543322972591801\n",
      "Iteration 3310 Loss: 0.07543248047508186\n",
      "Iteration 3311 Loss: 0.07543173252239252\n",
      "Iteration 3312 Loss: 0.07543098586488747\n",
      "Iteration 3313 Loss: 0.07543024049961118\n",
      "Iteration 3314 Loss: 0.07542949642361471\n",
      "Iteration 3315 Loss: 0.07542875363395611\n",
      "Iteration 3316 Loss: 0.07542801212770017\n",
      "Iteration 3317 Loss: 0.07542727190191835\n",
      "Iteration 3318 Loss: 0.07542653295368881\n",
      "Iteration 3319 Loss: 0.07542579528009659\n",
      "Iteration 3320 Loss: 0.07542505887823334\n",
      "Iteration 3321 Loss: 0.07542432374519732\n",
      "Iteration 3322 Loss: 0.07542358987809368\n",
      "Iteration 3323 Loss: 0.075422857274034\n",
      "Iteration 3324 Loss: 0.07542212593013671\n",
      "Iteration 3325 Loss: 0.07542139584352667\n",
      "Iteration 3326 Loss: 0.07542066701133547\n",
      "Iteration 3327 Loss: 0.07541993943070133\n",
      "Iteration 3328 Loss: 0.0754192130987689\n",
      "Iteration 3329 Loss: 0.07541848801268962\n",
      "Iteration 3330 Loss: 0.07541776416962129\n",
      "Iteration 3331 Loss: 0.07541704156672832\n",
      "Iteration 3332 Loss: 0.07541632020118168\n",
      "Iteration 3333 Loss: 0.07541560007015877\n",
      "Iteration 3334 Loss: 0.07541488117084356\n",
      "Iteration 3335 Loss: 0.07541416350042646\n",
      "Iteration 3336 Loss: 0.07541344705610435\n",
      "Iteration 3337 Loss: 0.07541273183508052\n",
      "Iteration 3338 Loss: 0.07541201783456472\n",
      "Iteration 3339 Loss: 0.07541130505177318\n",
      "Iteration 3340 Loss: 0.07541059348392842\n",
      "Iteration 3341 Loss: 0.0754098831282595\n",
      "Iteration 3342 Loss: 0.07540917398200166\n",
      "Iteration 3343 Loss: 0.07540846604239668\n",
      "Iteration 3344 Loss: 0.07540775930669252\n",
      "Iteration 3345 Loss: 0.07540705377214366\n",
      "Iteration 3346 Loss: 0.07540634943601077\n",
      "Iteration 3347 Loss: 0.07540564629556074\n",
      "Iteration 3348 Loss: 0.07540494434806697\n",
      "Iteration 3349 Loss: 0.07540424359080898\n",
      "Iteration 3350 Loss: 0.0754035440210726\n",
      "Iteration 3351 Loss: 0.07540284563614984\n",
      "Iteration 3352 Loss: 0.07540214843333906\n",
      "Iteration 3353 Loss: 0.07540145240994468\n",
      "Iteration 3354 Loss: 0.07540075756327744\n",
      "Iteration 3355 Loss: 0.07540006389065426\n",
      "Iteration 3356 Loss: 0.07539937138939812\n",
      "Iteration 3357 Loss: 0.07539868005683834\n",
      "Iteration 3358 Loss: 0.07539798989031019\n",
      "Iteration 3359 Loss: 0.07539730088715521\n",
      "Iteration 3360 Loss: 0.07539661304472098\n",
      "Iteration 3361 Loss: 0.07539592636036122\n",
      "Iteration 3362 Loss: 0.07539524083143576\n",
      "Iteration 3363 Loss: 0.0753945564553104\n",
      "Iteration 3364 Loss: 0.07539387322935712\n",
      "Iteration 3365 Loss: 0.07539319115095386\n",
      "Iteration 3366 Loss: 0.07539251021748464\n",
      "Iteration 3367 Loss: 0.07539183042633953\n",
      "Iteration 3368 Loss: 0.0753911517749145\n",
      "Iteration 3369 Loss: 0.07539047426061153\n",
      "Iteration 3370 Loss: 0.0753897978808387\n",
      "Iteration 3371 Loss: 0.07538912263300991\n",
      "Iteration 3372 Loss: 0.07538844851454511\n",
      "Iteration 3373 Loss: 0.07538777552287007\n",
      "Iteration 3374 Loss: 0.0753871036554166\n",
      "Iteration 3375 Loss: 0.07538643290962241\n",
      "Iteration 3376 Loss: 0.07538576328293096\n",
      "Iteration 3377 Loss: 0.07538509477279176\n",
      "Iteration 3378 Loss: 0.07538442737666011\n",
      "Iteration 3379 Loss: 0.07538376109199721\n",
      "Iteration 3380 Loss: 0.07538309591626997\n",
      "Iteration 3381 Loss: 0.07538243184695133\n",
      "Iteration 3382 Loss: 0.0753817688815198\n",
      "Iteration 3383 Loss: 0.07538110701745995\n",
      "Iteration 3384 Loss: 0.07538044625226203\n",
      "Iteration 3385 Loss: 0.0753797865834219\n",
      "Iteration 3386 Loss: 0.07537912800844146\n",
      "Iteration 3387 Loss: 0.07537847052482805\n",
      "Iteration 3388 Loss: 0.07537781413009512\n",
      "Iteration 3389 Loss: 0.07537715882176155\n",
      "Iteration 3390 Loss: 0.07537650459735196\n",
      "Iteration 3391 Loss: 0.07537585145439686\n",
      "Iteration 3392 Loss: 0.0753751993904321\n",
      "Iteration 3393 Loss: 0.07537454840299948\n",
      "Iteration 3394 Loss: 0.07537389848964647\n",
      "Iteration 3395 Loss: 0.07537324964792591\n",
      "Iteration 3396 Loss: 0.07537260187539657\n",
      "Iteration 3397 Loss: 0.07537195516962265\n",
      "Iteration 3398 Loss: 0.07537130952817397\n",
      "Iteration 3399 Loss: 0.0753706649486261\n",
      "Iteration 3400 Loss: 0.07537002142855993\n",
      "Iteration 3401 Loss: 0.07536937896556216\n",
      "Iteration 3402 Loss: 0.07536873755722487\n",
      "Iteration 3403 Loss: 0.07536809720114573\n",
      "Iteration 3404 Loss: 0.075367457894928\n",
      "Iteration 3405 Loss: 0.07536681963618033\n",
      "Iteration 3406 Loss: 0.07536618242251705\n",
      "Iteration 3407 Loss: 0.0753655462515578\n",
      "Iteration 3408 Loss: 0.07536491112092766\n",
      "Iteration 3409 Loss: 0.0753642770282575\n",
      "Iteration 3410 Loss: 0.07536364397118327\n",
      "Iteration 3411 Loss: 0.07536301194734647\n",
      "Iteration 3412 Loss: 0.07536238095439421\n",
      "Iteration 3413 Loss: 0.07536175098997874\n",
      "Iteration 3414 Loss: 0.07536112205175784\n",
      "Iteration 3415 Loss: 0.07536049413739475\n",
      "Iteration 3416 Loss: 0.0753598672445579\n",
      "Iteration 3417 Loss: 0.07535924137092129\n",
      "Iteration 3418 Loss: 0.07535861651416409\n",
      "Iteration 3419 Loss: 0.07535799267197092\n",
      "Iteration 3420 Loss: 0.0753573698420317\n",
      "Iteration 3421 Loss: 0.07535674802204166\n",
      "Iteration 3422 Loss: 0.0753561272097013\n",
      "Iteration 3423 Loss: 0.0753555074027165\n",
      "Iteration 3424 Loss: 0.07535488859879827\n",
      "Iteration 3425 Loss: 0.07535427079566309\n",
      "Iteration 3426 Loss: 0.07535365399103242\n",
      "Iteration 3427 Loss: 0.07535303818263324\n",
      "Iteration 3428 Loss: 0.07535242336819757\n",
      "Iteration 3429 Loss: 0.07535180954546275\n",
      "Iteration 3430 Loss: 0.07535119671217125\n",
      "Iteration 3431 Loss: 0.07535058486607078\n",
      "Iteration 3432 Loss: 0.07534997400491426\n",
      "Iteration 3433 Loss: 0.07534936412645968\n",
      "Iteration 3434 Loss: 0.07534875522847026\n",
      "Iteration 3435 Loss: 0.07534814730871434\n",
      "Iteration 3436 Loss: 0.07534754036496544\n",
      "Iteration 3437 Loss: 0.07534693439500215\n",
      "Iteration 3438 Loss: 0.07534632939660812\n",
      "Iteration 3439 Loss: 0.0753457253675723\n",
      "Iteration 3440 Loss: 0.0753451223056885\n",
      "Iteration 3441 Loss: 0.07534452020875564\n",
      "Iteration 3442 Loss: 0.0753439190745779\n",
      "Iteration 3443 Loss: 0.07534331890096424\n",
      "Iteration 3444 Loss: 0.07534271968572881\n",
      "Iteration 3445 Loss: 0.07534212142669088\n",
      "Iteration 3446 Loss: 0.07534152412167447\n",
      "Iteration 3447 Loss: 0.07534092776850884\n",
      "Iteration 3448 Loss: 0.07534033236502813\n",
      "Iteration 3449 Loss: 0.07533973790907153\n",
      "Iteration 3450 Loss: 0.0753391443984831\n",
      "Iteration 3451 Loss: 0.075338551831112\n",
      "Iteration 3452 Loss: 0.07533796020481218\n",
      "Iteration 3453 Loss: 0.07533736951744267\n",
      "Iteration 3454 Loss: 0.07533677976686731\n",
      "Iteration 3455 Loss: 0.0753361909509549\n",
      "Iteration 3456 Loss: 0.07533560306757922\n",
      "Iteration 3457 Loss: 0.07533501611461882\n",
      "Iteration 3458 Loss: 0.07533443008995713\n",
      "Iteration 3459 Loss: 0.07533384499148257\n",
      "Iteration 3460 Loss: 0.07533326081708833\n",
      "Iteration 3461 Loss: 0.0753326775646724\n",
      "Iteration 3462 Loss: 0.07533209523213771\n",
      "Iteration 3463 Loss: 0.07533151381739202\n",
      "Iteration 3464 Loss: 0.07533093331834773\n",
      "Iteration 3465 Loss: 0.07533035373292224\n",
      "Iteration 3466 Loss: 0.07532977505903764\n",
      "Iteration 3467 Loss: 0.0753291972946208\n",
      "Iteration 3468 Loss: 0.07532862043760345\n",
      "Iteration 3469 Loss: 0.07532804448592197\n",
      "Iteration 3470 Loss: 0.07532746943751752\n",
      "Iteration 3471 Loss: 0.07532689529033601\n",
      "Iteration 3472 Loss: 0.07532632204232799\n",
      "Iteration 3473 Loss: 0.07532574969144894\n",
      "Iteration 3474 Loss: 0.07532517823565882\n",
      "Iteration 3475 Loss: 0.07532460767292233\n",
      "Iteration 3476 Loss: 0.0753240380012089\n",
      "Iteration 3477 Loss: 0.07532346921849269\n",
      "Iteration 3478 Loss: 0.07532290132275243\n",
      "Iteration 3479 Loss: 0.07532233431197147\n",
      "Iteration 3480 Loss: 0.0753217681841378\n",
      "Iteration 3481 Loss: 0.07532120293724422\n",
      "Iteration 3482 Loss: 0.07532063856928786\n",
      "Iteration 3483 Loss: 0.07532007507827071\n",
      "Iteration 3484 Loss: 0.07531951246219926\n",
      "Iteration 3485 Loss: 0.07531895071908445\n",
      "Iteration 3486 Loss: 0.07531838984694213\n",
      "Iteration 3487 Loss: 0.0753178298437923\n",
      "Iteration 3488 Loss: 0.07531727070765985\n",
      "Iteration 3489 Loss: 0.07531671243657405\n",
      "Iteration 3490 Loss: 0.0753161550285687\n",
      "Iteration 3491 Loss: 0.07531559848168215\n",
      "Iteration 3492 Loss: 0.07531504279395737\n",
      "Iteration 3493 Loss: 0.07531448796344173\n",
      "Iteration 3494 Loss: 0.07531393398818695\n",
      "Iteration 3495 Loss: 0.07531338086624953\n",
      "Iteration 3496 Loss: 0.07531282859569018\n",
      "Iteration 3497 Loss: 0.07531227717457423\n",
      "Iteration 3498 Loss: 0.07531172660097142\n",
      "Iteration 3499 Loss: 0.07531117687295583\n",
      "Iteration 3500 Loss: 0.07531062798860619\n",
      "Iteration 3501 Loss: 0.0753100799460054\n",
      "Iteration 3502 Loss: 0.07530953274324095\n",
      "Iteration 3503 Loss: 0.07530898637840465\n",
      "Iteration 3504 Loss: 0.07530844084959269\n",
      "Iteration 3505 Loss: 0.07530789615490574\n",
      "Iteration 3506 Loss: 0.0753073522924487\n",
      "Iteration 3507 Loss: 0.07530680926033087\n",
      "Iteration 3508 Loss: 0.0753062670566659\n",
      "Iteration 3509 Loss: 0.07530572567957187\n",
      "Iteration 3510 Loss: 0.07530518512717116\n",
      "Iteration 3511 Loss: 0.07530464539759027\n",
      "Iteration 3512 Loss: 0.07530410648896033\n",
      "Iteration 3513 Loss: 0.07530356839941652\n",
      "Iteration 3514 Loss: 0.07530303112709835\n",
      "Iteration 3515 Loss: 0.07530249467014974\n",
      "Iteration 3516 Loss: 0.07530195902671877\n",
      "Iteration 3517 Loss: 0.07530142419495775\n",
      "Iteration 3518 Loss: 0.07530089017302331\n",
      "Iteration 3519 Loss: 0.07530035695907634\n",
      "Iteration 3520 Loss: 0.07529982455128194\n",
      "Iteration 3521 Loss: 0.07529929294780928\n",
      "Iteration 3522 Loss: 0.07529876214683204\n",
      "Iteration 3523 Loss: 0.07529823214652788\n",
      "Iteration 3524 Loss: 0.07529770294507868\n",
      "Iteration 3525 Loss: 0.07529717454067048\n",
      "Iteration 3526 Loss: 0.07529664693149365\n",
      "Iteration 3527 Loss: 0.0752961201157425\n",
      "Iteration 3528 Loss: 0.07529559409161568\n",
      "Iteration 3529 Loss: 0.07529506885731588\n",
      "Iteration 3530 Loss: 0.07529454441104996\n",
      "Iteration 3531 Loss: 0.0752940207510289\n",
      "Iteration 3532 Loss: 0.0752934978754678\n",
      "Iteration 3533 Loss: 0.07529297578258584\n",
      "Iteration 3534 Loss: 0.07529245447060633\n",
      "Iteration 3535 Loss: 0.07529193393775661\n",
      "Iteration 3536 Loss: 0.07529141418226819\n",
      "Iteration 3537 Loss: 0.0752908952023766\n",
      "Iteration 3538 Loss: 0.07529037699632136\n",
      "Iteration 3539 Loss: 0.07528985956234613\n",
      "Iteration 3540 Loss: 0.07528934289869861\n",
      "Iteration 3541 Loss: 0.07528882700363046\n",
      "Iteration 3542 Loss: 0.07528831187539742\n",
      "Iteration 3543 Loss: 0.07528779751225928\n",
      "Iteration 3544 Loss: 0.07528728391247971\n",
      "Iteration 3545 Loss: 0.07528677107432642\n",
      "Iteration 3546 Loss: 0.07528625899607125\n",
      "Iteration 3547 Loss: 0.07528574767598976\n",
      "Iteration 3548 Loss: 0.07528523711236171\n",
      "Iteration 3549 Loss: 0.0752847273034706\n",
      "Iteration 3550 Loss: 0.07528421824760409\n",
      "Iteration 3551 Loss: 0.07528370994305364\n",
      "Iteration 3552 Loss: 0.0752832023881147\n",
      "Iteration 3553 Loss: 0.07528269558108659\n",
      "Iteration 3554 Loss: 0.07528218952027263\n",
      "Iteration 3555 Loss: 0.07528168420397996\n",
      "Iteration 3556 Loss: 0.07528117963051957\n",
      "Iteration 3557 Loss: 0.07528067579820648\n",
      "Iteration 3558 Loss: 0.07528017270535954\n",
      "Iteration 3559 Loss: 0.07527967035030134\n",
      "Iteration 3560 Loss: 0.0752791687313585\n",
      "Iteration 3561 Loss: 0.07527866784686137\n",
      "Iteration 3562 Loss: 0.0752781676951442\n",
      "Iteration 3563 Loss: 0.0752776682745451\n",
      "Iteration 3564 Loss: 0.07527716958340591\n",
      "Iteration 3565 Loss: 0.07527667162007226\n",
      "Iteration 3566 Loss: 0.07527617438289376\n",
      "Iteration 3567 Loss: 0.07527567787022371\n",
      "Iteration 3568 Loss: 0.07527518208041918\n",
      "Iteration 3569 Loss: 0.07527468701184097\n",
      "Iteration 3570 Loss: 0.07527419266285384\n",
      "Iteration 3571 Loss: 0.07527369903182615\n",
      "Iteration 3572 Loss: 0.07527320611712995\n",
      "Iteration 3573 Loss: 0.07527271391714134\n",
      "Iteration 3574 Loss: 0.07527222243023968\n",
      "Iteration 3575 Loss: 0.07527173165480863\n",
      "Iteration 3576 Loss: 0.07527124158923501\n",
      "Iteration 3577 Loss: 0.07527075223190972\n",
      "Iteration 3578 Loss: 0.07527026358122732\n",
      "Iteration 3579 Loss: 0.07526977563558587\n",
      "Iteration 3580 Loss: 0.07526928839338727\n",
      "Iteration 3581 Loss: 0.0752688018530371\n",
      "Iteration 3582 Loss: 0.07526831601294452\n",
      "Iteration 3583 Loss: 0.07526783087152245\n",
      "Iteration 3584 Loss: 0.07526734642718738\n",
      "Iteration 3585 Loss: 0.07526686267835941\n",
      "Iteration 3586 Loss: 0.07526637962346239\n",
      "Iteration 3587 Loss: 0.07526589726092382\n",
      "Iteration 3588 Loss: 0.07526541558917454\n",
      "Iteration 3589 Loss: 0.07526493460664938\n",
      "Iteration 3590 Loss: 0.07526445431178647\n",
      "Iteration 3591 Loss: 0.07526397470302768\n",
      "Iteration 3592 Loss: 0.0752634957788185\n",
      "Iteration 3593 Loss: 0.07526301753760782\n",
      "Iteration 3594 Loss: 0.07526253997784826\n",
      "Iteration 3595 Loss: 0.0752620630979959\n",
      "Iteration 3596 Loss: 0.07526158689651041\n",
      "Iteration 3597 Loss: 0.0752611113718551\n",
      "Iteration 3598 Loss: 0.07526063652249662\n",
      "Iteration 3599 Loss: 0.07526016234690532\n",
      "Iteration 3600 Loss: 0.07525968884355491\n",
      "Iteration 3601 Loss: 0.07525921601092275\n",
      "Iteration 3602 Loss: 0.07525874384748966\n",
      "Iteration 3603 Loss: 0.07525827235173994\n",
      "Iteration 3604 Loss: 0.0752578015221613\n",
      "Iteration 3605 Loss: 0.07525733135724508\n",
      "Iteration 3606 Loss: 0.07525686185548604\n",
      "Iteration 3607 Loss: 0.0752563930153823\n",
      "Iteration 3608 Loss: 0.07525592483543556\n",
      "Iteration 3609 Loss: 0.07525545731415084\n",
      "Iteration 3610 Loss: 0.07525499045003689\n",
      "Iteration 3611 Loss: 0.07525452424160536\n",
      "Iteration 3612 Loss: 0.07525405868737196\n",
      "Iteration 3613 Loss: 0.07525359378585524\n",
      "Iteration 3614 Loss: 0.07525312953557754\n",
      "Iteration 3615 Loss: 0.07525266593506441\n",
      "Iteration 3616 Loss: 0.07525220298284492\n",
      "Iteration 3617 Loss: 0.07525174067745137\n",
      "Iteration 3618 Loss: 0.07525127901741961\n",
      "Iteration 3619 Loss: 0.07525081800128869\n",
      "Iteration 3620 Loss: 0.07525035762760111\n",
      "Iteration 3621 Loss: 0.07524989789490273\n",
      "Iteration 3622 Loss: 0.0752494388017427\n",
      "Iteration 3623 Loss: 0.07524898034667359\n",
      "Iteration 3624 Loss: 0.0752485225282512\n",
      "Iteration 3625 Loss: 0.0752480653450347\n",
      "Iteration 3626 Loss: 0.07524760879558665\n",
      "Iteration 3627 Loss: 0.07524715287847274\n",
      "Iteration 3628 Loss: 0.07524669759226207\n",
      "Iteration 3629 Loss: 0.07524624293552712\n",
      "Iteration 3630 Loss: 0.07524578890684344\n",
      "Iteration 3631 Loss: 0.07524533550479003\n",
      "Iteration 3632 Loss: 0.07524488272794908\n",
      "Iteration 3633 Loss: 0.07524443057490604\n",
      "Iteration 3634 Loss: 0.07524397904424959\n",
      "Iteration 3635 Loss: 0.07524352813457187\n",
      "Iteration 3636 Loss: 0.0752430778444679\n",
      "Iteration 3637 Loss: 0.07524262817253617\n",
      "Iteration 3638 Loss: 0.07524217911737838\n",
      "Iteration 3639 Loss: 0.07524173067759937\n",
      "Iteration 3640 Loss: 0.07524128285180724\n",
      "Iteration 3641 Loss: 0.07524083563861321\n",
      "Iteration 3642 Loss: 0.07524038903663194\n",
      "Iteration 3643 Loss: 0.07523994304448087\n",
      "Iteration 3644 Loss: 0.07523949766078096\n",
      "Iteration 3645 Loss: 0.07523905288415625\n",
      "Iteration 3646 Loss: 0.07523860871323385\n",
      "Iteration 3647 Loss: 0.07523816514664416\n",
      "Iteration 3648 Loss: 0.07523772218302062\n",
      "Iteration 3649 Loss: 0.07523727982099991\n",
      "Iteration 3650 Loss: 0.07523683805922174\n",
      "Iteration 3651 Loss: 0.07523639689632905\n",
      "Iteration 3652 Loss: 0.07523595633096784\n",
      "Iteration 3653 Loss: 0.0752355163617873\n",
      "Iteration 3654 Loss: 0.0752350769874395\n",
      "Iteration 3655 Loss: 0.07523463820657998\n",
      "Iteration 3656 Loss: 0.07523420001786706\n",
      "Iteration 3657 Loss: 0.07523376241996224\n",
      "Iteration 3658 Loss: 0.07523332541153015\n",
      "Iteration 3659 Loss: 0.07523288899123845\n",
      "Iteration 3660 Loss: 0.07523245315775788\n",
      "Iteration 3661 Loss: 0.07523201790976224\n",
      "Iteration 3662 Loss: 0.0752315832459283\n",
      "Iteration 3663 Loss: 0.075231149164936\n",
      "Iteration 3664 Loss: 0.0752307156654682\n",
      "Iteration 3665 Loss: 0.07523028274621088\n",
      "Iteration 3666 Loss: 0.07522985040585306\n",
      "Iteration 3667 Loss: 0.07522941864308662\n",
      "Iteration 3668 Loss: 0.07522898745660658\n",
      "Iteration 3669 Loss: 0.07522855684511098\n",
      "Iteration 3670 Loss: 0.07522812680730077\n",
      "Iteration 3671 Loss: 0.07522769734187991\n",
      "Iteration 3672 Loss: 0.07522726844755537\n",
      "Iteration 3673 Loss: 0.07522684012303707\n",
      "Iteration 3674 Loss: 0.07522641236703789\n",
      "Iteration 3675 Loss: 0.07522598517827374\n",
      "Iteration 3676 Loss: 0.07522555855546337\n",
      "Iteration 3677 Loss: 0.07522513249732853\n",
      "Iteration 3678 Loss: 0.07522470700259394\n",
      "Iteration 3679 Loss: 0.07522428206998724\n",
      "Iteration 3680 Loss: 0.07522385769823894\n",
      "Iteration 3681 Loss: 0.07522343388608255\n",
      "Iteration 3682 Loss: 0.07522301063225445\n",
      "Iteration 3683 Loss: 0.0752225879354939\n",
      "Iteration 3684 Loss: 0.07522216579454313\n",
      "Iteration 3685 Loss: 0.0752217442081472\n",
      "Iteration 3686 Loss: 0.07522132317505408\n",
      "Iteration 3687 Loss: 0.07522090269401467\n",
      "Iteration 3688 Loss: 0.07522048276378261\n",
      "Iteration 3689 Loss: 0.07522006338311446\n",
      "Iteration 3690 Loss: 0.07521964455076982\n",
      "Iteration 3691 Loss: 0.0752192262655108\n",
      "Iteration 3692 Loss: 0.07521880852610274\n",
      "Iteration 3693 Loss: 0.07521839133131346\n",
      "Iteration 3694 Loss: 0.07521797467991391\n",
      "Iteration 3695 Loss: 0.07521755857067765\n",
      "Iteration 3696 Loss: 0.07521714300238115\n",
      "Iteration 3697 Loss: 0.07521672797380372\n",
      "Iteration 3698 Loss: 0.07521631348372748\n",
      "Iteration 3699 Loss: 0.07521589953093726\n",
      "Iteration 3700 Loss: 0.07521548611422076\n",
      "Iteration 3701 Loss: 0.0752150732323684\n",
      "Iteration 3702 Loss: 0.0752146608841735\n",
      "Iteration 3703 Loss: 0.07521424906843213\n",
      "Iteration 3704 Loss: 0.07521383778394304\n",
      "Iteration 3705 Loss: 0.0752134270295077\n",
      "Iteration 3706 Loss: 0.07521301680393053\n",
      "Iteration 3707 Loss: 0.0752126071060186\n",
      "Iteration 3708 Loss: 0.07521219793458171\n",
      "Iteration 3709 Loss: 0.07521178928843233\n",
      "Iteration 3710 Loss: 0.07521138116638582\n",
      "Iteration 3711 Loss: 0.07521097356726014\n",
      "Iteration 3712 Loss: 0.075210566489876\n",
      "Iteration 3713 Loss: 0.07521015993305688\n",
      "Iteration 3714 Loss: 0.0752097538956289\n",
      "Iteration 3715 Loss: 0.07520934837642082\n",
      "Iteration 3716 Loss: 0.07520894337426429\n",
      "Iteration 3717 Loss: 0.07520853888799349\n",
      "Iteration 3718 Loss: 0.07520813491644525\n",
      "Iteration 3719 Loss: 0.07520773145845923\n",
      "Iteration 3720 Loss: 0.0752073285128776\n",
      "Iteration 3721 Loss: 0.07520692607854529\n",
      "Iteration 3722 Loss: 0.07520652415430991\n",
      "Iteration 3723 Loss: 0.07520612273902158\n",
      "Iteration 3724 Loss: 0.07520572183153323\n",
      "Iteration 3725 Loss: 0.07520532143070029\n",
      "Iteration 3726 Loss: 0.07520492153538094\n",
      "Iteration 3727 Loss: 0.07520452214443596\n",
      "Iteration 3728 Loss: 0.07520412325672862\n",
      "Iteration 3729 Loss: 0.07520372487112496\n",
      "Iteration 3730 Loss: 0.07520332698649357\n",
      "Iteration 3731 Loss: 0.07520292960170563\n",
      "Iteration 3732 Loss: 0.07520253271563496\n",
      "Iteration 3733 Loss: 0.07520213632715789\n",
      "Iteration 3734 Loss: 0.07520174043515342\n",
      "Iteration 3735 Loss: 0.0752013450385031\n",
      "Iteration 3736 Loss: 0.07520095013609104\n",
      "Iteration 3737 Loss: 0.07520055572680388\n",
      "Iteration 3738 Loss: 0.07520016180953089\n",
      "Iteration 3739 Loss: 0.07519976838316389\n",
      "Iteration 3740 Loss: 0.07519937544659719\n",
      "Iteration 3741 Loss: 0.07519898299872763\n",
      "Iteration 3742 Loss: 0.07519859103845472\n",
      "Iteration 3743 Loss: 0.07519819956468043\n",
      "Iteration 3744 Loss: 0.07519780857630912\n",
      "Iteration 3745 Loss: 0.07519741807224785\n",
      "Iteration 3746 Loss: 0.07519702805140616\n",
      "Iteration 3747 Loss: 0.07519663851269606\n",
      "Iteration 3748 Loss: 0.07519624945503205\n",
      "Iteration 3749 Loss: 0.07519586087733113\n",
      "Iteration 3750 Loss: 0.07519547277851282\n",
      "Iteration 3751 Loss: 0.07519508515749912\n",
      "Iteration 3752 Loss: 0.07519469801321452\n",
      "Iteration 3753 Loss: 0.07519431134458592\n",
      "Iteration 3754 Loss: 0.07519392515054277\n",
      "Iteration 3755 Loss: 0.07519353943001697\n",
      "Iteration 3756 Loss: 0.07519315418194279\n",
      "Iteration 3757 Loss: 0.07519276940525702\n",
      "Iteration 3758 Loss: 0.07519238509889889\n",
      "Iteration 3759 Loss: 0.07519200126181011\n",
      "Iteration 3760 Loss: 0.0751916178929347\n",
      "Iteration 3761 Loss: 0.07519123499121926\n",
      "Iteration 3762 Loss: 0.07519085255561268\n",
      "Iteration 3763 Loss: 0.07519047058506638\n",
      "Iteration 3764 Loss: 0.07519008907853413\n",
      "Iteration 3765 Loss: 0.07518970803497205\n",
      "Iteration 3766 Loss: 0.0751893274533388\n",
      "Iteration 3767 Loss: 0.07518894733259533\n",
      "Iteration 3768 Loss: 0.07518856767170502\n",
      "Iteration 3769 Loss: 0.07518818846963357\n",
      "Iteration 3770 Loss: 0.07518780972534918\n",
      "Iteration 3771 Loss: 0.07518743143782228\n",
      "Iteration 3772 Loss: 0.07518705360602587\n",
      "Iteration 3773 Loss: 0.07518667622893506\n",
      "Iteration 3774 Loss: 0.07518629930552741\n",
      "Iteration 3775 Loss: 0.07518592283478301\n",
      "Iteration 3776 Loss: 0.07518554681568398\n",
      "Iteration 3777 Loss: 0.07518517124721509\n",
      "Iteration 3778 Loss: 0.0751847961283632\n",
      "Iteration 3779 Loss: 0.07518442145811763\n",
      "Iteration 3780 Loss: 0.07518404723547002\n",
      "Iteration 3781 Loss: 0.07518367345941435\n",
      "Iteration 3782 Loss: 0.0751833001289467\n",
      "Iteration 3783 Loss: 0.07518292724306572\n",
      "Iteration 3784 Loss: 0.0751825548007723\n",
      "Iteration 3785 Loss: 0.07518218280106961\n",
      "Iteration 3786 Loss: 0.075181811242963\n",
      "Iteration 3787 Loss: 0.07518144012546027\n",
      "Iteration 3788 Loss: 0.07518106944757146\n",
      "Iteration 3789 Loss: 0.0751806992083088\n",
      "Iteration 3790 Loss: 0.0751803294066869\n",
      "Iteration 3791 Loss: 0.07517996004172259\n",
      "Iteration 3792 Loss: 0.07517959111243493\n",
      "Iteration 3793 Loss: 0.07517922261784528\n",
      "Iteration 3794 Loss: 0.07517885455697718\n",
      "Iteration 3795 Loss: 0.07517848692885658\n",
      "Iteration 3796 Loss: 0.07517811973251151\n",
      "Iteration 3797 Loss: 0.07517775296697225\n",
      "Iteration 3798 Loss: 0.07517738663127145\n",
      "Iteration 3799 Loss: 0.0751770207244437\n",
      "Iteration 3800 Loss: 0.07517665524552622\n",
      "Iteration 3801 Loss: 0.07517629019355807\n",
      "Iteration 3802 Loss: 0.0751759255675807\n",
      "Iteration 3803 Loss: 0.07517556136663767\n",
      "Iteration 3804 Loss: 0.0751751975897749\n",
      "Iteration 3805 Loss: 0.0751748342360403\n",
      "Iteration 3806 Loss: 0.07517447130448418\n",
      "Iteration 3807 Loss: 0.07517410879415883\n",
      "Iteration 3808 Loss: 0.07517374670411883\n",
      "Iteration 3809 Loss: 0.075173385033421\n",
      "Iteration 3810 Loss: 0.07517302378112412\n",
      "Iteration 3811 Loss: 0.07517266294628937\n",
      "Iteration 3812 Loss: 0.07517230252797993\n",
      "Iteration 3813 Loss: 0.07517194252526119\n",
      "Iteration 3814 Loss: 0.07517158293720066\n",
      "Iteration 3815 Loss: 0.07517122376286806\n",
      "Iteration 3816 Loss: 0.07517086500133523\n",
      "Iteration 3817 Loss: 0.07517050665167595\n",
      "Iteration 3818 Loss: 0.0751701487129665\n",
      "Iteration 3819 Loss: 0.07516979118428505\n",
      "Iteration 3820 Loss: 0.0751694340647118\n",
      "Iteration 3821 Loss: 0.07516907735332932\n",
      "Iteration 3822 Loss: 0.07516872104922212\n",
      "Iteration 3823 Loss: 0.0751683651514768\n",
      "Iteration 3824 Loss: 0.07516800965918218\n",
      "Iteration 3825 Loss: 0.07516765457142909\n",
      "Iteration 3826 Loss: 0.07516729988731047\n",
      "Iteration 3827 Loss: 0.07516694560592135\n",
      "Iteration 3828 Loss: 0.07516659172635878\n",
      "Iteration 3829 Loss: 0.07516623824772198\n",
      "Iteration 3830 Loss: 0.07516588516911228\n",
      "Iteration 3831 Loss: 0.0751655324896329\n",
      "Iteration 3832 Loss: 0.07516518020838919\n",
      "Iteration 3833 Loss: 0.07516482832448874\n",
      "Iteration 3834 Loss: 0.07516447683704089\n",
      "Iteration 3835 Loss: 0.07516412574515725\n",
      "Iteration 3836 Loss: 0.0751637750479514\n",
      "Iteration 3837 Loss: 0.07516342474453892\n",
      "Iteration 3838 Loss: 0.07516307483403754\n",
      "Iteration 3839 Loss: 0.07516272531556686\n",
      "Iteration 3840 Loss: 0.07516237618824859\n",
      "Iteration 3841 Loss: 0.07516202745120655\n",
      "Iteration 3842 Loss: 0.07516167910356641\n",
      "Iteration 3843 Loss: 0.07516133114445589\n",
      "Iteration 3844 Loss: 0.07516098357300478\n",
      "Iteration 3845 Loss: 0.07516063638834487\n",
      "Iteration 3846 Loss: 0.07516028958960988\n",
      "Iteration 3847 Loss: 0.07515994317593551\n",
      "Iteration 3848 Loss: 0.07515959714645959\n",
      "Iteration 3849 Loss: 0.07515925150032172\n",
      "Iteration 3850 Loss: 0.07515890623666374\n",
      "Iteration 3851 Loss: 0.07515856135462916\n",
      "Iteration 3852 Loss: 0.07515821685336369\n",
      "Iteration 3853 Loss: 0.07515787273201498\n",
      "Iteration 3854 Loss: 0.07515752898973248\n",
      "Iteration 3855 Loss: 0.0751571856256678\n",
      "Iteration 3856 Loss: 0.0751568426389744\n",
      "Iteration 3857 Loss: 0.07515650002880761\n",
      "Iteration 3858 Loss: 0.07515615779432483\n",
      "Iteration 3859 Loss: 0.07515581593468537\n",
      "Iteration 3860 Loss: 0.07515547444905045\n",
      "Iteration 3861 Loss: 0.07515513333658322\n",
      "Iteration 3862 Loss: 0.07515479259644871\n",
      "Iteration 3863 Loss: 0.07515445222781399\n",
      "Iteration 3864 Loss: 0.07515411222984797\n",
      "Iteration 3865 Loss: 0.0751537726017214\n",
      "Iteration 3866 Loss: 0.07515343334260709\n",
      "Iteration 3867 Loss: 0.07515309445167959\n",
      "Iteration 3868 Loss: 0.07515275592811553\n",
      "Iteration 3869 Loss: 0.07515241777109324\n",
      "Iteration 3870 Loss: 0.07515207997979308\n",
      "Iteration 3871 Loss: 0.07515174255339721\n",
      "Iteration 3872 Loss: 0.07515140549108978\n",
      "Iteration 3873 Loss: 0.07515106879205669\n",
      "Iteration 3874 Loss: 0.07515073245548576\n",
      "Iteration 3875 Loss: 0.07515039648056669\n",
      "Iteration 3876 Loss: 0.07515006086649105\n",
      "Iteration 3877 Loss: 0.0751497256124522\n",
      "Iteration 3878 Loss: 0.07514939071764551\n",
      "Iteration 3879 Loss: 0.07514905618126803\n",
      "Iteration 3880 Loss: 0.07514872200251868\n",
      "Iteration 3881 Loss: 0.07514838818059835\n",
      "Iteration 3882 Loss: 0.07514805471470967\n",
      "Iteration 3883 Loss: 0.0751477216040571\n",
      "Iteration 3884 Loss: 0.07514738884784686\n",
      "Iteration 3885 Loss: 0.07514705644528721\n",
      "Iteration 3886 Loss: 0.07514672439558813\n",
      "Iteration 3887 Loss: 0.07514639269796125\n",
      "Iteration 3888 Loss: 0.07514606135162019\n",
      "Iteration 3889 Loss: 0.0751457303557804\n",
      "Iteration 3890 Loss: 0.07514539970965903\n",
      "Iteration 3891 Loss: 0.07514506941247509\n",
      "Iteration 3892 Loss: 0.07514473946344938\n",
      "Iteration 3893 Loss: 0.07514440986180432\n",
      "Iteration 3894 Loss: 0.07514408060676454\n",
      "Iteration 3895 Loss: 0.07514375169755601\n",
      "Iteration 3896 Loss: 0.07514342313340672\n",
      "Iteration 3897 Loss: 0.07514309491354632\n",
      "Iteration 3898 Loss: 0.07514276703720639\n",
      "Iteration 3899 Loss: 0.07514243950362\n",
      "Iteration 3900 Loss: 0.07514211231202236\n",
      "Iteration 3901 Loss: 0.07514178546165017\n",
      "Iteration 3902 Loss: 0.0751414589517418\n",
      "Iteration 3903 Loss: 0.07514113278153768\n",
      "Iteration 3904 Loss: 0.07514080695027983\n",
      "Iteration 3905 Loss: 0.07514048145721185\n",
      "Iteration 3906 Loss: 0.07514015630157944\n",
      "Iteration 3907 Loss: 0.07513983148262968\n",
      "Iteration 3908 Loss: 0.07513950699961161\n",
      "Iteration 3909 Loss: 0.0751391828517759\n",
      "Iteration 3910 Loss: 0.07513885903837494\n",
      "Iteration 3911 Loss: 0.07513853555866287\n",
      "Iteration 3912 Loss: 0.07513821241189558\n",
      "Iteration 3913 Loss: 0.07513788959733056\n",
      "Iteration 3914 Loss: 0.07513756711422714\n",
      "Iteration 3915 Loss: 0.07513724496184622\n",
      "Iteration 3916 Loss: 0.0751369231394505\n",
      "Iteration 3917 Loss: 0.07513660164630434\n",
      "Iteration 3918 Loss: 0.07513628048167374\n",
      "Iteration 3919 Loss: 0.07513595964482656\n",
      "Iteration 3920 Loss: 0.07513563913503206\n",
      "Iteration 3921 Loss: 0.07513531895156152\n",
      "Iteration 3922 Loss: 0.07513499909368755\n",
      "Iteration 3923 Loss: 0.0751346795606847\n",
      "Iteration 3924 Loss: 0.07513436035182906\n",
      "Iteration 3925 Loss: 0.07513404146639842\n",
      "Iteration 3926 Loss: 0.07513372290367223\n",
      "Iteration 3927 Loss: 0.07513340466293159\n",
      "Iteration 3928 Loss: 0.0751330867434592\n",
      "Iteration 3929 Loss: 0.07513276914453956\n",
      "Iteration 3930 Loss: 0.07513245186545855\n",
      "Iteration 3931 Loss: 0.07513213490550402\n",
      "Iteration 3932 Loss: 0.07513181826396523\n",
      "Iteration 3933 Loss: 0.07513150194013316\n",
      "Iteration 3934 Loss: 0.07513118593330036\n",
      "Iteration 3935 Loss: 0.07513087024276108\n",
      "Iteration 3936 Loss: 0.07513055486781119\n",
      "Iteration 3937 Loss: 0.07513023980774798\n",
      "Iteration 3938 Loss: 0.07512992506187069\n",
      "Iteration 3939 Loss: 0.07512961062948001\n",
      "Iteration 3940 Loss: 0.07512929650987814\n",
      "Iteration 3941 Loss: 0.07512898270236898\n",
      "Iteration 3942 Loss: 0.07512866920625808\n",
      "Iteration 3943 Loss: 0.07512835602085252\n",
      "Iteration 3944 Loss: 0.07512804314546095\n",
      "Iteration 3945 Loss: 0.0751277305793937\n",
      "Iteration 3946 Loss: 0.07512741832196251\n",
      "Iteration 3947 Loss: 0.07512710637248096\n",
      "Iteration 3948 Loss: 0.07512679473026398\n",
      "Iteration 3949 Loss: 0.07512648339462827\n",
      "Iteration 3950 Loss: 0.07512617236489186\n",
      "Iteration 3951 Loss: 0.07512586164037453\n",
      "Iteration 3952 Loss: 0.0751255512203976\n",
      "Iteration 3953 Loss: 0.07512524110428392\n",
      "Iteration 3954 Loss: 0.07512493129135792\n",
      "Iteration 3955 Loss: 0.07512462178094552\n",
      "Iteration 3956 Loss: 0.07512431257237426\n",
      "Iteration 3957 Loss: 0.07512400366497327\n",
      "Iteration 3958 Loss: 0.07512369505807304\n",
      "Iteration 3959 Loss: 0.07512338675100573\n",
      "Iteration 3960 Loss: 0.07512307874310509\n",
      "Iteration 3961 Loss: 0.07512277103370624\n",
      "Iteration 3962 Loss: 0.07512246362214603\n",
      "Iteration 3963 Loss: 0.07512215650776269\n",
      "Iteration 3964 Loss: 0.07512184968989591\n",
      "Iteration 3965 Loss: 0.07512154316788708\n",
      "Iteration 3966 Loss: 0.07512123694107897\n",
      "Iteration 3967 Loss: 0.07512093100881598\n",
      "Iteration 3968 Loss: 0.07512062537044388\n",
      "Iteration 3969 Loss: 0.07512032002531002\n",
      "Iteration 3970 Loss: 0.07512001497276324\n",
      "Iteration 3971 Loss: 0.07511971021215388\n",
      "Iteration 3972 Loss: 0.07511940574283378\n",
      "Iteration 3973 Loss: 0.07511910156415622\n",
      "Iteration 3974 Loss: 0.07511879767547607\n",
      "Iteration 3975 Loss: 0.0751184940761496\n",
      "Iteration 3976 Loss: 0.07511819076553447\n",
      "Iteration 3977 Loss: 0.07511788774299008\n",
      "Iteration 3978 Loss: 0.07511758500787705\n",
      "Iteration 3979 Loss: 0.07511728255955766\n",
      "Iteration 3980 Loss: 0.07511698039739545\n",
      "Iteration 3981 Loss: 0.07511667852075563\n",
      "Iteration 3982 Loss: 0.07511637692900465\n",
      "Iteration 3983 Loss: 0.07511607562151071\n",
      "Iteration 3984 Loss: 0.07511577459764318\n",
      "Iteration 3985 Loss: 0.07511547385677304\n",
      "Iteration 3986 Loss: 0.07511517339827264\n",
      "Iteration 3987 Loss: 0.07511487322151582\n",
      "Iteration 3988 Loss: 0.07511457332587786\n",
      "Iteration 3989 Loss: 0.07511427371073542\n",
      "Iteration 3990 Loss: 0.0751139743754666\n",
      "Iteration 3991 Loss: 0.07511367531945104\n",
      "Iteration 3992 Loss: 0.0751133765420697\n",
      "Iteration 3993 Loss: 0.07511307804270503\n",
      "Iteration 3994 Loss: 0.07511277982074074\n",
      "Iteration 3995 Loss: 0.07511248187556221\n",
      "Iteration 3996 Loss: 0.07511218420655599\n",
      "Iteration 3997 Loss: 0.07511188681311025\n",
      "Iteration 3998 Loss: 0.0751115896946144\n",
      "Iteration 3999 Loss: 0.07511129285045928\n",
      "Iteration 4000 Loss: 0.07511099628003726\n",
      "Iteration 4001 Loss: 0.075110699982742\n",
      "Iteration 4002 Loss: 0.07511040395796849\n",
      "Iteration 4003 Loss: 0.07511010820511324\n",
      "Iteration 4004 Loss: 0.07510981272357416\n",
      "Iteration 4005 Loss: 0.07510951751275033\n",
      "Iteration 4006 Loss: 0.07510922257204256\n",
      "Iteration 4007 Loss: 0.07510892790085268\n",
      "Iteration 4008 Loss: 0.07510863349858411\n",
      "Iteration 4009 Loss: 0.0751083393646416\n",
      "Iteration 4010 Loss: 0.07510804549843124\n",
      "Iteration 4011 Loss: 0.0751077518993605\n",
      "Iteration 4012 Loss: 0.0751074585668382\n",
      "Iteration 4013 Loss: 0.07510716550027452\n",
      "Iteration 4014 Loss: 0.07510687269908103\n",
      "Iteration 4015 Loss: 0.0751065801626707\n",
      "Iteration 4016 Loss: 0.07510628789045766\n",
      "Iteration 4017 Loss: 0.07510599588185751\n",
      "Iteration 4018 Loss: 0.0751057041362873\n",
      "Iteration 4019 Loss: 0.0751054126531652\n",
      "Iteration 4020 Loss: 0.07510512143191081\n",
      "Iteration 4021 Loss: 0.07510483047194522\n",
      "Iteration 4022 Loss: 0.07510453977269058\n",
      "Iteration 4023 Loss: 0.07510424933357052\n",
      "Iteration 4024 Loss: 0.07510395915401005\n",
      "Iteration 4025 Loss: 0.07510366923343528\n",
      "Iteration 4026 Loss: 0.07510337957127394\n",
      "Iteration 4027 Loss: 0.07510309016695481\n",
      "Iteration 4028 Loss: 0.07510280101990816\n",
      "Iteration 4029 Loss: 0.07510251212956542\n",
      "Iteration 4030 Loss: 0.07510222349535949\n",
      "Iteration 4031 Loss: 0.07510193511672451\n",
      "Iteration 4032 Loss: 0.07510164699309577\n",
      "Iteration 4033 Loss: 0.07510135912391017\n",
      "Iteration 4034 Loss: 0.07510107150860554\n",
      "Iteration 4035 Loss: 0.07510078414662136\n",
      "Iteration 4036 Loss: 0.07510049703739807\n",
      "Iteration 4037 Loss: 0.07510021018037767\n",
      "Iteration 4038 Loss: 0.07509992357500325\n",
      "Iteration 4039 Loss: 0.07509963722071933\n",
      "Iteration 4040 Loss: 0.07509935111697157\n",
      "Iteration 4041 Loss: 0.07509906526320693\n",
      "Iteration 4042 Loss: 0.07509877965887381\n",
      "Iteration 4043 Loss: 0.07509849430342158\n",
      "Iteration 4044 Loss: 0.07509820919630115\n",
      "Iteration 4045 Loss: 0.07509792433696454\n",
      "Iteration 4046 Loss: 0.07509763972486504\n",
      "Iteration 4047 Loss: 0.07509735535945736\n",
      "Iteration 4048 Loss: 0.07509707124019716\n",
      "Iteration 4049 Loss: 0.07509678736654161\n",
      "Iteration 4050 Loss: 0.07509650373794907\n",
      "Iteration 4051 Loss: 0.07509622035387907\n",
      "Iteration 4052 Loss: 0.0750959372137924\n",
      "Iteration 4053 Loss: 0.07509565431715112\n",
      "Iteration 4054 Loss: 0.07509537166341865\n",
      "Iteration 4055 Loss: 0.07509508925205935\n",
      "Iteration 4056 Loss: 0.07509480708253906\n",
      "Iteration 4057 Loss: 0.0750945251543247\n",
      "Iteration 4058 Loss: 0.07509424346688463\n",
      "Iteration 4059 Loss: 0.07509396201968808\n",
      "Iteration 4060 Loss: 0.07509368081220583\n",
      "Iteration 4061 Loss: 0.0750933998439097\n",
      "Iteration 4062 Loss: 0.07509311911427281\n",
      "Iteration 4063 Loss: 0.07509283862276946\n",
      "Iteration 4064 Loss: 0.07509255836887503\n",
      "Iteration 4065 Loss: 0.07509227835206642\n",
      "Iteration 4066 Loss: 0.07509199857182139\n",
      "Iteration 4067 Loss: 0.07509171902761913\n",
      "Iteration 4068 Loss: 0.07509143971893986\n",
      "Iteration 4069 Loss: 0.07509116064526515\n",
      "Iteration 4070 Loss: 0.07509088180607768\n",
      "Iteration 4071 Loss: 0.07509060320086132\n",
      "Iteration 4072 Loss: 0.07509032482910119\n",
      "Iteration 4073 Loss: 0.07509004669028344\n",
      "Iteration 4074 Loss: 0.07508976878389555\n",
      "Iteration 4075 Loss: 0.0750894911094262\n",
      "Iteration 4076 Loss: 0.07508921366636503\n",
      "Iteration 4077 Loss: 0.07508893645420314\n",
      "Iteration 4078 Loss: 0.07508865947243257\n",
      "Iteration 4079 Loss: 0.07508838272054662\n",
      "Iteration 4080 Loss: 0.0750881061980398\n",
      "Iteration 4081 Loss: 0.07508782990440765\n",
      "Iteration 4082 Loss: 0.07508755383914707\n",
      "Iteration 4083 Loss: 0.07508727800175588\n",
      "Iteration 4084 Loss: 0.07508700239173323\n",
      "Iteration 4085 Loss: 0.07508672700857935\n",
      "Iteration 4086 Loss: 0.07508645185179563\n",
      "Iteration 4087 Loss: 0.0750861769208846\n",
      "Iteration 4088 Loss: 0.07508590221534994\n",
      "Iteration 4089 Loss: 0.07508562773469643\n",
      "Iteration 4090 Loss: 0.07508535347843012\n",
      "Iteration 4091 Loss: 0.07508507944605805\n",
      "Iteration 4092 Loss: 0.07508480563708846\n",
      "Iteration 4093 Loss: 0.07508453205103069\n",
      "Iteration 4094 Loss: 0.07508425868739518\n",
      "Iteration 4095 Loss: 0.07508398554569365\n",
      "Iteration 4096 Loss: 0.07508371262543878\n",
      "Iteration 4097 Loss: 0.07508343992614437\n",
      "Iteration 4098 Loss: 0.07508316744732546\n",
      "Iteration 4099 Loss: 0.07508289518849806\n",
      "Iteration 4100 Loss: 0.07508262314917943\n",
      "Iteration 4101 Loss: 0.07508235132888784\n",
      "Iteration 4102 Loss: 0.07508207972714274\n",
      "Iteration 4103 Loss: 0.0750818083434646\n",
      "Iteration 4104 Loss: 0.07508153717737506\n",
      "Iteration 4105 Loss: 0.07508126622839684\n",
      "Iteration 4106 Loss: 0.07508099549605371\n",
      "Iteration 4107 Loss: 0.07508072497987063\n",
      "Iteration 4108 Loss: 0.07508045467937366\n",
      "Iteration 4109 Loss: 0.07508018459408976\n",
      "Iteration 4110 Loss: 0.0750799147235471\n",
      "Iteration 4111 Loss: 0.07507964506727509\n",
      "Iteration 4112 Loss: 0.07507937562480396\n",
      "Iteration 4113 Loss: 0.07507910639566513\n",
      "Iteration 4114 Loss: 0.07507883737939114\n",
      "Iteration 4115 Loss: 0.07507856857551555\n",
      "Iteration 4116 Loss: 0.07507829998357293\n",
      "Iteration 4117 Loss: 0.07507803160309916\n",
      "Iteration 4118 Loss: 0.07507776343363086\n",
      "Iteration 4119 Loss: 0.07507749547470599\n",
      "Iteration 4120 Loss: 0.07507722772586341\n",
      "Iteration 4121 Loss: 0.07507696018664303\n",
      "Iteration 4122 Loss: 0.07507669285658598\n",
      "Iteration 4123 Loss: 0.07507642573523424\n",
      "Iteration 4124 Loss: 0.0750761588221311\n",
      "Iteration 4125 Loss: 0.07507589211682046\n",
      "Iteration 4126 Loss: 0.07507562561884779\n",
      "Iteration 4127 Loss: 0.0750753593277593\n",
      "Iteration 4128 Loss: 0.07507509324310227\n",
      "Iteration 4129 Loss: 0.07507482736442508\n",
      "Iteration 4130 Loss: 0.0750745616912772\n",
      "Iteration 4131 Loss: 0.07507429622320891\n",
      "Iteration 4132 Loss: 0.07507403095977169\n",
      "Iteration 4133 Loss: 0.07507376590051822\n",
      "Iteration 4134 Loss: 0.07507350104500178\n",
      "Iteration 4135 Loss: 0.07507323639277705\n",
      "Iteration 4136 Loss: 0.07507297194339961\n",
      "Iteration 4137 Loss: 0.07507270769642602\n",
      "Iteration 4138 Loss: 0.07507244365141388\n",
      "Iteration 4139 Loss: 0.07507217980792183\n",
      "Iteration 4140 Loss: 0.07507191616550954\n",
      "Iteration 4141 Loss: 0.07507165272373759\n",
      "Iteration 4142 Loss: 0.07507138948216771\n",
      "Iteration 4143 Loss: 0.07507112644036257\n",
      "Iteration 4144 Loss: 0.07507086359788577\n",
      "Iteration 4145 Loss: 0.0750706009543021\n",
      "Iteration 4146 Loss: 0.07507033850917715\n",
      "Iteration 4147 Loss: 0.0750700762620776\n",
      "Iteration 4148 Loss: 0.07506981421257122\n",
      "Iteration 4149 Loss: 0.07506955236022654\n",
      "Iteration 4150 Loss: 0.0750692907046133\n",
      "Iteration 4151 Loss: 0.0750690292453021\n",
      "Iteration 4152 Loss: 0.07506876798186468\n",
      "Iteration 4153 Loss: 0.07506850691387343\n",
      "Iteration 4154 Loss: 0.07506824604090219\n",
      "Iteration 4155 Loss: 0.07506798536252547\n",
      "Iteration 4156 Loss: 0.0750677248783187\n",
      "Iteration 4157 Loss: 0.07506746458785854\n",
      "Iteration 4158 Loss: 0.07506720449072252\n",
      "Iteration 4159 Loss: 0.07506694458648899\n",
      "Iteration 4160 Loss: 0.07506668487473746\n",
      "Iteration 4161 Loss: 0.07506642535504834\n",
      "Iteration 4162 Loss: 0.07506616602700303\n",
      "Iteration 4163 Loss: 0.07506590689018379\n",
      "Iteration 4164 Loss: 0.07506564794417396\n",
      "Iteration 4165 Loss: 0.0750653891885578\n",
      "Iteration 4166 Loss: 0.07506513062292056\n",
      "Iteration 4167 Loss: 0.07506487224684827\n",
      "Iteration 4168 Loss: 0.07506461405992815\n",
      "Iteration 4169 Loss: 0.07506435606174826\n",
      "Iteration 4170 Loss: 0.07506409825189755\n",
      "Iteration 4171 Loss: 0.07506384062996595\n",
      "Iteration 4172 Loss: 0.07506358319554447\n",
      "Iteration 4173 Loss: 0.07506332594822483\n",
      "Iteration 4174 Loss: 0.07506306888759988\n",
      "Iteration 4175 Loss: 0.0750628120132633\n",
      "Iteration 4176 Loss: 0.0750625553248097\n",
      "Iteration 4177 Loss: 0.07506229882183466\n",
      "Iteration 4178 Loss: 0.07506204250393472\n",
      "Iteration 4179 Loss: 0.07506178637070723\n",
      "Iteration 4180 Loss: 0.07506153042175065\n",
      "Iteration 4181 Loss: 0.07506127465666419\n",
      "Iteration 4182 Loss: 0.07506101907504806\n",
      "Iteration 4183 Loss: 0.07506076367650331\n",
      "Iteration 4184 Loss: 0.07506050846063209\n",
      "Iteration 4185 Loss: 0.07506025342703725\n",
      "Iteration 4186 Loss: 0.07505999857532267\n",
      "Iteration 4187 Loss: 0.07505974390509319\n",
      "Iteration 4188 Loss: 0.07505948941595442\n",
      "Iteration 4189 Loss: 0.0750592351075129\n",
      "Iteration 4190 Loss: 0.07505898097937619\n",
      "Iteration 4191 Loss: 0.07505872703115266\n",
      "Iteration 4192 Loss: 0.07505847326245155\n",
      "Iteration 4193 Loss: 0.07505821967288309\n",
      "Iteration 4194 Loss: 0.07505796626205838\n",
      "Iteration 4195 Loss: 0.07505771302958937\n",
      "Iteration 4196 Loss: 0.07505745997508892\n",
      "Iteration 4197 Loss: 0.07505720709817078\n",
      "Iteration 4198 Loss: 0.07505695439844959\n",
      "Iteration 4199 Loss: 0.0750567018755409\n",
      "Iteration 4200 Loss: 0.07505644952906107\n",
      "Iteration 4201 Loss: 0.07505619735862747\n",
      "Iteration 4202 Loss: 0.07505594536385823\n",
      "Iteration 4203 Loss: 0.0750556935443724\n",
      "Iteration 4204 Loss: 0.07505544189978987\n",
      "Iteration 4205 Loss: 0.0750551904297315\n",
      "Iteration 4206 Loss: 0.07505493913381889\n",
      "Iteration 4207 Loss: 0.07505468801167471\n",
      "Iteration 4208 Loss: 0.07505443706292217\n",
      "Iteration 4209 Loss: 0.07505418628718569\n",
      "Iteration 4210 Loss: 0.07505393568409034\n",
      "Iteration 4211 Loss: 0.07505368525326213\n",
      "Iteration 4212 Loss: 0.07505343499432787\n",
      "Iteration 4213 Loss: 0.0750531849069153\n",
      "Iteration 4214 Loss: 0.07505293499065306\n",
      "Iteration 4215 Loss: 0.07505268524517046\n",
      "Iteration 4216 Loss: 0.07505243567009781\n",
      "Iteration 4217 Loss: 0.07505218626506624\n",
      "Iteration 4218 Loss: 0.07505193702970771\n",
      "Iteration 4219 Loss: 0.075051687963655\n",
      "Iteration 4220 Loss: 0.07505143906654187\n",
      "Iteration 4221 Loss: 0.07505119033800271\n",
      "Iteration 4222 Loss: 0.07505094177767288\n",
      "Iteration 4223 Loss: 0.07505069338518856\n",
      "Iteration 4224 Loss: 0.07505044516018675\n",
      "Iteration 4225 Loss: 0.07505019710230534\n",
      "Iteration 4226 Loss: 0.07504994921118299\n",
      "Iteration 4227 Loss: 0.0750497014864592\n",
      "Iteration 4228 Loss: 0.07504945392777426\n",
      "Iteration 4229 Loss: 0.07504920653476937\n",
      "Iteration 4230 Loss: 0.07504895930708654\n",
      "Iteration 4231 Loss: 0.07504871224436853\n",
      "Iteration 4232 Loss: 0.075048465346259\n",
      "Iteration 4233 Loss: 0.07504821861240238\n",
      "Iteration 4234 Loss: 0.07504797204244393\n",
      "Iteration 4235 Loss: 0.07504772563602972\n",
      "Iteration 4236 Loss: 0.07504747939280673\n",
      "Iteration 4237 Loss: 0.07504723331242254\n",
      "Iteration 4238 Loss: 0.07504698739452574\n",
      "Iteration 4239 Loss: 0.07504674163876562\n",
      "Iteration 4240 Loss: 0.0750464960447923\n",
      "Iteration 4241 Loss: 0.07504625061225667\n",
      "Iteration 4242 Loss: 0.07504600534081059\n",
      "Iteration 4243 Loss: 0.07504576023010642\n",
      "Iteration 4244 Loss: 0.07504551527979768\n",
      "Iteration 4245 Loss: 0.07504527048953832\n",
      "Iteration 4246 Loss: 0.07504502585898339\n",
      "Iteration 4247 Loss: 0.0750447813877884\n",
      "Iteration 4248 Loss: 0.07504453707561007\n",
      "Iteration 4249 Loss: 0.07504429292210564\n",
      "Iteration 4250 Loss: 0.07504404892693314\n",
      "Iteration 4251 Loss: 0.07504380508975143\n",
      "Iteration 4252 Loss: 0.07504356141022021\n",
      "Iteration 4253 Loss: 0.0750433178879999\n",
      "Iteration 4254 Loss: 0.07504307452275168\n",
      "Iteration 4255 Loss: 0.07504283131413755\n",
      "Iteration 4256 Loss: 0.0750425882618202\n",
      "Iteration 4257 Loss: 0.07504234536546336\n",
      "Iteration 4258 Loss: 0.0750421026247312\n",
      "Iteration 4259 Loss: 0.07504186003928881\n",
      "Iteration 4260 Loss: 0.07504161760880208\n",
      "Iteration 4261 Loss: 0.0750413753329376\n",
      "Iteration 4262 Loss: 0.07504113321136271\n",
      "Iteration 4263 Loss: 0.0750408912437457\n",
      "Iteration 4264 Loss: 0.07504064942975533\n",
      "Iteration 4265 Loss: 0.07504040776906132\n",
      "Iteration 4266 Loss: 0.07504016626133417\n",
      "Iteration 4267 Loss: 0.075039924906245\n",
      "Iteration 4268 Loss: 0.07503968370346577\n",
      "Iteration 4269 Loss: 0.07503944265266912\n",
      "Iteration 4270 Loss: 0.07503920175352863\n",
      "Iteration 4271 Loss: 0.07503896100571837\n",
      "Iteration 4272 Loss: 0.07503872040891332\n",
      "Iteration 4273 Loss: 0.07503847996278923\n",
      "Iteration 4274 Loss: 0.07503823966702247\n",
      "Iteration 4275 Loss: 0.07503799952129023\n",
      "Iteration 4276 Loss: 0.0750377595252705\n",
      "Iteration 4277 Loss: 0.07503751967864185\n",
      "Iteration 4278 Loss: 0.07503727998108374\n",
      "Iteration 4279 Loss: 0.07503704043227624\n",
      "Iteration 4280 Loss: 0.07503680103190033\n",
      "Iteration 4281 Loss: 0.07503656177963755\n",
      "Iteration 4282 Loss: 0.07503632267517016\n",
      "Iteration 4283 Loss: 0.07503608371818137\n",
      "Iteration 4284 Loss: 0.07503584490835488\n",
      "Iteration 4285 Loss: 0.07503560624537524\n",
      "Iteration 4286 Loss: 0.07503536772892769\n",
      "Iteration 4287 Loss: 0.0750351293586982\n",
      "Iteration 4288 Loss: 0.07503489113437346\n",
      "Iteration 4289 Loss: 0.0750346530556408\n",
      "Iteration 4290 Loss: 0.07503441512218852\n",
      "Iteration 4291 Loss: 0.07503417733370525\n",
      "Iteration 4292 Loss: 0.07503393968988072\n",
      "Iteration 4293 Loss: 0.07503370219040512\n",
      "Iteration 4294 Loss: 0.07503346483496937\n",
      "Iteration 4295 Loss: 0.07503322762326528\n",
      "Iteration 4296 Loss: 0.07503299055498522\n",
      "Iteration 4297 Loss: 0.0750327536298222\n",
      "Iteration 4298 Loss: 0.07503251684747017\n",
      "Iteration 4299 Loss: 0.07503228020762354\n",
      "Iteration 4300 Loss: 0.07503204370997758\n",
      "Iteration 4301 Loss: 0.07503180735422814\n",
      "Iteration 4302 Loss: 0.07503157114007189\n",
      "Iteration 4303 Loss: 0.07503133506720616\n",
      "Iteration 4304 Loss: 0.07503109913532886\n",
      "Iteration 4305 Loss: 0.07503086334413873\n",
      "Iteration 4306 Loss: 0.07503062769333521\n",
      "Iteration 4307 Loss: 0.07503039218261835\n",
      "Iteration 4308 Loss: 0.07503015681168884\n",
      "Iteration 4309 Loss: 0.07502992158024827\n",
      "Iteration 4310 Loss: 0.07502968648799872\n",
      "Iteration 4311 Loss: 0.07502945153464302\n",
      "Iteration 4312 Loss: 0.07502921671988465\n",
      "Iteration 4313 Loss: 0.07502898204342782\n",
      "Iteration 4314 Loss: 0.07502874750497743\n",
      "Iteration 4315 Loss: 0.075028513104239\n",
      "Iteration 4316 Loss: 0.07502827884091863\n",
      "Iteration 4317 Loss: 0.07502804471472346\n",
      "Iteration 4318 Loss: 0.07502781072536084\n",
      "Iteration 4319 Loss: 0.07502757687253916\n",
      "Iteration 4320 Loss: 0.07502734315596726\n",
      "Iteration 4321 Loss: 0.07502710957535474\n",
      "Iteration 4322 Loss: 0.07502687613041187\n",
      "Iteration 4323 Loss: 0.07502664282084945\n",
      "Iteration 4324 Loss: 0.07502640964637916\n",
      "Iteration 4325 Loss: 0.07502617660671325\n",
      "Iteration 4326 Loss: 0.07502594370156457\n",
      "Iteration 4327 Loss: 0.07502571093064671\n",
      "Iteration 4328 Loss: 0.07502547829367381\n",
      "Iteration 4329 Loss: 0.07502524579036082\n",
      "Iteration 4330 Loss: 0.07502501342042325\n",
      "Iteration 4331 Loss: 0.07502478118357729\n",
      "Iteration 4332 Loss: 0.07502454907953977\n",
      "Iteration 4333 Loss: 0.07502431710802812\n",
      "Iteration 4334 Loss: 0.07502408526876053\n",
      "Iteration 4335 Loss: 0.07502385356145577\n",
      "Iteration 4336 Loss: 0.07502362198583323\n",
      "Iteration 4337 Loss: 0.07502339054161301\n",
      "Iteration 4338 Loss: 0.07502315922851578\n",
      "Iteration 4339 Loss: 0.07502292804626295\n",
      "Iteration 4340 Loss: 0.07502269699457649\n",
      "Iteration 4341 Loss: 0.075022466073179\n",
      "Iteration 4342 Loss: 0.07502223528179376\n",
      "Iteration 4343 Loss: 0.0750220046201447\n",
      "Iteration 4344 Loss: 0.07502177408795634\n",
      "Iteration 4345 Loss: 0.07502154368495387\n",
      "Iteration 4346 Loss: 0.075021313410863\n",
      "Iteration 4347 Loss: 0.07502108326541021\n",
      "Iteration 4348 Loss: 0.07502085324832261\n",
      "Iteration 4349 Loss: 0.07502062335932788\n",
      "Iteration 4350 Loss: 0.07502039359815424\n",
      "Iteration 4351 Loss: 0.07502016396453073\n",
      "Iteration 4352 Loss: 0.07501993445818683\n",
      "Iteration 4353 Loss: 0.07501970507885272\n",
      "Iteration 4354 Loss: 0.07501947582625926\n",
      "Iteration 4355 Loss: 0.07501924670013778\n",
      "Iteration 4356 Loss: 0.07501901770022033\n",
      "Iteration 4357 Loss: 0.07501878882623961\n",
      "Iteration 4358 Loss: 0.07501856007792881\n",
      "Iteration 4359 Loss: 0.0750183314550219\n",
      "Iteration 4360 Loss: 0.07501810295725328\n",
      "Iteration 4361 Loss: 0.07501787458435805\n",
      "Iteration 4362 Loss: 0.07501764633607193\n",
      "Iteration 4363 Loss: 0.07501741821213118\n",
      "Iteration 4364 Loss: 0.0750171902122728\n",
      "Iteration 4365 Loss: 0.07501696233623421\n",
      "Iteration 4366 Loss: 0.07501673458375369\n",
      "Iteration 4367 Loss: 0.07501650695456977\n",
      "Iteration 4368 Loss: 0.07501627944842186\n",
      "Iteration 4369 Loss: 0.07501605206504987\n",
      "Iteration 4370 Loss: 0.07501582480419433\n",
      "Iteration 4371 Loss: 0.07501559766559637\n",
      "Iteration 4372 Loss: 0.07501537064899763\n",
      "Iteration 4373 Loss: 0.07501514375414045\n",
      "Iteration 4374 Loss: 0.0750149169807677\n",
      "Iteration 4375 Loss: 0.07501469032862292\n",
      "Iteration 4376 Loss: 0.07501446379745011\n",
      "Iteration 4377 Loss: 0.07501423738699402\n",
      "Iteration 4378 Loss: 0.07501401109699976\n",
      "Iteration 4379 Loss: 0.07501378492721325\n",
      "Iteration 4380 Loss: 0.07501355887738088\n",
      "Iteration 4381 Loss: 0.07501333294724963\n",
      "Iteration 4382 Loss: 0.0750131071365671\n",
      "Iteration 4383 Loss: 0.07501288144508146\n",
      "Iteration 4384 Loss: 0.07501265587254141\n",
      "Iteration 4385 Loss: 0.07501243041869628\n",
      "Iteration 4386 Loss: 0.07501220508329591\n",
      "Iteration 4387 Loss: 0.0750119798660908\n",
      "Iteration 4388 Loss: 0.075011754766832\n",
      "Iteration 4389 Loss: 0.07501152978527108\n",
      "Iteration 4390 Loss: 0.07501130492116022\n",
      "Iteration 4391 Loss: 0.07501108017425215\n",
      "Iteration 4392 Loss: 0.07501085554430022\n",
      "Iteration 4393 Loss: 0.07501063103105829\n",
      "Iteration 4394 Loss: 0.07501040663428075\n",
      "Iteration 4395 Loss: 0.0750101823537227\n",
      "Iteration 4396 Loss: 0.07500995818913961\n",
      "Iteration 4397 Loss: 0.07500973414028769\n",
      "Iteration 4398 Loss: 0.07500951020692362\n",
      "Iteration 4399 Loss: 0.07500928638880458\n",
      "Iteration 4400 Loss: 0.07500906268568844\n",
      "Iteration 4401 Loss: 0.07500883909733347\n",
      "Iteration 4402 Loss: 0.07500861562349874\n",
      "Iteration 4403 Loss: 0.07500839226394355\n",
      "Iteration 4404 Loss: 0.07500816901842806\n",
      "Iteration 4405 Loss: 0.07500794588671274\n",
      "Iteration 4406 Loss: 0.07500772286855877\n",
      "Iteration 4407 Loss: 0.07500749996372778\n",
      "Iteration 4408 Loss: 0.0750072771719821\n",
      "Iteration 4409 Loss: 0.07500705449308429\n",
      "Iteration 4410 Loss: 0.0750068319267978\n",
      "Iteration 4411 Loss: 0.07500660947288644\n",
      "Iteration 4412 Loss: 0.07500638713111464\n",
      "Iteration 4413 Loss: 0.07500616490124726\n",
      "Iteration 4414 Loss: 0.07500594278304983\n",
      "Iteration 4415 Loss: 0.07500572077628835\n",
      "Iteration 4416 Loss: 0.07500549888072931\n",
      "Iteration 4417 Loss: 0.07500527709613994\n",
      "Iteration 4418 Loss: 0.07500505542228772\n",
      "Iteration 4419 Loss: 0.07500483385894077\n",
      "Iteration 4420 Loss: 0.07500461240586793\n",
      "Iteration 4421 Loss: 0.07500439106283836\n",
      "Iteration 4422 Loss: 0.07500416982962166\n",
      "Iteration 4423 Loss: 0.0750039487059883\n",
      "Iteration 4424 Loss: 0.07500372769170897\n",
      "Iteration 4425 Loss: 0.07500350678655503\n",
      "Iteration 4426 Loss: 0.0750032859902983\n",
      "Iteration 4427 Loss: 0.0750030653027112\n",
      "Iteration 4428 Loss: 0.07500284472356662\n",
      "Iteration 4429 Loss: 0.07500262425263791\n",
      "Iteration 4430 Loss: 0.07500240388969914\n",
      "Iteration 4431 Loss: 0.07500218363452464\n",
      "Iteration 4432 Loss: 0.07500196348688945\n",
      "Iteration 4433 Loss: 0.075001743446569\n",
      "Iteration 4434 Loss: 0.07500152351333936\n",
      "Iteration 4435 Loss: 0.075001303686977\n",
      "Iteration 4436 Loss: 0.07500108396725905\n",
      "Iteration 4437 Loss: 0.07500086435396297\n",
      "Iteration 4438 Loss: 0.0750006448468668\n",
      "Iteration 4439 Loss: 0.07500042544574917\n",
      "Iteration 4440 Loss: 0.07500020615038908\n",
      "Iteration 4441 Loss: 0.0749999869605662\n",
      "Iteration 4442 Loss: 0.07499976787606051\n",
      "Iteration 4443 Loss: 0.07499954889665268\n",
      "Iteration 4444 Loss: 0.07499933002212378\n",
      "Iteration 4445 Loss: 0.07499911125225542\n",
      "Iteration 4446 Loss: 0.07499889258682965\n",
      "Iteration 4447 Loss: 0.07499867402562911\n",
      "Iteration 4448 Loss: 0.07499845556843687\n",
      "Iteration 4449 Loss: 0.07499823721503657\n",
      "Iteration 4450 Loss: 0.0749980189652123\n",
      "Iteration 4451 Loss: 0.07499780081874852\n",
      "Iteration 4452 Loss: 0.0749975827754305\n",
      "Iteration 4453 Loss: 0.07499736483504368\n",
      "Iteration 4454 Loss: 0.07499714699737416\n",
      "Iteration 4455 Loss: 0.07499692926220855\n",
      "Iteration 4456 Loss: 0.07499671162933383\n",
      "Iteration 4457 Loss: 0.07499649409853756\n",
      "Iteration 4458 Loss: 0.07499627666960776\n",
      "Iteration 4459 Loss: 0.07499605934233286\n",
      "Iteration 4460 Loss: 0.07499584211650201\n",
      "Iteration 4461 Loss: 0.07499562499190461\n",
      "Iteration 4462 Loss: 0.07499540796833058\n",
      "Iteration 4463 Loss: 0.07499519104557044\n",
      "Iteration 4464 Loss: 0.07499497422341506\n",
      "Iteration 4465 Loss: 0.07499475750165589\n",
      "Iteration 4466 Loss: 0.07499454088008482\n",
      "Iteration 4467 Loss: 0.0749943243584941\n",
      "Iteration 4468 Loss: 0.07499410793667667\n",
      "Iteration 4469 Loss: 0.07499389161442585\n",
      "Iteration 4470 Loss: 0.07499367539153531\n",
      "Iteration 4471 Loss: 0.0749934592677995\n",
      "Iteration 4472 Loss: 0.07499324324301294\n",
      "Iteration 4473 Loss: 0.07499302731697102\n",
      "Iteration 4474 Loss: 0.07499281148946928\n",
      "Iteration 4475 Loss: 0.07499259576030401\n",
      "Iteration 4476 Loss: 0.07499238012927159\n",
      "Iteration 4477 Loss: 0.07499216459616928\n",
      "Iteration 4478 Loss: 0.07499194916079455\n",
      "Iteration 4479 Loss: 0.07499173382294543\n",
      "Iteration 4480 Loss: 0.07499151858242037\n",
      "Iteration 4481 Loss: 0.07499130343901833\n",
      "Iteration 4482 Loss: 0.07499108839253871\n",
      "Iteration 4483 Loss: 0.07499087344278134\n",
      "Iteration 4484 Loss: 0.07499065858954648\n",
      "Iteration 4485 Loss: 0.074990443832635\n",
      "Iteration 4486 Loss: 0.07499022917184803\n",
      "Iteration 4487 Loss: 0.07499001460698732\n",
      "Iteration 4488 Loss: 0.07498980013785501\n",
      "Iteration 4489 Loss: 0.07498958576425371\n",
      "Iteration 4490 Loss: 0.0749893714859864\n",
      "Iteration 4491 Loss: 0.0749891573028566\n",
      "Iteration 4492 Loss: 0.07498894321466827\n",
      "Iteration 4493 Loss: 0.07498872922122582\n",
      "Iteration 4494 Loss: 0.07498851532233405\n",
      "Iteration 4495 Loss: 0.07498830151779833\n",
      "Iteration 4496 Loss: 0.07498808780742433\n",
      "Iteration 4497 Loss: 0.07498787419101824\n",
      "Iteration 4498 Loss: 0.07498766066838673\n",
      "Iteration 4499 Loss: 0.07498744723933688\n",
      "Iteration 4500 Loss: 0.0749872339036762\n",
      "Iteration 4501 Loss: 0.07498702066121261\n",
      "Iteration 4502 Loss: 0.07498680751175456\n",
      "Iteration 4503 Loss: 0.07498659445511083\n",
      "Iteration 4504 Loss: 0.07498638149109074\n",
      "Iteration 4505 Loss: 0.07498616861950404\n",
      "Iteration 4506 Loss: 0.0749859558401608\n",
      "Iteration 4507 Loss: 0.07498574315287165\n",
      "Iteration 4508 Loss: 0.07498553055744764\n",
      "Iteration 4509 Loss: 0.07498531805370019\n",
      "Iteration 4510 Loss: 0.07498510564144119\n",
      "Iteration 4511 Loss: 0.07498489332048294\n",
      "Iteration 4512 Loss: 0.07498468109063823\n",
      "Iteration 4513 Loss: 0.07498446895172022\n",
      "Iteration 4514 Loss: 0.0749842569035425\n",
      "Iteration 4515 Loss: 0.07498404494591918\n",
      "Iteration 4516 Loss: 0.07498383307866464\n",
      "Iteration 4517 Loss: 0.0749836213015938\n",
      "Iteration 4518 Loss: 0.07498340961452193\n",
      "Iteration 4519 Loss: 0.07498319801726486\n",
      "Iteration 4520 Loss: 0.07498298650963864\n",
      "Iteration 4521 Loss: 0.07498277509145994\n",
      "Iteration 4522 Loss: 0.07498256376254568\n",
      "Iteration 4523 Loss: 0.07498235252271339\n",
      "Iteration 4524 Loss: 0.07498214137178086\n",
      "Iteration 4525 Loss: 0.07498193030956621\n",
      "Iteration 4526 Loss: 0.07498171933588833\n",
      "Iteration 4527 Loss: 0.0749815084505662\n",
      "Iteration 4528 Loss: 0.07498129765341935\n",
      "Iteration 4529 Loss: 0.07498108694426764\n",
      "Iteration 4530 Loss: 0.07498087632293149\n",
      "Iteration 4531 Loss: 0.0749806657892316\n",
      "Iteration 4532 Loss: 0.07498045534298911\n",
      "Iteration 4533 Loss: 0.07498024498402565\n",
      "Iteration 4534 Loss: 0.07498003471216312\n",
      "Iteration 4535 Loss: 0.07497982452722393\n",
      "Iteration 4536 Loss: 0.0749796144290309\n",
      "Iteration 4537 Loss: 0.07497940441740716\n",
      "Iteration 4538 Loss: 0.07497919449217638\n",
      "Iteration 4539 Loss: 0.07497898465316252\n",
      "Iteration 4540 Loss: 0.07497877490019003\n",
      "Iteration 4541 Loss: 0.0749785652330837\n",
      "Iteration 4542 Loss: 0.07497835565166877\n",
      "Iteration 4543 Loss: 0.07497814615577084\n",
      "Iteration 4544 Loss: 0.07497793674521587\n",
      "Iteration 4545 Loss: 0.0749777274198303\n",
      "Iteration 4546 Loss: 0.0749775181794411\n",
      "Iteration 4547 Loss: 0.07497730902387523\n",
      "Iteration 4548 Loss: 0.07497709995296044\n",
      "Iteration 4549 Loss: 0.07497689096652464\n",
      "Iteration 4550 Loss: 0.07497668206439642\n",
      "Iteration 4551 Loss: 0.0749764732464044\n",
      "Iteration 4552 Loss: 0.07497626451237778\n",
      "Iteration 4553 Loss: 0.07497605586214617\n",
      "Iteration 4554 Loss: 0.07497584729553955\n",
      "Iteration 4555 Loss: 0.07497563881238818\n",
      "Iteration 4556 Loss: 0.07497543041252294\n",
      "Iteration 4557 Loss: 0.07497522209577481\n",
      "Iteration 4558 Loss: 0.07497501386197547\n",
      "Iteration 4559 Loss: 0.07497480571095674\n",
      "Iteration 4560 Loss: 0.07497459764255089\n",
      "Iteration 4561 Loss: 0.07497438965659063\n",
      "Iteration 4562 Loss: 0.07497418175290899\n",
      "Iteration 4563 Loss: 0.07497397393133945\n",
      "Iteration 4564 Loss: 0.07497376619171585\n",
      "Iteration 4565 Loss: 0.0749735585338723\n",
      "Iteration 4566 Loss: 0.07497335095764349\n",
      "Iteration 4567 Loss: 0.07497314346286435\n",
      "Iteration 4568 Loss: 0.07497293604937018\n",
      "Iteration 4569 Loss: 0.07497272871699676\n",
      "Iteration 4570 Loss: 0.07497252146558016\n",
      "Iteration 4571 Loss: 0.07497231429495685\n",
      "Iteration 4572 Loss: 0.07497210720496371\n",
      "Iteration 4573 Loss: 0.0749719001954379\n",
      "Iteration 4574 Loss: 0.07497169326621707\n",
      "Iteration 4575 Loss: 0.07497148641713916\n",
      "Iteration 4576 Loss: 0.07497127964804251\n",
      "Iteration 4577 Loss: 0.07497107295876584\n",
      "Iteration 4578 Loss: 0.07497086634914821\n",
      "Iteration 4579 Loss: 0.07497065981902909\n",
      "Iteration 4580 Loss: 0.07497045336824835\n",
      "Iteration 4581 Loss: 0.07497024699664603\n",
      "Iteration 4582 Loss: 0.07497004070406274\n",
      "Iteration 4583 Loss: 0.07496983449033952\n",
      "Iteration 4584 Loss: 0.07496962835531751\n",
      "Iteration 4585 Loss: 0.07496942229883839\n",
      "Iteration 4586 Loss: 0.07496921632074419\n",
      "Iteration 4587 Loss: 0.07496901042087725\n",
      "Iteration 4588 Loss: 0.07496880459908029\n",
      "Iteration 4589 Loss: 0.07496859885519647\n",
      "Iteration 4590 Loss: 0.07496839318906914\n",
      "Iteration 4591 Loss: 0.07496818760054218\n",
      "Iteration 4592 Loss: 0.07496798208945979\n",
      "Iteration 4593 Loss: 0.07496777665566638\n",
      "Iteration 4594 Loss: 0.07496757129900695\n",
      "Iteration 4595 Loss: 0.07496736601932667\n",
      "Iteration 4596 Loss: 0.07496716081647117\n",
      "Iteration 4597 Loss: 0.07496695569028636\n",
      "Iteration 4598 Loss: 0.07496675064061853\n",
      "Iteration 4599 Loss: 0.07496654566731442\n",
      "Iteration 4600 Loss: 0.07496634077022098\n",
      "Iteration 4601 Loss: 0.07496613594918548\n",
      "Iteration 4602 Loss: 0.07496593120405573\n",
      "Iteration 4603 Loss: 0.07496572653467977\n",
      "Iteration 4604 Loss: 0.07496552194090597\n",
      "Iteration 4605 Loss: 0.07496531742258308\n",
      "Iteration 4606 Loss: 0.07496511297956021\n",
      "Iteration 4607 Loss: 0.07496490861168684\n",
      "Iteration 4608 Loss: 0.0749647043188127\n",
      "Iteration 4609 Loss: 0.07496450010078792\n",
      "Iteration 4610 Loss: 0.07496429595746294\n",
      "Iteration 4611 Loss: 0.07496409188868872\n",
      "Iteration 4612 Loss: 0.07496388789431628\n",
      "Iteration 4613 Loss: 0.07496368397419717\n",
      "Iteration 4614 Loss: 0.07496348012818324\n",
      "Iteration 4615 Loss: 0.07496327635612668\n",
      "Iteration 4616 Loss: 0.07496307265787996\n",
      "Iteration 4617 Loss: 0.07496286903329595\n",
      "Iteration 4618 Loss: 0.07496266548222792\n",
      "Iteration 4619 Loss: 0.07496246200452927\n",
      "Iteration 4620 Loss: 0.07496225860005394\n",
      "Iteration 4621 Loss: 0.07496205526865618\n",
      "Iteration 4622 Loss: 0.07496185201019044\n",
      "Iteration 4623 Loss: 0.07496164882451159\n",
      "Iteration 4624 Loss: 0.07496144571147494\n",
      "Iteration 4625 Loss: 0.0749612426709359\n",
      "Iteration 4626 Loss: 0.07496103970275042\n",
      "Iteration 4627 Loss: 0.07496083680677457\n",
      "Iteration 4628 Loss: 0.07496063398286505\n",
      "Iteration 4629 Loss: 0.07496043123087859\n",
      "Iteration 4630 Loss: 0.07496022855067248\n",
      "Iteration 4631 Loss: 0.07496002594210402\n",
      "Iteration 4632 Loss: 0.07495982340503125\n",
      "Iteration 4633 Loss: 0.07495962093931226\n",
      "Iteration 4634 Loss: 0.07495941854480556\n",
      "Iteration 4635 Loss: 0.07495921622136988\n",
      "Iteration 4636 Loss: 0.07495901396886445\n",
      "Iteration 4637 Loss: 0.07495881178714862\n",
      "Iteration 4638 Loss: 0.07495860967608234\n",
      "Iteration 4639 Loss: 0.0749584076355256\n",
      "Iteration 4640 Loss: 0.0749582056653388\n",
      "Iteration 4641 Loss: 0.07495800376538271\n",
      "Iteration 4642 Loss: 0.07495780193551835\n",
      "Iteration 4643 Loss: 0.07495760017560718\n",
      "Iteration 4644 Loss: 0.07495739848551083\n",
      "Iteration 4645 Loss: 0.07495719686509139\n",
      "Iteration 4646 Loss: 0.07495699531421104\n",
      "Iteration 4647 Loss: 0.07495679383273256\n",
      "Iteration 4648 Loss: 0.07495659242051889\n",
      "Iteration 4649 Loss: 0.07495639107743327\n",
      "Iteration 4650 Loss: 0.07495618980333926\n",
      "Iteration 4651 Loss: 0.07495598859810089\n",
      "Iteration 4652 Loss: 0.07495578746158213\n",
      "Iteration 4653 Loss: 0.07495558639364779\n",
      "Iteration 4654 Loss: 0.07495538539416245\n",
      "Iteration 4655 Loss: 0.07495518446299146\n",
      "Iteration 4656 Loss: 0.07495498360000008\n",
      "Iteration 4657 Loss: 0.07495478280505426\n",
      "Iteration 4658 Loss: 0.0749545820780199\n",
      "Iteration 4659 Loss: 0.0749543814187635\n",
      "Iteration 4660 Loss: 0.07495418082715162\n",
      "Iteration 4661 Loss: 0.07495398030305131\n",
      "Iteration 4662 Loss: 0.07495377984632991\n",
      "Iteration 4663 Loss: 0.07495357945685492\n",
      "Iteration 4664 Loss: 0.07495337913449433\n",
      "Iteration 4665 Loss: 0.07495317887911626\n",
      "Iteration 4666 Loss: 0.07495297869058923\n",
      "Iteration 4667 Loss: 0.07495277856878209\n",
      "Iteration 4668 Loss: 0.07495257851356385\n",
      "Iteration 4669 Loss: 0.07495237852480403\n",
      "Iteration 4670 Loss: 0.07495217860237229\n",
      "Iteration 4671 Loss: 0.07495197874613854\n",
      "Iteration 4672 Loss: 0.07495177895597326\n",
      "Iteration 4673 Loss: 0.07495157923174688\n",
      "Iteration 4674 Loss: 0.07495137957333038\n",
      "Iteration 4675 Loss: 0.07495117998059493\n",
      "Iteration 4676 Loss: 0.07495098045341193\n",
      "Iteration 4677 Loss: 0.0749507809916533\n",
      "Iteration 4678 Loss: 0.0749505815951911\n",
      "Iteration 4679 Loss: 0.07495038226389761\n",
      "Iteration 4680 Loss: 0.07495018299764547\n",
      "Iteration 4681 Loss: 0.07494998379630771\n",
      "Iteration 4682 Loss: 0.07494978465975755\n",
      "Iteration 4683 Loss: 0.07494958558786854\n",
      "Iteration 4684 Loss: 0.07494938658051443\n",
      "Iteration 4685 Loss: 0.07494918763756944\n",
      "Iteration 4686 Loss: 0.0749489887589078\n",
      "Iteration 4687 Loss: 0.07494878994440435\n",
      "Iteration 4688 Loss: 0.07494859119393402\n",
      "Iteration 4689 Loss: 0.07494839250737198\n",
      "Iteration 4690 Loss: 0.07494819388459388\n",
      "Iteration 4691 Loss: 0.07494799532547558\n",
      "Iteration 4692 Loss: 0.07494779682989305\n",
      "Iteration 4693 Loss: 0.07494759839772278\n",
      "Iteration 4694 Loss: 0.07494740002884151\n",
      "Iteration 4695 Loss: 0.07494720172312605\n",
      "Iteration 4696 Loss: 0.07494700348045377\n",
      "Iteration 4697 Loss: 0.07494680530070216\n",
      "Iteration 4698 Loss: 0.07494660718374901\n",
      "Iteration 4699 Loss: 0.07494640912947237\n",
      "Iteration 4700 Loss: 0.0749462111377507\n",
      "Iteration 4701 Loss: 0.07494601320846259\n",
      "Iteration 4702 Loss: 0.07494581534148699\n",
      "Iteration 4703 Loss: 0.07494561753670306\n",
      "Iteration 4704 Loss: 0.0749454197939903\n",
      "Iteration 4705 Loss: 0.07494522211322842\n",
      "Iteration 4706 Loss: 0.07494502449429752\n",
      "Iteration 4707 Loss: 0.07494482693707784\n",
      "Iteration 4708 Loss: 0.07494462944144997\n",
      "Iteration 4709 Loss: 0.07494443200729481\n",
      "Iteration 4710 Loss: 0.07494423463449341\n",
      "Iteration 4711 Loss: 0.07494403732292725\n",
      "Iteration 4712 Loss: 0.07494384007247787\n",
      "Iteration 4713 Loss: 0.07494364288302735\n",
      "Iteration 4714 Loss: 0.07494344575445777\n",
      "Iteration 4715 Loss: 0.0749432486866517\n",
      "Iteration 4716 Loss: 0.07494305167949189\n",
      "Iteration 4717 Loss: 0.07494285473286125\n",
      "Iteration 4718 Loss: 0.07494265784664318\n",
      "Iteration 4719 Loss: 0.07494246102072118\n",
      "Iteration 4720 Loss: 0.07494226425497902\n",
      "Iteration 4721 Loss: 0.07494206754930086\n",
      "Iteration 4722 Loss: 0.07494187090357105\n",
      "Iteration 4723 Loss: 0.07494167431767416\n",
      "Iteration 4724 Loss: 0.07494147779149503\n",
      "Iteration 4725 Loss: 0.07494128132491891\n",
      "Iteration 4726 Loss: 0.0749410849178311\n",
      "Iteration 4727 Loss: 0.07494088857011735\n",
      "Iteration 4728 Loss: 0.07494069228166356\n",
      "Iteration 4729 Loss: 0.07494049605235585\n",
      "Iteration 4730 Loss: 0.07494029988208079\n",
      "Iteration 4731 Loss: 0.07494010377072495\n",
      "Iteration 4732 Loss: 0.07493990771817544\n",
      "Iteration 4733 Loss: 0.07493971172431939\n",
      "Iteration 4734 Loss: 0.07493951578904437\n",
      "Iteration 4735 Loss: 0.07493931991223805\n",
      "Iteration 4736 Loss: 0.0749391240937884\n",
      "Iteration 4737 Loss: 0.07493892833358382\n",
      "Iteration 4738 Loss: 0.0749387326315127\n",
      "Iteration 4739 Loss: 0.07493853698746385\n",
      "Iteration 4740 Loss: 0.0749383414013263\n",
      "Iteration 4741 Loss: 0.07493814587298926\n",
      "Iteration 4742 Loss: 0.07493795040234237\n",
      "Iteration 4743 Loss: 0.07493775498927535\n",
      "Iteration 4744 Loss: 0.07493755963367818\n",
      "Iteration 4745 Loss: 0.07493736433544126\n",
      "Iteration 4746 Loss: 0.07493716909445508\n",
      "Iteration 4747 Loss: 0.07493697391061038\n",
      "Iteration 4748 Loss: 0.07493677878379826\n",
      "Iteration 4749 Loss: 0.07493658371391002\n",
      "Iteration 4750 Loss: 0.07493638870083721\n",
      "Iteration 4751 Loss: 0.07493619374447151\n",
      "Iteration 4752 Loss: 0.074935998844705\n",
      "Iteration 4753 Loss: 0.07493580400143005\n",
      "Iteration 4754 Loss: 0.07493560921453903\n",
      "Iteration 4755 Loss: 0.07493541448392485\n",
      "Iteration 4756 Loss: 0.07493521980948051\n",
      "Iteration 4757 Loss: 0.07493502519109921\n",
      "Iteration 4758 Loss: 0.0749348306286744\n",
      "Iteration 4759 Loss: 0.07493463612210002\n",
      "Iteration 4760 Loss: 0.07493444167126997\n",
      "Iteration 4761 Loss: 0.0749342472760784\n",
      "Iteration 4762 Loss: 0.07493405293641989\n",
      "Iteration 4763 Loss: 0.07493385865218918\n",
      "Iteration 4764 Loss: 0.07493366442328114\n",
      "Iteration 4765 Loss: 0.07493347024959104\n",
      "Iteration 4766 Loss: 0.07493327613101433\n",
      "Iteration 4767 Loss: 0.07493308206744664\n",
      "Iteration 4768 Loss: 0.07493288805878393\n",
      "Iteration 4769 Loss: 0.0749326941049223\n",
      "Iteration 4770 Loss: 0.07493250020575817\n",
      "Iteration 4771 Loss: 0.07493230636118828\n",
      "Iteration 4772 Loss: 0.07493211257110939\n",
      "Iteration 4773 Loss: 0.07493191883541854\n",
      "Iteration 4774 Loss: 0.07493172515401318\n",
      "Iteration 4775 Loss: 0.07493153152679091\n",
      "Iteration 4776 Loss: 0.07493133795364945\n",
      "Iteration 4777 Loss: 0.0749311444344869\n",
      "Iteration 4778 Loss: 0.07493095096920147\n",
      "Iteration 4779 Loss: 0.07493075755769175\n",
      "Iteration 4780 Loss: 0.07493056419985646\n",
      "Iteration 4781 Loss: 0.07493037089559454\n",
      "Iteration 4782 Loss: 0.07493017764480527\n",
      "Iteration 4783 Loss: 0.07492998444738797\n",
      "Iteration 4784 Loss: 0.0749297913032424\n",
      "Iteration 4785 Loss: 0.07492959821226838\n",
      "Iteration 4786 Loss: 0.07492940517436611\n",
      "Iteration 4787 Loss: 0.07492921218943593\n",
      "Iteration 4788 Loss: 0.07492901925737838\n",
      "Iteration 4789 Loss: 0.07492882637809431\n",
      "Iteration 4790 Loss: 0.0749286335514847\n",
      "Iteration 4791 Loss: 0.07492844077745087\n",
      "Iteration 4792 Loss: 0.07492824805589433\n",
      "Iteration 4793 Loss: 0.07492805538671667\n",
      "Iteration 4794 Loss: 0.07492786276981991\n",
      "Iteration 4795 Loss: 0.07492767020510618\n",
      "Iteration 4796 Loss: 0.07492747769247791\n",
      "Iteration 4797 Loss: 0.07492728523183778\n",
      "Iteration 4798 Loss: 0.07492709282308843\n",
      "Iteration 4799 Loss: 0.07492690046613307\n",
      "Iteration 4800 Loss: 0.07492670816087485\n",
      "Iteration 4801 Loss: 0.07492651590721743\n",
      "Iteration 4802 Loss: 0.07492632370506438\n",
      "Iteration 4803 Loss: 0.07492613155431971\n",
      "Iteration 4804 Loss: 0.07492593945488757\n",
      "Iteration 4805 Loss: 0.07492574740667228\n",
      "Iteration 4806 Loss: 0.07492555540957853\n",
      "Iteration 4807 Loss: 0.0749253634635111\n",
      "Iteration 4808 Loss: 0.074925171568375\n",
      "Iteration 4809 Loss: 0.07492497972407546\n",
      "Iteration 4810 Loss: 0.07492478793051797\n",
      "Iteration 4811 Loss: 0.07492459618760829\n",
      "Iteration 4812 Loss: 0.07492440449525217\n",
      "Iteration 4813 Loss: 0.07492421285335585\n",
      "Iteration 4814 Loss: 0.07492402126182557\n",
      "Iteration 4815 Loss: 0.07492382972056791\n",
      "Iteration 4816 Loss: 0.07492363822948962\n",
      "Iteration 4817 Loss: 0.07492344678849767\n",
      "Iteration 4818 Loss: 0.07492325539749926\n",
      "Iteration 4819 Loss: 0.07492306405640176\n",
      "Iteration 4820 Loss: 0.07492287276511278\n",
      "Iteration 4821 Loss: 0.07492268152354016\n",
      "Iteration 4822 Loss: 0.07492249033159185\n",
      "Iteration 4823 Loss: 0.07492229918917621\n",
      "Iteration 4824 Loss: 0.07492210809620162\n",
      "Iteration 4825 Loss: 0.0749219170525767\n",
      "Iteration 4826 Loss: 0.07492172605821038\n",
      "Iteration 4827 Loss: 0.0749215351130117\n",
      "Iteration 4828 Loss: 0.07492134421688998\n",
      "Iteration 4829 Loss: 0.07492115336975463\n",
      "Iteration 4830 Loss: 0.07492096257151544\n",
      "Iteration 4831 Loss: 0.07492077182208227\n",
      "Iteration 4832 Loss: 0.07492058112136527\n",
      "Iteration 4833 Loss: 0.07492039046927466\n",
      "Iteration 4834 Loss: 0.07492019986572104\n",
      "Iteration 4835 Loss: 0.07492000931061508\n",
      "Iteration 4836 Loss: 0.0749198188038678\n",
      "Iteration 4837 Loss: 0.07491962834539026\n",
      "Iteration 4838 Loss: 0.07491943793509379\n",
      "Iteration 4839 Loss: 0.07491924757288997\n",
      "Iteration 4840 Loss: 0.07491905725869048\n",
      "Iteration 4841 Loss: 0.07491886699240732\n",
      "Iteration 4842 Loss: 0.07491867677395264\n",
      "Iteration 4843 Loss: 0.07491848660323874\n",
      "Iteration 4844 Loss: 0.0749182964801782\n",
      "Iteration 4845 Loss: 0.07491810640468373\n",
      "Iteration 4846 Loss: 0.07491791637666836\n",
      "Iteration 4847 Loss: 0.07491772639604506\n",
      "Iteration 4848 Loss: 0.07491753646272732\n",
      "Iteration 4849 Loss: 0.07491734657662866\n",
      "Iteration 4850 Loss: 0.07491715673766278\n",
      "Iteration 4851 Loss: 0.07491696694574364\n",
      "Iteration 4852 Loss: 0.07491677720078538\n",
      "Iteration 4853 Loss: 0.07491658750270228\n",
      "Iteration 4854 Loss: 0.07491639785140886\n",
      "Iteration 4855 Loss: 0.07491620824681991\n",
      "Iteration 4856 Loss: 0.07491601868885037\n",
      "Iteration 4857 Loss: 0.07491582917741521\n",
      "Iteration 4858 Loss: 0.07491563971242984\n",
      "Iteration 4859 Loss: 0.07491545029380972\n",
      "Iteration 4860 Loss: 0.07491526092147054\n",
      "Iteration 4861 Loss: 0.07491507159532815\n",
      "Iteration 4862 Loss: 0.07491488231529872\n",
      "Iteration 4863 Loss: 0.07491469308129844\n",
      "Iteration 4864 Loss: 0.07491450389324378\n",
      "Iteration 4865 Loss: 0.07491431475105143\n",
      "Iteration 4866 Loss: 0.07491412565463812\n",
      "Iteration 4867 Loss: 0.07491393660392102\n",
      "Iteration 4868 Loss: 0.07491374759881722\n",
      "Iteration 4869 Loss: 0.07491355863924422\n",
      "Iteration 4870 Loss: 0.07491336972511958\n",
      "Iteration 4871 Loss: 0.07491318085636108\n",
      "Iteration 4872 Loss: 0.07491299203288673\n",
      "Iteration 4873 Loss: 0.07491280325461461\n",
      "Iteration 4874 Loss: 0.07491261452146314\n",
      "Iteration 4875 Loss: 0.07491242583335087\n",
      "Iteration 4876 Loss: 0.07491223719019645\n",
      "Iteration 4877 Loss: 0.07491204859191884\n",
      "Iteration 4878 Loss: 0.07491186003843707\n",
      "Iteration 4879 Loss: 0.07491167152967047\n",
      "Iteration 4880 Loss: 0.0749114830655385\n",
      "Iteration 4881 Loss: 0.0749112946459608\n",
      "Iteration 4882 Loss: 0.0749111062708572\n",
      "Iteration 4883 Loss: 0.07491091794014765\n",
      "Iteration 4884 Loss: 0.07491072965375241\n",
      "Iteration 4885 Loss: 0.07491054141159184\n",
      "Iteration 4886 Loss: 0.07491035321358654\n",
      "Iteration 4887 Loss: 0.07491016505965715\n",
      "Iteration 4888 Loss: 0.07490997694972468\n",
      "Iteration 4889 Loss: 0.07490978888371018\n",
      "Iteration 4890 Loss: 0.07490960086153496\n",
      "Iteration 4891 Loss: 0.0749094128831205\n",
      "Iteration 4892 Loss: 0.07490922494838834\n",
      "Iteration 4893 Loss: 0.07490903705726042\n",
      "Iteration 4894 Loss: 0.0749088492096587\n",
      "Iteration 4895 Loss: 0.07490866140550533\n",
      "Iteration 4896 Loss: 0.07490847364472268\n",
      "Iteration 4897 Loss: 0.0749082859272333\n",
      "Iteration 4898 Loss: 0.07490809825295994\n",
      "Iteration 4899 Loss: 0.07490791062182536\n",
      "Iteration 4900 Loss: 0.07490772303375268\n",
      "Iteration 4901 Loss: 0.07490753548866515\n",
      "Iteration 4902 Loss: 0.07490734798648623\n",
      "Iteration 4903 Loss: 0.0749071605271394\n",
      "Iteration 4904 Loss: 0.07490697311054853\n",
      "Iteration 4905 Loss: 0.07490678573663749\n",
      "Iteration 4906 Loss: 0.0749065984053304\n",
      "Iteration 4907 Loss: 0.07490641111655161\n",
      "Iteration 4908 Loss: 0.07490622387022543\n",
      "Iteration 4909 Loss: 0.07490603666627658\n",
      "Iteration 4910 Loss: 0.07490584950462989\n",
      "Iteration 4911 Loss: 0.07490566238521026\n",
      "Iteration 4912 Loss: 0.07490547530794296\n",
      "Iteration 4913 Loss: 0.07490528827275315\n",
      "Iteration 4914 Loss: 0.0749051012795664\n",
      "Iteration 4915 Loss: 0.07490491432830834\n",
      "Iteration 4916 Loss: 0.07490472741890483\n",
      "Iteration 4917 Loss: 0.0749045405512818\n",
      "Iteration 4918 Loss: 0.0749043537253655\n",
      "Iteration 4919 Loss: 0.07490416694108218\n",
      "Iteration 4920 Loss: 0.0749039801983584\n",
      "Iteration 4921 Loss: 0.07490379349712076\n",
      "Iteration 4922 Loss: 0.07490360683729616\n",
      "Iteration 4923 Loss: 0.0749034202188116\n",
      "Iteration 4924 Loss: 0.0749032336415943\n",
      "Iteration 4925 Loss: 0.07490304710557141\n",
      "Iteration 4926 Loss: 0.0749028606106706\n",
      "Iteration 4927 Loss: 0.0749026741568195\n",
      "Iteration 4928 Loss: 0.0749024877439459\n",
      "Iteration 4929 Loss: 0.07490230137197784\n",
      "Iteration 4930 Loss: 0.07490211504084349\n",
      "Iteration 4931 Loss: 0.07490192875047114\n",
      "Iteration 4932 Loss: 0.07490174250078932\n",
      "Iteration 4933 Loss: 0.07490155629172664\n",
      "Iteration 4934 Loss: 0.07490137012321194\n",
      "Iteration 4935 Loss: 0.07490118399517418\n",
      "Iteration 4936 Loss: 0.07490099790754254\n",
      "Iteration 4937 Loss: 0.07490081186024626\n",
      "Iteration 4938 Loss: 0.07490062585321486\n",
      "Iteration 4939 Loss: 0.07490043988637791\n",
      "Iteration 4940 Loss: 0.07490025395966529\n",
      "Iteration 4941 Loss: 0.07490006807300684\n",
      "Iteration 4942 Loss: 0.07489988222633272\n",
      "Iteration 4943 Loss: 0.07489969641957317\n",
      "Iteration 4944 Loss: 0.07489951065265868\n",
      "Iteration 4945 Loss: 0.07489932492551965\n",
      "Iteration 4946 Loss: 0.07489913923808704\n",
      "Iteration 4947 Loss: 0.07489895359029168\n",
      "Iteration 4948 Loss: 0.07489876798206452\n",
      "Iteration 4949 Loss: 0.07489858241333693\n",
      "Iteration 4950 Loss: 0.07489839688404018\n",
      "Iteration 4951 Loss: 0.07489821139410584\n",
      "Iteration 4952 Loss: 0.07489802594346562\n",
      "Iteration 4953 Loss: 0.07489784053205126\n",
      "Iteration 4954 Loss: 0.07489765515979488\n",
      "Iteration 4955 Loss: 0.07489746982662848\n",
      "Iteration 4956 Loss: 0.07489728453248452\n",
      "Iteration 4957 Loss: 0.07489709927729542\n",
      "Iteration 4958 Loss: 0.0748969140609937\n",
      "Iteration 4959 Loss: 0.07489672888351225\n",
      "Iteration 4960 Loss: 0.07489654374478392\n",
      "Iteration 4961 Loss: 0.07489635864474184\n",
      "Iteration 4962 Loss: 0.0748961735833192\n",
      "Iteration 4963 Loss: 0.0748959885604493\n",
      "Iteration 4964 Loss: 0.07489580357606583\n",
      "Iteration 4965 Loss: 0.07489561863010234\n",
      "Iteration 4966 Loss: 0.07489543372249277\n",
      "Iteration 4967 Loss: 0.07489524885317099\n",
      "Iteration 4968 Loss: 0.07489506402207127\n",
      "Iteration 4969 Loss: 0.07489487922912783\n",
      "Iteration 4970 Loss: 0.07489469447427506\n",
      "Iteration 4971 Loss: 0.07489450975744762\n",
      "Iteration 4972 Loss: 0.07489432507858018\n",
      "Iteration 4973 Loss: 0.07489414043760764\n",
      "Iteration 4974 Loss: 0.07489395583446509\n",
      "Iteration 4975 Loss: 0.07489377126908772\n",
      "Iteration 4976 Loss: 0.07489358674141074\n",
      "Iteration 4977 Loss: 0.07489340225136971\n",
      "Iteration 4978 Loss: 0.07489321779890026\n",
      "Iteration 4979 Loss: 0.07489303338393813\n",
      "Iteration 4980 Loss: 0.07489284900641927\n",
      "Iteration 4981 Loss: 0.07489266466627971\n",
      "Iteration 4982 Loss: 0.07489248036345567\n",
      "Iteration 4983 Loss: 0.0748922960978835\n",
      "Iteration 4984 Loss: 0.07489211186949969\n",
      "Iteration 4985 Loss: 0.07489192767824092\n",
      "Iteration 4986 Loss: 0.0748917435240439\n",
      "Iteration 4987 Loss: 0.07489155940684572\n",
      "Iteration 4988 Loss: 0.07489137532658323\n",
      "Iteration 4989 Loss: 0.07489119128319385\n",
      "Iteration 4990 Loss: 0.07489100727661487\n",
      "Iteration 4991 Loss: 0.07489082330678376\n",
      "Iteration 4992 Loss: 0.07489063937363827\n",
      "Iteration 4993 Loss: 0.07489045547711601\n",
      "Iteration 4994 Loss: 0.07489027161715515\n",
      "Iteration 4995 Loss: 0.07489008779369359\n",
      "Iteration 4996 Loss: 0.07488990400666957\n",
      "Iteration 4997 Loss: 0.0748897202560215\n",
      "Iteration 4998 Loss: 0.07488953654168787\n",
      "Iteration 4999 Loss: 0.07488935286360728\n",
      "Iteration 5000 Loss: 0.07488916922171854\n",
      "Iteration 5001 Loss: 0.07488898561596054\n",
      "Iteration 5002 Loss: 0.0748888020462723\n",
      "Iteration 5003 Loss: 0.07488861851259312\n",
      "Iteration 5004 Loss: 0.07488843501486223\n",
      "Iteration 5005 Loss: 0.0748882515530192\n",
      "Iteration 5006 Loss: 0.07488806812700356\n",
      "Iteration 5007 Loss: 0.07488788473675509\n",
      "Iteration 5008 Loss: 0.07488770138221366\n",
      "Iteration 5009 Loss: 0.07488751806331934\n",
      "Iteration 5010 Loss: 0.07488733478001222\n",
      "Iteration 5011 Loss: 0.07488715153223262\n",
      "Iteration 5012 Loss: 0.07488696831992096\n",
      "Iteration 5013 Loss: 0.0748867851430179\n",
      "Iteration 5014 Loss: 0.07488660200146399\n",
      "Iteration 5015 Loss: 0.07488641889520016\n",
      "Iteration 5016 Loss: 0.07488623582416737\n",
      "Iteration 5017 Loss: 0.07488605278830675\n",
      "Iteration 5018 Loss: 0.07488586978755951\n",
      "Iteration 5019 Loss: 0.07488568682186701\n",
      "Iteration 5020 Loss: 0.0748855038911708\n",
      "Iteration 5021 Loss: 0.0748853209954125\n",
      "Iteration 5022 Loss: 0.07488513813453383\n",
      "Iteration 5023 Loss: 0.07488495530847682\n",
      "Iteration 5024 Loss: 0.07488477251718337\n",
      "Iteration 5025 Loss: 0.07488458976059582\n",
      "Iteration 5026 Loss: 0.07488440703865633\n",
      "Iteration 5027 Loss: 0.07488422435130736\n",
      "Iteration 5028 Loss: 0.07488404169849154\n",
      "Iteration 5029 Loss: 0.0748838590801515\n",
      "Iteration 5030 Loss: 0.07488367649623014\n",
      "Iteration 5031 Loss: 0.07488349394667038\n",
      "Iteration 5032 Loss: 0.07488331143141524\n",
      "Iteration 5033 Loss: 0.07488312895040804\n",
      "Iteration 5034 Loss: 0.07488294650359215\n",
      "Iteration 5035 Loss: 0.07488276409091091\n",
      "Iteration 5036 Loss: 0.07488258171230806\n",
      "Iteration 5037 Loss: 0.07488239936772716\n",
      "Iteration 5038 Loss: 0.07488221705711229\n",
      "Iteration 5039 Loss: 0.07488203478040731\n",
      "Iteration 5040 Loss: 0.07488185253755637\n",
      "Iteration 5041 Loss: 0.07488167032850368\n",
      "Iteration 5042 Loss: 0.0748814881531937\n",
      "Iteration 5043 Loss: 0.07488130601157084\n",
      "Iteration 5044 Loss: 0.07488112390357973\n",
      "Iteration 5045 Loss: 0.0748809418291652\n",
      "Iteration 5046 Loss: 0.07488075978827205\n",
      "Iteration 5047 Loss: 0.0748805777808453\n",
      "Iteration 5048 Loss: 0.0748803958068301\n",
      "Iteration 5049 Loss: 0.07488021386617165\n",
      "Iteration 5050 Loss: 0.07488003195881537\n",
      "Iteration 5051 Loss: 0.07487985008470681\n",
      "Iteration 5052 Loss: 0.07487966824379148\n",
      "Iteration 5053 Loss: 0.07487948643601527\n",
      "Iteration 5054 Loss: 0.0748793046613239\n",
      "Iteration 5055 Loss: 0.07487912291966344\n",
      "Iteration 5056 Loss: 0.07487894121098004\n",
      "Iteration 5057 Loss: 0.07487875953521989\n",
      "Iteration 5058 Loss: 0.0748785778923294\n",
      "Iteration 5059 Loss: 0.07487839628225501\n",
      "Iteration 5060 Loss: 0.07487821470494341\n",
      "Iteration 5061 Loss: 0.07487803316034121\n",
      "Iteration 5062 Loss: 0.07487785164839533\n",
      "Iteration 5063 Loss: 0.07487767016905278\n",
      "Iteration 5064 Loss: 0.07487748872226059\n",
      "Iteration 5065 Loss: 0.07487730730796598\n",
      "Iteration 5066 Loss: 0.07487712592611635\n",
      "Iteration 5067 Loss: 0.07487694457665905\n",
      "Iteration 5068 Loss: 0.0748767632595417\n",
      "Iteration 5069 Loss: 0.07487658197471204\n",
      "Iteration 5070 Loss: 0.07487640072211788\n",
      "Iteration 5071 Loss: 0.07487621950170704\n",
      "Iteration 5072 Loss: 0.0748760383134277\n",
      "Iteration 5073 Loss: 0.07487585715722786\n",
      "Iteration 5074 Loss: 0.07487567603305603\n",
      "Iteration 5075 Loss: 0.07487549494086047\n",
      "Iteration 5076 Loss: 0.07487531388058968\n",
      "Iteration 5077 Loss: 0.07487513285219241\n",
      "Iteration 5078 Loss: 0.0748749518556173\n",
      "Iteration 5079 Loss: 0.07487477089081328\n",
      "Iteration 5080 Loss: 0.07487458995772937\n",
      "Iteration 5081 Loss: 0.07487440905631466\n",
      "Iteration 5082 Loss: 0.07487422818651826\n",
      "Iteration 5083 Loss: 0.07487404734828967\n",
      "Iteration 5084 Loss: 0.07487386654157817\n",
      "Iteration 5085 Loss: 0.07487368576633349\n",
      "Iteration 5086 Loss: 0.0748735050225052\n",
      "Iteration 5087 Loss: 0.07487332431004318\n",
      "Iteration 5088 Loss: 0.07487314362889723\n",
      "Iteration 5089 Loss: 0.07487296297901752\n",
      "Iteration 5090 Loss: 0.07487278236035402\n",
      "Iteration 5091 Loss: 0.07487260177285711\n",
      "Iteration 5092 Loss: 0.07487242121647711\n",
      "Iteration 5093 Loss: 0.07487224069116441\n",
      "Iteration 5094 Loss: 0.07487206019686976\n",
      "Iteration 5095 Loss: 0.07487187973354378\n",
      "Iteration 5096 Loss: 0.07487169930113727\n",
      "Iteration 5097 Loss: 0.0748715188996012\n",
      "Iteration 5098 Loss: 0.07487133852888656\n",
      "Iteration 5099 Loss: 0.07487115818894445\n",
      "Iteration 5100 Loss: 0.07487097787972632\n",
      "Iteration 5101 Loss: 0.0748707976011834\n",
      "Iteration 5102 Loss: 0.07487061735326712\n",
      "Iteration 5103 Loss: 0.07487043713592918\n",
      "Iteration 5104 Loss: 0.07487025694912128\n",
      "Iteration 5105 Loss: 0.07487007679279514\n",
      "Iteration 5106 Loss: 0.07486989666690272\n",
      "Iteration 5107 Loss: 0.07486971657139614\n",
      "Iteration 5108 Loss: 0.07486953650622742\n",
      "Iteration 5109 Loss: 0.07486935647134889\n",
      "Iteration 5110 Loss: 0.07486917646671284\n",
      "Iteration 5111 Loss: 0.07486899649227179\n",
      "Iteration 5112 Loss: 0.07486881654797832\n",
      "Iteration 5113 Loss: 0.074868636633785\n",
      "Iteration 5114 Loss: 0.07486845674964476\n",
      "Iteration 5115 Loss: 0.07486827689551045\n",
      "Iteration 5116 Loss: 0.0748680970713351\n",
      "Iteration 5117 Loss: 0.07486791727707169\n",
      "Iteration 5118 Loss: 0.07486773751267366\n",
      "Iteration 5119 Loss: 0.07486755777809412\n",
      "Iteration 5120 Loss: 0.0748673780732866\n",
      "Iteration 5121 Loss: 0.07486719839820459\n",
      "Iteration 5122 Loss: 0.07486701875280179\n",
      "Iteration 5123 Loss: 0.07486683913703196\n",
      "Iteration 5124 Loss: 0.07486665955084884\n",
      "Iteration 5125 Loss: 0.07486647999420652\n",
      "Iteration 5126 Loss: 0.07486630046705893\n",
      "Iteration 5127 Loss: 0.07486612096936032\n",
      "Iteration 5128 Loss: 0.07486594150106503\n",
      "Iteration 5129 Loss: 0.07486576206212728\n",
      "Iteration 5130 Loss: 0.0748655826525016\n",
      "Iteration 5131 Loss: 0.07486540327214264\n",
      "Iteration 5132 Loss: 0.07486522392100499\n",
      "Iteration 5133 Loss: 0.0748650445990435\n",
      "Iteration 5134 Loss: 0.07486486530621303\n",
      "Iteration 5135 Loss: 0.07486468604246858\n",
      "Iteration 5136 Loss: 0.07486450680776527\n",
      "Iteration 5137 Loss: 0.07486432760205818\n",
      "Iteration 5138 Loss: 0.0748641484253028\n",
      "Iteration 5139 Loss: 0.07486396927745442\n",
      "Iteration 5140 Loss: 0.07486379015846853\n",
      "Iteration 5141 Loss: 0.0748636110683008\n",
      "Iteration 5142 Loss: 0.07486343200690682\n",
      "Iteration 5143 Loss: 0.07486325297424253\n",
      "Iteration 5144 Loss: 0.07486307397026375\n",
      "Iteration 5145 Loss: 0.07486289499492646\n",
      "Iteration 5146 Loss: 0.07486271604818688\n",
      "Iteration 5147 Loss: 0.07486253713000113\n",
      "Iteration 5148 Loss: 0.07486235824032553\n",
      "Iteration 5149 Loss: 0.07486217937911639\n",
      "Iteration 5150 Loss: 0.07486200054633045\n",
      "Iteration 5151 Loss: 0.07486182174192406\n",
      "Iteration 5152 Loss: 0.07486164296585407\n",
      "Iteration 5153 Loss: 0.07486146421807727\n",
      "Iteration 5154 Loss: 0.07486128549855045\n",
      "Iteration 5155 Loss: 0.07486110680723082\n",
      "Iteration 5156 Loss: 0.07486092814407515\n",
      "Iteration 5157 Loss: 0.07486074950904083\n",
      "Iteration 5158 Loss: 0.07486057090208523\n",
      "Iteration 5159 Loss: 0.07486039232316552\n",
      "Iteration 5160 Loss: 0.07486021377223936\n",
      "Iteration 5161 Loss: 0.07486003524926425\n",
      "Iteration 5162 Loss: 0.07485985675419786\n",
      "Iteration 5163 Loss: 0.07485967828699798\n",
      "Iteration 5164 Loss: 0.07485949984762244\n",
      "Iteration 5165 Loss: 0.07485932143602923\n",
      "Iteration 5166 Loss: 0.0748591430521764\n",
      "Iteration 5167 Loss: 0.07485896469602216\n",
      "Iteration 5168 Loss: 0.07485878636752458\n",
      "Iteration 5169 Loss: 0.07485860806664223\n",
      "Iteration 5170 Loss: 0.07485842979333338\n",
      "Iteration 5171 Loss: 0.07485825154755665\n",
      "Iteration 5172 Loss: 0.07485807332927064\n",
      "Iteration 5173 Loss: 0.07485789513843406\n",
      "Iteration 5174 Loss: 0.07485771697500575\n",
      "Iteration 5175 Loss: 0.07485753883894455\n",
      "Iteration 5176 Loss: 0.07485736073020952\n",
      "Iteration 5177 Loss: 0.07485718264875975\n",
      "Iteration 5178 Loss: 0.07485700459455437\n",
      "Iteration 5179 Loss: 0.07485682656755274\n",
      "Iteration 5180 Loss: 0.07485664856771418\n",
      "Iteration 5181 Loss: 0.0748564705949981\n",
      "Iteration 5182 Loss: 0.07485629264936422\n",
      "Iteration 5183 Loss: 0.074856114730772\n",
      "Iteration 5184 Loss: 0.07485593683918128\n",
      "Iteration 5185 Loss: 0.07485575897455185\n",
      "Iteration 5186 Loss: 0.07485558113684365\n",
      "Iteration 5187 Loss: 0.07485540332601671\n",
      "Iteration 5188 Loss: 0.07485522554203113\n",
      "Iteration 5189 Loss: 0.07485504778484708\n",
      "Iteration 5190 Loss: 0.07485487005442482\n",
      "Iteration 5191 Loss: 0.07485469235072477\n",
      "Iteration 5192 Loss: 0.07485451467370742\n",
      "Iteration 5193 Loss: 0.07485433702333319\n",
      "Iteration 5194 Loss: 0.07485415939956287\n",
      "Iteration 5195 Loss: 0.0748539818023571\n",
      "Iteration 5196 Loss: 0.07485380423167676\n",
      "Iteration 5197 Loss: 0.0748536266874827\n",
      "Iteration 5198 Loss: 0.074853449169736\n",
      "Iteration 5199 Loss: 0.07485327167839764\n",
      "Iteration 5200 Loss: 0.07485309421342885\n",
      "Iteration 5201 Loss: 0.07485291677479096\n",
      "Iteration 5202 Loss: 0.07485273936244521\n",
      "Iteration 5203 Loss: 0.07485256197635308\n",
      "Iteration 5204 Loss: 0.07485238461647614\n",
      "Iteration 5205 Loss: 0.07485220728277593\n",
      "Iteration 5206 Loss: 0.07485202997521416\n",
      "Iteration 5207 Loss: 0.07485185269375276\n",
      "Iteration 5208 Loss: 0.07485167543835335\n",
      "Iteration 5209 Loss: 0.0748514982089781\n",
      "Iteration 5210 Loss: 0.07485132100558893\n",
      "Iteration 5211 Loss: 0.07485114382814811\n",
      "Iteration 5212 Loss: 0.07485096667661766\n",
      "Iteration 5213 Loss: 0.07485078955096011\n",
      "Iteration 5214 Loss: 0.07485061245113767\n",
      "Iteration 5215 Loss: 0.07485043537711292\n",
      "Iteration 5216 Loss: 0.07485025832884833\n",
      "Iteration 5217 Loss: 0.07485008130630666\n",
      "Iteration 5218 Loss: 0.07484990430945057\n",
      "Iteration 5219 Loss: 0.07484972733824284\n",
      "Iteration 5220 Loss: 0.07484955039264646\n",
      "Iteration 5221 Loss: 0.0748493734726244\n",
      "Iteration 5222 Loss: 0.07484919657813957\n",
      "Iteration 5223 Loss: 0.07484901970915532\n",
      "Iteration 5224 Loss: 0.0748488428656348\n",
      "Iteration 5225 Loss: 0.07484866604754133\n",
      "Iteration 5226 Loss: 0.07484848925483827\n",
      "Iteration 5227 Loss: 0.0748483124874892\n",
      "Iteration 5228 Loss: 0.07484813574545761\n",
      "Iteration 5229 Loss: 0.07484795902870715\n",
      "Iteration 5230 Loss: 0.07484778233720162\n",
      "Iteration 5231 Loss: 0.07484760567090476\n",
      "Iteration 5232 Loss: 0.07484742902978045\n",
      "Iteration 5233 Loss: 0.07484725241379273\n",
      "Iteration 5234 Loss: 0.0748470758229057\n",
      "Iteration 5235 Loss: 0.07484689925708336\n",
      "Iteration 5236 Loss: 0.07484672271629\n",
      "Iteration 5237 Loss: 0.07484654620048999\n",
      "Iteration 5238 Loss: 0.07484636970964759\n",
      "Iteration 5239 Loss: 0.07484619324372735\n",
      "Iteration 5240 Loss: 0.0748460168026938\n",
      "Iteration 5241 Loss: 0.07484584038651157\n",
      "Iteration 5242 Loss: 0.07484566399514529\n",
      "Iteration 5243 Loss: 0.07484548762855987\n",
      "Iteration 5244 Loss: 0.07484531128672008\n",
      "Iteration 5245 Loss: 0.07484513496959086\n",
      "Iteration 5246 Loss: 0.07484495867713727\n",
      "Iteration 5247 Loss: 0.07484478240932449\n",
      "Iteration 5248 Loss: 0.07484460616611756\n",
      "Iteration 5249 Loss: 0.07484442994748179\n",
      "Iteration 5250 Loss: 0.07484425375338254\n",
      "Iteration 5251 Loss: 0.0748440775837852\n",
      "Iteration 5252 Loss: 0.07484390143865531\n",
      "Iteration 5253 Loss: 0.07484372531795845\n",
      "Iteration 5254 Loss: 0.07484354922166024\n",
      "Iteration 5255 Loss: 0.07484337314972637\n",
      "Iteration 5256 Loss: 0.07484319710212269\n",
      "Iteration 5257 Loss: 0.07484302107881512\n",
      "Iteration 5258 Loss: 0.07484284507976957\n",
      "Iteration 5259 Loss: 0.07484266910495208\n",
      "Iteration 5260 Loss: 0.07484249315432881\n",
      "Iteration 5261 Loss: 0.0748423172278659\n",
      "Iteration 5262 Loss: 0.0748421413255297\n",
      "Iteration 5263 Loss: 0.07484196544728641\n",
      "Iteration 5264 Loss: 0.07484178959310259\n",
      "Iteration 5265 Loss: 0.07484161376294471\n",
      "Iteration 5266 Loss: 0.07484143795677929\n",
      "Iteration 5267 Loss: 0.07484126217457301\n",
      "Iteration 5268 Loss: 0.07484108641629268\n",
      "Iteration 5269 Loss: 0.07484091068190492\n",
      "Iteration 5270 Loss: 0.07484073497137674\n",
      "Iteration 5271 Loss: 0.07484055928467505\n",
      "Iteration 5272 Loss: 0.0748403836217669\n",
      "Iteration 5273 Loss: 0.07484020798261934\n",
      "Iteration 5274 Loss: 0.07484003236719959\n",
      "Iteration 5275 Loss: 0.07483985677547488\n",
      "Iteration 5276 Loss: 0.07483968120741252\n",
      "Iteration 5277 Loss: 0.07483950566297995\n",
      "Iteration 5278 Loss: 0.07483933014214464\n",
      "Iteration 5279 Loss: 0.07483915464487409\n",
      "Iteration 5280 Loss: 0.07483897917113587\n",
      "Iteration 5281 Loss: 0.07483880372089775\n",
      "Iteration 5282 Loss: 0.0748386282941275\n",
      "Iteration 5283 Loss: 0.07483845289079297\n",
      "Iteration 5284 Loss: 0.07483827751086194\n",
      "Iteration 5285 Loss: 0.07483810215430262\n",
      "Iteration 5286 Loss: 0.07483792682108285\n",
      "Iteration 5287 Loss: 0.07483775151117082\n",
      "Iteration 5288 Loss: 0.0748375762245348\n",
      "Iteration 5289 Loss: 0.07483740096114297\n",
      "Iteration 5290 Loss: 0.07483722572096371\n",
      "Iteration 5291 Loss: 0.0748370505039654\n",
      "Iteration 5292 Loss: 0.07483687531011662\n",
      "Iteration 5293 Loss: 0.0748367001393858\n",
      "Iteration 5294 Loss: 0.07483652499174165\n",
      "Iteration 5295 Loss: 0.07483634986715289\n",
      "Iteration 5296 Loss: 0.07483617476558821\n",
      "Iteration 5297 Loss: 0.0748359996870165\n",
      "Iteration 5298 Loss: 0.07483582463140667\n",
      "Iteration 5299 Loss: 0.07483564959872765\n",
      "Iteration 5300 Loss: 0.07483547458894857\n",
      "Iteration 5301 Loss: 0.07483529960203852\n",
      "Iteration 5302 Loss: 0.0748351246379667\n",
      "Iteration 5303 Loss: 0.0748349496967023\n",
      "Iteration 5304 Loss: 0.07483477477821476\n",
      "Iteration 5305 Loss: 0.07483459988247335\n",
      "Iteration 5306 Loss: 0.07483442500944766\n",
      "Iteration 5307 Loss: 0.07483425015910716\n",
      "Iteration 5308 Loss: 0.07483407533142147\n",
      "Iteration 5309 Loss: 0.07483390052636027\n",
      "Iteration 5310 Loss: 0.07483372574389333\n",
      "Iteration 5311 Loss: 0.07483355098399043\n",
      "Iteration 5312 Loss: 0.07483337624662134\n",
      "Iteration 5313 Loss: 0.07483320153175625\n",
      "Iteration 5314 Loss: 0.07483302683936496\n",
      "Iteration 5315 Loss: 0.07483285216941771\n",
      "Iteration 5316 Loss: 0.07483267752188455\n",
      "Iteration 5317 Loss: 0.07483250289673578\n",
      "Iteration 5318 Loss: 0.07483232829394156\n",
      "Iteration 5319 Loss: 0.07483215371347232\n",
      "Iteration 5320 Loss: 0.07483197915529852\n",
      "Iteration 5321 Loss: 0.07483180461939057\n",
      "Iteration 5322 Loss: 0.07483163010571907\n",
      "Iteration 5323 Loss: 0.07483145561425461\n",
      "Iteration 5324 Loss: 0.07483128114496793\n",
      "Iteration 5325 Loss: 0.0748311066978297\n",
      "Iteration 5326 Loss: 0.07483093227281083\n",
      "Iteration 5327 Loss: 0.07483075786988211\n",
      "Iteration 5328 Loss: 0.07483058348901458\n",
      "Iteration 5329 Loss: 0.07483040913017923\n",
      "Iteration 5330 Loss: 0.07483023479334706\n",
      "Iteration 5331 Loss: 0.0748300604784893\n",
      "Iteration 5332 Loss: 0.07482988618557715\n",
      "Iteration 5333 Loss: 0.07482971191458195\n",
      "Iteration 5334 Loss: 0.07482953766547489\n",
      "Iteration 5335 Loss: 0.07482936343822748\n",
      "Iteration 5336 Loss: 0.07482918923281116\n",
      "Iteration 5337 Loss: 0.07482901504919745\n",
      "Iteration 5338 Loss: 0.07482884088735797\n",
      "Iteration 5339 Loss: 0.07482866674726443\n",
      "Iteration 5340 Loss: 0.0748284926288885\n",
      "Iteration 5341 Loss: 0.07482831853220195\n",
      "Iteration 5342 Loss: 0.07482814445717667\n",
      "Iteration 5343 Loss: 0.07482797040378464\n",
      "Iteration 5344 Loss: 0.0748277963719977\n",
      "Iteration 5345 Loss: 0.074827622361788\n",
      "Iteration 5346 Loss: 0.07482744837312763\n",
      "Iteration 5347 Loss: 0.07482727440598876\n",
      "Iteration 5348 Loss: 0.07482710046034355\n",
      "Iteration 5349 Loss: 0.07482692653616448\n",
      "Iteration 5350 Loss: 0.07482675263342364\n",
      "Iteration 5351 Loss: 0.07482657875209373\n",
      "Iteration 5352 Loss: 0.07482640489214704\n",
      "Iteration 5353 Loss: 0.07482623105355617\n",
      "Iteration 5354 Loss: 0.07482605723629374\n",
      "Iteration 5355 Loss: 0.07482588344033245\n",
      "Iteration 5356 Loss: 0.07482570966564497\n",
      "Iteration 5357 Loss: 0.07482553591220412\n",
      "Iteration 5358 Loss: 0.07482536217998273\n",
      "Iteration 5359 Loss: 0.07482518846895378\n",
      "Iteration 5360 Loss: 0.07482501477909016\n",
      "Iteration 5361 Loss: 0.07482484111036494\n",
      "Iteration 5362 Loss: 0.07482466746275127\n",
      "Iteration 5363 Loss: 0.07482449383622225\n",
      "Iteration 5364 Loss: 0.07482432023075107\n",
      "Iteration 5365 Loss: 0.07482414664631112\n",
      "Iteration 5366 Loss: 0.07482397308287562\n",
      "Iteration 5367 Loss: 0.07482379954041804\n",
      "Iteration 5368 Loss: 0.07482362601891182\n",
      "Iteration 5369 Loss: 0.07482345251833049\n",
      "Iteration 5370 Loss: 0.07482327903864758\n",
      "Iteration 5371 Loss: 0.07482310557983679\n",
      "Iteration 5372 Loss: 0.07482293214187179\n",
      "Iteration 5373 Loss: 0.07482275872472635\n",
      "Iteration 5374 Loss: 0.07482258532837427\n",
      "Iteration 5375 Loss: 0.07482241195278941\n",
      "Iteration 5376 Loss: 0.07482223859794575\n",
      "Iteration 5377 Loss: 0.07482206526381724\n",
      "Iteration 5378 Loss: 0.074821891950378\n",
      "Iteration 5379 Loss: 0.07482171865760201\n",
      "Iteration 5380 Loss: 0.07482154538546357\n",
      "Iteration 5381 Loss: 0.07482137213393683\n",
      "Iteration 5382 Loss: 0.07482119890299616\n",
      "Iteration 5383 Loss: 0.0748210256926158\n",
      "Iteration 5384 Loss: 0.07482085250277015\n",
      "Iteration 5385 Loss: 0.07482067933343375\n",
      "Iteration 5386 Loss: 0.07482050618458105\n",
      "Iteration 5387 Loss: 0.07482033305618667\n",
      "Iteration 5388 Loss: 0.07482015994822522\n",
      "Iteration 5389 Loss: 0.0748199868606714\n",
      "Iteration 5390 Loss: 0.07481981379349993\n",
      "Iteration 5391 Loss: 0.07481964074668558\n",
      "Iteration 5392 Loss: 0.07481946772020327\n",
      "Iteration 5393 Loss: 0.07481929471402786\n",
      "Iteration 5394 Loss: 0.07481912172813437\n",
      "Iteration 5395 Loss: 0.07481894876249781\n",
      "Iteration 5396 Loss: 0.07481877581709329\n",
      "Iteration 5397 Loss: 0.07481860289189589\n",
      "Iteration 5398 Loss: 0.07481842998688085\n",
      "Iteration 5399 Loss: 0.07481825710202342\n",
      "Iteration 5400 Loss: 0.07481808423729883\n",
      "Iteration 5401 Loss: 0.0748179113926826\n",
      "Iteration 5402 Loss: 0.07481773856815001\n",
      "Iteration 5403 Loss: 0.07481756576367657\n",
      "Iteration 5404 Loss: 0.07481739297923781\n",
      "Iteration 5405 Loss: 0.07481722021480934\n",
      "Iteration 5406 Loss: 0.07481704747036678\n",
      "Iteration 5407 Loss: 0.07481687474588578\n",
      "Iteration 5408 Loss: 0.0748167020413422\n",
      "Iteration 5409 Loss: 0.07481652935671168\n",
      "Iteration 5410 Loss: 0.07481635669197026\n",
      "Iteration 5411 Loss: 0.0748161840470937\n",
      "Iteration 5412 Loss: 0.07481601142205803\n",
      "Iteration 5413 Loss: 0.07481583881683927\n",
      "Iteration 5414 Loss: 0.07481566623141347\n",
      "Iteration 5415 Loss: 0.07481549366575682\n",
      "Iteration 5416 Loss: 0.07481532111984543\n",
      "Iteration 5417 Loss: 0.0748151485936556\n",
      "Iteration 5418 Loss: 0.07481497608716353\n",
      "Iteration 5419 Loss: 0.07481480360034562\n",
      "Iteration 5420 Loss: 0.07481463113317824\n",
      "Iteration 5421 Loss: 0.07481445868563787\n",
      "Iteration 5422 Loss: 0.07481428625770097\n",
      "Iteration 5423 Loss: 0.0748141138493441\n",
      "Iteration 5424 Loss: 0.0748139414605439\n",
      "Iteration 5425 Loss: 0.07481376909127704\n",
      "Iteration 5426 Loss: 0.07481359674152017\n",
      "Iteration 5427 Loss: 0.07481342441125009\n",
      "Iteration 5428 Loss: 0.07481325210044357\n",
      "Iteration 5429 Loss: 0.07481307980907756\n",
      "Iteration 5430 Loss: 0.07481290753712891\n",
      "Iteration 5431 Loss: 0.07481273528457463\n",
      "Iteration 5432 Loss: 0.07481256305139175\n",
      "Iteration 5433 Loss: 0.07481239083755724\n",
      "Iteration 5434 Loss: 0.07481221864304839\n",
      "Iteration 5435 Loss: 0.0748120464678423\n",
      "Iteration 5436 Loss: 0.07481187431191616\n",
      "Iteration 5437 Loss: 0.07481170217524728\n",
      "Iteration 5438 Loss: 0.07481153005781303\n",
      "Iteration 5439 Loss: 0.07481135795959075\n",
      "Iteration 5440 Loss: 0.07481118588055788\n",
      "Iteration 5441 Loss: 0.07481101382069189\n",
      "Iteration 5442 Loss: 0.07481084177997037\n",
      "Iteration 5443 Loss: 0.07481066975837078\n",
      "Iteration 5444 Loss: 0.07481049775587094\n",
      "Iteration 5445 Loss: 0.07481032577244842\n",
      "Iteration 5446 Loss: 0.0748101538080809\n",
      "Iteration 5447 Loss: 0.07480998186274632\n",
      "Iteration 5448 Loss: 0.07480980993642236\n",
      "Iteration 5449 Loss: 0.07480963802908705\n",
      "Iteration 5450 Loss: 0.07480946614071822\n",
      "Iteration 5451 Loss: 0.07480929427129383\n",
      "Iteration 5452 Loss: 0.074809122420792\n",
      "Iteration 5453 Loss: 0.07480895058919082\n",
      "Iteration 5454 Loss: 0.07480877877646834\n",
      "Iteration 5455 Loss: 0.07480860698260279\n",
      "Iteration 5456 Loss: 0.07480843520757238\n",
      "Iteration 5457 Loss: 0.07480826345135541\n",
      "Iteration 5458 Loss: 0.07480809171393021\n",
      "Iteration 5459 Loss: 0.07480791999527511\n",
      "Iteration 5460 Loss: 0.0748077482953686\n",
      "Iteration 5461 Loss: 0.07480757661418912\n",
      "Iteration 5462 Loss: 0.0748074049517152\n",
      "Iteration 5463 Loss: 0.07480723330792544\n",
      "Iteration 5464 Loss: 0.07480706168279834\n",
      "Iteration 5465 Loss: 0.0748068900763127\n",
      "Iteration 5466 Loss: 0.07480671848844712\n",
      "Iteration 5467 Loss: 0.0748065469191805\n",
      "Iteration 5468 Loss: 0.07480637536849151\n",
      "Iteration 5469 Loss: 0.07480620383635905\n",
      "Iteration 5470 Loss: 0.07480603232276213\n",
      "Iteration 5471 Loss: 0.07480586082767954\n",
      "Iteration 5472 Loss: 0.07480568935109036\n",
      "Iteration 5473 Loss: 0.07480551789297363\n",
      "Iteration 5474 Loss: 0.07480534645330843\n",
      "Iteration 5475 Loss: 0.07480517503207391\n",
      "Iteration 5476 Loss: 0.07480500362924927\n",
      "Iteration 5477 Loss: 0.07480483224481371\n",
      "Iteration 5478 Loss: 0.07480466087874649\n",
      "Iteration 5479 Loss: 0.07480448953102699\n",
      "Iteration 5480 Loss: 0.07480431820163456\n",
      "Iteration 5481 Loss: 0.07480414689054866\n",
      "Iteration 5482 Loss: 0.07480397559774862\n",
      "Iteration 5483 Loss: 0.07480380432321412\n",
      "Iteration 5484 Loss: 0.07480363306692461\n",
      "Iteration 5485 Loss: 0.0748034618288597\n",
      "Iteration 5486 Loss: 0.07480329060899908\n",
      "Iteration 5487 Loss: 0.07480311940732244\n",
      "Iteration 5488 Loss: 0.07480294822380948\n",
      "Iteration 5489 Loss: 0.07480277705843999\n",
      "Iteration 5490 Loss: 0.07480260591119386\n",
      "Iteration 5491 Loss: 0.07480243478205083\n",
      "Iteration 5492 Loss: 0.07480226367099099\n",
      "Iteration 5493 Loss: 0.07480209257799415\n",
      "Iteration 5494 Loss: 0.07480192150304045\n",
      "Iteration 5495 Loss: 0.0748017504461099\n",
      "Iteration 5496 Loss: 0.07480157940718254\n",
      "Iteration 5497 Loss: 0.07480140838623854\n",
      "Iteration 5498 Loss: 0.07480123738325811\n",
      "Iteration 5499 Loss: 0.07480106639822152\n",
      "Iteration 5500 Loss: 0.07480089543110902\n",
      "Iteration 5501 Loss: 0.07480072448190081\n",
      "Iteration 5502 Loss: 0.07480055355057744\n",
      "Iteration 5503 Loss: 0.07480038263711919\n",
      "Iteration 5504 Loss: 0.07480021174150657\n",
      "Iteration 5505 Loss: 0.07480004086372012\n",
      "Iteration 5506 Loss: 0.07479987000374025\n",
      "Iteration 5507 Loss: 0.07479969916154763\n",
      "Iteration 5508 Loss: 0.07479952833712292\n",
      "Iteration 5509 Loss: 0.07479935753044667\n",
      "Iteration 5510 Loss: 0.07479918674149973\n",
      "Iteration 5511 Loss: 0.07479901597026278\n",
      "Iteration 5512 Loss: 0.07479884521671658\n",
      "Iteration 5513 Loss: 0.07479867448084213\n",
      "Iteration 5514 Loss: 0.07479850376262014\n",
      "Iteration 5515 Loss: 0.0747983330620316\n",
      "Iteration 5516 Loss: 0.07479816237905754\n",
      "Iteration 5517 Loss: 0.0747979917136789\n",
      "Iteration 5518 Loss: 0.07479782106587676\n",
      "Iteration 5519 Loss: 0.07479765043563222\n",
      "Iteration 5520 Loss: 0.07479747982292637\n",
      "Iteration 5521 Loss: 0.07479730922774046\n",
      "Iteration 5522 Loss: 0.07479713865005573\n",
      "Iteration 5523 Loss: 0.07479696808985338\n",
      "Iteration 5524 Loss: 0.07479679754711474\n",
      "Iteration 5525 Loss: 0.07479662702182119\n",
      "Iteration 5526 Loss: 0.07479645651395407\n",
      "Iteration 5527 Loss: 0.07479628602349483\n",
      "Iteration 5528 Loss: 0.07479611555042502\n",
      "Iteration 5529 Loss: 0.07479594509472598\n",
      "Iteration 5530 Loss: 0.07479577465637952\n",
      "Iteration 5531 Loss: 0.07479560423536705\n",
      "Iteration 5532 Loss: 0.07479543383167025\n",
      "Iteration 5533 Loss: 0.0747952634452708\n",
      "Iteration 5534 Loss: 0.07479509307615044\n",
      "Iteration 5535 Loss: 0.07479492272429092\n",
      "Iteration 5536 Loss: 0.07479475238967404\n",
      "Iteration 5537 Loss: 0.07479458207228167\n",
      "Iteration 5538 Loss: 0.07479441177209566\n",
      "Iteration 5539 Loss: 0.07479424148909793\n",
      "Iteration 5540 Loss: 0.07479407122327052\n",
      "Iteration 5541 Loss: 0.07479390097459535\n",
      "Iteration 5542 Loss: 0.07479373074305452\n",
      "Iteration 5543 Loss: 0.07479356052863004\n",
      "Iteration 5544 Loss: 0.07479339033130412\n",
      "Iteration 5545 Loss: 0.0747932201510589\n",
      "Iteration 5546 Loss: 0.07479304998787659\n",
      "Iteration 5547 Loss: 0.0747928798417394\n",
      "Iteration 5548 Loss: 0.07479270971262961\n",
      "Iteration 5549 Loss: 0.0747925396005296\n",
      "Iteration 5550 Loss: 0.0747923695054217\n",
      "Iteration 5551 Loss: 0.07479219942728832\n",
      "Iteration 5552 Loss: 0.07479202936611193\n",
      "Iteration 5553 Loss: 0.07479185932187497\n",
      "Iteration 5554 Loss: 0.07479168929455997\n",
      "Iteration 5555 Loss: 0.07479151928414948\n",
      "Iteration 5556 Loss: 0.0747913492906261\n",
      "Iteration 5557 Loss: 0.0747911793139725\n",
      "Iteration 5558 Loss: 0.07479100935417131\n",
      "Iteration 5559 Loss: 0.07479083941120532\n",
      "Iteration 5560 Loss: 0.07479066948505722\n",
      "Iteration 5561 Loss: 0.0747904995757098\n",
      "Iteration 5562 Loss: 0.07479032968314583\n",
      "Iteration 5563 Loss: 0.07479015980734834\n",
      "Iteration 5564 Loss: 0.0747899899483001\n",
      "Iteration 5565 Loss: 0.0747898201059842\n",
      "Iteration 5566 Loss: 0.0747896502803834\n",
      "Iteration 5567 Loss: 0.0747894804714809\n",
      "Iteration 5568 Loss: 0.07478931067925969\n",
      "Iteration 5569 Loss: 0.07478914090370292\n",
      "Iteration 5570 Loss: 0.07478897114479363\n",
      "Iteration 5571 Loss: 0.07478880140251506\n",
      "Iteration 5572 Loss: 0.07478863167685038\n",
      "Iteration 5573 Loss: 0.07478846196778295\n",
      "Iteration 5574 Loss: 0.07478829227529582\n",
      "Iteration 5575 Loss: 0.07478812259937258\n",
      "Iteration 5576 Loss: 0.07478795293999638\n",
      "Iteration 5577 Loss: 0.0747877832971507\n",
      "Iteration 5578 Loss: 0.07478761367081896\n",
      "Iteration 5579 Loss: 0.0747874440609847\n",
      "Iteration 5580 Loss: 0.07478727446763131\n",
      "Iteration 5581 Loss: 0.07478710489074239\n",
      "Iteration 5582 Loss: 0.07478693533030153\n",
      "Iteration 5583 Loss: 0.07478676578629231\n",
      "Iteration 5584 Loss: 0.0747865962586984\n",
      "Iteration 5585 Loss: 0.07478642674750348\n",
      "Iteration 5586 Loss: 0.0747862572526913\n",
      "Iteration 5587 Loss: 0.07478608777424559\n",
      "Iteration 5588 Loss: 0.07478591831215013\n",
      "Iteration 5589 Loss: 0.07478574886638881\n",
      "Iteration 5590 Loss: 0.07478557943694548\n",
      "Iteration 5591 Loss: 0.07478541002380405\n",
      "Iteration 5592 Loss: 0.07478524062694838\n",
      "Iteration 5593 Loss: 0.07478507124636258\n",
      "Iteration 5594 Loss: 0.0747849018820306\n",
      "Iteration 5595 Loss: 0.07478473253393643\n",
      "Iteration 5596 Loss: 0.07478456320206417\n",
      "Iteration 5597 Loss: 0.0747843938863981\n",
      "Iteration 5598 Loss: 0.07478422458692215\n",
      "Iteration 5599 Loss: 0.07478405530362062\n",
      "Iteration 5600 Loss: 0.07478388603647772\n",
      "Iteration 5601 Loss: 0.07478371678547771\n",
      "Iteration 5602 Loss: 0.07478354755060489\n",
      "Iteration 5603 Loss: 0.07478337833184359\n",
      "Iteration 5604 Loss: 0.07478320912917816\n",
      "Iteration 5605 Loss: 0.074783039942593\n",
      "Iteration 5606 Loss: 0.07478287077207255\n",
      "Iteration 5607 Loss: 0.07478270161760124\n",
      "Iteration 5608 Loss: 0.07478253247916364\n",
      "Iteration 5609 Loss: 0.07478236335674425\n",
      "Iteration 5610 Loss: 0.07478219425032766\n",
      "Iteration 5611 Loss: 0.07478202515989847\n",
      "Iteration 5612 Loss: 0.0747818560854413\n",
      "Iteration 5613 Loss: 0.07478168702694081\n",
      "Iteration 5614 Loss: 0.07478151798438173\n",
      "Iteration 5615 Loss: 0.07478134895774882\n",
      "Iteration 5616 Loss: 0.07478117994702682\n",
      "Iteration 5617 Loss: 0.07478101095220058\n",
      "Iteration 5618 Loss: 0.07478084197325488\n",
      "Iteration 5619 Loss: 0.07478067301017471\n",
      "Iteration 5620 Loss: 0.07478050406294486\n",
      "Iteration 5621 Loss: 0.07478033513155033\n",
      "Iteration 5622 Loss: 0.07478016621597605\n",
      "Iteration 5623 Loss: 0.07477999731620721\n",
      "Iteration 5624 Loss: 0.07477982843222858\n",
      "Iteration 5625 Loss: 0.07477965956402535\n",
      "Iteration 5626 Loss: 0.07477949071158273\n",
      "Iteration 5627 Loss: 0.07477932187488576\n",
      "Iteration 5628 Loss: 0.07477915305391962\n",
      "Iteration 5629 Loss: 0.07477898424866955\n",
      "Iteration 5630 Loss: 0.0747788154591208\n",
      "Iteration 5631 Loss: 0.0747786466852586\n",
      "Iteration 5632 Loss: 0.07477847792706828\n",
      "Iteration 5633 Loss: 0.07477830918453522\n",
      "Iteration 5634 Loss: 0.07477814045764468\n",
      "Iteration 5635 Loss: 0.0747779717463822\n",
      "Iteration 5636 Loss: 0.07477780305073314\n",
      "Iteration 5637 Loss: 0.07477763437068292\n",
      "Iteration 5638 Loss: 0.0747774657062172\n",
      "Iteration 5639 Loss: 0.07477729705732136\n",
      "Iteration 5640 Loss: 0.0747771284239811\n",
      "Iteration 5641 Loss: 0.0747769598061819\n",
      "Iteration 5642 Loss: 0.07477679120390941\n",
      "Iteration 5643 Loss: 0.07477662261714932\n",
      "Iteration 5644 Loss: 0.07477645404588733\n",
      "Iteration 5645 Loss: 0.07477628549010913\n",
      "Iteration 5646 Loss: 0.07477611694980049\n",
      "Iteration 5647 Loss: 0.0747759484249472\n",
      "Iteration 5648 Loss: 0.07477577991553516\n",
      "Iteration 5649 Loss: 0.07477561142155006\n",
      "Iteration 5650 Loss: 0.07477544294297793\n",
      "Iteration 5651 Loss: 0.07477527447980457\n",
      "Iteration 5652 Loss: 0.07477510603201605\n",
      "Iteration 5653 Loss: 0.07477493759959816\n",
      "Iteration 5654 Loss: 0.07477476918253714\n",
      "Iteration 5655 Loss: 0.0747746007808189\n",
      "Iteration 5656 Loss: 0.07477443239442941\n",
      "Iteration 5657 Loss: 0.07477426402335494\n",
      "Iteration 5658 Loss: 0.07477409566758153\n",
      "Iteration 5659 Loss: 0.07477392732709541\n",
      "Iteration 5660 Loss: 0.07477375900188266\n",
      "Iteration 5661 Loss: 0.0747735906919296\n",
      "Iteration 5662 Loss: 0.07477342239722247\n",
      "Iteration 5663 Loss: 0.07477325411774756\n",
      "Iteration 5664 Loss: 0.0747730858534911\n",
      "Iteration 5665 Loss: 0.07477291760443949\n",
      "Iteration 5666 Loss: 0.07477274937057912\n",
      "Iteration 5667 Loss: 0.0747725811518964\n",
      "Iteration 5668 Loss: 0.07477241294837776\n",
      "Iteration 5669 Loss: 0.07477224476000961\n",
      "Iteration 5670 Loss: 0.07477207658677847\n",
      "Iteration 5671 Loss: 0.0747719084286709\n",
      "Iteration 5672 Loss: 0.07477174028567343\n",
      "Iteration 5673 Loss: 0.07477157215777262\n",
      "Iteration 5674 Loss: 0.0747714040449551\n",
      "Iteration 5675 Loss: 0.07477123594720757\n",
      "Iteration 5676 Loss: 0.07477106786451661\n",
      "Iteration 5677 Loss: 0.07477089979686893\n",
      "Iteration 5678 Loss: 0.07477073174425135\n",
      "Iteration 5679 Loss: 0.07477056370665051\n",
      "Iteration 5680 Loss: 0.0747703956840533\n",
      "Iteration 5681 Loss: 0.07477022767644648\n",
      "Iteration 5682 Loss: 0.07477005968381692\n",
      "Iteration 5683 Loss: 0.07476989170615152\n",
      "Iteration 5684 Loss: 0.07476972374343713\n",
      "Iteration 5685 Loss: 0.07476955579566072\n",
      "Iteration 5686 Loss: 0.07476938786280925\n",
      "Iteration 5687 Loss: 0.0747692199448697\n",
      "Iteration 5688 Loss: 0.07476905204182907\n",
      "Iteration 5689 Loss: 0.07476888415367455\n",
      "Iteration 5690 Loss: 0.07476871628039297\n",
      "Iteration 5691 Loss: 0.07476854842197159\n",
      "Iteration 5692 Loss: 0.07476838057839759\n",
      "Iteration 5693 Loss: 0.07476821274965803\n",
      "Iteration 5694 Loss: 0.07476804493574019\n",
      "Iteration 5695 Loss: 0.07476787713663116\n",
      "Iteration 5696 Loss: 0.07476770935231833\n",
      "Iteration 5697 Loss: 0.07476754158278895\n",
      "Iteration 5698 Loss: 0.07476737382803021\n",
      "Iteration 5699 Loss: 0.0747672060880295\n",
      "Iteration 5700 Loss: 0.07476703836277432\n",
      "Iteration 5701 Loss: 0.07476687065225185\n",
      "Iteration 5702 Loss: 0.07476670295644965\n",
      "Iteration 5703 Loss: 0.07476653527535505\n",
      "Iteration 5704 Loss: 0.07476636760895564\n",
      "Iteration 5705 Loss: 0.07476619995723884\n",
      "Iteration 5706 Loss: 0.07476603232019216\n",
      "Iteration 5707 Loss: 0.07476586469780326\n",
      "Iteration 5708 Loss: 0.07476569709005959\n",
      "Iteration 5709 Loss: 0.07476552949694884\n",
      "Iteration 5710 Loss: 0.07476536191845863\n",
      "Iteration 5711 Loss: 0.07476519435457663\n",
      "Iteration 5712 Loss: 0.07476502680529049\n",
      "Iteration 5713 Loss: 0.07476485927058799\n",
      "Iteration 5714 Loss: 0.07476469175045676\n",
      "Iteration 5715 Loss: 0.07476452424488478\n",
      "Iteration 5716 Loss: 0.07476435675385965\n",
      "Iteration 5717 Loss: 0.0747641892773692\n",
      "Iteration 5718 Loss: 0.07476402181540145\n",
      "Iteration 5719 Loss: 0.07476385436794412\n",
      "Iteration 5720 Loss: 0.07476368693498521\n",
      "Iteration 5721 Loss: 0.07476351951651258\n",
      "Iteration 5722 Loss: 0.0747633521125142\n",
      "Iteration 5723 Loss: 0.07476318472297812\n",
      "Iteration 5724 Loss: 0.0747630173478923\n",
      "Iteration 5725 Loss: 0.07476284998724478\n",
      "Iteration 5726 Loss: 0.07476268264102363\n",
      "Iteration 5727 Loss: 0.07476251530921697\n",
      "Iteration 5728 Loss: 0.07476234799181285\n",
      "Iteration 5729 Loss: 0.07476218068879945\n",
      "Iteration 5730 Loss: 0.07476201340016489\n",
      "Iteration 5731 Loss: 0.07476184612589751\n",
      "Iteration 5732 Loss: 0.07476167886598543\n",
      "Iteration 5733 Loss: 0.0747615116204168\n",
      "Iteration 5734 Loss: 0.0747613443891801\n",
      "Iteration 5735 Loss: 0.07476117717226347\n",
      "Iteration 5736 Loss: 0.07476100996965528\n",
      "Iteration 5737 Loss: 0.07476084278134384\n",
      "Iteration 5738 Loss: 0.07476067560731761\n",
      "Iteration 5739 Loss: 0.07476050844756493\n",
      "Iteration 5740 Loss: 0.07476034130207429\n",
      "Iteration 5741 Loss: 0.07476017417083403\n",
      "Iteration 5742 Loss: 0.07476000705383276\n",
      "Iteration 5743 Loss: 0.07475983995105887\n",
      "Iteration 5744 Loss: 0.07475967286250089\n",
      "Iteration 5745 Loss: 0.07475950578814754\n",
      "Iteration 5746 Loss: 0.07475933872798715\n",
      "Iteration 5747 Loss: 0.07475917168200848\n",
      "Iteration 5748 Loss: 0.0747590046502001\n",
      "Iteration 5749 Loss: 0.0747588376325508\n",
      "Iteration 5750 Loss: 0.07475867062904895\n",
      "Iteration 5751 Loss: 0.07475850363968362\n",
      "Iteration 5752 Loss: 0.07475833666444329\n",
      "Iteration 5753 Loss: 0.07475816970331674\n",
      "Iteration 5754 Loss: 0.07475800275629282\n",
      "Iteration 5755 Loss: 0.07475783582336029\n",
      "Iteration 5756 Loss: 0.07475766890450804\n",
      "Iteration 5757 Loss: 0.07475750199972478\n",
      "Iteration 5758 Loss: 0.07475733510899954\n",
      "Iteration 5759 Loss: 0.0747571682323211\n",
      "Iteration 5760 Loss: 0.07475700136967847\n",
      "Iteration 5761 Loss: 0.07475683452106051\n",
      "Iteration 5762 Loss: 0.07475666768645625\n",
      "Iteration 5763 Loss: 0.07475650086585472\n",
      "Iteration 5764 Loss: 0.0747563340592449\n",
      "Iteration 5765 Loss: 0.07475616726661578\n",
      "Iteration 5766 Loss: 0.0747560004879565\n",
      "Iteration 5767 Loss: 0.07475583372325612\n",
      "Iteration 5768 Loss: 0.0747556669725038\n",
      "Iteration 5769 Loss: 0.07475550023568862\n",
      "Iteration 5770 Loss: 0.07475533351279981\n",
      "Iteration 5771 Loss: 0.07475516680382646\n",
      "Iteration 5772 Loss: 0.07475500010875782\n",
      "Iteration 5773 Loss: 0.0747548334275832\n",
      "Iteration 5774 Loss: 0.07475466676029177\n",
      "Iteration 5775 Loss: 0.07475450010687286\n",
      "Iteration 5776 Loss: 0.07475433346731568\n",
      "Iteration 5777 Loss: 0.07475416684160975\n",
      "Iteration 5778 Loss: 0.07475400022974424\n",
      "Iteration 5779 Loss: 0.07475383363170857\n",
      "Iteration 5780 Loss: 0.07475366704749219\n",
      "Iteration 5781 Loss: 0.07475350047708453\n",
      "Iteration 5782 Loss: 0.07475333392047492\n",
      "Iteration 5783 Loss: 0.07475316737765292\n",
      "Iteration 5784 Loss: 0.074753000848608\n",
      "Iteration 5785 Loss: 0.07475283433332969\n",
      "Iteration 5786 Loss: 0.07475266783180753\n",
      "Iteration 5787 Loss: 0.07475250134403102\n",
      "Iteration 5788 Loss: 0.07475233486998978\n",
      "Iteration 5789 Loss: 0.07475216840967343\n",
      "Iteration 5790 Loss: 0.0747520019630716\n",
      "Iteration 5791 Loss: 0.07475183553017392\n",
      "Iteration 5792 Loss: 0.07475166911097007\n",
      "Iteration 5793 Loss: 0.0747515027054497\n",
      "Iteration 5794 Loss: 0.07475133631360262\n",
      "Iteration 5795 Loss: 0.07475116993541853\n",
      "Iteration 5796 Loss: 0.07475100357088708\n",
      "Iteration 5797 Loss: 0.07475083721999823\n",
      "Iteration 5798 Loss: 0.07475067088274176\n",
      "Iteration 5799 Loss: 0.0747505045591074\n",
      "Iteration 5800 Loss: 0.07475033824908506\n",
      "Iteration 5801 Loss: 0.07475017195266465\n",
      "Iteration 5802 Loss: 0.07475000566983597\n",
      "Iteration 5803 Loss: 0.07474983940058902\n",
      "Iteration 5804 Loss: 0.0747496731449137\n",
      "Iteration 5805 Loss: 0.0747495069028\n",
      "Iteration 5806 Loss: 0.07474934067423784\n",
      "Iteration 5807 Loss: 0.07474917445921733\n",
      "Iteration 5808 Loss: 0.07474900825772848\n",
      "Iteration 5809 Loss: 0.0747488420697612\n",
      "Iteration 5810 Loss: 0.07474867589530573\n",
      "Iteration 5811 Loss: 0.07474850973435204\n",
      "Iteration 5812 Loss: 0.07474834358689034\n",
      "Iteration 5813 Loss: 0.07474817745291068\n",
      "Iteration 5814 Loss: 0.07474801133240333\n",
      "Iteration 5815 Loss: 0.07474784522535834\n",
      "Iteration 5816 Loss: 0.07474767913176601\n",
      "Iteration 5817 Loss: 0.07474751305161646\n",
      "Iteration 5818 Loss: 0.07474734698490001\n",
      "Iteration 5819 Loss: 0.0747471809316069\n",
      "Iteration 5820 Loss: 0.07474701489172748\n",
      "Iteration 5821 Loss: 0.07474684886525193\n",
      "Iteration 5822 Loss: 0.07474668285217065\n",
      "Iteration 5823 Loss: 0.07474651685247402\n",
      "Iteration 5824 Loss: 0.0747463508661523\n",
      "Iteration 5825 Loss: 0.07474618489319602\n",
      "Iteration 5826 Loss: 0.07474601893359545\n",
      "Iteration 5827 Loss: 0.07474585298734113\n",
      "Iteration 5828 Loss: 0.07474568705442348\n",
      "Iteration 5829 Loss: 0.07474552113483295\n",
      "Iteration 5830 Loss: 0.07474535522856005\n",
      "Iteration 5831 Loss: 0.07474518933559532\n",
      "Iteration 5832 Loss: 0.07474502345592919\n",
      "Iteration 5833 Loss: 0.0747448575895524\n",
      "Iteration 5834 Loss: 0.0747446917364554\n",
      "Iteration 5835 Loss: 0.07474452589662879\n",
      "Iteration 5836 Loss: 0.0747443600700632\n",
      "Iteration 5837 Loss: 0.07474419425674925\n",
      "Iteration 5838 Loss: 0.07474402845667764\n",
      "Iteration 5839 Loss: 0.07474386266983905\n",
      "Iteration 5840 Loss: 0.07474369689622419\n",
      "Iteration 5841 Loss: 0.07474353113582367\n",
      "Iteration 5842 Loss: 0.07474336538862837\n",
      "Iteration 5843 Loss: 0.07474319965462892\n",
      "Iteration 5844 Loss: 0.07474303393381625\n",
      "Iteration 5845 Loss: 0.07474286822618097\n",
      "Iteration 5846 Loss: 0.07474270253171404\n",
      "Iteration 5847 Loss: 0.07474253685040631\n",
      "Iteration 5848 Loss: 0.07474237118224854\n",
      "Iteration 5849 Loss: 0.07474220552723164\n",
      "Iteration 5850 Loss: 0.07474203988534656\n",
      "Iteration 5851 Loss: 0.07474187425658416\n",
      "Iteration 5852 Loss: 0.07474170864093546\n",
      "Iteration 5853 Loss: 0.07474154303839127\n",
      "Iteration 5854 Loss: 0.07474137744894269\n",
      "Iteration 5855 Loss: 0.07474121187258069\n",
      "Iteration 5856 Loss: 0.0747410463092963\n",
      "Iteration 5857 Loss: 0.07474088075908049\n",
      "Iteration 5858 Loss: 0.07474071522192441\n",
      "Iteration 5859 Loss: 0.07474054969781901\n",
      "Iteration 5860 Loss: 0.0747403841867555\n",
      "Iteration 5861 Loss: 0.07474021868872491\n",
      "Iteration 5862 Loss: 0.07474005320371843\n",
      "Iteration 5863 Loss: 0.07473988773172721\n",
      "Iteration 5864 Loss: 0.07473972227274242\n",
      "Iteration 5865 Loss: 0.07473955682675515\n",
      "Iteration 5866 Loss: 0.07473939139375674\n",
      "Iteration 5867 Loss: 0.07473922597373835\n",
      "Iteration 5868 Loss: 0.07473906056669125\n",
      "Iteration 5869 Loss: 0.07473889517260668\n",
      "Iteration 5870 Loss: 0.07473872979147594\n",
      "Iteration 5871 Loss: 0.07473856442329035\n",
      "Iteration 5872 Loss: 0.07473839906804125\n",
      "Iteration 5873 Loss: 0.07473823372571993\n",
      "Iteration 5874 Loss: 0.07473806839631776\n",
      "Iteration 5875 Loss: 0.07473790307982608\n",
      "Iteration 5876 Loss: 0.07473773777623635\n",
      "Iteration 5877 Loss: 0.07473757248554005\n",
      "Iteration 5878 Loss: 0.07473740720772844\n",
      "Iteration 5879 Loss: 0.07473724194279308\n",
      "Iteration 5880 Loss: 0.0747370766907255\n",
      "Iteration 5881 Loss: 0.07473691145151702\n",
      "Iteration 5882 Loss: 0.0747367462251592\n",
      "Iteration 5883 Loss: 0.07473658101164372\n",
      "Iteration 5884 Loss: 0.07473641581096196\n",
      "Iteration 5885 Loss: 0.0747362506231055\n",
      "Iteration 5886 Loss: 0.07473608544806605\n",
      "Iteration 5887 Loss: 0.07473592028583503\n",
      "Iteration 5888 Loss: 0.07473575513640421\n",
      "Iteration 5889 Loss: 0.07473558999976508\n",
      "Iteration 5890 Loss: 0.07473542487590941\n",
      "Iteration 5891 Loss: 0.07473525976482878\n",
      "Iteration 5892 Loss: 0.07473509466651497\n",
      "Iteration 5893 Loss: 0.07473492958095963\n",
      "Iteration 5894 Loss: 0.07473476450815449\n",
      "Iteration 5895 Loss: 0.07473459944809129\n",
      "Iteration 5896 Loss: 0.07473443440076186\n",
      "Iteration 5897 Loss: 0.0747342693661579\n",
      "Iteration 5898 Loss: 0.07473410434427115\n",
      "Iteration 5899 Loss: 0.07473393933509363\n",
      "Iteration 5900 Loss: 0.07473377433861693\n",
      "Iteration 5901 Loss: 0.07473360935483302\n",
      "Iteration 5902 Loss: 0.07473344438373378\n",
      "Iteration 5903 Loss: 0.07473327942531109\n",
      "Iteration 5904 Loss: 0.07473311447955681\n",
      "Iteration 5905 Loss: 0.07473294954646284\n",
      "Iteration 5906 Loss: 0.0747327846260212\n",
      "Iteration 5907 Loss: 0.07473261971822381\n",
      "Iteration 5908 Loss: 0.07473245482306254\n",
      "Iteration 5909 Loss: 0.07473228994052955\n",
      "Iteration 5910 Loss: 0.07473212507061673\n",
      "Iteration 5911 Loss: 0.07473196021331621\n",
      "Iteration 5912 Loss: 0.07473179536861986\n",
      "Iteration 5913 Loss: 0.07473163053651984\n",
      "Iteration 5914 Loss: 0.07473146571700821\n",
      "Iteration 5915 Loss: 0.07473130091007708\n",
      "Iteration 5916 Loss: 0.07473113611571856\n",
      "Iteration 5917 Loss: 0.07473097133392478\n",
      "Iteration 5918 Loss: 0.0747308065646878\n",
      "Iteration 5919 Loss: 0.07473064180799982\n",
      "Iteration 5920 Loss: 0.07473047706385309\n",
      "Iteration 5921 Loss: 0.07473031233223973\n",
      "Iteration 5922 Loss: 0.07473014761315196\n",
      "Iteration 5923 Loss: 0.074729982906582\n",
      "Iteration 5924 Loss: 0.07472981821252209\n",
      "Iteration 5925 Loss: 0.07472965353096447\n",
      "Iteration 5926 Loss: 0.07472948886190148\n",
      "Iteration 5927 Loss: 0.07472932420532542\n",
      "Iteration 5928 Loss: 0.0747291595612285\n",
      "Iteration 5929 Loss: 0.0747289949296031\n",
      "Iteration 5930 Loss: 0.07472883031044156\n",
      "Iteration 5931 Loss: 0.07472866570373626\n",
      "Iteration 5932 Loss: 0.07472850110947958\n",
      "Iteration 5933 Loss: 0.07472833652766382\n",
      "Iteration 5934 Loss: 0.07472817195828148\n",
      "Iteration 5935 Loss: 0.07472800740132493\n",
      "Iteration 5936 Loss: 0.07472784285678664\n",
      "Iteration 5937 Loss: 0.07472767832465906\n",
      "Iteration 5938 Loss: 0.07472751380493466\n",
      "Iteration 5939 Loss: 0.07472734929760601\n",
      "Iteration 5940 Loss: 0.07472718480266541\n",
      "Iteration 5941 Loss: 0.0747270203201056\n",
      "Iteration 5942 Loss: 0.07472685584991896\n",
      "Iteration 5943 Loss: 0.07472669139209809\n",
      "Iteration 5944 Loss: 0.07472652694663559\n",
      "Iteration 5945 Loss: 0.07472636251352403\n",
      "Iteration 5946 Loss: 0.07472619809275599\n",
      "Iteration 5947 Loss: 0.07472603368432407\n",
      "Iteration 5948 Loss: 0.07472586928822092\n",
      "Iteration 5949 Loss: 0.07472570490443926\n",
      "Iteration 5950 Loss: 0.07472554053297166\n",
      "Iteration 5951 Loss: 0.07472537617381085\n",
      "Iteration 5952 Loss: 0.07472521182694944\n",
      "Iteration 5953 Loss: 0.07472504749238025\n",
      "Iteration 5954 Loss: 0.07472488317009593\n",
      "Iteration 5955 Loss: 0.07472471886008923\n",
      "Iteration 5956 Loss: 0.07472455456235297\n",
      "Iteration 5957 Loss: 0.0747243902768798\n",
      "Iteration 5958 Loss: 0.07472422600366264\n",
      "Iteration 5959 Loss: 0.07472406174269422\n",
      "Iteration 5960 Loss: 0.07472389749396736\n",
      "Iteration 5961 Loss: 0.07472373325747494\n",
      "Iteration 5962 Loss: 0.0747235690332097\n",
      "Iteration 5963 Loss: 0.0747234048211646\n",
      "Iteration 5964 Loss: 0.07472324062133254\n",
      "Iteration 5965 Loss: 0.07472307643370638\n",
      "Iteration 5966 Loss: 0.07472291225827891\n",
      "Iteration 5967 Loss: 0.07472274809504326\n",
      "Iteration 5968 Loss: 0.07472258394399219\n",
      "Iteration 5969 Loss: 0.07472241980511878\n",
      "Iteration 5970 Loss: 0.07472225567841594\n",
      "Iteration 5971 Loss: 0.07472209156387669\n",
      "Iteration 5972 Loss: 0.07472192746149403\n",
      "Iteration 5973 Loss: 0.07472176337126094\n",
      "Iteration 5974 Loss: 0.07472159929317042\n",
      "Iteration 5975 Loss: 0.07472143522721557\n",
      "Iteration 5976 Loss: 0.07472127117338945\n",
      "Iteration 5977 Loss: 0.07472110713168509\n",
      "Iteration 5978 Loss: 0.07472094310209568\n",
      "Iteration 5979 Loss: 0.07472077908461419\n",
      "Iteration 5980 Loss: 0.07472061507923382\n",
      "Iteration 5981 Loss: 0.07472045108594769\n",
      "Iteration 5982 Loss: 0.07472028710474884\n",
      "Iteration 5983 Loss: 0.07472012313563062\n",
      "Iteration 5984 Loss: 0.07471995917858608\n",
      "Iteration 5985 Loss: 0.07471979523360846\n",
      "Iteration 5986 Loss: 0.07471963130069094\n",
      "Iteration 5987 Loss: 0.07471946737982678\n",
      "Iteration 5988 Loss: 0.07471930347100912\n",
      "Iteration 5989 Loss: 0.07471913957423132\n",
      "Iteration 5990 Loss: 0.07471897568948656\n",
      "Iteration 5991 Loss: 0.07471881181676816\n",
      "Iteration 5992 Loss: 0.07471864795606942\n",
      "Iteration 5993 Loss: 0.07471848410738358\n",
      "Iteration 5994 Loss: 0.07471832027070408\n",
      "Iteration 5995 Loss: 0.07471815644602406\n",
      "Iteration 5996 Loss: 0.07471799263333709\n",
      "Iteration 5997 Loss: 0.0747178288326364\n",
      "Iteration 5998 Loss: 0.07471766504391539\n",
      "Iteration 5999 Loss: 0.07471750126716745\n",
      "Iteration 6000 Loss: 0.07471733750238596\n",
      "Iteration 6001 Loss: 0.07471717374956441\n",
      "Iteration 6002 Loss: 0.07471701000869621\n",
      "Iteration 6003 Loss: 0.07471684627977475\n",
      "Iteration 6004 Loss: 0.07471668256279344\n",
      "Iteration 6005 Loss: 0.07471651885774593\n",
      "Iteration 6006 Loss: 0.07471635516462563\n",
      "Iteration 6007 Loss: 0.07471619148342593\n",
      "Iteration 6008 Loss: 0.07471602781414052\n",
      "Iteration 6009 Loss: 0.07471586415676283\n",
      "Iteration 6010 Loss: 0.07471570051128637\n",
      "Iteration 6011 Loss: 0.0747155368777048\n",
      "Iteration 6012 Loss: 0.07471537325601163\n",
      "Iteration 6013 Loss: 0.0747152096462004\n",
      "Iteration 6014 Loss: 0.0747150460482648\n",
      "Iteration 6015 Loss: 0.07471488246219841\n",
      "Iteration 6016 Loss: 0.07471471888799483\n",
      "Iteration 6017 Loss: 0.07471455532564768\n",
      "Iteration 6018 Loss: 0.07471439177515066\n",
      "Iteration 6019 Loss: 0.07471422823649736\n",
      "Iteration 6020 Loss: 0.07471406470968156\n",
      "Iteration 6021 Loss: 0.07471390119469683\n",
      "Iteration 6022 Loss: 0.07471373769153707\n",
      "Iteration 6023 Loss: 0.07471357420019578\n",
      "Iteration 6024 Loss: 0.0747134107206668\n",
      "Iteration 6025 Loss: 0.07471324725294384\n",
      "Iteration 6026 Loss: 0.07471308379702073\n",
      "Iteration 6027 Loss: 0.0747129203528911\n",
      "Iteration 6028 Loss: 0.07471275692054886\n",
      "Iteration 6029 Loss: 0.07471259349998771\n",
      "Iteration 6030 Loss: 0.07471243009120157\n",
      "Iteration 6031 Loss: 0.0747122666941842\n",
      "Iteration 6032 Loss: 0.07471210330892938\n",
      "Iteration 6033 Loss: 0.07471193993543111\n",
      "Iteration 6034 Loss: 0.07471177657368307\n",
      "Iteration 6035 Loss: 0.07471161322367921\n",
      "Iteration 6036 Loss: 0.07471144988541353\n",
      "Iteration 6037 Loss: 0.07471128655887978\n",
      "Iteration 6038 Loss: 0.07471112324407195\n",
      "Iteration 6039 Loss: 0.07471095994098387\n",
      "Iteration 6040 Loss: 0.07471079664960963\n",
      "Iteration 6041 Loss: 0.07471063336994306\n",
      "Iteration 6042 Loss: 0.07471047010197811\n",
      "Iteration 6043 Loss: 0.0747103068457089\n",
      "Iteration 6044 Loss: 0.07471014360112929\n",
      "Iteration 6045 Loss: 0.07470998036823331\n",
      "Iteration 6046 Loss: 0.07470981714701504\n",
      "Iteration 6047 Loss: 0.07470965393746842\n",
      "Iteration 6048 Loss: 0.07470949073958746\n",
      "Iteration 6049 Loss: 0.07470932755336634\n",
      "Iteration 6050 Loss: 0.07470916437879906\n",
      "Iteration 6051 Loss: 0.07470900121587967\n",
      "Iteration 6052 Loss: 0.0747088380646023\n",
      "Iteration 6053 Loss: 0.074708674924961\n",
      "Iteration 6054 Loss: 0.07470851179695001\n",
      "Iteration 6055 Loss: 0.07470834868056334\n",
      "Iteration 6056 Loss: 0.0747081855757951\n",
      "Iteration 6057 Loss: 0.07470802248263955\n",
      "Iteration 6058 Loss: 0.07470785940109081\n",
      "Iteration 6059 Loss: 0.07470769633114305\n",
      "Iteration 6060 Loss: 0.07470753327279038\n",
      "Iteration 6061 Loss: 0.07470737022602712\n",
      "Iteration 6062 Loss: 0.07470720719084752\n",
      "Iteration 6063 Loss: 0.07470704416724562\n",
      "Iteration 6064 Loss: 0.07470688115521582\n",
      "Iteration 6065 Loss: 0.07470671815475227\n",
      "Iteration 6066 Loss: 0.07470655516584926\n",
      "Iteration 6067 Loss: 0.07470639218850114\n",
      "Iteration 6068 Loss: 0.07470622922270209\n",
      "Iteration 6069 Loss: 0.07470606626844639\n",
      "Iteration 6070 Loss: 0.07470590332572846\n",
      "Iteration 6071 Loss: 0.07470574039454254\n",
      "Iteration 6072 Loss: 0.07470557747488304\n",
      "Iteration 6073 Loss: 0.0747054145667442\n",
      "Iteration 6074 Loss: 0.07470525167012043\n",
      "Iteration 6075 Loss: 0.0747050887850061\n",
      "Iteration 6076 Loss: 0.07470492591139558\n",
      "Iteration 6077 Loss: 0.07470476304928324\n",
      "Iteration 6078 Loss: 0.07470460019866357\n",
      "Iteration 6079 Loss: 0.07470443735953083\n",
      "Iteration 6080 Loss: 0.07470427453187956\n",
      "Iteration 6081 Loss: 0.0747041117157043\n",
      "Iteration 6082 Loss: 0.07470394891099916\n",
      "Iteration 6083 Loss: 0.07470378611775895\n",
      "Iteration 6084 Loss: 0.07470362333597799\n",
      "Iteration 6085 Loss: 0.07470346056565078\n",
      "Iteration 6086 Loss: 0.07470329780677179\n",
      "Iteration 6087 Loss: 0.07470313505933561\n",
      "Iteration 6088 Loss: 0.07470297232333666\n",
      "Iteration 6089 Loss: 0.07470280959876953\n",
      "Iteration 6090 Loss: 0.07470264688562875\n",
      "Iteration 6091 Loss: 0.07470248418390883\n",
      "Iteration 6092 Loss: 0.07470232149360441\n",
      "Iteration 6093 Loss: 0.07470215881471001\n",
      "Iteration 6094 Loss: 0.0747019961472202\n",
      "Iteration 6095 Loss: 0.07470183349112966\n",
      "Iteration 6096 Loss: 0.07470167084643291\n",
      "Iteration 6097 Loss: 0.07470150821312464\n",
      "Iteration 6098 Loss: 0.07470134559119948\n",
      "Iteration 6099 Loss: 0.07470118298065198\n",
      "Iteration 6100 Loss: 0.07470102038147688\n",
      "Iteration 6101 Loss: 0.07470085779366886\n",
      "Iteration 6102 Loss: 0.07470069521722249\n",
      "Iteration 6103 Loss: 0.07470053265213258\n",
      "Iteration 6104 Loss: 0.07470037009839374\n",
      "Iteration 6105 Loss: 0.07470020755600078\n",
      "Iteration 6106 Loss: 0.07470004502494829\n",
      "Iteration 6107 Loss: 0.07469988250523107\n",
      "Iteration 6108 Loss: 0.07469971999684386\n",
      "Iteration 6109 Loss: 0.0746995574997814\n",
      "Iteration 6110 Loss: 0.07469939501403851\n",
      "Iteration 6111 Loss: 0.07469923253960983\n",
      "Iteration 6112 Loss: 0.07469907007649032\n",
      "Iteration 6113 Loss: 0.07469890762467461\n",
      "Iteration 6114 Loss: 0.0746987451841576\n",
      "Iteration 6115 Loss: 0.07469858275493416\n",
      "Iteration 6116 Loss: 0.074698420336999\n",
      "Iteration 6117 Loss: 0.07469825793034703\n",
      "Iteration 6118 Loss: 0.0746980955349731\n",
      "Iteration 6119 Loss: 0.07469793315087199\n",
      "Iteration 6120 Loss: 0.07469777077803867\n",
      "Iteration 6121 Loss: 0.07469760841646798\n",
      "Iteration 6122 Loss: 0.07469744606615482\n",
      "Iteration 6123 Loss: 0.0746972837270941\n",
      "Iteration 6124 Loss: 0.07469712139928067\n",
      "Iteration 6125 Loss: 0.07469695908270951\n",
      "Iteration 6126 Loss: 0.07469679677737563\n",
      "Iteration 6127 Loss: 0.07469663448327384\n",
      "Iteration 6128 Loss: 0.07469647220039917\n",
      "Iteration 6129 Loss: 0.07469630992874651\n",
      "Iteration 6130 Loss: 0.07469614766831094\n",
      "Iteration 6131 Loss: 0.07469598541908742\n",
      "Iteration 6132 Loss: 0.07469582318107089\n",
      "Iteration 6133 Loss: 0.07469566095425641\n",
      "Iteration 6134 Loss: 0.07469549873863894\n",
      "Iteration 6135 Loss: 0.07469533653421359\n",
      "Iteration 6136 Loss: 0.07469517434097536\n",
      "Iteration 6137 Loss: 0.07469501215891924\n",
      "Iteration 6138 Loss: 0.07469484998804041\n",
      "Iteration 6139 Loss: 0.07469468782833384\n",
      "Iteration 6140 Loss: 0.07469452567979463\n",
      "Iteration 6141 Loss: 0.07469436354241789\n",
      "Iteration 6142 Loss: 0.0746942014161987\n",
      "Iteration 6143 Loss: 0.07469403930113219\n",
      "Iteration 6144 Loss: 0.0746938771972134\n",
      "Iteration 6145 Loss: 0.0746937151044376\n",
      "Iteration 6146 Loss: 0.07469355302279987\n",
      "Iteration 6147 Loss: 0.0746933909522953\n",
      "Iteration 6148 Loss: 0.07469322889291909\n",
      "Iteration 6149 Loss: 0.0746930668446664\n",
      "Iteration 6150 Loss: 0.0746929048075324\n",
      "Iteration 6151 Loss: 0.07469274278151235\n",
      "Iteration 6152 Loss: 0.07469258076660136\n",
      "Iteration 6153 Loss: 0.07469241876279473\n",
      "Iteration 6154 Loss: 0.07469225677008755\n",
      "Iteration 6155 Loss: 0.07469209478847516\n",
      "Iteration 6156 Loss: 0.07469193281795279\n",
      "Iteration 6157 Loss: 0.07469177085851561\n",
      "Iteration 6158 Loss: 0.07469160891015893\n",
      "Iteration 6159 Loss: 0.074691446972878\n",
      "Iteration 6160 Loss: 0.07469128504666811\n",
      "Iteration 6161 Loss: 0.0746911231315246\n",
      "Iteration 6162 Loss: 0.0746909612274426\n",
      "Iteration 6163 Loss: 0.07469079933441766\n",
      "Iteration 6164 Loss: 0.0746906374524449\n",
      "Iteration 6165 Loss: 0.07469047558151971\n",
      "Iteration 6166 Loss: 0.07469031372163744\n",
      "Iteration 6167 Loss: 0.0746901518727934\n",
      "Iteration 6168 Loss: 0.07468999003498297\n",
      "Iteration 6169 Loss: 0.0746898282082015\n",
      "Iteration 6170 Loss: 0.07468966639244433\n",
      "Iteration 6171 Loss: 0.07468950458770693\n",
      "Iteration 6172 Loss: 0.0746893427939846\n",
      "Iteration 6173 Loss: 0.07468918101127284\n",
      "Iteration 6174 Loss: 0.07468901923956692\n",
      "Iteration 6175 Loss: 0.07468885747886239\n",
      "Iteration 6176 Loss: 0.0746886957291546\n",
      "Iteration 6177 Loss: 0.0746885339904391\n",
      "Iteration 6178 Loss: 0.07468837226271115\n",
      "Iteration 6179 Loss: 0.07468821054596639\n",
      "Iteration 6180 Loss: 0.07468804884020017\n",
      "Iteration 6181 Loss: 0.07468788714540803\n",
      "Iteration 6182 Loss: 0.07468772546158542\n",
      "Iteration 6183 Loss: 0.0746875637887278\n",
      "Iteration 6184 Loss: 0.07468740212683075\n",
      "Iteration 6185 Loss: 0.07468724047588976\n",
      "Iteration 6186 Loss: 0.0746870788359004\n",
      "Iteration 6187 Loss: 0.07468691720685801\n",
      "Iteration 6188 Loss: 0.07468675558875833\n",
      "Iteration 6189 Loss: 0.07468659398159685\n",
      "Iteration 6190 Loss: 0.07468643238536912\n",
      "Iteration 6191 Loss: 0.07468627080007063\n",
      "Iteration 6192 Loss: 0.07468610922569711\n",
      "Iteration 6193 Loss: 0.07468594766224398\n",
      "Iteration 6194 Loss: 0.07468578610970703\n",
      "Iteration 6195 Loss: 0.07468562456808167\n",
      "Iteration 6196 Loss: 0.0746854630373636\n",
      "Iteration 6197 Loss: 0.07468530151754847\n",
      "Iteration 6198 Loss: 0.07468514000863187\n",
      "Iteration 6199 Loss: 0.07468497851060944\n",
      "Iteration 6200 Loss: 0.07468481702347671\n",
      "Iteration 6201 Loss: 0.07468465554722961\n",
      "Iteration 6202 Loss: 0.07468449408186355\n",
      "Iteration 6203 Loss: 0.07468433262737435\n",
      "Iteration 6204 Loss: 0.07468417118375766\n",
      "Iteration 6205 Loss: 0.07468400975100908\n",
      "Iteration 6206 Loss: 0.07468384832912449\n",
      "Iteration 6207 Loss: 0.07468368691809936\n",
      "Iteration 6208 Loss: 0.07468352551792963\n",
      "Iteration 6209 Loss: 0.07468336412861092\n",
      "Iteration 6210 Loss: 0.07468320275013898\n",
      "Iteration 6211 Loss: 0.07468304138250954\n",
      "Iteration 6212 Loss: 0.07468288002571837\n",
      "Iteration 6213 Loss: 0.07468271867976123\n",
      "Iteration 6214 Loss: 0.07468255734463387\n",
      "Iteration 6215 Loss: 0.07468239602033211\n",
      "Iteration 6216 Loss: 0.07468223470685169\n",
      "Iteration 6217 Loss: 0.07468207340418839\n",
      "Iteration 6218 Loss: 0.0746819121123381\n",
      "Iteration 6219 Loss: 0.07468175083129658\n",
      "Iteration 6220 Loss: 0.07468158956105957\n",
      "Iteration 6221 Loss: 0.07468142830162304\n",
      "Iteration 6222 Loss: 0.07468126705298272\n",
      "Iteration 6223 Loss: 0.07468110581513451\n",
      "Iteration 6224 Loss: 0.07468094458807428\n",
      "Iteration 6225 Loss: 0.07468078337179784\n",
      "Iteration 6226 Loss: 0.0746806221663011\n",
      "Iteration 6227 Loss: 0.07468046097157985\n",
      "Iteration 6228 Loss: 0.07468029978763013\n",
      "Iteration 6229 Loss: 0.07468013861444762\n",
      "Iteration 6230 Loss: 0.07467997745202848\n",
      "Iteration 6231 Loss: 0.07467981630036849\n",
      "Iteration 6232 Loss: 0.07467965515946355\n",
      "Iteration 6233 Loss: 0.07467949402930958\n",
      "Iteration 6234 Loss: 0.07467933290990258\n",
      "Iteration 6235 Loss: 0.07467917180123848\n",
      "Iteration 6236 Loss: 0.07467901070331324\n",
      "Iteration 6237 Loss: 0.07467884961612267\n",
      "Iteration 6238 Loss: 0.074678688539663\n",
      "Iteration 6239 Loss: 0.07467852747392999\n",
      "Iteration 6240 Loss: 0.07467836641891974\n",
      "Iteration 6241 Loss: 0.07467820537462819\n",
      "Iteration 6242 Loss: 0.07467804434105138\n",
      "Iteration 6243 Loss: 0.07467788331818528\n",
      "Iteration 6244 Loss: 0.07467772230602598\n",
      "Iteration 6245 Loss: 0.07467756130456937\n",
      "Iteration 6246 Loss: 0.0746774003138116\n",
      "Iteration 6247 Loss: 0.07467723933374873\n",
      "Iteration 6248 Loss: 0.0746770783643767\n",
      "Iteration 6249 Loss: 0.0746769174056916\n",
      "Iteration 6250 Loss: 0.07467675645768954\n",
      "Iteration 6251 Loss: 0.07467659552036657\n",
      "Iteration 6252 Loss: 0.07467643459371873\n",
      "Iteration 6253 Loss: 0.07467627367774221\n",
      "Iteration 6254 Loss: 0.07467611277243298\n",
      "Iteration 6255 Loss: 0.07467595187778726\n",
      "Iteration 6256 Loss: 0.07467579099380105\n",
      "Iteration 6257 Loss: 0.07467563012047057\n",
      "Iteration 6258 Loss: 0.07467546925779187\n",
      "Iteration 6259 Loss: 0.07467530840576117\n",
      "Iteration 6260 Loss: 0.0746751475643745\n",
      "Iteration 6261 Loss: 0.07467498673362812\n",
      "Iteration 6262 Loss: 0.07467482591351811\n",
      "Iteration 6263 Loss: 0.07467466510404068\n",
      "Iteration 6264 Loss: 0.07467450430519193\n",
      "Iteration 6265 Loss: 0.07467434351696811\n",
      "Iteration 6266 Loss: 0.07467418273936541\n",
      "Iteration 6267 Loss: 0.07467402197238\n",
      "Iteration 6268 Loss: 0.07467386121600812\n",
      "Iteration 6269 Loss: 0.07467370047024599\n",
      "Iteration 6270 Loss: 0.0746735397350897\n",
      "Iteration 6271 Loss: 0.0746733790105356\n",
      "Iteration 6272 Loss: 0.07467321829657991\n",
      "Iteration 6273 Loss: 0.07467305759321888\n",
      "Iteration 6274 Loss: 0.0746728969004487\n",
      "Iteration 6275 Loss: 0.07467273621826562\n",
      "Iteration 6276 Loss: 0.07467257554666604\n",
      "Iteration 6277 Loss: 0.07467241488564606\n",
      "Iteration 6278 Loss: 0.07467225423520205\n",
      "Iteration 6279 Loss: 0.07467209359533025\n",
      "Iteration 6280 Loss: 0.074671932966027\n",
      "Iteration 6281 Loss: 0.07467177234728856\n",
      "Iteration 6282 Loss: 0.07467161173911126\n",
      "Iteration 6283 Loss: 0.07467145114149142\n",
      "Iteration 6284 Loss: 0.07467129055442538\n",
      "Iteration 6285 Loss: 0.0746711299779094\n",
      "Iteration 6286 Loss: 0.07467096941193992\n",
      "Iteration 6287 Loss: 0.07467080885651314\n",
      "Iteration 6288 Loss: 0.07467064831162554\n",
      "Iteration 6289 Loss: 0.07467048777727348\n",
      "Iteration 6290 Loss: 0.07467032725345317\n",
      "Iteration 6291 Loss: 0.07467016674016116\n",
      "Iteration 6292 Loss: 0.07467000623739375\n",
      "Iteration 6293 Loss: 0.07466984574514736\n",
      "Iteration 6294 Loss: 0.0746696852634183\n",
      "Iteration 6295 Loss: 0.0746695247922031\n",
      "Iteration 6296 Loss: 0.07466936433149804\n",
      "Iteration 6297 Loss: 0.0746692038812996\n",
      "Iteration 6298 Loss: 0.07466904344160426\n",
      "Iteration 6299 Loss: 0.0746688830124084\n",
      "Iteration 6300 Loss: 0.07466872259370837\n",
      "Iteration 6301 Loss: 0.07466856218550075\n",
      "Iteration 6302 Loss: 0.07466840178778192\n",
      "Iteration 6303 Loss: 0.07466824140054837\n",
      "Iteration 6304 Loss: 0.07466808102379648\n",
      "Iteration 6305 Loss: 0.07466792065752281\n",
      "Iteration 6306 Loss: 0.07466776030172381\n",
      "Iteration 6307 Loss: 0.07466759995639598\n",
      "Iteration 6308 Loss: 0.0746674396215358\n",
      "Iteration 6309 Loss: 0.07466727929713977\n",
      "Iteration 6310 Loss: 0.0746671189832044\n",
      "Iteration 6311 Loss: 0.07466695867972617\n",
      "Iteration 6312 Loss: 0.07466679838670162\n",
      "Iteration 6313 Loss: 0.07466663810412735\n",
      "Iteration 6314 Loss: 0.0746664778319998\n",
      "Iteration 6315 Loss: 0.07466631757031555\n",
      "Iteration 6316 Loss: 0.07466615731907114\n",
      "Iteration 6317 Loss: 0.07466599707826307\n",
      "Iteration 6318 Loss: 0.074665836847888\n",
      "Iteration 6319 Loss: 0.07466567662794239\n",
      "Iteration 6320 Loss: 0.07466551641842294\n",
      "Iteration 6321 Loss: 0.07466535621932616\n",
      "Iteration 6322 Loss: 0.07466519603064857\n",
      "Iteration 6323 Loss: 0.07466503585238689\n",
      "Iteration 6324 Loss: 0.07466487568453761\n",
      "Iteration 6325 Loss: 0.07466471552709746\n",
      "Iteration 6326 Loss: 0.07466455538006293\n",
      "Iteration 6327 Loss: 0.07466439524343071\n",
      "Iteration 6328 Loss: 0.07466423511719744\n",
      "Iteration 6329 Loss: 0.0746640750013597\n",
      "Iteration 6330 Loss: 0.07466391489591417\n",
      "Iteration 6331 Loss: 0.07466375480085749\n",
      "Iteration 6332 Loss: 0.0746635947161863\n",
      "Iteration 6333 Loss: 0.07466343464189726\n",
      "Iteration 6334 Loss: 0.07466327457798708\n",
      "Iteration 6335 Loss: 0.07466311452445235\n",
      "Iteration 6336 Loss: 0.07466295448128984\n",
      "Iteration 6337 Loss: 0.07466279444849615\n",
      "Iteration 6338 Loss: 0.074662634426068\n",
      "Iteration 6339 Loss: 0.07466247441400212\n",
      "Iteration 6340 Loss: 0.07466231441229523\n",
      "Iteration 6341 Loss: 0.07466215442094398\n",
      "Iteration 6342 Loss: 0.07466199443994515\n",
      "Iteration 6343 Loss: 0.07466183446929536\n",
      "Iteration 6344 Loss: 0.07466167450899146\n",
      "Iteration 6345 Loss: 0.07466151455903013\n",
      "Iteration 6346 Loss: 0.07466135461940816\n",
      "Iteration 6347 Loss: 0.07466119469012221\n",
      "Iteration 6348 Loss: 0.07466103477116912\n",
      "Iteration 6349 Loss: 0.07466087486254562\n",
      "Iteration 6350 Loss: 0.07466071496424845\n",
      "Iteration 6351 Loss: 0.07466055507627448\n",
      "Iteration 6352 Loss: 0.07466039519862033\n",
      "Iteration 6353 Loss: 0.07466023533128292\n",
      "Iteration 6354 Loss: 0.07466007547425899\n",
      "Iteration 6355 Loss: 0.0746599156275454\n",
      "Iteration 6356 Loss: 0.0746597557911389\n",
      "Iteration 6357 Loss: 0.07465959596503625\n",
      "Iteration 6358 Loss: 0.07465943614923437\n",
      "Iteration 6359 Loss: 0.07465927634373007\n",
      "Iteration 6360 Loss: 0.07465911654852012\n",
      "Iteration 6361 Loss: 0.0746589567636014\n",
      "Iteration 6362 Loss: 0.07465879698897075\n",
      "Iteration 6363 Loss: 0.07465863722462504\n",
      "Iteration 6364 Loss: 0.07465847747056105\n",
      "Iteration 6365 Loss: 0.0746583177267757\n",
      "Iteration 6366 Loss: 0.07465815799326586\n",
      "Iteration 6367 Loss: 0.07465799827002836\n",
      "Iteration 6368 Loss: 0.07465783855706012\n",
      "Iteration 6369 Loss: 0.07465767885435806\n",
      "Iteration 6370 Loss: 0.07465751916191898\n",
      "Iteration 6371 Loss: 0.07465735947973981\n",
      "Iteration 6372 Loss: 0.07465719980781747\n",
      "Iteration 6373 Loss: 0.07465704014614885\n",
      "Iteration 6374 Loss: 0.07465688049473092\n",
      "Iteration 6375 Loss: 0.07465672085356054\n",
      "Iteration 6376 Loss: 0.0746565612226346\n",
      "Iteration 6377 Loss: 0.07465640160195015\n",
      "Iteration 6378 Loss: 0.07465624199150407\n",
      "Iteration 6379 Loss: 0.07465608239129325\n",
      "Iteration 6380 Loss: 0.07465592280131468\n",
      "Iteration 6381 Loss: 0.07465576322156538\n",
      "Iteration 6382 Loss: 0.07465560365204219\n",
      "Iteration 6383 Loss: 0.07465544409274219\n",
      "Iteration 6384 Loss: 0.07465528454366227\n",
      "Iteration 6385 Loss: 0.07465512500479954\n",
      "Iteration 6386 Loss: 0.07465496547615078\n",
      "Iteration 6387 Loss: 0.07465480595771315\n",
      "Iteration 6388 Loss: 0.07465464644948358\n",
      "Iteration 6389 Loss: 0.07465448695145904\n",
      "Iteration 6390 Loss: 0.07465432746363657\n",
      "Iteration 6391 Loss: 0.07465416798601328\n",
      "Iteration 6392 Loss: 0.07465400851858602\n",
      "Iteration 6393 Loss: 0.07465384906135193\n",
      "Iteration 6394 Loss: 0.07465368961430796\n",
      "Iteration 6395 Loss: 0.07465353017745127\n",
      "Iteration 6396 Loss: 0.07465337075077881\n",
      "Iteration 6397 Loss: 0.0746532113342876\n",
      "Iteration 6398 Loss: 0.07465305192797478\n",
      "Iteration 6399 Loss: 0.07465289253183731\n",
      "Iteration 6400 Loss: 0.07465273314587234\n",
      "Iteration 6401 Loss: 0.07465257377007688\n",
      "Iteration 6402 Loss: 0.07465241440444807\n",
      "Iteration 6403 Loss: 0.07465225504898296\n",
      "Iteration 6404 Loss: 0.07465209570367858\n",
      "Iteration 6405 Loss: 0.0746519363685321\n",
      "Iteration 6406 Loss: 0.0746517770435406\n",
      "Iteration 6407 Loss: 0.07465161772870121\n",
      "Iteration 6408 Loss: 0.07465145842401094\n",
      "Iteration 6409 Loss: 0.074651299129467\n",
      "Iteration 6410 Loss: 0.07465113984506648\n",
      "Iteration 6411 Loss: 0.07465098057080645\n",
      "Iteration 6412 Loss: 0.07465082130668416\n",
      "Iteration 6413 Loss: 0.07465066205269666\n",
      "Iteration 6414 Loss: 0.07465050280884107\n",
      "Iteration 6415 Loss: 0.07465034357511457\n",
      "Iteration 6416 Loss: 0.07465018435151434\n",
      "Iteration 6417 Loss: 0.07465002513803758\n",
      "Iteration 6418 Loss: 0.07464986593468129\n",
      "Iteration 6419 Loss: 0.0746497067414428\n",
      "Iteration 6420 Loss: 0.07464954755831914\n",
      "Iteration 6421 Loss: 0.07464938838530766\n",
      "Iteration 6422 Loss: 0.07464922922240538\n",
      "Iteration 6423 Loss: 0.07464907006960962\n",
      "Iteration 6424 Loss: 0.0746489109269175\n",
      "Iteration 6425 Loss: 0.07464875179432624\n",
      "Iteration 6426 Loss: 0.07464859267183302\n",
      "Iteration 6427 Loss: 0.07464843355943515\n",
      "Iteration 6428 Loss: 0.07464827445712965\n",
      "Iteration 6429 Loss: 0.07464811536491392\n",
      "Iteration 6430 Loss: 0.07464795628278517\n",
      "Iteration 6431 Loss: 0.07464779721074054\n",
      "Iteration 6432 Loss: 0.07464763814877731\n",
      "Iteration 6433 Loss: 0.07464747909689269\n",
      "Iteration 6434 Loss: 0.07464732005508408\n",
      "Iteration 6435 Loss: 0.07464716102334853\n",
      "Iteration 6436 Loss: 0.07464700200168335\n",
      "Iteration 6437 Loss: 0.07464684299008588\n",
      "Iteration 6438 Loss: 0.07464668398855334\n",
      "Iteration 6439 Loss: 0.07464652499708305\n",
      "Iteration 6440 Loss: 0.07464636601567218\n",
      "Iteration 6441 Loss: 0.0746462070443181\n",
      "Iteration 6442 Loss: 0.07464604808301807\n",
      "Iteration 6443 Loss: 0.07464588913176937\n",
      "Iteration 6444 Loss: 0.07464573019056935\n",
      "Iteration 6445 Loss: 0.07464557125941526\n",
      "Iteration 6446 Loss: 0.07464541233830443\n",
      "Iteration 6447 Loss: 0.07464525342723417\n",
      "Iteration 6448 Loss: 0.07464509452620183\n",
      "Iteration 6449 Loss: 0.07464493563520465\n",
      "Iteration 6450 Loss: 0.07464477675424001\n",
      "Iteration 6451 Loss: 0.07464461788330529\n",
      "Iteration 6452 Loss: 0.07464445902239773\n",
      "Iteration 6453 Loss: 0.07464430017151473\n",
      "Iteration 6454 Loss: 0.07464414133065367\n",
      "Iteration 6455 Loss: 0.07464398249981183\n",
      "Iteration 6456 Loss: 0.07464382367898666\n",
      "Iteration 6457 Loss: 0.07464366486817546\n",
      "Iteration 6458 Loss: 0.0746435060673755\n",
      "Iteration 6459 Loss: 0.07464334727658434\n",
      "Iteration 6460 Loss: 0.07464318849579928\n",
      "Iteration 6461 Loss: 0.07464302972501766\n",
      "Iteration 6462 Loss: 0.07464287096423693\n",
      "Iteration 6463 Loss: 0.07464271221345442\n",
      "Iteration 6464 Loss: 0.07464255347266761\n",
      "Iteration 6465 Loss: 0.07464239474187377\n",
      "Iteration 6466 Loss: 0.07464223602107045\n",
      "Iteration 6467 Loss: 0.07464207731025503\n",
      "Iteration 6468 Loss: 0.07464191860942485\n",
      "Iteration 6469 Loss: 0.07464175991857744\n",
      "Iteration 6470 Loss: 0.07464160123771012\n",
      "Iteration 6471 Loss: 0.07464144256682036\n",
      "Iteration 6472 Loss: 0.07464128390590559\n",
      "Iteration 6473 Loss: 0.07464112525496328\n",
      "Iteration 6474 Loss: 0.0746409666139908\n",
      "Iteration 6475 Loss: 0.07464080798298574\n",
      "Iteration 6476 Loss: 0.07464064936194541\n",
      "Iteration 6477 Loss: 0.07464049075086734\n",
      "Iteration 6478 Loss: 0.07464033214974893\n",
      "Iteration 6479 Loss: 0.07464017355858776\n",
      "Iteration 6480 Loss: 0.07464001497738124\n",
      "Iteration 6481 Loss: 0.0746398564061268\n",
      "Iteration 6482 Loss: 0.07463969784482198\n",
      "Iteration 6483 Loss: 0.07463953929346431\n",
      "Iteration 6484 Loss: 0.07463938075205116\n",
      "Iteration 6485 Loss: 0.07463922222058013\n",
      "Iteration 6486 Loss: 0.07463906369904867\n",
      "Iteration 6487 Loss: 0.07463890518745428\n",
      "Iteration 6488 Loss: 0.07463874668579454\n",
      "Iteration 6489 Loss: 0.07463858819406684\n",
      "Iteration 6490 Loss: 0.07463842971226879\n",
      "Iteration 6491 Loss: 0.0746382712403979\n",
      "Iteration 6492 Loss: 0.07463811277845174\n",
      "Iteration 6493 Loss: 0.07463795432642772\n",
      "Iteration 6494 Loss: 0.07463779588432348\n",
      "Iteration 6495 Loss: 0.07463763745213652\n",
      "Iteration 6496 Loss: 0.07463747902986437\n",
      "Iteration 6497 Loss: 0.07463732061750464\n",
      "Iteration 6498 Loss: 0.07463716221505484\n",
      "Iteration 6499 Loss: 0.07463700382251258\n",
      "Iteration 6500 Loss: 0.07463684543987531\n",
      "Iteration 6501 Loss: 0.07463668706714073\n",
      "Iteration 6502 Loss: 0.07463652870430633\n",
      "Iteration 6503 Loss: 0.0746363703513697\n",
      "Iteration 6504 Loss: 0.07463621200832844\n",
      "Iteration 6505 Loss: 0.07463605367518014\n",
      "Iteration 6506 Loss: 0.07463589535192236\n",
      "Iteration 6507 Loss: 0.07463573703855275\n",
      "Iteration 6508 Loss: 0.07463557873506879\n",
      "Iteration 6509 Loss: 0.07463542044146823\n",
      "Iteration 6510 Loss: 0.07463526215774859\n",
      "Iteration 6511 Loss: 0.07463510388390748\n",
      "Iteration 6512 Loss: 0.07463494561994259\n",
      "Iteration 6513 Loss: 0.07463478736585143\n",
      "Iteration 6514 Loss: 0.07463462912163173\n",
      "Iteration 6515 Loss: 0.07463447088728101\n",
      "Iteration 6516 Loss: 0.07463431266279703\n",
      "Iteration 6517 Loss: 0.07463415444817731\n",
      "Iteration 6518 Loss: 0.07463399624341957\n",
      "Iteration 6519 Loss: 0.0746338380485215\n",
      "Iteration 6520 Loss: 0.07463367986348059\n",
      "Iteration 6521 Loss: 0.0746335216882946\n",
      "Iteration 6522 Loss: 0.07463336352296117\n",
      "Iteration 6523 Loss: 0.07463320536747806\n",
      "Iteration 6524 Loss: 0.07463304722184276\n",
      "Iteration 6525 Loss: 0.07463288908605303\n",
      "Iteration 6526 Loss: 0.07463273096010654\n",
      "Iteration 6527 Loss: 0.07463257284400096\n",
      "Iteration 6528 Loss: 0.07463241473773403\n",
      "Iteration 6529 Loss: 0.07463225664130334\n",
      "Iteration 6530 Loss: 0.07463209855470668\n",
      "Iteration 6531 Loss: 0.07463194047794172\n",
      "Iteration 6532 Loss: 0.07463178241100611\n",
      "Iteration 6533 Loss: 0.07463162435389756\n",
      "Iteration 6534 Loss: 0.0746314663066139\n",
      "Iteration 6535 Loss: 0.07463130826915268\n",
      "Iteration 6536 Loss: 0.0746311502415117\n",
      "Iteration 6537 Loss: 0.07463099222368864\n",
      "Iteration 6538 Loss: 0.07463083421568131\n",
      "Iteration 6539 Loss: 0.07463067621748734\n",
      "Iteration 6540 Loss: 0.07463051822910453\n",
      "Iteration 6541 Loss: 0.07463036025053058\n",
      "Iteration 6542 Loss: 0.07463020228176327\n",
      "Iteration 6543 Loss: 0.07463004432280028\n",
      "Iteration 6544 Loss: 0.07462988637363943\n",
      "Iteration 6545 Loss: 0.07462972843427844\n",
      "Iteration 6546 Loss: 0.07462957050471505\n",
      "Iteration 6547 Loss: 0.07462941258494707\n",
      "Iteration 6548 Loss: 0.0746292546749722\n",
      "Iteration 6549 Loss: 0.07462909677478831\n",
      "Iteration 6550 Loss: 0.074628938884393\n",
      "Iteration 6551 Loss: 0.07462878100378423\n",
      "Iteration 6552 Loss: 0.0746286231329597\n",
      "Iteration 6553 Loss: 0.07462846527191727\n",
      "Iteration 6554 Loss: 0.07462830742065454\n",
      "Iteration 6555 Loss: 0.0746281495791695\n",
      "Iteration 6556 Loss: 0.0746279917474599\n",
      "Iteration 6557 Loss: 0.07462783392552345\n",
      "Iteration 6558 Loss: 0.07462767611335804\n",
      "Iteration 6559 Loss: 0.07462751831096145\n",
      "Iteration 6560 Loss: 0.07462736051833155\n",
      "Iteration 6561 Loss: 0.07462720273546605\n",
      "Iteration 6562 Loss: 0.07462704496236286\n",
      "Iteration 6563 Loss: 0.07462688719901973\n",
      "Iteration 6564 Loss: 0.07462672944543455\n",
      "Iteration 6565 Loss: 0.07462657170160515\n",
      "Iteration 6566 Loss: 0.07462641396752935\n",
      "Iteration 6567 Loss: 0.07462625624320493\n",
      "Iteration 6568 Loss: 0.07462609852862992\n",
      "Iteration 6569 Loss: 0.07462594082380193\n",
      "Iteration 6570 Loss: 0.0746257831287189\n",
      "Iteration 6571 Loss: 0.0746256254433787\n",
      "Iteration 6572 Loss: 0.07462546776777923\n",
      "Iteration 6573 Loss: 0.07462531010191831\n",
      "Iteration 6574 Loss: 0.07462515244579385\n",
      "Iteration 6575 Loss: 0.07462499479940361\n",
      "Iteration 6576 Loss: 0.07462483716274557\n",
      "Iteration 6577 Loss: 0.07462467953581751\n",
      "Iteration 6578 Loss: 0.07462452191861745\n",
      "Iteration 6579 Loss: 0.07462436431114317\n",
      "Iteration 6580 Loss: 0.07462420671339255\n",
      "Iteration 6581 Loss: 0.07462404912536351\n",
      "Iteration 6582 Loss: 0.074623891547054\n",
      "Iteration 6583 Loss: 0.07462373397846186\n",
      "Iteration 6584 Loss: 0.07462357641958497\n",
      "Iteration 6585 Loss: 0.07462341887042125\n",
      "Iteration 6586 Loss: 0.07462326133096868\n",
      "Iteration 6587 Loss: 0.0746231038012251\n",
      "Iteration 6588 Loss: 0.07462294628118846\n",
      "Iteration 6589 Loss: 0.07462278877085664\n",
      "Iteration 6590 Loss: 0.07462263127022765\n",
      "Iteration 6591 Loss: 0.07462247377929934\n",
      "Iteration 6592 Loss: 0.07462231629806966\n",
      "Iteration 6593 Loss: 0.07462215882653656\n",
      "Iteration 6594 Loss: 0.07462200136469799\n",
      "Iteration 6595 Loss: 0.07462184391255185\n",
      "Iteration 6596 Loss: 0.07462168647009608\n",
      "Iteration 6597 Loss: 0.07462152903732869\n",
      "Iteration 6598 Loss: 0.07462137161424766\n",
      "Iteration 6599 Loss: 0.0746212142008508\n",
      "Iteration 6600 Loss: 0.07462105679713622\n",
      "Iteration 6601 Loss: 0.0746208994031018\n",
      "Iteration 6602 Loss: 0.07462074201874555\n",
      "Iteration 6603 Loss: 0.07462058464406543\n",
      "Iteration 6604 Loss: 0.07462042727905942\n",
      "Iteration 6605 Loss: 0.07462026992372542\n",
      "Iteration 6606 Loss: 0.07462011257806156\n",
      "Iteration 6607 Loss: 0.07461995524206573\n",
      "Iteration 6608 Loss: 0.07461979791573589\n",
      "Iteration 6609 Loss: 0.07461964059907007\n",
      "Iteration 6610 Loss: 0.07461948329206633\n",
      "Iteration 6611 Loss: 0.07461932599472257\n",
      "Iteration 6612 Loss: 0.07461916870703686\n",
      "Iteration 6613 Loss: 0.07461901142900715\n",
      "Iteration 6614 Loss: 0.07461885416063149\n",
      "Iteration 6615 Loss: 0.0746186969019079\n",
      "Iteration 6616 Loss: 0.07461853965283435\n",
      "Iteration 6617 Loss: 0.07461838241340889\n",
      "Iteration 6618 Loss: 0.07461822518362957\n",
      "Iteration 6619 Loss: 0.07461806796349427\n",
      "Iteration 6620 Loss: 0.07461791075300127\n",
      "Iteration 6621 Loss: 0.07461775355214836\n",
      "Iteration 6622 Loss: 0.07461759636093378\n",
      "Iteration 6623 Loss: 0.07461743917935544\n",
      "Iteration 6624 Loss: 0.07461728200741145\n",
      "Iteration 6625 Loss: 0.07461712484509973\n",
      "Iteration 6626 Loss: 0.0746169676924185\n",
      "Iteration 6627 Loss: 0.07461681054936573\n",
      "Iteration 6628 Loss: 0.07461665341593944\n",
      "Iteration 6629 Loss: 0.07461649629213776\n",
      "Iteration 6630 Loss: 0.07461633917795875\n",
      "Iteration 6631 Loss: 0.07461618207340041\n",
      "Iteration 6632 Loss: 0.07461602497846087\n",
      "Iteration 6633 Loss: 0.07461586789313818\n",
      "Iteration 6634 Loss: 0.07461571081743043\n",
      "Iteration 6635 Loss: 0.07461555375133568\n",
      "Iteration 6636 Loss: 0.07461539669485201\n",
      "Iteration 6637 Loss: 0.07461523964797756\n",
      "Iteration 6638 Loss: 0.07461508261071033\n",
      "Iteration 6639 Loss: 0.07461492558304848\n",
      "Iteration 6640 Loss: 0.07461476856499012\n",
      "Iteration 6641 Loss: 0.07461461155653326\n",
      "Iteration 6642 Loss: 0.0746144545576761\n",
      "Iteration 6643 Loss: 0.07461429756841667\n",
      "Iteration 6644 Loss: 0.07461414058875313\n",
      "Iteration 6645 Loss: 0.07461398361868354\n",
      "Iteration 6646 Loss: 0.07461382665820615\n",
      "Iteration 6647 Loss: 0.07461366970731886\n",
      "Iteration 6648 Loss: 0.07461351276601995\n",
      "Iteration 6649 Loss: 0.07461335583430748\n",
      "Iteration 6650 Loss: 0.07461319891217963\n",
      "Iteration 6651 Loss: 0.07461304199963448\n",
      "Iteration 6652 Loss: 0.07461288509667023\n",
      "Iteration 6653 Loss: 0.0746127282032849\n",
      "Iteration 6654 Loss: 0.07461257131947674\n",
      "Iteration 6655 Loss: 0.07461241444524382\n",
      "Iteration 6656 Loss: 0.07461225758058436\n",
      "Iteration 6657 Loss: 0.07461210072549646\n",
      "Iteration 6658 Loss: 0.07461194387997828\n",
      "Iteration 6659 Loss: 0.07461178704402796\n",
      "Iteration 6660 Loss: 0.07461163021764372\n",
      "Iteration 6661 Loss: 0.0746114734008237\n",
      "Iteration 6662 Loss: 0.07461131659356593\n",
      "Iteration 6663 Loss: 0.07461115979586876\n",
      "Iteration 6664 Loss: 0.07461100300773034\n",
      "Iteration 6665 Loss: 0.07461084622914871\n",
      "Iteration 6666 Loss: 0.07461068946012217\n",
      "Iteration 6667 Loss: 0.07461053270064887\n",
      "Iteration 6668 Loss: 0.07461037595072695\n",
      "Iteration 6669 Loss: 0.07461021921035475\n",
      "Iteration 6670 Loss: 0.07461006247953018\n",
      "Iteration 6671 Loss: 0.0746099057582517\n",
      "Iteration 6672 Loss: 0.07460974904651738\n",
      "Iteration 6673 Loss: 0.07460959234432543\n",
      "Iteration 6674 Loss: 0.07460943565167405\n",
      "Iteration 6675 Loss: 0.07460927896856144\n",
      "Iteration 6676 Loss: 0.07460912229498587\n",
      "Iteration 6677 Loss: 0.07460896563094546\n",
      "Iteration 6678 Loss: 0.07460880897643844\n",
      "Iteration 6679 Loss: 0.07460865233146313\n",
      "Iteration 6680 Loss: 0.07460849569601757\n",
      "Iteration 6681 Loss: 0.07460833907010014\n",
      "Iteration 6682 Loss: 0.07460818245370904\n",
      "Iteration 6683 Loss: 0.07460802584684241\n",
      "Iteration 6684 Loss: 0.07460786924949858\n",
      "Iteration 6685 Loss: 0.07460771266167568\n",
      "Iteration 6686 Loss: 0.07460755608337204\n",
      "Iteration 6687 Loss: 0.07460739951458584\n",
      "Iteration 6688 Loss: 0.07460724295531533\n",
      "Iteration 6689 Loss: 0.07460708640555883\n",
      "Iteration 6690 Loss: 0.07460692986531448\n",
      "Iteration 6691 Loss: 0.07460677333458059\n",
      "Iteration 6692 Loss: 0.07460661681335543\n",
      "Iteration 6693 Loss: 0.07460646030163721\n",
      "Iteration 6694 Loss: 0.07460630379942423\n",
      "Iteration 6695 Loss: 0.0746061473067147\n",
      "Iteration 6696 Loss: 0.07460599082350691\n",
      "Iteration 6697 Loss: 0.07460583434979914\n",
      "Iteration 6698 Loss: 0.07460567788558978\n",
      "Iteration 6699 Loss: 0.0746055214308768\n",
      "Iteration 6700 Loss: 0.07460536498565873\n",
      "Iteration 6701 Loss: 0.0746052085499338\n",
      "Iteration 6702 Loss: 0.07460505212370025\n",
      "Iteration 6703 Loss: 0.07460489570695637\n",
      "Iteration 6704 Loss: 0.07460473929970045\n",
      "Iteration 6705 Loss: 0.07460458290193087\n",
      "Iteration 6706 Loss: 0.07460442651364577\n",
      "Iteration 6707 Loss: 0.07460427013484357\n",
      "Iteration 6708 Loss: 0.07460411376552248\n",
      "Iteration 6709 Loss: 0.07460395740568088\n",
      "Iteration 6710 Loss: 0.07460380105531701\n",
      "Iteration 6711 Loss: 0.07460364471442923\n",
      "Iteration 6712 Loss: 0.07460348838301575\n",
      "Iteration 6713 Loss: 0.074603332061075\n",
      "Iteration 6714 Loss: 0.07460317574860528\n",
      "Iteration 6715 Loss: 0.07460301944560485\n",
      "Iteration 6716 Loss: 0.07460286315207207\n",
      "Iteration 6717 Loss: 0.07460270686800527\n",
      "Iteration 6718 Loss: 0.0746025505934027\n",
      "Iteration 6719 Loss: 0.07460239432826277\n",
      "Iteration 6720 Loss: 0.07460223807258384\n",
      "Iteration 6721 Loss: 0.07460208182636413\n",
      "Iteration 6722 Loss: 0.07460192558960207\n",
      "Iteration 6723 Loss: 0.07460176936229604\n",
      "Iteration 6724 Loss: 0.0746016131444442\n",
      "Iteration 6725 Loss: 0.07460145693604506\n",
      "Iteration 6726 Loss: 0.07460130073709692\n",
      "Iteration 6727 Loss: 0.0746011445475981\n",
      "Iteration 6728 Loss: 0.07460098836754699\n",
      "Iteration 6729 Loss: 0.07460083219694194\n",
      "Iteration 6730 Loss: 0.07460067603578131\n",
      "Iteration 6731 Loss: 0.07460051988406344\n",
      "Iteration 6732 Loss: 0.07460036374178672\n",
      "Iteration 6733 Loss: 0.07460020760894948\n",
      "Iteration 6734 Loss: 0.07460005148555013\n",
      "Iteration 6735 Loss: 0.07459989537158698\n",
      "Iteration 6736 Loss: 0.07459973926705848\n",
      "Iteration 6737 Loss: 0.07459958317196298\n",
      "Iteration 6738 Loss: 0.07459942708629883\n",
      "Iteration 6739 Loss: 0.07459927101006444\n",
      "Iteration 6740 Loss: 0.07459911494325819\n",
      "Iteration 6741 Loss: 0.07459895888587847\n",
      "Iteration 6742 Loss: 0.07459880283792367\n",
      "Iteration 6743 Loss: 0.07459864679939214\n",
      "Iteration 6744 Loss: 0.07459849077028233\n",
      "Iteration 6745 Loss: 0.0745983347505926\n",
      "Iteration 6746 Loss: 0.07459817874032137\n",
      "Iteration 6747 Loss: 0.07459802273946708\n",
      "Iteration 6748 Loss: 0.07459786674802803\n",
      "Iteration 6749 Loss: 0.07459771076600272\n",
      "Iteration 6750 Loss: 0.07459755479338953\n",
      "Iteration 6751 Loss: 0.07459739883018687\n",
      "Iteration 6752 Loss: 0.0745972428763931\n",
      "Iteration 6753 Loss: 0.07459708693200677\n",
      "Iteration 6754 Loss: 0.07459693099702616\n",
      "Iteration 6755 Loss: 0.07459677507144973\n",
      "Iteration 6756 Loss: 0.07459661915527603\n",
      "Iteration 6757 Loss: 0.07459646324850336\n",
      "Iteration 6758 Loss: 0.0745963073511301\n",
      "Iteration 6759 Loss: 0.07459615146315479\n",
      "Iteration 6760 Loss: 0.07459599558457584\n",
      "Iteration 6761 Loss: 0.07459583971539163\n",
      "Iteration 6762 Loss: 0.07459568385560074\n",
      "Iteration 6763 Loss: 0.07459552800520144\n",
      "Iteration 6764 Loss: 0.07459537216419228\n",
      "Iteration 6765 Loss: 0.07459521633257168\n",
      "Iteration 6766 Loss: 0.07459506051033808\n",
      "Iteration 6767 Loss: 0.07459490469748989\n",
      "Iteration 6768 Loss: 0.0745947488940257\n",
      "Iteration 6769 Loss: 0.07459459309994385\n",
      "Iteration 6770 Loss: 0.07459443731524276\n",
      "Iteration 6771 Loss: 0.07459428153992104\n",
      "Iteration 6772 Loss: 0.07459412577397703\n",
      "Iteration 6773 Loss: 0.07459397001740922\n",
      "Iteration 6774 Loss: 0.07459381427021616\n",
      "Iteration 6775 Loss: 0.07459365853239616\n",
      "Iteration 6776 Loss: 0.07459350280394789\n",
      "Iteration 6777 Loss: 0.07459334708486964\n",
      "Iteration 6778 Loss: 0.07459319137516003\n",
      "Iteration 6779 Loss: 0.07459303567481745\n",
      "Iteration 6780 Loss: 0.07459287998384044\n",
      "Iteration 6781 Loss: 0.07459272430222744\n",
      "Iteration 6782 Loss: 0.07459256862997693\n",
      "Iteration 6783 Loss: 0.07459241296708743\n",
      "Iteration 6784 Loss: 0.07459225731355744\n",
      "Iteration 6785 Loss: 0.07459210166938543\n",
      "Iteration 6786 Loss: 0.0745919460345699\n",
      "Iteration 6787 Loss: 0.07459179040910938\n",
      "Iteration 6788 Loss: 0.07459163479300232\n",
      "Iteration 6789 Loss: 0.07459147918624724\n",
      "Iteration 6790 Loss: 0.07459132358884264\n",
      "Iteration 6791 Loss: 0.07459116800078709\n",
      "Iteration 6792 Loss: 0.07459101242207904\n",
      "Iteration 6793 Loss: 0.07459085685271702\n",
      "Iteration 6794 Loss: 0.07459070129269949\n",
      "Iteration 6795 Loss: 0.07459054574202503\n",
      "Iteration 6796 Loss: 0.07459039020069216\n",
      "Iteration 6797 Loss: 0.07459023466869943\n",
      "Iteration 6798 Loss: 0.07459007914604521\n",
      "Iteration 6799 Loss: 0.07458992363272818\n",
      "Iteration 6800 Loss: 0.07458976812874689\n",
      "Iteration 6801 Loss: 0.07458961263409973\n",
      "Iteration 6802 Loss: 0.07458945714878532\n",
      "Iteration 6803 Loss: 0.0745893016728022\n",
      "Iteration 6804 Loss: 0.07458914620614888\n",
      "Iteration 6805 Loss: 0.07458899074882389\n",
      "Iteration 6806 Loss: 0.0745888353008258\n",
      "Iteration 6807 Loss: 0.07458867986215312\n",
      "Iteration 6808 Loss: 0.07458852443280446\n",
      "Iteration 6809 Loss: 0.07458836901277828\n",
      "Iteration 6810 Loss: 0.0745882136020732\n",
      "Iteration 6811 Loss: 0.07458805820068778\n",
      "Iteration 6812 Loss: 0.07458790280862052\n",
      "Iteration 6813 Loss: 0.07458774742586995\n",
      "Iteration 6814 Loss: 0.0745875920524347\n",
      "Iteration 6815 Loss: 0.07458743668831337\n",
      "Iteration 6816 Loss: 0.07458728133350441\n",
      "Iteration 6817 Loss: 0.07458712598800644\n",
      "Iteration 6818 Loss: 0.07458697065181803\n",
      "Iteration 6819 Loss: 0.07458681532493776\n",
      "Iteration 6820 Loss: 0.07458666000736418\n",
      "Iteration 6821 Loss: 0.07458650469909586\n",
      "Iteration 6822 Loss: 0.0745863494001314\n",
      "Iteration 6823 Loss: 0.07458619411046938\n",
      "Iteration 6824 Loss: 0.07458603883010835\n",
      "Iteration 6825 Loss: 0.07458588355904686\n",
      "Iteration 6826 Loss: 0.07458572829728363\n",
      "Iteration 6827 Loss: 0.07458557304481712\n",
      "Iteration 6828 Loss: 0.07458541780164593\n",
      "Iteration 6829 Loss: 0.07458526256776875\n",
      "Iteration 6830 Loss: 0.07458510734318406\n",
      "Iteration 6831 Loss: 0.07458495212789051\n",
      "Iteration 6832 Loss: 0.0745847969218867\n",
      "Iteration 6833 Loss: 0.0745846417251712\n",
      "Iteration 6834 Loss: 0.07458448653774259\n",
      "Iteration 6835 Loss: 0.07458433135959959\n",
      "Iteration 6836 Loss: 0.07458417619074068\n",
      "Iteration 6837 Loss: 0.07458402103116449\n",
      "Iteration 6838 Loss: 0.07458386588086965\n",
      "Iteration 6839 Loss: 0.0745837107398548\n",
      "Iteration 6840 Loss: 0.07458355560811858\n",
      "Iteration 6841 Loss: 0.07458340048565944\n",
      "Iteration 6842 Loss: 0.07458324537247622\n",
      "Iteration 6843 Loss: 0.07458309026856735\n",
      "Iteration 6844 Loss: 0.0745829351739316\n",
      "Iteration 6845 Loss: 0.07458278008856753\n",
      "Iteration 6846 Loss: 0.07458262501247373\n",
      "Iteration 6847 Loss: 0.07458246994564881\n",
      "Iteration 6848 Loss: 0.07458231488809153\n",
      "Iteration 6849 Loss: 0.0745821598398004\n",
      "Iteration 6850 Loss: 0.07458200480077413\n",
      "Iteration 6851 Loss: 0.07458184977101133\n",
      "Iteration 6852 Loss: 0.07458169475051064\n",
      "Iteration 6853 Loss: 0.07458153973927065\n",
      "Iteration 6854 Loss: 0.07458138473729002\n",
      "Iteration 6855 Loss: 0.07458122974456746\n",
      "Iteration 6856 Loss: 0.07458107476110157\n",
      "Iteration 6857 Loss: 0.074580919786891\n",
      "Iteration 6858 Loss: 0.0745807648219344\n",
      "Iteration 6859 Loss: 0.07458060986623043\n",
      "Iteration 6860 Loss: 0.07458045491977769\n",
      "Iteration 6861 Loss: 0.07458029998257494\n",
      "Iteration 6862 Loss: 0.07458014505462077\n",
      "Iteration 6863 Loss: 0.07457999013591385\n",
      "Iteration 6864 Loss: 0.07457983522645283\n",
      "Iteration 6865 Loss: 0.07457968032623637\n",
      "Iteration 6866 Loss: 0.07457952543526315\n",
      "Iteration 6867 Loss: 0.07457937055353184\n",
      "Iteration 6868 Loss: 0.07457921568104113\n",
      "Iteration 6869 Loss: 0.07457906081778967\n",
      "Iteration 6870 Loss: 0.07457890596377606\n",
      "Iteration 6871 Loss: 0.07457875111899914\n",
      "Iteration 6872 Loss: 0.07457859628345748\n",
      "Iteration 6873 Loss: 0.07457844145714972\n",
      "Iteration 6874 Loss: 0.07457828664007461\n",
      "Iteration 6875 Loss: 0.0745781318322308\n",
      "Iteration 6876 Loss: 0.07457797703361703\n",
      "Iteration 6877 Loss: 0.0745778222442319\n",
      "Iteration 6878 Loss: 0.07457766746407417\n",
      "Iteration 6879 Loss: 0.07457751269314253\n",
      "Iteration 6880 Loss: 0.07457735793143556\n",
      "Iteration 6881 Loss: 0.07457720317895218\n",
      "Iteration 6882 Loss: 0.07457704843569082\n",
      "Iteration 6883 Loss: 0.0745768937016503\n",
      "Iteration 6884 Loss: 0.0745767389768294\n",
      "Iteration 6885 Loss: 0.0745765842612267\n",
      "Iteration 6886 Loss: 0.07457642955484094\n",
      "Iteration 6887 Loss: 0.07457627485767084\n",
      "Iteration 6888 Loss: 0.07457612016971511\n",
      "Iteration 6889 Loss: 0.07457596549097248\n",
      "Iteration 6890 Loss: 0.07457581082144159\n",
      "Iteration 6891 Loss: 0.07457565616112118\n",
      "Iteration 6892 Loss: 0.07457550151000998\n",
      "Iteration 6893 Loss: 0.0745753468681067\n",
      "Iteration 6894 Loss: 0.07457519223541002\n",
      "Iteration 6895 Loss: 0.07457503761191872\n",
      "Iteration 6896 Loss: 0.07457488299763154\n",
      "Iteration 6897 Loss: 0.07457472839254711\n",
      "Iteration 6898 Loss: 0.07457457379666424\n",
      "Iteration 6899 Loss: 0.07457441920998158\n",
      "Iteration 6900 Loss: 0.07457426463249792\n",
      "Iteration 6901 Loss: 0.074574110064212\n",
      "Iteration 6902 Loss: 0.07457395550512251\n",
      "Iteration 6903 Loss: 0.07457380095522818\n",
      "Iteration 6904 Loss: 0.07457364641452778\n",
      "Iteration 6905 Loss: 0.07457349188302007\n",
      "Iteration 6906 Loss: 0.07457333736070368\n",
      "Iteration 6907 Loss: 0.07457318284757739\n",
      "Iteration 6908 Loss: 0.07457302834364003\n",
      "Iteration 6909 Loss: 0.0745728738488903\n",
      "Iteration 6910 Loss: 0.0745727193633269\n",
      "Iteration 6911 Loss: 0.07457256488694866\n",
      "Iteration 6912 Loss: 0.07457241041975424\n",
      "Iteration 6913 Loss: 0.07457225596174237\n",
      "Iteration 6914 Loss: 0.07457210151291202\n",
      "Iteration 6915 Loss: 0.07457194707326169\n",
      "Iteration 6916 Loss: 0.07457179264279026\n",
      "Iteration 6917 Loss: 0.07457163822149646\n",
      "Iteration 6918 Loss: 0.07457148380937903\n",
      "Iteration 6919 Loss: 0.07457132940643676\n",
      "Iteration 6920 Loss: 0.07457117501266848\n",
      "Iteration 6921 Loss: 0.07457102062807278\n",
      "Iteration 6922 Loss: 0.07457086625264862\n",
      "Iteration 6923 Loss: 0.07457071188639461\n",
      "Iteration 6924 Loss: 0.07457055752930966\n",
      "Iteration 6925 Loss: 0.07457040318139241\n",
      "Iteration 6926 Loss: 0.07457024884264171\n",
      "Iteration 6927 Loss: 0.07457009451305638\n",
      "Iteration 6928 Loss: 0.07456994019263509\n",
      "Iteration 6929 Loss: 0.07456978588137665\n",
      "Iteration 6930 Loss: 0.07456963157927986\n",
      "Iteration 6931 Loss: 0.07456947728634353\n",
      "Iteration 6932 Loss: 0.07456932300256643\n",
      "Iteration 6933 Loss: 0.07456916872794725\n",
      "Iteration 6934 Loss: 0.07456901446248491\n",
      "Iteration 6935 Loss: 0.07456886020617817\n",
      "Iteration 6936 Loss: 0.07456870595902575\n",
      "Iteration 6937 Loss: 0.07456855172102646\n",
      "Iteration 6938 Loss: 0.07456839749217911\n",
      "Iteration 6939 Loss: 0.07456824327248257\n",
      "Iteration 6940 Loss: 0.07456808906193557\n",
      "Iteration 6941 Loss: 0.07456793486053685\n",
      "Iteration 6942 Loss: 0.0745677806682853\n",
      "Iteration 6943 Loss: 0.0745676264851797\n",
      "Iteration 6944 Loss: 0.07456747231121881\n",
      "Iteration 6945 Loss: 0.0745673181464015\n",
      "Iteration 6946 Loss: 0.07456716399072656\n",
      "Iteration 6947 Loss: 0.07456700984419275\n",
      "Iteration 6948 Loss: 0.07456685570679891\n",
      "Iteration 6949 Loss: 0.07456670157854392\n",
      "Iteration 6950 Loss: 0.07456654745942645\n",
      "Iteration 6951 Loss: 0.07456639334944537\n",
      "Iteration 6952 Loss: 0.07456623924859958\n",
      "Iteration 6953 Loss: 0.07456608515688783\n",
      "Iteration 6954 Loss: 0.07456593107430891\n",
      "Iteration 6955 Loss: 0.07456577700086173\n",
      "Iteration 6956 Loss: 0.07456562293654503\n",
      "Iteration 6957 Loss: 0.07456546888135765\n",
      "Iteration 6958 Loss: 0.07456531483529841\n",
      "Iteration 6959 Loss: 0.07456516079836621\n",
      "Iteration 6960 Loss: 0.0745650067705598\n",
      "Iteration 6961 Loss: 0.07456485275187799\n",
      "Iteration 6962 Loss: 0.07456469874231972\n",
      "Iteration 6963 Loss: 0.07456454474188368\n",
      "Iteration 6964 Loss: 0.07456439075056887\n",
      "Iteration 6965 Loss: 0.07456423676837395\n",
      "Iteration 6966 Loss: 0.07456408279529789\n",
      "Iteration 6967 Loss: 0.07456392883133953\n",
      "Iteration 6968 Loss: 0.0745637748764976\n",
      "Iteration 6969 Loss: 0.07456362093077104\n",
      "Iteration 6970 Loss: 0.07456346699415868\n",
      "Iteration 6971 Loss: 0.07456331306665931\n",
      "Iteration 6972 Loss: 0.07456315914827183\n",
      "Iteration 6973 Loss: 0.07456300523899509\n",
      "Iteration 6974 Loss: 0.07456285133882794\n",
      "Iteration 6975 Loss: 0.07456269744776921\n",
      "Iteration 6976 Loss: 0.07456254356581771\n",
      "Iteration 6977 Loss: 0.07456238969297244\n",
      "Iteration 6978 Loss: 0.07456223582923206\n",
      "Iteration 6979 Loss: 0.07456208197459559\n",
      "Iteration 6980 Loss: 0.07456192812906177\n",
      "Iteration 6981 Loss: 0.0745617742926296\n",
      "Iteration 6982 Loss: 0.07456162046529777\n",
      "Iteration 6983 Loss: 0.07456146664706527\n",
      "Iteration 6984 Loss: 0.07456131283793101\n",
      "Iteration 6985 Loss: 0.07456115903789366\n",
      "Iteration 6986 Loss: 0.07456100524695226\n",
      "Iteration 6987 Loss: 0.07456085146510563\n",
      "Iteration 6988 Loss: 0.07456069769235257\n",
      "Iteration 6989 Loss: 0.074560543928692\n",
      "Iteration 6990 Loss: 0.0745603901741229\n",
      "Iteration 6991 Loss: 0.07456023642864403\n",
      "Iteration 6992 Loss: 0.0745600826922543\n",
      "Iteration 6993 Loss: 0.07455992896495259\n",
      "Iteration 6994 Loss: 0.0745597752467377\n",
      "Iteration 6995 Loss: 0.07455962153760867\n",
      "Iteration 6996 Loss: 0.07455946783756422\n",
      "Iteration 6997 Loss: 0.0745593141466033\n",
      "Iteration 6998 Loss: 0.07455916046472488\n",
      "Iteration 6999 Loss: 0.07455900679192773\n",
      "Iteration 7000 Loss: 0.07455885312821078\n",
      "Iteration 7001 Loss: 0.07455869947357292\n",
      "Iteration 7002 Loss: 0.074558545828013\n",
      "Iteration 7003 Loss: 0.07455839219152999\n",
      "Iteration 7004 Loss: 0.07455823856412275\n",
      "Iteration 7005 Loss: 0.07455808494579017\n",
      "Iteration 7006 Loss: 0.07455793133653113\n",
      "Iteration 7007 Loss: 0.0745577777363445\n",
      "Iteration 7008 Loss: 0.07455762414522933\n",
      "Iteration 7009 Loss: 0.07455747056318436\n",
      "Iteration 7010 Loss: 0.07455731699020851\n",
      "Iteration 7011 Loss: 0.07455716342630078\n",
      "Iteration 7012 Loss: 0.07455700987145997\n",
      "Iteration 7013 Loss: 0.07455685632568507\n",
      "Iteration 7014 Loss: 0.07455670278897497\n",
      "Iteration 7015 Loss: 0.07455654926132846\n",
      "Iteration 7016 Loss: 0.07455639574274464\n",
      "Iteration 7017 Loss: 0.07455624223322233\n",
      "Iteration 7018 Loss: 0.07455608873276046\n",
      "Iteration 7019 Loss: 0.07455593524135787\n",
      "Iteration 7020 Loss: 0.07455578175901356\n",
      "Iteration 7021 Loss: 0.07455562828572641\n",
      "Iteration 7022 Loss: 0.07455547482149541\n",
      "Iteration 7023 Loss: 0.07455532136631933\n",
      "Iteration 7024 Loss: 0.07455516792019727\n",
      "Iteration 7025 Loss: 0.07455501448312804\n",
      "Iteration 7026 Loss: 0.07455486105511056\n",
      "Iteration 7027 Loss: 0.0745547076361438\n",
      "Iteration 7028 Loss: 0.07455455422622675\n",
      "Iteration 7029 Loss: 0.07455440082535818\n",
      "Iteration 7030 Loss: 0.07455424743353709\n",
      "Iteration 7031 Loss: 0.07455409405076249\n",
      "Iteration 7032 Loss: 0.07455394067703318\n",
      "Iteration 7033 Loss: 0.07455378731234825\n",
      "Iteration 7034 Loss: 0.07455363395670644\n",
      "Iteration 7035 Loss: 0.0745534806101068\n",
      "Iteration 7036 Loss: 0.07455332727254835\n",
      "Iteration 7037 Loss: 0.07455317394402983\n",
      "Iteration 7038 Loss: 0.07455302062455033\n",
      "Iteration 7039 Loss: 0.07455286731410873\n",
      "Iteration 7040 Loss: 0.07455271401270401\n",
      "Iteration 7041 Loss: 0.07455256072033507\n",
      "Iteration 7042 Loss: 0.07455240743700094\n",
      "Iteration 7043 Loss: 0.07455225416270042\n",
      "Iteration 7044 Loss: 0.07455210089743261\n",
      "Iteration 7045 Loss: 0.07455194764119637\n",
      "Iteration 7046 Loss: 0.07455179439399062\n",
      "Iteration 7047 Loss: 0.07455164115581443\n",
      "Iteration 7048 Loss: 0.07455148792666669\n",
      "Iteration 7049 Loss: 0.07455133470654632\n",
      "Iteration 7050 Loss: 0.0745511814954523\n",
      "Iteration 7051 Loss: 0.07455102829338363\n",
      "Iteration 7052 Loss: 0.07455087510033923\n",
      "Iteration 7053 Loss: 0.074550721916318\n",
      "Iteration 7054 Loss: 0.07455056874131907\n",
      "Iteration 7055 Loss: 0.0745504155753412\n",
      "Iteration 7056 Loss: 0.0745502624183835\n",
      "Iteration 7057 Loss: 0.07455010927044484\n",
      "Iteration 7058 Loss: 0.07454995613152425\n",
      "Iteration 7059 Loss: 0.07454980300162067\n",
      "Iteration 7060 Loss: 0.0745496498807331\n",
      "Iteration 7061 Loss: 0.07454949676886045\n",
      "Iteration 7062 Loss: 0.0745493436660017\n",
      "Iteration 7063 Loss: 0.07454919057215588\n",
      "Iteration 7064 Loss: 0.07454903748732196\n",
      "Iteration 7065 Loss: 0.07454888441149878\n",
      "Iteration 7066 Loss: 0.07454873134468551\n",
      "Iteration 7067 Loss: 0.07454857828688104\n",
      "Iteration 7068 Loss: 0.07454842523808428\n",
      "Iteration 7069 Loss: 0.0745482721982943\n",
      "Iteration 7070 Loss: 0.07454811916751\n",
      "Iteration 7071 Loss: 0.07454796614573052\n",
      "Iteration 7072 Loss: 0.07454781313295464\n",
      "Iteration 7073 Loss: 0.07454766012918149\n",
      "Iteration 7074 Loss: 0.07454750713441\n",
      "Iteration 7075 Loss: 0.07454735414863913\n",
      "Iteration 7076 Loss: 0.07454720117186794\n",
      "Iteration 7077 Loss: 0.07454704820409533\n",
      "Iteration 7078 Loss: 0.07454689524532039\n",
      "Iteration 7079 Loss: 0.07454674229554206\n",
      "Iteration 7080 Loss: 0.07454658935475925\n",
      "Iteration 7081 Loss: 0.0745464364229711\n",
      "Iteration 7082 Loss: 0.07454628350017656\n",
      "Iteration 7083 Loss: 0.07454613058637453\n",
      "Iteration 7084 Loss: 0.07454597768156412\n",
      "Iteration 7085 Loss: 0.07454582478574431\n",
      "Iteration 7086 Loss: 0.07454567189891413\n",
      "Iteration 7087 Loss: 0.07454551902107245\n",
      "Iteration 7088 Loss: 0.07454536615221838\n",
      "Iteration 7089 Loss: 0.07454521329235089\n",
      "Iteration 7090 Loss: 0.07454506044146905\n",
      "Iteration 7091 Loss: 0.0745449075995717\n",
      "Iteration 7092 Loss: 0.07454475476665803\n",
      "Iteration 7093 Loss: 0.07454460194272693\n",
      "Iteration 7094 Loss: 0.07454444912777747\n",
      "Iteration 7095 Loss: 0.07454429632180867\n",
      "Iteration 7096 Loss: 0.07454414352481946\n",
      "Iteration 7097 Loss: 0.07454399073680895\n",
      "Iteration 7098 Loss: 0.07454383795777601\n",
      "Iteration 7099 Loss: 0.0745436851877199\n",
      "Iteration 7100 Loss: 0.07454353242663939\n",
      "Iteration 7101 Loss: 0.07454337967453359\n",
      "Iteration 7102 Loss: 0.07454322693140152\n",
      "Iteration 7103 Loss: 0.07454307419724224\n",
      "Iteration 7104 Loss: 0.07454292147205473\n",
      "Iteration 7105 Loss: 0.07454276875583798\n",
      "Iteration 7106 Loss: 0.07454261604859104\n",
      "Iteration 7107 Loss: 0.07454246335031296\n",
      "Iteration 7108 Loss: 0.07454231066100277\n",
      "Iteration 7109 Loss: 0.0745421579806594\n",
      "Iteration 7110 Loss: 0.074542005309282\n",
      "Iteration 7111 Loss: 0.07454185264686948\n",
      "Iteration 7112 Loss: 0.07454169999342104\n",
      "Iteration 7113 Loss: 0.07454154734893552\n",
      "Iteration 7114 Loss: 0.07454139471341202\n",
      "Iteration 7115 Loss: 0.07454124208684967\n",
      "Iteration 7116 Loss: 0.07454108946924733\n",
      "Iteration 7117 Loss: 0.07454093686060417\n",
      "Iteration 7118 Loss: 0.07454078426091917\n",
      "Iteration 7119 Loss: 0.07454063167019147\n",
      "Iteration 7120 Loss: 0.07454047908841986\n",
      "Iteration 7121 Loss: 0.07454032651560363\n",
      "Iteration 7122 Loss: 0.07454017395174169\n",
      "Iteration 7123 Loss: 0.0745400213968331\n",
      "Iteration 7124 Loss: 0.07453986885087693\n",
      "Iteration 7125 Loss: 0.07453971631387218\n",
      "Iteration 7126 Loss: 0.07453956378581804\n",
      "Iteration 7127 Loss: 0.07453941126671335\n",
      "Iteration 7128 Loss: 0.07453925875655729\n",
      "Iteration 7129 Loss: 0.0745391062553488\n",
      "Iteration 7130 Loss: 0.07453895376308706\n",
      "Iteration 7131 Loss: 0.07453880127977101\n",
      "Iteration 7132 Loss: 0.07453864880539979\n",
      "Iteration 7133 Loss: 0.0745384963399724\n",
      "Iteration 7134 Loss: 0.07453834388348786\n",
      "Iteration 7135 Loss: 0.07453819143594528\n",
      "Iteration 7136 Loss: 0.07453803899734368\n",
      "Iteration 7137 Loss: 0.07453788656768215\n",
      "Iteration 7138 Loss: 0.07453773414695976\n",
      "Iteration 7139 Loss: 0.07453758173517551\n",
      "Iteration 7140 Loss: 0.0745374293323285\n",
      "Iteration 7141 Loss: 0.07453727693841776\n",
      "Iteration 7142 Loss: 0.07453712455344236\n",
      "Iteration 7143 Loss: 0.07453697217740145\n",
      "Iteration 7144 Loss: 0.07453681981029389\n",
      "Iteration 7145 Loss: 0.07453666745211897\n",
      "Iteration 7146 Loss: 0.07453651510287561\n",
      "Iteration 7147 Loss: 0.0745363627625629\n",
      "Iteration 7148 Loss: 0.07453621043118\n",
      "Iteration 7149 Loss: 0.07453605810872585\n",
      "Iteration 7150 Loss: 0.07453590579519961\n",
      "Iteration 7151 Loss: 0.0745357534906003\n",
      "Iteration 7152 Loss: 0.07453560119492708\n",
      "Iteration 7153 Loss: 0.07453544890817883\n",
      "Iteration 7154 Loss: 0.07453529663035481\n",
      "Iteration 7155 Loss: 0.074535144361454\n",
      "Iteration 7156 Loss: 0.07453499210147553\n",
      "Iteration 7157 Loss: 0.07453483985041846\n",
      "Iteration 7158 Loss: 0.07453468760828186\n",
      "Iteration 7159 Loss: 0.07453453537506477\n",
      "Iteration 7160 Loss: 0.07453438315076637\n",
      "Iteration 7161 Loss: 0.07453423093538561\n",
      "Iteration 7162 Loss: 0.07453407872892166\n",
      "Iteration 7163 Loss: 0.07453392653137358\n",
      "Iteration 7164 Loss: 0.07453377434274047\n",
      "Iteration 7165 Loss: 0.0745336221630214\n",
      "Iteration 7166 Loss: 0.07453346999221544\n",
      "Iteration 7167 Loss: 0.07453331783032172\n",
      "Iteration 7168 Loss: 0.07453316567733925\n",
      "Iteration 7169 Loss: 0.07453301353326718\n",
      "Iteration 7170 Loss: 0.0745328613981046\n",
      "Iteration 7171 Loss: 0.0745327092718506\n",
      "Iteration 7172 Loss: 0.07453255715450421\n",
      "Iteration 7173 Loss: 0.07453240504606459\n",
      "Iteration 7174 Loss: 0.0745322529465309\n",
      "Iteration 7175 Loss: 0.074532100855902\n",
      "Iteration 7176 Loss: 0.07453194877417721\n",
      "Iteration 7177 Loss: 0.07453179670135558\n",
      "Iteration 7178 Loss: 0.07453164463743615\n",
      "Iteration 7179 Loss: 0.074531492582418\n",
      "Iteration 7180 Loss: 0.07453134053630027\n",
      "Iteration 7181 Loss: 0.07453118849908211\n",
      "Iteration 7182 Loss: 0.0745310364707626\n",
      "Iteration 7183 Loss: 0.07453088445134073\n",
      "Iteration 7184 Loss: 0.07453073244081573\n",
      "Iteration 7185 Loss: 0.07453058043918667\n",
      "Iteration 7186 Loss: 0.07453042844645266\n",
      "Iteration 7187 Loss: 0.07453027646261269\n",
      "Iteration 7188 Loss: 0.07453012448766608\n",
      "Iteration 7189 Loss: 0.07452997252161175\n",
      "Iteration 7190 Loss: 0.07452982056444894\n",
      "Iteration 7191 Loss: 0.07452966861617671\n",
      "Iteration 7192 Loss: 0.07452951667679411\n",
      "Iteration 7193 Loss: 0.07452936474630037\n",
      "Iteration 7194 Loss: 0.07452921282469446\n",
      "Iteration 7195 Loss: 0.07452906091197566\n",
      "Iteration 7196 Loss: 0.0745289090081429\n",
      "Iteration 7197 Loss: 0.07452875711319541\n",
      "Iteration 7198 Loss: 0.07452860522713228\n",
      "Iteration 7199 Loss: 0.07452845334995264\n",
      "Iteration 7200 Loss: 0.07452830148165564\n",
      "Iteration 7201 Loss: 0.0745281496222403\n",
      "Iteration 7202 Loss: 0.07452799777170585\n",
      "Iteration 7203 Loss: 0.07452784593005132\n",
      "Iteration 7204 Loss: 0.07452769409727586\n",
      "Iteration 7205 Loss: 0.07452754227337857\n",
      "Iteration 7206 Loss: 0.07452739045835863\n",
      "Iteration 7207 Loss: 0.07452723865221515\n",
      "Iteration 7208 Loss: 0.07452708685494722\n",
      "Iteration 7209 Loss: 0.07452693506655401\n",
      "Iteration 7210 Loss: 0.07452678328703462\n",
      "Iteration 7211 Loss: 0.07452663151638814\n",
      "Iteration 7212 Loss: 0.0745264797546138\n",
      "Iteration 7213 Loss: 0.07452632800171062\n",
      "Iteration 7214 Loss: 0.0745261762576778\n",
      "Iteration 7215 Loss: 0.07452602452251442\n",
      "Iteration 7216 Loss: 0.07452587279621965\n",
      "Iteration 7217 Loss: 0.07452572107879266\n",
      "Iteration 7218 Loss: 0.07452556937023243\n",
      "Iteration 7219 Loss: 0.07452541767053827\n",
      "Iteration 7220 Loss: 0.07452526597970927\n",
      "Iteration 7221 Loss: 0.07452511429774449\n",
      "Iteration 7222 Loss: 0.07452496262464312\n",
      "Iteration 7223 Loss: 0.07452481096040434\n",
      "Iteration 7224 Loss: 0.07452465930502719\n",
      "Iteration 7225 Loss: 0.07452450765851089\n",
      "Iteration 7226 Loss: 0.07452435602085458\n",
      "Iteration 7227 Loss: 0.07452420439205729\n",
      "Iteration 7228 Loss: 0.07452405277211835\n",
      "Iteration 7229 Loss: 0.07452390116103677\n",
      "Iteration 7230 Loss: 0.07452374955881169\n",
      "Iteration 7231 Loss: 0.07452359796544235\n",
      "Iteration 7232 Loss: 0.07452344638092781\n",
      "Iteration 7233 Loss: 0.07452329480526723\n",
      "Iteration 7234 Loss: 0.0745231432384598\n",
      "Iteration 7235 Loss: 0.07452299168050458\n",
      "Iteration 7236 Loss: 0.07452284013140079\n",
      "Iteration 7237 Loss: 0.0745226885911476\n",
      "Iteration 7238 Loss: 0.07452253705974417\n",
      "Iteration 7239 Loss: 0.07452238553718948\n",
      "Iteration 7240 Loss: 0.07452223402348286\n",
      "Iteration 7241 Loss: 0.07452208251862344\n",
      "Iteration 7242 Loss: 0.07452193102261037\n",
      "Iteration 7243 Loss: 0.07452177953544274\n",
      "Iteration 7244 Loss: 0.07452162805711977\n",
      "Iteration 7245 Loss: 0.07452147658764055\n",
      "Iteration 7246 Loss: 0.07452132512700435\n",
      "Iteration 7247 Loss: 0.0745211736752102\n",
      "Iteration 7248 Loss: 0.07452102223225733\n",
      "Iteration 7249 Loss: 0.07452087079814491\n",
      "Iteration 7250 Loss: 0.07452071937287208\n",
      "Iteration 7251 Loss: 0.07452056795643798\n",
      "Iteration 7252 Loss: 0.07452041654884184\n",
      "Iteration 7253 Loss: 0.07452026515008273\n",
      "Iteration 7254 Loss: 0.07452011376015988\n",
      "Iteration 7255 Loss: 0.07451996237907235\n",
      "Iteration 7256 Loss: 0.07451981100681951\n",
      "Iteration 7257 Loss: 0.07451965964340035\n",
      "Iteration 7258 Loss: 0.07451950828881411\n",
      "Iteration 7259 Loss: 0.07451935694305989\n",
      "Iteration 7260 Loss: 0.07451920560613697\n",
      "Iteration 7261 Loss: 0.07451905427804442\n",
      "Iteration 7262 Loss: 0.07451890295878143\n",
      "Iteration 7263 Loss: 0.07451875164834723\n",
      "Iteration 7264 Loss: 0.07451860034674096\n",
      "Iteration 7265 Loss: 0.07451844905396172\n",
      "Iteration 7266 Loss: 0.07451829777000882\n",
      "Iteration 7267 Loss: 0.07451814649488128\n",
      "Iteration 7268 Loss: 0.07451799522857842\n",
      "Iteration 7269 Loss: 0.0745178439710993\n",
      "Iteration 7270 Loss: 0.07451769272244316\n",
      "Iteration 7271 Loss: 0.07451754148260921\n",
      "Iteration 7272 Loss: 0.07451739025159652\n",
      "Iteration 7273 Loss: 0.07451723902940435\n",
      "Iteration 7274 Loss: 0.07451708781603189\n",
      "Iteration 7275 Loss: 0.07451693661147826\n",
      "Iteration 7276 Loss: 0.07451678541574264\n",
      "Iteration 7277 Loss: 0.0745166342288243\n",
      "Iteration 7278 Loss: 0.07451648305072237\n",
      "Iteration 7279 Loss: 0.07451633188143598\n",
      "Iteration 7280 Loss: 0.07451618072096437\n",
      "Iteration 7281 Loss: 0.07451602956930672\n",
      "Iteration 7282 Loss: 0.07451587842646225\n",
      "Iteration 7283 Loss: 0.07451572729243007\n",
      "Iteration 7284 Loss: 0.07451557616720947\n",
      "Iteration 7285 Loss: 0.07451542505079949\n",
      "Iteration 7286 Loss: 0.07451527394319943\n",
      "Iteration 7287 Loss: 0.07451512284440852\n",
      "Iteration 7288 Loss: 0.07451497175442579\n",
      "Iteration 7289 Loss: 0.07451482067325058\n",
      "Iteration 7290 Loss: 0.07451466960088199\n",
      "Iteration 7291 Loss: 0.07451451853731926\n",
      "Iteration 7292 Loss: 0.0745143674825616\n",
      "Iteration 7293 Loss: 0.07451421643660815\n",
      "Iteration 7294 Loss: 0.07451406539945812\n",
      "Iteration 7295 Loss: 0.07451391437111064\n",
      "Iteration 7296 Loss: 0.07451376335156513\n",
      "Iteration 7297 Loss: 0.07451361234082055\n",
      "Iteration 7298 Loss: 0.07451346133887621\n",
      "Iteration 7299 Loss: 0.07451331034573125\n",
      "Iteration 7300 Loss: 0.07451315936138495\n",
      "Iteration 7301 Loss: 0.07451300838583644\n",
      "Iteration 7302 Loss: 0.07451285741908494\n",
      "Iteration 7303 Loss: 0.07451270646112965\n",
      "Iteration 7304 Loss: 0.07451255551196975\n",
      "Iteration 7305 Loss: 0.07451240457160452\n",
      "Iteration 7306 Loss: 0.07451225364003304\n",
      "Iteration 7307 Loss: 0.07451210271725464\n",
      "Iteration 7308 Loss: 0.07451195180326844\n",
      "Iteration 7309 Loss: 0.07451180089807366\n",
      "Iteration 7310 Loss: 0.07451165000166962\n",
      "Iteration 7311 Loss: 0.07451149911405536\n",
      "Iteration 7312 Loss: 0.07451134823523012\n",
      "Iteration 7313 Loss: 0.07451119736519318\n",
      "Iteration 7314 Loss: 0.07451104650394373\n",
      "Iteration 7315 Loss: 0.07451089565148092\n",
      "Iteration 7316 Loss: 0.07451074480780404\n",
      "Iteration 7317 Loss: 0.07451059397291225\n",
      "Iteration 7318 Loss: 0.07451044314680477\n",
      "Iteration 7319 Loss: 0.0745102923294808\n",
      "Iteration 7320 Loss: 0.0745101415209396\n",
      "Iteration 7321 Loss: 0.07450999072118034\n",
      "Iteration 7322 Loss: 0.0745098399302023\n",
      "Iteration 7323 Loss: 0.07450968914800453\n",
      "Iteration 7324 Loss: 0.0745095383745865\n",
      "Iteration 7325 Loss: 0.07450938760994721\n",
      "Iteration 7326 Loss: 0.07450923685408596\n",
      "Iteration 7327 Loss: 0.07450908610700199\n",
      "Iteration 7328 Loss: 0.07450893536869443\n",
      "Iteration 7329 Loss: 0.07450878463916262\n",
      "Iteration 7330 Loss: 0.07450863391840565\n",
      "Iteration 7331 Loss: 0.07450848320642287\n",
      "Iteration 7332 Loss: 0.07450833250321348\n",
      "Iteration 7333 Loss: 0.07450818180877661\n",
      "Iteration 7334 Loss: 0.07450803112311152\n",
      "Iteration 7335 Loss: 0.0745078804462175\n",
      "Iteration 7336 Loss: 0.07450772977809372\n",
      "Iteration 7337 Loss: 0.07450757911873931\n",
      "Iteration 7338 Loss: 0.07450742846815375\n",
      "Iteration 7339 Loss: 0.07450727782633607\n",
      "Iteration 7340 Loss: 0.07450712719328548\n",
      "Iteration 7341 Loss: 0.0745069765690013\n",
      "Iteration 7342 Loss: 0.07450682595348272\n",
      "Iteration 7343 Loss: 0.07450667534672897\n",
      "Iteration 7344 Loss: 0.07450652474873927\n",
      "Iteration 7345 Loss: 0.07450637415951286\n",
      "Iteration 7346 Loss: 0.07450622357904901\n",
      "Iteration 7347 Loss: 0.0745060730073469\n",
      "Iteration 7348 Loss: 0.07450592244440575\n",
      "Iteration 7349 Loss: 0.07450577189022486\n",
      "Iteration 7350 Loss: 0.07450562134480343\n",
      "Iteration 7351 Loss: 0.07450547080814066\n",
      "Iteration 7352 Loss: 0.07450532028023578\n",
      "Iteration 7353 Loss: 0.07450516976108808\n",
      "Iteration 7354 Loss: 0.07450501925069677\n",
      "Iteration 7355 Loss: 0.07450486874906112\n",
      "Iteration 7356 Loss: 0.07450471825618028\n",
      "Iteration 7357 Loss: 0.07450456777205364\n",
      "Iteration 7358 Loss: 0.0745044172966803\n",
      "Iteration 7359 Loss: 0.07450426683005953\n",
      "Iteration 7360 Loss: 0.07450411637219054\n",
      "Iteration 7361 Loss: 0.07450396592307265\n",
      "Iteration 7362 Loss: 0.07450381548270509\n",
      "Iteration 7363 Loss: 0.07450366505108705\n",
      "Iteration 7364 Loss: 0.07450351462821776\n",
      "Iteration 7365 Loss: 0.07450336421409658\n",
      "Iteration 7366 Loss: 0.0745032138087226\n",
      "Iteration 7367 Loss: 0.0745030634120952\n",
      "Iteration 7368 Loss: 0.07450291302421352\n",
      "Iteration 7369 Loss: 0.0745027626450769\n",
      "Iteration 7370 Loss: 0.07450261227468448\n",
      "Iteration 7371 Loss: 0.0745024619130356\n",
      "Iteration 7372 Loss: 0.07450231156012946\n",
      "Iteration 7373 Loss: 0.07450216121596528\n",
      "Iteration 7374 Loss: 0.07450201088054238\n",
      "Iteration 7375 Loss: 0.07450186055386003\n",
      "Iteration 7376 Loss: 0.07450171023591731\n",
      "Iteration 7377 Loss: 0.0745015599267136\n",
      "Iteration 7378 Loss: 0.07450140962624821\n",
      "Iteration 7379 Loss: 0.07450125933452027\n",
      "Iteration 7380 Loss: 0.0745011090515291\n",
      "Iteration 7381 Loss: 0.07450095877727393\n",
      "Iteration 7382 Loss: 0.07450080851175403\n",
      "Iteration 7383 Loss: 0.07450065825496864\n",
      "Iteration 7384 Loss: 0.07450050800691699\n",
      "Iteration 7385 Loss: 0.07450035776759836\n",
      "Iteration 7386 Loss: 0.07450020753701203\n",
      "Iteration 7387 Loss: 0.07450005731515723\n",
      "Iteration 7388 Loss: 0.07449990710203325\n",
      "Iteration 7389 Loss: 0.07449975689763923\n",
      "Iteration 7390 Loss: 0.07449960670197459\n",
      "Iteration 7391 Loss: 0.07449945651503848\n",
      "Iteration 7392 Loss: 0.07449930633683019\n",
      "Iteration 7393 Loss: 0.07449915616734899\n",
      "Iteration 7394 Loss: 0.07449900600659423\n",
      "Iteration 7395 Loss: 0.07449885585456491\n",
      "Iteration 7396 Loss: 0.07449870571126063\n",
      "Iteration 7397 Loss: 0.07449855557668032\n",
      "Iteration 7398 Loss: 0.07449840545082347\n",
      "Iteration 7399 Loss: 0.0744982553336893\n",
      "Iteration 7400 Loss: 0.07449810522527707\n",
      "Iteration 7401 Loss: 0.07449795512558599\n",
      "Iteration 7402 Loss: 0.07449780503461538\n",
      "Iteration 7403 Loss: 0.07449765495236445\n",
      "Iteration 7404 Loss: 0.07449750487883254\n",
      "Iteration 7405 Loss: 0.07449735481401885\n",
      "Iteration 7406 Loss: 0.07449720475792272\n",
      "Iteration 7407 Loss: 0.07449705471054337\n",
      "Iteration 7408 Loss: 0.07449690467188005\n",
      "Iteration 7409 Loss: 0.07449675464193209\n",
      "Iteration 7410 Loss: 0.07449660462069867\n",
      "Iteration 7411 Loss: 0.0744964546081792\n",
      "Iteration 7412 Loss: 0.0744963046043728\n",
      "Iteration 7413 Loss: 0.07449615460927883\n",
      "Iteration 7414 Loss: 0.07449600462289654\n",
      "Iteration 7415 Loss: 0.0744958546452252\n",
      "Iteration 7416 Loss: 0.07449570467626412\n",
      "Iteration 7417 Loss: 0.07449555471601252\n",
      "Iteration 7418 Loss: 0.0744954047644697\n",
      "Iteration 7419 Loss: 0.07449525482163492\n",
      "Iteration 7420 Loss: 0.07449510488750748\n",
      "Iteration 7421 Loss: 0.07449495496208665\n",
      "Iteration 7422 Loss: 0.07449480504537166\n",
      "Iteration 7423 Loss: 0.07449465513736185\n",
      "Iteration 7424 Loss: 0.0744945052380565\n",
      "Iteration 7425 Loss: 0.07449435534745481\n",
      "Iteration 7426 Loss: 0.07449420546555616\n",
      "Iteration 7427 Loss: 0.07449405559235978\n",
      "Iteration 7428 Loss: 0.07449390572786493\n",
      "Iteration 7429 Loss: 0.07449375587207092\n",
      "Iteration 7430 Loss: 0.07449360602497708\n",
      "Iteration 7431 Loss: 0.07449345618658255\n",
      "Iteration 7432 Loss: 0.0744933063568867\n",
      "Iteration 7433 Loss: 0.07449315653588884\n",
      "Iteration 7434 Loss: 0.07449300672358822\n",
      "Iteration 7435 Loss: 0.07449285691998415\n",
      "Iteration 7436 Loss: 0.07449270712507586\n",
      "Iteration 7437 Loss: 0.07449255733886269\n",
      "Iteration 7438 Loss: 0.07449240756134391\n",
      "Iteration 7439 Loss: 0.07449225779251875\n",
      "Iteration 7440 Loss: 0.07449210803238658\n",
      "Iteration 7441 Loss: 0.07449195828094665\n",
      "Iteration 7442 Loss: 0.07449180853819823\n",
      "Iteration 7443 Loss: 0.07449165880414066\n",
      "Iteration 7444 Loss: 0.0744915090787732\n",
      "Iteration 7445 Loss: 0.07449135936209508\n",
      "Iteration 7446 Loss: 0.07449120965410572\n",
      "Iteration 7447 Loss: 0.07449105995480429\n",
      "Iteration 7448 Loss: 0.07449091026419015\n",
      "Iteration 7449 Loss: 0.07449076058226253\n",
      "Iteration 7450 Loss: 0.0744906109090208\n",
      "Iteration 7451 Loss: 0.0744904612444642\n",
      "Iteration 7452 Loss: 0.07449031158859203\n",
      "Iteration 7453 Loss: 0.07449016194140361\n",
      "Iteration 7454 Loss: 0.07449001230289816\n",
      "Iteration 7455 Loss: 0.07448986267307507\n",
      "Iteration 7456 Loss: 0.07448971305193355\n",
      "Iteration 7457 Loss: 0.074489563439473\n",
      "Iteration 7458 Loss: 0.07448941383569256\n",
      "Iteration 7459 Loss: 0.07448926424059171\n",
      "Iteration 7460 Loss: 0.07448911465416966\n",
      "Iteration 7461 Loss: 0.07448896507642569\n",
      "Iteration 7462 Loss: 0.07448881550735909\n",
      "Iteration 7463 Loss: 0.07448866594696915\n",
      "Iteration 7464 Loss: 0.07448851639525525\n",
      "Iteration 7465 Loss: 0.07448836685221663\n",
      "Iteration 7466 Loss: 0.07448821731785259\n",
      "Iteration 7467 Loss: 0.07448806779216241\n",
      "Iteration 7468 Loss: 0.07448791827514548\n",
      "Iteration 7469 Loss: 0.07448776876680101\n",
      "Iteration 7470 Loss: 0.07448761926712832\n",
      "Iteration 7471 Loss: 0.07448746977612677\n",
      "Iteration 7472 Loss: 0.0744873202937956\n",
      "Iteration 7473 Loss: 0.07448717082013412\n",
      "Iteration 7474 Loss: 0.07448702135514168\n",
      "Iteration 7475 Loss: 0.07448687189881754\n",
      "Iteration 7476 Loss: 0.07448672245116102\n",
      "Iteration 7477 Loss: 0.07448657301217143\n",
      "Iteration 7478 Loss: 0.07448642358184801\n",
      "Iteration 7479 Loss: 0.07448627416019019\n",
      "Iteration 7480 Loss: 0.07448612474719718\n",
      "Iteration 7481 Loss: 0.07448597534286835\n",
      "Iteration 7482 Loss: 0.07448582594720296\n",
      "Iteration 7483 Loss: 0.07448567656020036\n",
      "Iteration 7484 Loss: 0.07448552718185979\n",
      "Iteration 7485 Loss: 0.07448537781218065\n",
      "Iteration 7486 Loss: 0.07448522845116219\n",
      "Iteration 7487 Loss: 0.07448507909880372\n",
      "Iteration 7488 Loss: 0.0744849297551046\n",
      "Iteration 7489 Loss: 0.07448478042006408\n",
      "Iteration 7490 Loss: 0.07448463109368146\n",
      "Iteration 7491 Loss: 0.07448448177595617\n",
      "Iteration 7492 Loss: 0.07448433246688743\n",
      "Iteration 7493 Loss: 0.07448418316647454\n",
      "Iteration 7494 Loss: 0.07448403387471687\n",
      "Iteration 7495 Loss: 0.07448388459161363\n",
      "Iteration 7496 Loss: 0.07448373531716437\n",
      "Iteration 7497 Loss: 0.07448358605136811\n",
      "Iteration 7498 Loss: 0.07448343679422433\n",
      "Iteration 7499 Loss: 0.07448328754573232\n",
      "Iteration 7500 Loss: 0.07448313830589141\n",
      "Iteration 7501 Loss: 0.0744829890747009\n",
      "Iteration 7502 Loss: 0.07448283985216009\n",
      "Iteration 7503 Loss: 0.07448269063826835\n",
      "Iteration 7504 Loss: 0.07448254143302489\n",
      "Iteration 7505 Loss: 0.07448239223642922\n",
      "Iteration 7506 Loss: 0.07448224304848049\n",
      "Iteration 7507 Loss: 0.07448209386917808\n",
      "Iteration 7508 Loss: 0.0744819446985213\n",
      "Iteration 7509 Loss: 0.07448179553650944\n",
      "Iteration 7510 Loss: 0.07448164638314189\n",
      "Iteration 7511 Loss: 0.07448149723841792\n",
      "Iteration 7512 Loss: 0.07448134810233695\n",
      "Iteration 7513 Loss: 0.07448119897489812\n",
      "Iteration 7514 Loss: 0.0744810498561009\n",
      "Iteration 7515 Loss: 0.07448090074594454\n",
      "Iteration 7516 Loss: 0.07448075164442845\n",
      "Iteration 7517 Loss: 0.07448060255155185\n",
      "Iteration 7518 Loss: 0.07448045346731416\n",
      "Iteration 7519 Loss: 0.07448030439171462\n",
      "Iteration 7520 Loss: 0.0744801553247526\n",
      "Iteration 7521 Loss: 0.07448000626642741\n",
      "Iteration 7522 Loss: 0.07447985721673842\n",
      "Iteration 7523 Loss: 0.07447970817568483\n",
      "Iteration 7524 Loss: 0.07447955914326618\n",
      "Iteration 7525 Loss: 0.07447941011948166\n",
      "Iteration 7526 Loss: 0.07447926110433059\n",
      "Iteration 7527 Loss: 0.0744791120978123\n",
      "Iteration 7528 Loss: 0.07447896309992622\n",
      "Iteration 7529 Loss: 0.07447881411067152\n",
      "Iteration 7530 Loss: 0.07447866513004772\n",
      "Iteration 7531 Loss: 0.07447851615805397\n",
      "Iteration 7532 Loss: 0.07447836719468967\n",
      "Iteration 7533 Loss: 0.0744782182399542\n",
      "Iteration 7534 Loss: 0.07447806929384684\n",
      "Iteration 7535 Loss: 0.07447792035636691\n",
      "Iteration 7536 Loss: 0.07447777142751381\n",
      "Iteration 7537 Loss: 0.07447762250728682\n",
      "Iteration 7538 Loss: 0.07447747359568531\n",
      "Iteration 7539 Loss: 0.07447732469270851\n",
      "Iteration 7540 Loss: 0.07447717579835592\n",
      "Iteration 7541 Loss: 0.07447702691262667\n",
      "Iteration 7542 Loss: 0.0744768780355203\n",
      "Iteration 7543 Loss: 0.07447672916703607\n",
      "Iteration 7544 Loss: 0.07447658030717329\n",
      "Iteration 7545 Loss: 0.07447643145593129\n",
      "Iteration 7546 Loss: 0.07447628261330945\n",
      "Iteration 7547 Loss: 0.07447613377930709\n",
      "Iteration 7548 Loss: 0.07447598495392352\n",
      "Iteration 7549 Loss: 0.07447583613715814\n",
      "Iteration 7550 Loss: 0.07447568732901022\n",
      "Iteration 7551 Loss: 0.07447553852947912\n",
      "Iteration 7552 Loss: 0.07447538973856425\n",
      "Iteration 7553 Loss: 0.0744752409562649\n",
      "Iteration 7554 Loss: 0.0744750921825803\n",
      "Iteration 7555 Loss: 0.07447494341750997\n",
      "Iteration 7556 Loss: 0.07447479466105315\n",
      "Iteration 7557 Loss: 0.07447464591320922\n",
      "Iteration 7558 Loss: 0.0744744971739775\n",
      "Iteration 7559 Loss: 0.07447434844335735\n",
      "Iteration 7560 Loss: 0.07447419972134808\n",
      "Iteration 7561 Loss: 0.07447405100794909\n",
      "Iteration 7562 Loss: 0.07447390230315964\n",
      "Iteration 7563 Loss: 0.07447375360697917\n",
      "Iteration 7564 Loss: 0.07447360491940698\n",
      "Iteration 7565 Loss: 0.07447345624044237\n",
      "Iteration 7566 Loss: 0.07447330757008479\n",
      "Iteration 7567 Loss: 0.07447315890833353\n",
      "Iteration 7568 Loss: 0.07447301025518786\n",
      "Iteration 7569 Loss: 0.07447286161064726\n",
      "Iteration 7570 Loss: 0.074472712974711\n",
      "Iteration 7571 Loss: 0.07447256434737845\n",
      "Iteration 7572 Loss: 0.07447241572864896\n",
      "Iteration 7573 Loss: 0.07447226711852183\n",
      "Iteration 7574 Loss: 0.07447211851699645\n",
      "Iteration 7575 Loss: 0.07447196992407223\n",
      "Iteration 7576 Loss: 0.07447182133974842\n",
      "Iteration 7577 Loss: 0.07447167276402439\n",
      "Iteration 7578 Loss: 0.07447152419689954\n",
      "Iteration 7579 Loss: 0.07447137563837311\n",
      "Iteration 7580 Loss: 0.07447122708844463\n",
      "Iteration 7581 Loss: 0.07447107854711328\n",
      "Iteration 7582 Loss: 0.07447093001437852\n",
      "Iteration 7583 Loss: 0.0744707814902397\n",
      "Iteration 7584 Loss: 0.07447063297469603\n",
      "Iteration 7585 Loss: 0.0744704844677471\n",
      "Iteration 7586 Loss: 0.07447033596939205\n",
      "Iteration 7587 Loss: 0.07447018747963033\n",
      "Iteration 7588 Loss: 0.07447003899846129\n",
      "Iteration 7589 Loss: 0.07446989052588429\n",
      "Iteration 7590 Loss: 0.07446974206189867\n",
      "Iteration 7591 Loss: 0.07446959360650376\n",
      "Iteration 7592 Loss: 0.07446944515969892\n",
      "Iteration 7593 Loss: 0.07446929672148361\n",
      "Iteration 7594 Loss: 0.07446914829185702\n",
      "Iteration 7595 Loss: 0.07446899987081865\n",
      "Iteration 7596 Loss: 0.07446885145836775\n",
      "Iteration 7597 Loss: 0.07446870305450384\n",
      "Iteration 7598 Loss: 0.07446855465922604\n",
      "Iteration 7599 Loss: 0.0744684062725339\n",
      "Iteration 7600 Loss: 0.07446825789442665\n",
      "Iteration 7601 Loss: 0.07446810952490378\n",
      "Iteration 7602 Loss: 0.07446796116396456\n",
      "Iteration 7603 Loss: 0.07446781281160836\n",
      "Iteration 7604 Loss: 0.07446766446783457\n",
      "Iteration 7605 Loss: 0.07446751613264249\n",
      "Iteration 7606 Loss: 0.07446736780603155\n",
      "Iteration 7607 Loss: 0.07446721948800114\n",
      "Iteration 7608 Loss: 0.07446707117855045\n",
      "Iteration 7609 Loss: 0.07446692287767902\n",
      "Iteration 7610 Loss: 0.0744667745853862\n",
      "Iteration 7611 Loss: 0.07446662630167122\n",
      "Iteration 7612 Loss: 0.07446647802653354\n",
      "Iteration 7613 Loss: 0.07446632975997258\n",
      "Iteration 7614 Loss: 0.07446618150198756\n",
      "Iteration 7615 Loss: 0.07446603325257799\n",
      "Iteration 7616 Loss: 0.07446588501174313\n",
      "Iteration 7617 Loss: 0.07446573677948233\n",
      "Iteration 7618 Loss: 0.07446558855579506\n",
      "Iteration 7619 Loss: 0.07446544034068063\n",
      "Iteration 7620 Loss: 0.0744652921341384\n",
      "Iteration 7621 Loss: 0.07446514393616771\n",
      "Iteration 7622 Loss: 0.07446499574676799\n",
      "Iteration 7623 Loss: 0.0744648475659386\n",
      "Iteration 7624 Loss: 0.07446469939367885\n",
      "Iteration 7625 Loss: 0.07446455122998814\n",
      "Iteration 7626 Loss: 0.07446440307486592\n",
      "Iteration 7627 Loss: 0.07446425492831135\n",
      "Iteration 7628 Loss: 0.07446410679032398\n",
      "Iteration 7629 Loss: 0.07446395866090316\n",
      "Iteration 7630 Loss: 0.07446381054004822\n",
      "Iteration 7631 Loss: 0.07446366242775851\n",
      "Iteration 7632 Loss: 0.0744635143240335\n",
      "Iteration 7633 Loss: 0.07446336622887242\n",
      "Iteration 7634 Loss: 0.0744632181422747\n",
      "Iteration 7635 Loss: 0.07446307006423976\n",
      "Iteration 7636 Loss: 0.07446292199476694\n",
      "Iteration 7637 Loss: 0.07446277393385561\n",
      "Iteration 7638 Loss: 0.07446262588150514\n",
      "Iteration 7639 Loss: 0.07446247783771484\n",
      "Iteration 7640 Loss: 0.0744623298024842\n",
      "Iteration 7641 Loss: 0.07446218177581257\n",
      "Iteration 7642 Loss: 0.07446203375769922\n",
      "Iteration 7643 Loss: 0.07446188574814365\n",
      "Iteration 7644 Loss: 0.07446173774714515\n",
      "Iteration 7645 Loss: 0.07446158975470318\n",
      "Iteration 7646 Loss: 0.07446144177081704\n",
      "Iteration 7647 Loss: 0.0744612937954861\n",
      "Iteration 7648 Loss: 0.07446114582870979\n",
      "Iteration 7649 Loss: 0.07446099787048746\n",
      "Iteration 7650 Loss: 0.07446084992081847\n",
      "Iteration 7651 Loss: 0.0744607019797022\n",
      "Iteration 7652 Loss: 0.07446055404713806\n",
      "Iteration 7653 Loss: 0.0744604061231254\n",
      "Iteration 7654 Loss: 0.07446025820766367\n",
      "Iteration 7655 Loss: 0.07446011030075214\n",
      "Iteration 7656 Loss: 0.07445996240239025\n",
      "Iteration 7657 Loss: 0.07445981451257735\n",
      "Iteration 7658 Loss: 0.0744596666313128\n",
      "Iteration 7659 Loss: 0.0744595187585961\n",
      "Iteration 7660 Loss: 0.07445937089442647\n",
      "Iteration 7661 Loss: 0.07445922303880337\n",
      "Iteration 7662 Loss: 0.07445907519172623\n",
      "Iteration 7663 Loss: 0.07445892735319433\n",
      "Iteration 7664 Loss: 0.07445877952320712\n",
      "Iteration 7665 Loss: 0.07445863170176396\n",
      "Iteration 7666 Loss: 0.07445848388886421\n",
      "Iteration 7667 Loss: 0.07445833608450728\n",
      "Iteration 7668 Loss: 0.07445818828869254\n",
      "Iteration 7669 Loss: 0.0744580405014194\n",
      "Iteration 7670 Loss: 0.07445789272268721\n",
      "Iteration 7671 Loss: 0.07445774495249538\n",
      "Iteration 7672 Loss: 0.07445759719084326\n",
      "Iteration 7673 Loss: 0.07445744943773032\n",
      "Iteration 7674 Loss: 0.07445730169315579\n",
      "Iteration 7675 Loss: 0.07445715395711915\n",
      "Iteration 7676 Loss: 0.07445700622961979\n",
      "Iteration 7677 Loss: 0.07445685851065717\n",
      "Iteration 7678 Loss: 0.0744567108002305\n",
      "Iteration 7679 Loss: 0.0744565630983393\n",
      "Iteration 7680 Loss: 0.07445641540498289\n",
      "Iteration 7681 Loss: 0.0744562677201607\n",
      "Iteration 7682 Loss: 0.07445612004387206\n",
      "Iteration 7683 Loss: 0.07445597237611647\n",
      "Iteration 7684 Loss: 0.07445582471689323\n",
      "Iteration 7685 Loss: 0.07445567706620168\n",
      "Iteration 7686 Loss: 0.0744555294240413\n",
      "Iteration 7687 Loss: 0.07445538179041147\n",
      "Iteration 7688 Loss: 0.07445523416531155\n",
      "Iteration 7689 Loss: 0.07445508654874092\n",
      "Iteration 7690 Loss: 0.07445493894069902\n",
      "Iteration 7691 Loss: 0.0744547913411852\n",
      "Iteration 7692 Loss: 0.0744546437501988\n",
      "Iteration 7693 Loss: 0.07445449616773935\n",
      "Iteration 7694 Loss: 0.07445434859380615\n",
      "Iteration 7695 Loss: 0.07445420102839859\n",
      "Iteration 7696 Loss: 0.07445405347151611\n",
      "Iteration 7697 Loss: 0.074453905923158\n",
      "Iteration 7698 Loss: 0.07445375838332377\n",
      "Iteration 7699 Loss: 0.0744536108520127\n",
      "Iteration 7700 Loss: 0.07445346332922426\n",
      "Iteration 7701 Loss: 0.07445331581495787\n",
      "Iteration 7702 Loss: 0.07445316830921282\n",
      "Iteration 7703 Loss: 0.0744530208119886\n",
      "Iteration 7704 Loss: 0.07445287332328458\n",
      "Iteration 7705 Loss: 0.07445272584310011\n",
      "Iteration 7706 Loss: 0.07445257837143468\n",
      "Iteration 7707 Loss: 0.07445243090828754\n",
      "Iteration 7708 Loss: 0.0744522834536582\n",
      "Iteration 7709 Loss: 0.07445213600754605\n",
      "Iteration 7710 Loss: 0.07445198856995044\n",
      "Iteration 7711 Loss: 0.07445184114087078\n",
      "Iteration 7712 Loss: 0.07445169372030648\n",
      "Iteration 7713 Loss: 0.07445154630825693\n",
      "Iteration 7714 Loss: 0.07445139890472148\n",
      "Iteration 7715 Loss: 0.07445125150969964\n",
      "Iteration 7716 Loss: 0.0744511041231907\n",
      "Iteration 7717 Loss: 0.07445095674519409\n",
      "Iteration 7718 Loss: 0.07445080937570926\n",
      "Iteration 7719 Loss: 0.07445066201473553\n",
      "Iteration 7720 Loss: 0.07445051466227234\n",
      "Iteration 7721 Loss: 0.07445036731831908\n",
      "Iteration 7722 Loss: 0.07445021998287518\n",
      "Iteration 7723 Loss: 0.07445007265593999\n",
      "Iteration 7724 Loss: 0.07444992533751296\n",
      "Iteration 7725 Loss: 0.07444977802759345\n",
      "Iteration 7726 Loss: 0.07444963072618083\n",
      "Iteration 7727 Loss: 0.0744494834332746\n",
      "Iteration 7728 Loss: 0.07444933614887413\n",
      "Iteration 7729 Loss: 0.07444918887297872\n",
      "Iteration 7730 Loss: 0.07444904160558788\n",
      "Iteration 7731 Loss: 0.07444889434670099\n",
      "Iteration 7732 Loss: 0.07444874709631738\n",
      "Iteration 7733 Loss: 0.07444859985443661\n",
      "Iteration 7734 Loss: 0.07444845262105795\n",
      "Iteration 7735 Loss: 0.07444830539618082\n",
      "Iteration 7736 Loss: 0.07444815817980466\n",
      "Iteration 7737 Loss: 0.07444801097192887\n",
      "Iteration 7738 Loss: 0.07444786377255289\n",
      "Iteration 7739 Loss: 0.074447716581676\n",
      "Iteration 7740 Loss: 0.07444756939929772\n",
      "Iteration 7741 Loss: 0.0744474222254174\n",
      "Iteration 7742 Loss: 0.0744472750600345\n",
      "Iteration 7743 Loss: 0.07444712790314831\n",
      "Iteration 7744 Loss: 0.07444698075475839\n",
      "Iteration 7745 Loss: 0.07444683361486402\n",
      "Iteration 7746 Loss: 0.0744466864834647\n",
      "Iteration 7747 Loss: 0.07444653936055977\n",
      "Iteration 7748 Loss: 0.07444639224614866\n",
      "Iteration 7749 Loss: 0.07444624514023082\n",
      "Iteration 7750 Loss: 0.07444609804280558\n",
      "Iteration 7751 Loss: 0.07444595095387233\n",
      "Iteration 7752 Loss: 0.07444580387343058\n",
      "Iteration 7753 Loss: 0.07444565680147966\n",
      "Iteration 7754 Loss: 0.07444550973801907\n",
      "Iteration 7755 Loss: 0.07444536268304809\n",
      "Iteration 7756 Loss: 0.07444521563656616\n",
      "Iteration 7757 Loss: 0.07444506859857283\n",
      "Iteration 7758 Loss: 0.07444492156906736\n",
      "Iteration 7759 Loss: 0.07444477454804915\n",
      "Iteration 7760 Loss: 0.07444462753551777\n",
      "Iteration 7761 Loss: 0.07444448053147242\n",
      "Iteration 7762 Loss: 0.07444433353591265\n",
      "Iteration 7763 Loss: 0.07444418654883789\n",
      "Iteration 7764 Loss: 0.0744440395702474\n",
      "Iteration 7765 Loss: 0.07444389260014075\n",
      "Iteration 7766 Loss: 0.07444374563851727\n",
      "Iteration 7767 Loss: 0.07444359868537641\n",
      "Iteration 7768 Loss: 0.07444345174071754\n",
      "Iteration 7769 Loss: 0.07444330480454009\n",
      "Iteration 7770 Loss: 0.07444315787684352\n",
      "Iteration 7771 Loss: 0.0744430109576272\n",
      "Iteration 7772 Loss: 0.07444286404689052\n",
      "Iteration 7773 Loss: 0.07444271714463291\n",
      "Iteration 7774 Loss: 0.07444257025085381\n",
      "Iteration 7775 Loss: 0.07444242336555258\n",
      "Iteration 7776 Loss: 0.07444227648872871\n",
      "Iteration 7777 Loss: 0.07444212962038152\n",
      "Iteration 7778 Loss: 0.07444198276051062\n",
      "Iteration 7779 Loss: 0.07444183590911516\n",
      "Iteration 7780 Loss: 0.07444168906619478\n",
      "Iteration 7781 Loss: 0.0744415422317487\n",
      "Iteration 7782 Loss: 0.07444139540577642\n",
      "Iteration 7783 Loss: 0.07444124858827747\n",
      "Iteration 7784 Loss: 0.07444110177925109\n",
      "Iteration 7785 Loss: 0.07444095497869677\n",
      "Iteration 7786 Loss: 0.07444080818661396\n",
      "Iteration 7787 Loss: 0.07444066140300205\n",
      "Iteration 7788 Loss: 0.07444051462786043\n",
      "Iteration 7789 Loss: 0.07444036786118859\n",
      "Iteration 7790 Loss: 0.07444022110298583\n",
      "Iteration 7791 Loss: 0.0744400743532517\n",
      "Iteration 7792 Loss: 0.07443992761198555\n",
      "Iteration 7793 Loss: 0.07443978087918678\n",
      "Iteration 7794 Loss: 0.07443963415485486\n",
      "Iteration 7795 Loss: 0.0744394874389891\n",
      "Iteration 7796 Loss: 0.07443934073158913\n",
      "Iteration 7797 Loss: 0.07443919403265416\n",
      "Iteration 7798 Loss: 0.07443904734218368\n",
      "Iteration 7799 Loss: 0.0744389006601772\n",
      "Iteration 7800 Loss: 0.07443875398663401\n",
      "Iteration 7801 Loss: 0.07443860732155361\n",
      "Iteration 7802 Loss: 0.07443846066493535\n",
      "Iteration 7803 Loss: 0.07443831401677876\n",
      "Iteration 7804 Loss: 0.07443816737708316\n",
      "Iteration 7805 Loss: 0.074438020745848\n",
      "Iteration 7806 Loss: 0.07443787412307273\n",
      "Iteration 7807 Loss: 0.07443772750875673\n",
      "Iteration 7808 Loss: 0.07443758090289951\n",
      "Iteration 7809 Loss: 0.07443743430550037\n",
      "Iteration 7810 Loss: 0.0744372877165588\n",
      "Iteration 7811 Loss: 0.0744371411360742\n",
      "Iteration 7812 Loss: 0.0744369945640461\n",
      "Iteration 7813 Loss: 0.07443684800047372\n",
      "Iteration 7814 Loss: 0.07443670144535668\n",
      "Iteration 7815 Loss: 0.07443655489869427\n",
      "Iteration 7816 Loss: 0.07443640836048597\n",
      "Iteration 7817 Loss: 0.07443626183073124\n",
      "Iteration 7818 Loss: 0.07443611530942942\n",
      "Iteration 7819 Loss: 0.07443596879658\n",
      "Iteration 7820 Loss: 0.07443582229218235\n",
      "Iteration 7821 Loss: 0.07443567579623597\n",
      "Iteration 7822 Loss: 0.07443552930874023\n",
      "Iteration 7823 Loss: 0.07443538282969457\n",
      "Iteration 7824 Loss: 0.0744352363590984\n",
      "Iteration 7825 Loss: 0.07443508989695122\n",
      "Iteration 7826 Loss: 0.07443494344325238\n",
      "Iteration 7827 Loss: 0.0744347969980013\n",
      "Iteration 7828 Loss: 0.0744346505611975\n",
      "Iteration 7829 Loss: 0.07443450413284024\n",
      "Iteration 7830 Loss: 0.0744343577129292\n",
      "Iteration 7831 Loss: 0.07443421130146352\n",
      "Iteration 7832 Loss: 0.07443406489844283\n",
      "Iteration 7833 Loss: 0.07443391850386649\n",
      "Iteration 7834 Loss: 0.0744337721177339\n",
      "Iteration 7835 Loss: 0.0744336257400446\n",
      "Iteration 7836 Loss: 0.07443347937079789\n",
      "Iteration 7837 Loss: 0.07443333300999325\n",
      "Iteration 7838 Loss: 0.07443318665763014\n",
      "Iteration 7839 Loss: 0.07443304031370797\n",
      "Iteration 7840 Loss: 0.0744328939782261\n",
      "Iteration 7841 Loss: 0.07443274765118406\n",
      "Iteration 7842 Loss: 0.0744326013325813\n",
      "Iteration 7843 Loss: 0.0744324550224171\n",
      "Iteration 7844 Loss: 0.074432308720691\n",
      "Iteration 7845 Loss: 0.07443216242740247\n",
      "Iteration 7846 Loss: 0.07443201614255084\n",
      "Iteration 7847 Loss: 0.07443186986613556\n",
      "Iteration 7848 Loss: 0.07443172359815613\n",
      "Iteration 7849 Loss: 0.07443157733861201\n",
      "Iteration 7850 Loss: 0.07443143108750247\n",
      "Iteration 7851 Loss: 0.07443128484482707\n",
      "Iteration 7852 Loss: 0.07443113861058523\n",
      "Iteration 7853 Loss: 0.07443099238477632\n",
      "Iteration 7854 Loss: 0.07443084616739984\n",
      "Iteration 7855 Loss: 0.0744306999584552\n",
      "Iteration 7856 Loss: 0.07443055375794182\n",
      "Iteration 7857 Loss: 0.07443040756585917\n",
      "Iteration 7858 Loss: 0.07443026138220663\n",
      "Iteration 7859 Loss: 0.07443011520698367\n",
      "Iteration 7860 Loss: 0.07442996904018977\n",
      "Iteration 7861 Loss: 0.07442982288182429\n",
      "Iteration 7862 Loss: 0.07442967673188668\n",
      "Iteration 7863 Loss: 0.07442953059037637\n",
      "Iteration 7864 Loss: 0.0744293844572928\n",
      "Iteration 7865 Loss: 0.0744292383326355\n",
      "Iteration 7866 Loss: 0.07442909221640373\n",
      "Iteration 7867 Loss: 0.07442894610859704\n",
      "Iteration 7868 Loss: 0.07442880000921484\n",
      "Iteration 7869 Loss: 0.0744286539182566\n",
      "Iteration 7870 Loss: 0.07442850783572173\n",
      "Iteration 7871 Loss: 0.07442836176160962\n",
      "Iteration 7872 Loss: 0.0744282156959198\n",
      "Iteration 7873 Loss: 0.07442806963865166\n",
      "Iteration 7874 Loss: 0.07442792358980459\n",
      "Iteration 7875 Loss: 0.07442777754937811\n",
      "Iteration 7876 Loss: 0.07442763151737164\n",
      "Iteration 7877 Loss: 0.07442748549378457\n",
      "Iteration 7878 Loss: 0.07442733947861638\n",
      "Iteration 7879 Loss: 0.07442719347186644\n",
      "Iteration 7880 Loss: 0.07442704747353432\n",
      "Iteration 7881 Loss: 0.07442690148361937\n",
      "Iteration 7882 Loss: 0.07442675550212104\n",
      "Iteration 7883 Loss: 0.07442660952903878\n",
      "Iteration 7884 Loss: 0.07442646356437194\n",
      "Iteration 7885 Loss: 0.07442631760812018\n",
      "Iteration 7886 Loss: 0.07442617166028277\n",
      "Iteration 7887 Loss: 0.0744260257208591\n",
      "Iteration 7888 Loss: 0.07442587978984878\n",
      "Iteration 7889 Loss: 0.07442573386725114\n",
      "Iteration 7890 Loss: 0.07442558795306564\n",
      "Iteration 7891 Loss: 0.07442544204729176\n",
      "Iteration 7892 Loss: 0.07442529614992885\n",
      "Iteration 7893 Loss: 0.07442515026097646\n",
      "Iteration 7894 Loss: 0.07442500438043395\n",
      "Iteration 7895 Loss: 0.0744248585083008\n",
      "Iteration 7896 Loss: 0.07442471264457648\n",
      "Iteration 7897 Loss: 0.07442456678926038\n",
      "Iteration 7898 Loss: 0.07442442094235194\n",
      "Iteration 7899 Loss: 0.07442427510385063\n",
      "Iteration 7900 Loss: 0.07442412927375591\n",
      "Iteration 7901 Loss: 0.07442398345206717\n",
      "Iteration 7902 Loss: 0.07442383763878395\n",
      "Iteration 7903 Loss: 0.07442369183390556\n",
      "Iteration 7904 Loss: 0.07442354603743154\n",
      "Iteration 7905 Loss: 0.07442340024936128\n",
      "Iteration 7906 Loss: 0.07442325446969429\n",
      "Iteration 7907 Loss: 0.07442310869842994\n",
      "Iteration 7908 Loss: 0.07442296293556767\n",
      "Iteration 7909 Loss: 0.07442281718110702\n",
      "Iteration 7910 Loss: 0.07442267143504744\n",
      "Iteration 7911 Loss: 0.07442252569738815\n",
      "Iteration 7912 Loss: 0.07442237996812891\n",
      "Iteration 7913 Loss: 0.074422234247269\n",
      "Iteration 7914 Loss: 0.07442208853480778\n",
      "Iteration 7915 Loss: 0.07442194283074484\n",
      "Iteration 7916 Loss: 0.07442179713507961\n",
      "Iteration 7917 Loss: 0.07442165144781152\n",
      "Iteration 7918 Loss: 0.07442150576893998\n",
      "Iteration 7919 Loss: 0.07442136009846444\n",
      "Iteration 7920 Loss: 0.07442121443638441\n",
      "Iteration 7921 Loss: 0.07442106878269929\n",
      "Iteration 7922 Loss: 0.07442092313740847\n",
      "Iteration 7923 Loss: 0.07442077750051157\n",
      "Iteration 7924 Loss: 0.07442063187200787\n",
      "Iteration 7925 Loss: 0.07442048625189684\n",
      "Iteration 7926 Loss: 0.07442034064017804\n",
      "Iteration 7927 Loss: 0.07442019503685078\n",
      "Iteration 7928 Loss: 0.0744200494419146\n",
      "Iteration 7929 Loss: 0.07441990385536898\n",
      "Iteration 7930 Loss: 0.07441975827721321\n",
      "Iteration 7931 Loss: 0.07441961270744686\n",
      "Iteration 7932 Loss: 0.07441946714606942\n",
      "Iteration 7933 Loss: 0.07441932159308019\n",
      "Iteration 7934 Loss: 0.07441917604847874\n",
      "Iteration 7935 Loss: 0.07441903051226455\n",
      "Iteration 7936 Loss: 0.07441888498443694\n",
      "Iteration 7937 Loss: 0.07441873946499547\n",
      "Iteration 7938 Loss: 0.07441859395393947\n",
      "Iteration 7939 Loss: 0.07441844845126855\n",
      "Iteration 7940 Loss: 0.07441830295698201\n",
      "Iteration 7941 Loss: 0.07441815747107941\n",
      "Iteration 7942 Loss: 0.07441801199356016\n",
      "Iteration 7943 Loss: 0.07441786652442373\n",
      "Iteration 7944 Loss: 0.07441772106366952\n",
      "Iteration 7945 Loss: 0.07441757561129705\n",
      "Iteration 7946 Loss: 0.07441743016730569\n",
      "Iteration 7947 Loss: 0.07441728473169498\n",
      "Iteration 7948 Loss: 0.07441713930446432\n",
      "Iteration 7949 Loss: 0.07441699388561322\n",
      "Iteration 7950 Loss: 0.07441684847514103\n",
      "Iteration 7951 Loss: 0.07441670307304729\n",
      "Iteration 7952 Loss: 0.07441655767933135\n",
      "Iteration 7953 Loss: 0.07441641229399279\n",
      "Iteration 7954 Loss: 0.07441626691703104\n",
      "Iteration 7955 Loss: 0.0744161215484455\n",
      "Iteration 7956 Loss: 0.07441597618823564\n",
      "Iteration 7957 Loss: 0.07441583083640095\n",
      "Iteration 7958 Loss: 0.07441568549294082\n",
      "Iteration 7959 Loss: 0.07441554015785476\n",
      "Iteration 7960 Loss: 0.07441539483114215\n",
      "Iteration 7961 Loss: 0.07441524951280258\n",
      "Iteration 7962 Loss: 0.07441510420283537\n",
      "Iteration 7963 Loss: 0.07441495890124003\n",
      "Iteration 7964 Loss: 0.07441481360801604\n",
      "Iteration 7965 Loss: 0.07441466832316276\n",
      "Iteration 7966 Loss: 0.07441452304667986\n",
      "Iteration 7967 Loss: 0.07441437777856652\n",
      "Iteration 7968 Loss: 0.07441423251882237\n",
      "Iteration 7969 Loss: 0.07441408726744683\n",
      "Iteration 7970 Loss: 0.07441394202443936\n",
      "Iteration 7971 Loss: 0.07441379678979936\n",
      "Iteration 7972 Loss: 0.07441365156352642\n",
      "Iteration 7973 Loss: 0.07441350634561979\n",
      "Iteration 7974 Loss: 0.07441336113607912\n",
      "Iteration 7975 Loss: 0.0744132159349038\n",
      "Iteration 7976 Loss: 0.07441307074209322\n",
      "Iteration 7977 Loss: 0.07441292555764689\n",
      "Iteration 7978 Loss: 0.07441278038156433\n",
      "Iteration 7979 Loss: 0.07441263521384493\n",
      "Iteration 7980 Loss: 0.07441249005448816\n",
      "Iteration 7981 Loss: 0.07441234490349355\n",
      "Iteration 7982 Loss: 0.07441219976086036\n",
      "Iteration 7983 Loss: 0.07441205462658823\n",
      "Iteration 7984 Loss: 0.07441190950067655\n",
      "Iteration 7985 Loss: 0.07441176438312481\n",
      "Iteration 7986 Loss: 0.07441161927393247\n",
      "Iteration 7987 Loss: 0.07441147417309894\n",
      "Iteration 7988 Loss: 0.07441132908062373\n",
      "Iteration 7989 Loss: 0.07441118399650627\n",
      "Iteration 7990 Loss: 0.07441103892074599\n",
      "Iteration 7991 Loss: 0.07441089385334247\n",
      "Iteration 7992 Loss: 0.07441074879429506\n",
      "Iteration 7993 Loss: 0.07441060374360324\n",
      "Iteration 7994 Loss: 0.0744104587012665\n",
      "Iteration 7995 Loss: 0.0744103136672843\n",
      "Iteration 7996 Loss: 0.07441016864165598\n",
      "Iteration 7997 Loss: 0.0744100236243812\n",
      "Iteration 7998 Loss: 0.07440987861545931\n",
      "Iteration 7999 Loss: 0.07440973361488976\n",
      "Iteration 8000 Loss: 0.07440958862267204\n",
      "Iteration 8001 Loss: 0.07440944363880561\n",
      "Iteration 8002 Loss: 0.07440929866328996\n",
      "Iteration 8003 Loss: 0.07440915369612451\n",
      "Iteration 8004 Loss: 0.07440900873730871\n",
      "Iteration 8005 Loss: 0.07440886378684208\n",
      "Iteration 8006 Loss: 0.07440871884472404\n",
      "Iteration 8007 Loss: 0.07440857391095405\n",
      "Iteration 8008 Loss: 0.0744084289855316\n",
      "Iteration 8009 Loss: 0.07440828406845611\n",
      "Iteration 8010 Loss: 0.07440813915972708\n",
      "Iteration 8011 Loss: 0.07440799425934394\n",
      "Iteration 8012 Loss: 0.07440784936730621\n",
      "Iteration 8013 Loss: 0.07440770448361334\n",
      "Iteration 8014 Loss: 0.07440755960826476\n",
      "Iteration 8015 Loss: 0.07440741474125993\n",
      "Iteration 8016 Loss: 0.07440726988259835\n",
      "Iteration 8017 Loss: 0.07440712503227943\n",
      "Iteration 8018 Loss: 0.07440698019030276\n",
      "Iteration 8019 Loss: 0.07440683535666762\n",
      "Iteration 8020 Loss: 0.07440669053137361\n",
      "Iteration 8021 Loss: 0.07440654571442017\n",
      "Iteration 8022 Loss: 0.0744064009058067\n",
      "Iteration 8023 Loss: 0.07440625610553271\n",
      "Iteration 8024 Loss: 0.07440611131359773\n",
      "Iteration 8025 Loss: 0.07440596653000116\n",
      "Iteration 8026 Loss: 0.07440582175474242\n",
      "Iteration 8027 Loss: 0.07440567698782105\n",
      "Iteration 8028 Loss: 0.07440553222923652\n",
      "Iteration 8029 Loss: 0.07440538747898824\n",
      "Iteration 8030 Loss: 0.07440524273707572\n",
      "Iteration 8031 Loss: 0.07440509800349838\n",
      "Iteration 8032 Loss: 0.07440495327825578\n",
      "Iteration 8033 Loss: 0.07440480856134726\n",
      "Iteration 8034 Loss: 0.0744046638527724\n",
      "Iteration 8035 Loss: 0.07440451915253057\n",
      "Iteration 8036 Loss: 0.07440437446062136\n",
      "Iteration 8037 Loss: 0.0744042297770441\n",
      "Iteration 8038 Loss: 0.07440408510179837\n",
      "Iteration 8039 Loss: 0.07440394043488353\n",
      "Iteration 8040 Loss: 0.07440379577629916\n",
      "Iteration 8041 Loss: 0.07440365112604468\n",
      "Iteration 8042 Loss: 0.0744035064841195\n",
      "Iteration 8043 Loss: 0.07440336185052318\n",
      "Iteration 8044 Loss: 0.07440321722525516\n",
      "Iteration 8045 Loss: 0.07440307260831487\n",
      "Iteration 8046 Loss: 0.07440292799970182\n",
      "Iteration 8047 Loss: 0.07440278339941543\n",
      "Iteration 8048 Loss: 0.07440263880745526\n",
      "Iteration 8049 Loss: 0.07440249422382073\n",
      "Iteration 8050 Loss: 0.0744023496485112\n",
      "Iteration 8051 Loss: 0.07440220508152637\n",
      "Iteration 8052 Loss: 0.07440206052286553\n",
      "Iteration 8053 Loss: 0.07440191597252814\n",
      "Iteration 8054 Loss: 0.07440177143051382\n",
      "Iteration 8055 Loss: 0.07440162689682192\n",
      "Iteration 8056 Loss: 0.07440148237145194\n",
      "Iteration 8057 Loss: 0.07440133785440337\n",
      "Iteration 8058 Loss: 0.07440119334567567\n",
      "Iteration 8059 Loss: 0.07440104884526831\n",
      "Iteration 8060 Loss: 0.07440090435318074\n",
      "Iteration 8061 Loss: 0.07440075986941246\n",
      "Iteration 8062 Loss: 0.0744006153939629\n",
      "Iteration 8063 Loss: 0.07440047092683157\n",
      "Iteration 8064 Loss: 0.07440032646801789\n",
      "Iteration 8065 Loss: 0.07440018201752144\n",
      "Iteration 8066 Loss: 0.07440003757534158\n",
      "Iteration 8067 Loss: 0.07439989314147787\n",
      "Iteration 8068 Loss: 0.07439974871592969\n",
      "Iteration 8069 Loss: 0.0743996042986966\n",
      "Iteration 8070 Loss: 0.07439945988977802\n",
      "Iteration 8071 Loss: 0.07439931548917342\n",
      "Iteration 8072 Loss: 0.0743991710968823\n",
      "Iteration 8073 Loss: 0.07439902671290412\n",
      "Iteration 8074 Loss: 0.07439888233723835\n",
      "Iteration 8075 Loss: 0.07439873796988443\n",
      "Iteration 8076 Loss: 0.07439859361084192\n",
      "Iteration 8077 Loss: 0.07439844926011027\n",
      "Iteration 8078 Loss: 0.07439830491768884\n",
      "Iteration 8079 Loss: 0.07439816058357725\n",
      "Iteration 8080 Loss: 0.07439801625777487\n",
      "Iteration 8081 Loss: 0.07439787194028129\n",
      "Iteration 8082 Loss: 0.07439772763109584\n",
      "Iteration 8083 Loss: 0.07439758333021809\n",
      "Iteration 8084 Loss: 0.07439743903764753\n",
      "Iteration 8085 Loss: 0.07439729475338355\n",
      "Iteration 8086 Loss: 0.0743971504774257\n",
      "Iteration 8087 Loss: 0.07439700620977338\n",
      "Iteration 8088 Loss: 0.0743968619504261\n",
      "Iteration 8089 Loss: 0.07439671769938336\n",
      "Iteration 8090 Loss: 0.07439657345664469\n",
      "Iteration 8091 Loss: 0.07439642922220945\n",
      "Iteration 8092 Loss: 0.07439628499607713\n",
      "Iteration 8093 Loss: 0.07439614077824727\n",
      "Iteration 8094 Loss: 0.07439599656871929\n",
      "Iteration 8095 Loss: 0.07439585236749265\n",
      "Iteration 8096 Loss: 0.07439570817456695\n",
      "Iteration 8097 Loss: 0.07439556398994152\n",
      "Iteration 8098 Loss: 0.07439541981361597\n",
      "Iteration 8099 Loss: 0.0743952756455896\n",
      "Iteration 8100 Loss: 0.07439513148586208\n",
      "Iteration 8101 Loss: 0.07439498733443271\n",
      "Iteration 8102 Loss: 0.07439484319130114\n",
      "Iteration 8103 Loss: 0.07439469905646667\n",
      "Iteration 8104 Loss: 0.07439455492992893\n",
      "Iteration 8105 Loss: 0.07439441081168732\n",
      "Iteration 8106 Loss: 0.07439426670174129\n",
      "Iteration 8107 Loss: 0.07439412260009037\n",
      "Iteration 8108 Loss: 0.0743939785067341\n",
      "Iteration 8109 Loss: 0.07439383442167186\n",
      "Iteration 8110 Loss: 0.07439369034490309\n",
      "Iteration 8111 Loss: 0.07439354627642739\n",
      "Iteration 8112 Loss: 0.07439340221624413\n",
      "Iteration 8113 Loss: 0.0743932581643529\n",
      "Iteration 8114 Loss: 0.07439311412075308\n",
      "Iteration 8115 Loss: 0.07439297008544421\n",
      "Iteration 8116 Loss: 0.07439282605842572\n",
      "Iteration 8117 Loss: 0.07439268203969711\n",
      "Iteration 8118 Loss: 0.07439253802925785\n",
      "Iteration 8119 Loss: 0.07439239402710744\n",
      "Iteration 8120 Loss: 0.07439225003324536\n",
      "Iteration 8121 Loss: 0.07439210604767107\n",
      "Iteration 8122 Loss: 0.07439196207038401\n",
      "Iteration 8123 Loss: 0.07439181810138379\n",
      "Iteration 8124 Loss: 0.07439167414066984\n",
      "Iteration 8125 Loss: 0.07439153018824154\n",
      "Iteration 8126 Loss: 0.07439138624409841\n",
      "Iteration 8127 Loss: 0.07439124230823997\n",
      "Iteration 8128 Loss: 0.07439109838066575\n",
      "Iteration 8129 Loss: 0.07439095446137511\n",
      "Iteration 8130 Loss: 0.0743908105503676\n",
      "Iteration 8131 Loss: 0.0743906666476427\n",
      "Iteration 8132 Loss: 0.07439052275319992\n",
      "Iteration 8133 Loss: 0.07439037886703866\n",
      "Iteration 8134 Loss: 0.07439023498915845\n",
      "Iteration 8135 Loss: 0.07439009111955877\n",
      "Iteration 8136 Loss: 0.0743899472582391\n",
      "Iteration 8137 Loss: 0.07438980340519889\n",
      "Iteration 8138 Loss: 0.07438965956043768\n",
      "Iteration 8139 Loss: 0.07438951572395494\n",
      "Iteration 8140 Loss: 0.07438937189575008\n",
      "Iteration 8141 Loss: 0.07438922807582264\n",
      "Iteration 8142 Loss: 0.07438908426417211\n",
      "Iteration 8143 Loss: 0.07438894046079798\n",
      "Iteration 8144 Loss: 0.07438879666569971\n",
      "Iteration 8145 Loss: 0.07438865287887675\n",
      "Iteration 8146 Loss: 0.07438850910032865\n",
      "Iteration 8147 Loss: 0.07438836533005486\n",
      "Iteration 8148 Loss: 0.07438822156805489\n",
      "Iteration 8149 Loss: 0.07438807781432814\n",
      "Iteration 8150 Loss: 0.07438793406887419\n",
      "Iteration 8151 Loss: 0.07438779033169243\n",
      "Iteration 8152 Loss: 0.07438764660278242\n",
      "Iteration 8153 Loss: 0.07438750288214364\n",
      "Iteration 8154 Loss: 0.07438735916977553\n",
      "Iteration 8155 Loss: 0.07438721546567761\n",
      "Iteration 8156 Loss: 0.07438707176984935\n",
      "Iteration 8157 Loss: 0.07438692808229022\n",
      "Iteration 8158 Loss: 0.0743867844029998\n",
      "Iteration 8159 Loss: 0.07438664073197736\n",
      "Iteration 8160 Loss: 0.07438649706922265\n",
      "Iteration 8161 Loss: 0.07438635341473492\n",
      "Iteration 8162 Loss: 0.0743862097685138\n",
      "Iteration 8163 Loss: 0.07438606613055876\n",
      "Iteration 8164 Loss: 0.07438592250086921\n",
      "Iteration 8165 Loss: 0.07438577887944472\n",
      "Iteration 8166 Loss: 0.0743856352662847\n",
      "Iteration 8167 Loss: 0.07438549166138868\n",
      "Iteration 8168 Loss: 0.07438534806475613\n",
      "Iteration 8169 Loss: 0.07438520447638661\n",
      "Iteration 8170 Loss: 0.07438506089627946\n",
      "Iteration 8171 Loss: 0.07438491732443425\n",
      "Iteration 8172 Loss: 0.07438477376085055\n",
      "Iteration 8173 Loss: 0.07438463020552769\n",
      "Iteration 8174 Loss: 0.0743844866584652\n",
      "Iteration 8175 Loss: 0.07438434311966263\n",
      "Iteration 8176 Loss: 0.07438419958911943\n",
      "Iteration 8177 Loss: 0.07438405606683501\n",
      "Iteration 8178 Loss: 0.07438391255280902\n",
      "Iteration 8179 Loss: 0.07438376904704079\n",
      "Iteration 8180 Loss: 0.07438362554952996\n",
      "Iteration 8181 Loss: 0.07438348206027584\n",
      "Iteration 8182 Loss: 0.07438333857927805\n",
      "Iteration 8183 Loss: 0.07438319510653597\n",
      "Iteration 8184 Loss: 0.07438305164204921\n",
      "Iteration 8185 Loss: 0.07438290818581719\n",
      "Iteration 8186 Loss: 0.07438276473783942\n",
      "Iteration 8187 Loss: 0.07438262129811538\n",
      "Iteration 8188 Loss: 0.07438247786664454\n",
      "Iteration 8189 Loss: 0.07438233444342639\n",
      "Iteration 8190 Loss: 0.07438219102846041\n",
      "Iteration 8191 Loss: 0.07438204762174609\n",
      "Iteration 8192 Loss: 0.07438190422328306\n",
      "Iteration 8193 Loss: 0.07438176083307055\n",
      "Iteration 8194 Loss: 0.07438161745110823\n",
      "Iteration 8195 Loss: 0.07438147407739551\n",
      "Iteration 8196 Loss: 0.07438133071193195\n",
      "Iteration 8197 Loss: 0.07438118735471697\n",
      "Iteration 8198 Loss: 0.07438104400575014\n",
      "Iteration 8199 Loss: 0.07438090066503081\n",
      "Iteration 8200 Loss: 0.0743807573325586\n",
      "Iteration 8201 Loss: 0.07438061400833292\n",
      "Iteration 8202 Loss: 0.07438047069235335\n",
      "Iteration 8203 Loss: 0.07438032738461928\n",
      "Iteration 8204 Loss: 0.07438018408513028\n",
      "Iteration 8205 Loss: 0.07438004079388576\n",
      "Iteration 8206 Loss: 0.07437989751088527\n",
      "Iteration 8207 Loss: 0.07437975423612829\n",
      "Iteration 8208 Loss: 0.07437961096961429\n",
      "Iteration 8209 Loss: 0.07437946771134274\n",
      "Iteration 8210 Loss: 0.0743793244613132\n",
      "Iteration 8211 Loss: 0.07437918121952514\n",
      "Iteration 8212 Loss: 0.07437903798597803\n",
      "Iteration 8213 Loss: 0.07437889476067136\n",
      "Iteration 8214 Loss: 0.0743787515436046\n",
      "Iteration 8215 Loss: 0.07437860833477729\n",
      "Iteration 8216 Loss: 0.07437846513418886\n",
      "Iteration 8217 Loss: 0.07437832194183888\n",
      "Iteration 8218 Loss: 0.07437817875772679\n",
      "Iteration 8219 Loss: 0.07437803558185212\n",
      "Iteration 8220 Loss: 0.07437789241421429\n",
      "Iteration 8221 Loss: 0.07437774925481282\n",
      "Iteration 8222 Loss: 0.07437760610364724\n",
      "Iteration 8223 Loss: 0.07437746296071703\n",
      "Iteration 8224 Loss: 0.07437731982602162\n",
      "Iteration 8225 Loss: 0.07437717669956057\n",
      "Iteration 8226 Loss: 0.07437703358133338\n",
      "Iteration 8227 Loss: 0.07437689047133951\n",
      "Iteration 8228 Loss: 0.07437674736957849\n",
      "Iteration 8229 Loss: 0.07437660427604972\n",
      "Iteration 8230 Loss: 0.07437646119075281\n",
      "Iteration 8231 Loss: 0.07437631811368717\n",
      "Iteration 8232 Loss: 0.0743761750448523\n",
      "Iteration 8233 Loss: 0.07437603198424772\n",
      "Iteration 8234 Loss: 0.0743758889318729\n",
      "Iteration 8235 Loss: 0.0743757458877274\n",
      "Iteration 8236 Loss: 0.07437560285181063\n",
      "Iteration 8237 Loss: 0.0743754598241221\n",
      "Iteration 8238 Loss: 0.07437531680466132\n",
      "Iteration 8239 Loss: 0.07437517379342776\n",
      "Iteration 8240 Loss: 0.07437503079042099\n",
      "Iteration 8241 Loss: 0.07437488779564042\n",
      "Iteration 8242 Loss: 0.07437474480908557\n",
      "Iteration 8243 Loss: 0.07437460183075596\n",
      "Iteration 8244 Loss: 0.07437445886065101\n",
      "Iteration 8245 Loss: 0.07437431589877032\n",
      "Iteration 8246 Loss: 0.07437417294511331\n",
      "Iteration 8247 Loss: 0.07437402999967949\n",
      "Iteration 8248 Loss: 0.07437388706246834\n",
      "Iteration 8249 Loss: 0.07437374413347934\n",
      "Iteration 8250 Loss: 0.07437360121271211\n",
      "Iteration 8251 Loss: 0.07437345830016598\n",
      "Iteration 8252 Loss: 0.07437331539584054\n",
      "Iteration 8253 Loss: 0.07437317249973527\n",
      "Iteration 8254 Loss: 0.07437302961184963\n",
      "Iteration 8255 Loss: 0.07437288673218315\n",
      "Iteration 8256 Loss: 0.07437274386073535\n",
      "Iteration 8257 Loss: 0.07437260099750567\n",
      "Iteration 8258 Loss: 0.07437245814249367\n",
      "Iteration 8259 Loss: 0.07437231529569878\n",
      "Iteration 8260 Loss: 0.07437217245712047\n",
      "Iteration 8261 Loss: 0.0743720296267583\n",
      "Iteration 8262 Loss: 0.07437188680461175\n",
      "Iteration 8263 Loss: 0.07437174399068036\n",
      "Iteration 8264 Loss: 0.07437160118496358\n",
      "Iteration 8265 Loss: 0.07437145838746091\n",
      "Iteration 8266 Loss: 0.07437131559817181\n",
      "Iteration 8267 Loss: 0.07437117281709585\n",
      "Iteration 8268 Loss: 0.07437103004423244\n",
      "Iteration 8269 Loss: 0.07437088727958116\n",
      "Iteration 8270 Loss: 0.07437074452314148\n",
      "Iteration 8271 Loss: 0.0743706017749129\n",
      "Iteration 8272 Loss: 0.07437045903489492\n",
      "Iteration 8273 Loss: 0.07437031630308692\n",
      "Iteration 8274 Loss: 0.07437017357948861\n",
      "Iteration 8275 Loss: 0.07437003086409935\n",
      "Iteration 8276 Loss: 0.07436988815691861\n",
      "Iteration 8277 Loss: 0.07436974545794603\n",
      "Iteration 8278 Loss: 0.07436960276718099\n",
      "Iteration 8279 Loss: 0.074369460084623\n",
      "Iteration 8280 Loss: 0.07436931741027153\n",
      "Iteration 8281 Loss: 0.07436917474412623\n",
      "Iteration 8282 Loss: 0.07436903208618643\n",
      "Iteration 8283 Loss: 0.0743688894364517\n",
      "Iteration 8284 Loss: 0.07436874679492153\n",
      "Iteration 8285 Loss: 0.07436860416159542\n",
      "Iteration 8286 Loss: 0.07436846153647288\n",
      "Iteration 8287 Loss: 0.07436831891955338\n",
      "Iteration 8288 Loss: 0.07436817631083636\n",
      "Iteration 8289 Loss: 0.07436803371032151\n",
      "Iteration 8290 Loss: 0.07436789111800815\n",
      "Iteration 8291 Loss: 0.0743677485338958\n",
      "Iteration 8292 Loss: 0.07436760595798411\n",
      "Iteration 8293 Loss: 0.07436746339027239\n",
      "Iteration 8294 Loss: 0.07436732083076023\n",
      "Iteration 8295 Loss: 0.07436717827944712\n",
      "Iteration 8296 Loss: 0.07436703573633255\n",
      "Iteration 8297 Loss: 0.07436689320141603\n",
      "Iteration 8298 Loss: 0.07436675067469703\n",
      "Iteration 8299 Loss: 0.07436660815617509\n",
      "Iteration 8300 Loss: 0.07436646564584966\n",
      "Iteration 8301 Loss: 0.07436632314372031\n",
      "Iteration 8302 Loss: 0.07436618064978652\n",
      "Iteration 8303 Loss: 0.07436603816404769\n",
      "Iteration 8304 Loss: 0.07436589568650352\n",
      "Iteration 8305 Loss: 0.07436575321715336\n",
      "Iteration 8306 Loss: 0.07436561075599667\n",
      "Iteration 8307 Loss: 0.07436546830303307\n",
      "Iteration 8308 Loss: 0.07436532585826203\n",
      "Iteration 8309 Loss: 0.07436518342168302\n",
      "Iteration 8310 Loss: 0.07436504099329558\n",
      "Iteration 8311 Loss: 0.07436489857309911\n",
      "Iteration 8312 Loss: 0.07436475616109328\n",
      "Iteration 8313 Loss: 0.07436461375727743\n",
      "Iteration 8314 Loss: 0.0743644713616512\n",
      "Iteration 8315 Loss: 0.07436432897421398\n",
      "Iteration 8316 Loss: 0.07436418659496531\n",
      "Iteration 8317 Loss: 0.07436404422390472\n",
      "Iteration 8318 Loss: 0.07436390186103164\n",
      "Iteration 8319 Loss: 0.0743637595063456\n",
      "Iteration 8320 Loss: 0.07436361715984617\n",
      "Iteration 8321 Loss: 0.0743634748215328\n",
      "Iteration 8322 Loss: 0.07436333249140499\n",
      "Iteration 8323 Loss: 0.0743631901694622\n",
      "Iteration 8324 Loss: 0.07436304785570404\n",
      "Iteration 8325 Loss: 0.07436290555012992\n",
      "Iteration 8326 Loss: 0.07436276325273934\n",
      "Iteration 8327 Loss: 0.07436262096353188\n",
      "Iteration 8328 Loss: 0.07436247868250696\n",
      "Iteration 8329 Loss: 0.07436233640966412\n",
      "Iteration 8330 Loss: 0.07436219414500286\n",
      "Iteration 8331 Loss: 0.07436205188852271\n",
      "Iteration 8332 Loss: 0.07436190964022313\n",
      "Iteration 8333 Loss: 0.07436176740010363\n",
      "Iteration 8334 Loss: 0.07436162516816373\n",
      "Iteration 8335 Loss: 0.07436148294440288\n",
      "Iteration 8336 Loss: 0.07436134072882068\n",
      "Iteration 8337 Loss: 0.07436119852141658\n",
      "Iteration 8338 Loss: 0.07436105632219007\n",
      "Iteration 8339 Loss: 0.07436091413114068\n",
      "Iteration 8340 Loss: 0.0743607719482679\n",
      "Iteration 8341 Loss: 0.0743606297735712\n",
      "Iteration 8342 Loss: 0.07436048760705015\n",
      "Iteration 8343 Loss: 0.07436034544870423\n",
      "Iteration 8344 Loss: 0.0743602032985329\n",
      "Iteration 8345 Loss: 0.07436006115653573\n",
      "Iteration 8346 Loss: 0.07435991902271219\n",
      "Iteration 8347 Loss: 0.07435977689706177\n",
      "Iteration 8348 Loss: 0.07435963477958404\n",
      "Iteration 8349 Loss: 0.07435949267027842\n",
      "Iteration 8350 Loss: 0.07435935056914444\n",
      "Iteration 8351 Loss: 0.07435920847618166\n",
      "Iteration 8352 Loss: 0.07435906639138948\n",
      "Iteration 8353 Loss: 0.0743589243147675\n",
      "Iteration 8354 Loss: 0.07435878224631517\n",
      "Iteration 8355 Loss: 0.07435864018603203\n",
      "Iteration 8356 Loss: 0.07435849813391755\n",
      "Iteration 8357 Loss: 0.07435835608997127\n",
      "Iteration 8358 Loss: 0.0743582140541927\n",
      "Iteration 8359 Loss: 0.07435807202658126\n",
      "Iteration 8360 Loss: 0.07435793000713661\n",
      "Iteration 8361 Loss: 0.07435778799585813\n",
      "Iteration 8362 Loss: 0.07435764599274533\n",
      "Iteration 8363 Loss: 0.07435750399779775\n",
      "Iteration 8364 Loss: 0.0743573620110149\n",
      "Iteration 8365 Loss: 0.07435722003239631\n",
      "Iteration 8366 Loss: 0.07435707806194139\n",
      "Iteration 8367 Loss: 0.07435693609964975\n",
      "Iteration 8368 Loss: 0.0743567941455209\n",
      "Iteration 8369 Loss: 0.07435665219955424\n",
      "Iteration 8370 Loss: 0.07435651026174936\n",
      "Iteration 8371 Loss: 0.07435636833210571\n",
      "Iteration 8372 Loss: 0.07435622641062285\n",
      "Iteration 8373 Loss: 0.07435608449730031\n",
      "Iteration 8374 Loss: 0.07435594259213744\n",
      "Iteration 8375 Loss: 0.07435580069513394\n",
      "Iteration 8376 Loss: 0.07435565880628928\n",
      "Iteration 8377 Loss: 0.07435551692560285\n",
      "Iteration 8378 Loss: 0.0743553750530742\n",
      "Iteration 8379 Loss: 0.07435523318870296\n",
      "Iteration 8380 Loss: 0.07435509133248848\n",
      "Iteration 8381 Loss: 0.07435494948443036\n",
      "Iteration 8382 Loss: 0.07435480764452808\n",
      "Iteration 8383 Loss: 0.07435466581278116\n",
      "Iteration 8384 Loss: 0.07435452398918908\n",
      "Iteration 8385 Loss: 0.07435438217375134\n",
      "Iteration 8386 Loss: 0.07435424036646746\n",
      "Iteration 8387 Loss: 0.07435409856733696\n",
      "Iteration 8388 Loss: 0.0743539567763594\n",
      "Iteration 8389 Loss: 0.07435381499353415\n",
      "Iteration 8390 Loss: 0.07435367321886086\n",
      "Iteration 8391 Loss: 0.07435353145233892\n",
      "Iteration 8392 Loss: 0.07435338969396796\n",
      "Iteration 8393 Loss: 0.07435324794374737\n",
      "Iteration 8394 Loss: 0.07435310620167672\n",
      "Iteration 8395 Loss: 0.07435296446775554\n",
      "Iteration 8396 Loss: 0.0743528227419833\n",
      "Iteration 8397 Loss: 0.07435268102435943\n",
      "Iteration 8398 Loss: 0.07435253931488364\n",
      "Iteration 8399 Loss: 0.07435239761355526\n",
      "Iteration 8400 Loss: 0.07435225592037388\n",
      "Iteration 8401 Loss: 0.074352114235339\n",
      "Iteration 8402 Loss: 0.07435197255845011\n",
      "Iteration 8403 Loss: 0.07435183088970676\n",
      "Iteration 8404 Loss: 0.07435168922910837\n",
      "Iteration 8405 Loss: 0.07435154757665452\n",
      "Iteration 8406 Loss: 0.07435140593234471\n",
      "Iteration 8407 Loss: 0.07435126429617844\n",
      "Iteration 8408 Loss: 0.07435112266815526\n",
      "Iteration 8409 Loss: 0.07435098104827459\n",
      "Iteration 8410 Loss: 0.074350839436536\n",
      "Iteration 8411 Loss: 0.07435069783293904\n",
      "Iteration 8412 Loss: 0.07435055623748318\n",
      "Iteration 8413 Loss: 0.07435041465016787\n",
      "Iteration 8414 Loss: 0.0743502730709927\n",
      "Iteration 8415 Loss: 0.0743501314999571\n",
      "Iteration 8416 Loss: 0.07434998993706068\n",
      "Iteration 8417 Loss: 0.07434984838230288\n",
      "Iteration 8418 Loss: 0.07434970683568329\n",
      "Iteration 8419 Loss: 0.07434956529720127\n",
      "Iteration 8420 Loss: 0.07434942376685652\n",
      "Iteration 8421 Loss: 0.07434928224464839\n",
      "Iteration 8422 Loss: 0.07434914073057643\n",
      "Iteration 8423 Loss: 0.07434899922464024\n",
      "Iteration 8424 Loss: 0.07434885772683922\n",
      "Iteration 8425 Loss: 0.07434871623717294\n",
      "Iteration 8426 Loss: 0.07434857475564091\n",
      "Iteration 8427 Loss: 0.0743484332822426\n",
      "Iteration 8428 Loss: 0.0743482918169776\n",
      "Iteration 8429 Loss: 0.0743481503598453\n",
      "Iteration 8430 Loss: 0.0743480089108453\n",
      "Iteration 8431 Loss: 0.07434786746997711\n",
      "Iteration 8432 Loss: 0.07434772603724024\n",
      "Iteration 8433 Loss: 0.0743475846126341\n",
      "Iteration 8434 Loss: 0.07434744319615835\n",
      "Iteration 8435 Loss: 0.07434730178781243\n",
      "Iteration 8436 Loss: 0.07434716038759588\n",
      "Iteration 8437 Loss: 0.07434701899550815\n",
      "Iteration 8438 Loss: 0.07434687761154882\n",
      "Iteration 8439 Loss: 0.07434673623571733\n",
      "Iteration 8440 Loss: 0.0743465948680133\n",
      "Iteration 8441 Loss: 0.07434645350843613\n",
      "Iteration 8442 Loss: 0.07434631215698542\n",
      "Iteration 8443 Loss: 0.0743461708136606\n",
      "Iteration 8444 Loss: 0.07434602947846122\n",
      "Iteration 8445 Loss: 0.07434588815138679\n",
      "Iteration 8446 Loss: 0.07434574683243692\n",
      "Iteration 8447 Loss: 0.0743456055216109\n",
      "Iteration 8448 Loss: 0.07434546421890846\n",
      "Iteration 8449 Loss: 0.07434532292432901\n",
      "Iteration 8450 Loss: 0.07434518163787204\n",
      "Iteration 8451 Loss: 0.07434504035953715\n",
      "Iteration 8452 Loss: 0.07434489908932376\n",
      "Iteration 8453 Loss: 0.07434475782723142\n",
      "Iteration 8454 Loss: 0.07434461657325968\n",
      "Iteration 8455 Loss: 0.07434447532740801\n",
      "Iteration 8456 Loss: 0.07434433408967596\n",
      "Iteration 8457 Loss: 0.07434419286006297\n",
      "Iteration 8458 Loss: 0.07434405163856865\n",
      "Iteration 8459 Loss: 0.07434391042519246\n",
      "Iteration 8460 Loss: 0.07434376921993395\n",
      "Iteration 8461 Loss: 0.07434362802279251\n",
      "Iteration 8462 Loss: 0.07434348683376782\n",
      "Iteration 8463 Loss: 0.0743433456528593\n",
      "Iteration 8464 Loss: 0.07434320448006645\n",
      "Iteration 8465 Loss: 0.07434306331538887\n",
      "Iteration 8466 Loss: 0.07434292215882596\n",
      "Iteration 8467 Loss: 0.07434278101037736\n",
      "Iteration 8468 Loss: 0.0743426398700425\n",
      "Iteration 8469 Loss: 0.07434249873782088\n",
      "Iteration 8470 Loss: 0.07434235761371207\n",
      "Iteration 8471 Loss: 0.07434221649771558\n",
      "Iteration 8472 Loss: 0.07434207538983081\n",
      "Iteration 8473 Loss: 0.07434193429005753\n",
      "Iteration 8474 Loss: 0.07434179319839496\n",
      "Iteration 8475 Loss: 0.07434165211484285\n",
      "Iteration 8476 Loss: 0.07434151103940055\n",
      "Iteration 8477 Loss: 0.07434136997206767\n",
      "Iteration 8478 Loss: 0.0743412289128436\n",
      "Iteration 8479 Loss: 0.07434108786172805\n",
      "Iteration 8480 Loss: 0.07434094681872042\n",
      "Iteration 8481 Loss: 0.0743408057838202\n",
      "Iteration 8482 Loss: 0.07434066475702694\n",
      "Iteration 8483 Loss: 0.07434052373834017\n",
      "Iteration 8484 Loss: 0.07434038272775942\n",
      "Iteration 8485 Loss: 0.07434024172528415\n",
      "Iteration 8486 Loss: 0.07434010073091393\n",
      "Iteration 8487 Loss: 0.07433995974464822\n",
      "Iteration 8488 Loss: 0.07433981876648654\n",
      "Iteration 8489 Loss: 0.07433967779642843\n",
      "Iteration 8490 Loss: 0.07433953683447352\n",
      "Iteration 8491 Loss: 0.0743393958806211\n",
      "Iteration 8492 Loss: 0.07433925493487081\n",
      "Iteration 8493 Loss: 0.07433911399722219\n",
      "Iteration 8494 Loss: 0.0743389730676747\n",
      "Iteration 8495 Loss: 0.07433883214622779\n",
      "Iteration 8496 Loss: 0.0743386912328812\n",
      "Iteration 8497 Loss: 0.07433855032763427\n",
      "Iteration 8498 Loss: 0.07433840943048652\n",
      "Iteration 8499 Loss: 0.07433826854143749\n",
      "Iteration 8500 Loss: 0.07433812766048667\n",
      "Iteration 8501 Loss: 0.0743379867876337\n",
      "Iteration 8502 Loss: 0.07433784592287797\n",
      "Iteration 8503 Loss: 0.07433770506621902\n",
      "Iteration 8504 Loss: 0.07433756421765639\n",
      "Iteration 8505 Loss: 0.07433742337718958\n",
      "Iteration 8506 Loss: 0.07433728254481813\n",
      "Iteration 8507 Loss: 0.07433714172054154\n",
      "Iteration 8508 Loss: 0.07433700090435932\n",
      "Iteration 8509 Loss: 0.07433686009627101\n",
      "Iteration 8510 Loss: 0.07433671929627612\n",
      "Iteration 8511 Loss: 0.07433657850437411\n",
      "Iteration 8512 Loss: 0.07433643772056459\n",
      "Iteration 8513 Loss: 0.07433629694484703\n",
      "Iteration 8514 Loss: 0.07433615617722093\n",
      "Iteration 8515 Loss: 0.07433601541768588\n",
      "Iteration 8516 Loss: 0.07433587466624124\n",
      "Iteration 8517 Loss: 0.07433573392288671\n",
      "Iteration 8518 Loss: 0.07433559318762174\n",
      "Iteration 8519 Loss: 0.07433545246044583\n",
      "Iteration 8520 Loss: 0.07433531174135843\n",
      "Iteration 8521 Loss: 0.0743351710303592\n",
      "Iteration 8522 Loss: 0.0743350303274476\n",
      "Iteration 8523 Loss: 0.07433488963262311\n",
      "Iteration 8524 Loss: 0.07433474894588532\n",
      "Iteration 8525 Loss: 0.0743346082672337\n",
      "Iteration 8526 Loss: 0.0743344675966677\n",
      "Iteration 8527 Loss: 0.07433432693418704\n",
      "Iteration 8528 Loss: 0.074334186279791\n",
      "Iteration 8529 Loss: 0.07433404563347923\n",
      "Iteration 8530 Loss: 0.07433390499525125\n",
      "Iteration 8531 Loss: 0.07433376436510652\n",
      "Iteration 8532 Loss: 0.07433362374304467\n",
      "Iteration 8533 Loss: 0.07433348312906503\n",
      "Iteration 8534 Loss: 0.07433334252316728\n",
      "Iteration 8535 Loss: 0.07433320192535091\n",
      "Iteration 8536 Loss: 0.07433306133561546\n",
      "Iteration 8537 Loss: 0.07433292075396036\n",
      "Iteration 8538 Loss: 0.07433278018038518\n",
      "Iteration 8539 Loss: 0.07433263961488942\n",
      "Iteration 8540 Loss: 0.07433249905747266\n",
      "Iteration 8541 Loss: 0.07433235850813427\n",
      "Iteration 8542 Loss: 0.07433221796687388\n",
      "Iteration 8543 Loss: 0.07433207743369112\n",
      "Iteration 8544 Loss: 0.07433193690858533\n",
      "Iteration 8545 Loss: 0.07433179639155611\n",
      "Iteration 8546 Loss: 0.0743316558826029\n",
      "Iteration 8547 Loss: 0.07433151538172533\n",
      "Iteration 8548 Loss: 0.07433137488892289\n",
      "Iteration 8549 Loss: 0.07433123440419499\n",
      "Iteration 8550 Loss: 0.07433109392754131\n",
      "Iteration 8551 Loss: 0.07433095345896133\n",
      "Iteration 8552 Loss: 0.07433081299845448\n",
      "Iteration 8553 Loss: 0.07433067254602035\n",
      "Iteration 8554 Loss: 0.07433053210165845\n",
      "Iteration 8555 Loss: 0.07433039166536828\n",
      "Iteration 8556 Loss: 0.0743302512371494\n",
      "Iteration 8557 Loss: 0.07433011081700133\n",
      "Iteration 8558 Loss: 0.07432997040492353\n",
      "Iteration 8559 Loss: 0.07432983000091557\n",
      "Iteration 8560 Loss: 0.07432968960497699\n",
      "Iteration 8561 Loss: 0.07432954921710724\n",
      "Iteration 8562 Loss: 0.0743294088373059\n",
      "Iteration 8563 Loss: 0.07432926846557245\n",
      "Iteration 8564 Loss: 0.07432912810190646\n",
      "Iteration 8565 Loss: 0.0743289877463074\n",
      "Iteration 8566 Loss: 0.0743288473987748\n",
      "Iteration 8567 Loss: 0.07432870705930819\n",
      "Iteration 8568 Loss: 0.07432856672790718\n",
      "Iteration 8569 Loss: 0.07432842640457116\n",
      "Iteration 8570 Loss: 0.07432828608929962\n",
      "Iteration 8571 Loss: 0.07432814578209225\n",
      "Iteration 8572 Loss: 0.07432800548294846\n",
      "Iteration 8573 Loss: 0.07432786519186775\n",
      "Iteration 8574 Loss: 0.07432772490884973\n",
      "Iteration 8575 Loss: 0.07432758463389386\n",
      "Iteration 8576 Loss: 0.07432744436699963\n",
      "Iteration 8577 Loss: 0.07432730410816664\n",
      "Iteration 8578 Loss: 0.0743271638573944\n",
      "Iteration 8579 Loss: 0.0743270236146824\n",
      "Iteration 8580 Loss: 0.07432688338003013\n",
      "Iteration 8581 Loss: 0.07432674315343721\n",
      "Iteration 8582 Loss: 0.07432660293490308\n",
      "Iteration 8583 Loss: 0.07432646272442725\n",
      "Iteration 8584 Loss: 0.07432632252200934\n",
      "Iteration 8585 Loss: 0.07432618232764879\n",
      "Iteration 8586 Loss: 0.07432604214134511\n",
      "Iteration 8587 Loss: 0.07432590196309788\n",
      "Iteration 8588 Loss: 0.07432576179290662\n",
      "Iteration 8589 Loss: 0.07432562163077075\n",
      "Iteration 8590 Loss: 0.07432548147668994\n",
      "Iteration 8591 Loss: 0.07432534133066362\n",
      "Iteration 8592 Loss: 0.07432520119269134\n",
      "Iteration 8593 Loss: 0.07432506106277259\n",
      "Iteration 8594 Loss: 0.074324920940907\n",
      "Iteration 8595 Loss: 0.0743247808270939\n",
      "Iteration 8596 Loss: 0.07432464072133302\n",
      "Iteration 8597 Loss: 0.07432450062362372\n",
      "Iteration 8598 Loss: 0.07432436053396564\n",
      "Iteration 8599 Loss: 0.0743242204523582\n",
      "Iteration 8600 Loss: 0.074324080378801\n",
      "Iteration 8601 Loss: 0.07432394031329356\n",
      "Iteration 8602 Loss: 0.0743238002558354\n",
      "Iteration 8603 Loss: 0.074323660206426\n",
      "Iteration 8604 Loss: 0.07432352016506491\n",
      "Iteration 8605 Loss: 0.07432338013175169\n",
      "Iteration 8606 Loss: 0.07432324010648575\n",
      "Iteration 8607 Loss: 0.07432310008926672\n",
      "Iteration 8608 Loss: 0.07432296008009412\n",
      "Iteration 8609 Loss: 0.07432282007896743\n",
      "Iteration 8610 Loss: 0.0743226800858862\n",
      "Iteration 8611 Loss: 0.0743225401008499\n",
      "Iteration 8612 Loss: 0.07432240012385813\n",
      "Iteration 8613 Loss: 0.07432226015491039\n",
      "Iteration 8614 Loss: 0.07432212019400616\n",
      "Iteration 8615 Loss: 0.07432198024114503\n",
      "Iteration 8616 Loss: 0.07432184029632649\n",
      "Iteration 8617 Loss: 0.07432170035954998\n",
      "Iteration 8618 Loss: 0.07432156043081521\n",
      "Iteration 8619 Loss: 0.07432142051012157\n",
      "Iteration 8620 Loss: 0.07432128059746863\n",
      "Iteration 8621 Loss: 0.07432114069285592\n",
      "Iteration 8622 Loss: 0.07432100079628293\n",
      "Iteration 8623 Loss: 0.07432086090774914\n",
      "Iteration 8624 Loss: 0.07432072102725418\n",
      "Iteration 8625 Loss: 0.07432058115479756\n",
      "Iteration 8626 Loss: 0.07432044129037879\n",
      "Iteration 8627 Loss: 0.07432030143399733\n",
      "Iteration 8628 Loss: 0.07432016158565273\n",
      "Iteration 8629 Loss: 0.07432002174534455\n",
      "Iteration 8630 Loss: 0.07431988191307237\n",
      "Iteration 8631 Loss: 0.07431974208883559\n",
      "Iteration 8632 Loss: 0.07431960227263379\n",
      "Iteration 8633 Loss: 0.0743194624644665\n",
      "Iteration 8634 Loss: 0.07431932266433323\n",
      "Iteration 8635 Loss: 0.07431918287223356\n",
      "Iteration 8636 Loss: 0.07431904308816695\n",
      "Iteration 8637 Loss: 0.07431890331213296\n",
      "Iteration 8638 Loss: 0.07431876354413107\n",
      "Iteration 8639 Loss: 0.07431862378416083\n",
      "Iteration 8640 Loss: 0.07431848403222181\n",
      "Iteration 8641 Loss: 0.07431834428831352\n",
      "Iteration 8642 Loss: 0.07431820455243535\n",
      "Iteration 8643 Loss: 0.07431806482458703\n",
      "Iteration 8644 Loss: 0.074317925104768\n",
      "Iteration 8645 Loss: 0.07431778539297773\n",
      "Iteration 8646 Loss: 0.07431764568921587\n",
      "Iteration 8647 Loss: 0.07431750599348182\n",
      "Iteration 8648 Loss: 0.07431736630577515\n",
      "Iteration 8649 Loss: 0.07431722662609541\n",
      "Iteration 8650 Loss: 0.07431708695444213\n",
      "Iteration 8651 Loss: 0.07431694729081478\n",
      "Iteration 8652 Loss: 0.07431680763521292\n",
      "Iteration 8653 Loss: 0.07431666798763607\n",
      "Iteration 8654 Loss: 0.0743165283480838\n",
      "Iteration 8655 Loss: 0.07431638871655556\n",
      "Iteration 8656 Loss: 0.07431624909305093\n",
      "Iteration 8657 Loss: 0.07431610947756945\n",
      "Iteration 8658 Loss: 0.07431596987011055\n",
      "Iteration 8659 Loss: 0.07431583027067387\n",
      "Iteration 8660 Loss: 0.07431569067925889\n",
      "Iteration 8661 Loss: 0.07431555109586516\n",
      "Iteration 8662 Loss: 0.07431541152049215\n",
      "Iteration 8663 Loss: 0.07431527195313944\n",
      "Iteration 8664 Loss: 0.07431513239380654\n",
      "Iteration 8665 Loss: 0.07431499284249292\n",
      "Iteration 8666 Loss: 0.07431485329919826\n",
      "Iteration 8667 Loss: 0.07431471376392187\n",
      "Iteration 8668 Loss: 0.07431457423666345\n",
      "Iteration 8669 Loss: 0.07431443471742252\n",
      "Iteration 8670 Loss: 0.0743142952061985\n",
      "Iteration 8671 Loss: 0.074314155702991\n",
      "Iteration 8672 Loss: 0.07431401620779951\n",
      "Iteration 8673 Loss: 0.07431387672062362\n",
      "Iteration 8674 Loss: 0.07431373724146269\n",
      "Iteration 8675 Loss: 0.07431359777031639\n",
      "Iteration 8676 Loss: 0.07431345830718433\n",
      "Iteration 8677 Loss: 0.07431331885206585\n",
      "Iteration 8678 Loss: 0.07431317940496057\n",
      "Iteration 8679 Loss: 0.07431303996586802\n",
      "Iteration 8680 Loss: 0.0743129005347877\n",
      "Iteration 8681 Loss: 0.07431276111171917\n",
      "Iteration 8682 Loss: 0.07431262169666189\n",
      "Iteration 8683 Loss: 0.07431248228961544\n",
      "Iteration 8684 Loss: 0.07431234289057938\n",
      "Iteration 8685 Loss: 0.07431220349955314\n",
      "Iteration 8686 Loss: 0.07431206411653639\n",
      "Iteration 8687 Loss: 0.0743119247415285\n",
      "Iteration 8688 Loss: 0.07431178537452912\n",
      "Iteration 8689 Loss: 0.07431164601553772\n",
      "Iteration 8690 Loss: 0.07431150666455386\n",
      "Iteration 8691 Loss: 0.07431136732157706\n",
      "Iteration 8692 Loss: 0.0743112279866068\n",
      "Iteration 8693 Loss: 0.07431108865964266\n",
      "Iteration 8694 Loss: 0.07431094934068412\n",
      "Iteration 8695 Loss: 0.07431081002973075\n",
      "Iteration 8696 Loss: 0.07431067072678213\n",
      "Iteration 8697 Loss: 0.07431053143183768\n",
      "Iteration 8698 Loss: 0.07431039214489699\n",
      "Iteration 8699 Loss: 0.07431025286595959\n",
      "Iteration 8700 Loss: 0.07431011359502496\n",
      "Iteration 8701 Loss: 0.07430997433209267\n",
      "Iteration 8702 Loss: 0.07430983507716224\n",
      "Iteration 8703 Loss: 0.07430969583023324\n",
      "Iteration 8704 Loss: 0.07430955659130511\n",
      "Iteration 8705 Loss: 0.07430941736037744\n",
      "Iteration 8706 Loss: 0.07430927813744978\n",
      "Iteration 8707 Loss: 0.07430913892252163\n",
      "Iteration 8708 Loss: 0.07430899971559249\n",
      "Iteration 8709 Loss: 0.0743088605166619\n",
      "Iteration 8710 Loss: 0.07430872132572941\n",
      "Iteration 8711 Loss: 0.07430858214279457\n",
      "Iteration 8712 Loss: 0.07430844296785684\n",
      "Iteration 8713 Loss: 0.07430830380091587\n",
      "Iteration 8714 Loss: 0.07430816464197106\n",
      "Iteration 8715 Loss: 0.074308025491022\n",
      "Iteration 8716 Loss: 0.07430788634806822\n",
      "Iteration 8717 Loss: 0.07430774721310922\n",
      "Iteration 8718 Loss: 0.07430760808614453\n",
      "Iteration 8719 Loss: 0.07430746896717375\n",
      "Iteration 8720 Loss: 0.07430732985619634\n",
      "Iteration 8721 Loss: 0.07430719075321188\n",
      "Iteration 8722 Loss: 0.07430705165821982\n",
      "Iteration 8723 Loss: 0.07430691257121971\n",
      "Iteration 8724 Loss: 0.07430677349221117\n",
      "Iteration 8725 Loss: 0.07430663442119367\n",
      "Iteration 8726 Loss: 0.07430649535816673\n",
      "Iteration 8727 Loss: 0.07430635630312987\n",
      "Iteration 8728 Loss: 0.07430621725608268\n",
      "Iteration 8729 Loss: 0.07430607821702465\n",
      "Iteration 8730 Loss: 0.07430593918595522\n",
      "Iteration 8731 Loss: 0.07430580016287411\n",
      "Iteration 8732 Loss: 0.07430566114778067\n",
      "Iteration 8733 Loss: 0.07430552214067458\n",
      "Iteration 8734 Loss: 0.07430538314155526\n",
      "Iteration 8735 Loss: 0.0743052441504223\n",
      "Iteration 8736 Loss: 0.0743051051672752\n",
      "Iteration 8737 Loss: 0.07430496619211356\n",
      "Iteration 8738 Loss: 0.07430482722493675\n",
      "Iteration 8739 Loss: 0.0743046882657445\n",
      "Iteration 8740 Loss: 0.07430454931453619\n",
      "Iteration 8741 Loss: 0.07430441037131144\n",
      "Iteration 8742 Loss: 0.07430427143606971\n",
      "Iteration 8743 Loss: 0.0743041325088106\n",
      "Iteration 8744 Loss: 0.0743039935895336\n",
      "Iteration 8745 Loss: 0.07430385467823826\n",
      "Iteration 8746 Loss: 0.074303715774924\n",
      "Iteration 8747 Loss: 0.07430357687959063\n",
      "Iteration 8748 Loss: 0.07430343799223743\n",
      "Iteration 8749 Loss: 0.07430329911286397\n",
      "Iteration 8750 Loss: 0.07430316024146985\n",
      "Iteration 8751 Loss: 0.07430302137805456\n",
      "Iteration 8752 Loss: 0.07430288252261762\n",
      "Iteration 8753 Loss: 0.07430274367515859\n",
      "Iteration 8754 Loss: 0.07430260483567701\n",
      "Iteration 8755 Loss: 0.07430246600417234\n",
      "Iteration 8756 Loss: 0.07430232718064421\n",
      "Iteration 8757 Loss: 0.07430218836509214\n",
      "Iteration 8758 Loss: 0.0743020495575156\n",
      "Iteration 8759 Loss: 0.07430191075791417\n",
      "Iteration 8760 Loss: 0.07430177196628732\n",
      "Iteration 8761 Loss: 0.07430163318263461\n",
      "Iteration 8762 Loss: 0.07430149440695565\n",
      "Iteration 8763 Loss: 0.07430135563924982\n",
      "Iteration 8764 Loss: 0.07430121687951681\n",
      "Iteration 8765 Loss: 0.07430107812775605\n",
      "Iteration 8766 Loss: 0.07430093938396712\n",
      "Iteration 8767 Loss: 0.07430080064814955\n",
      "Iteration 8768 Loss: 0.07430066192030282\n",
      "Iteration 8769 Loss: 0.07430052320042653\n",
      "Iteration 8770 Loss: 0.07430038448852014\n",
      "Iteration 8771 Loss: 0.0743002457845833\n",
      "Iteration 8772 Loss: 0.07430010708861541\n",
      "Iteration 8773 Loss: 0.07429996840061608\n",
      "Iteration 8774 Loss: 0.07429982972058483\n",
      "Iteration 8775 Loss: 0.0742996910485212\n",
      "Iteration 8776 Loss: 0.07429955238442468\n",
      "Iteration 8777 Loss: 0.07429941372829482\n",
      "Iteration 8778 Loss: 0.07429927508013116\n",
      "Iteration 8779 Loss: 0.07429913643993327\n",
      "Iteration 8780 Loss: 0.07429899780770063\n",
      "Iteration 8781 Loss: 0.07429885918343282\n",
      "Iteration 8782 Loss: 0.0742987205671293\n",
      "Iteration 8783 Loss: 0.07429858195878963\n",
      "Iteration 8784 Loss: 0.07429844335841342\n",
      "Iteration 8785 Loss: 0.07429830476600015\n",
      "Iteration 8786 Loss: 0.0742981661815493\n",
      "Iteration 8787 Loss: 0.07429802760506045\n",
      "Iteration 8788 Loss: 0.07429788903653314\n",
      "Iteration 8789 Loss: 0.07429775047596696\n",
      "Iteration 8790 Loss: 0.0742976119233613\n",
      "Iteration 8791 Loss: 0.07429747337871581\n",
      "Iteration 8792 Loss: 0.07429733484202998\n",
      "Iteration 8793 Loss: 0.07429719631330334\n",
      "Iteration 8794 Loss: 0.07429705779253544\n",
      "Iteration 8795 Loss: 0.0742969192797258\n",
      "Iteration 8796 Loss: 0.07429678077487398\n",
      "Iteration 8797 Loss: 0.07429664227797947\n",
      "Iteration 8798 Loss: 0.07429650378904187\n",
      "Iteration 8799 Loss: 0.07429636530806058\n",
      "Iteration 8800 Loss: 0.07429622683503531\n",
      "Iteration 8801 Loss: 0.07429608836996551\n",
      "Iteration 8802 Loss: 0.07429594991285071\n",
      "Iteration 8803 Loss: 0.07429581146369035\n",
      "Iteration 8804 Loss: 0.07429567302248416\n",
      "Iteration 8805 Loss: 0.07429553458923156\n",
      "Iteration 8806 Loss: 0.07429539616393208\n",
      "Iteration 8807 Loss: 0.07429525774658527\n",
      "Iteration 8808 Loss: 0.07429511933719075\n",
      "Iteration 8809 Loss: 0.07429498093574784\n",
      "Iteration 8810 Loss: 0.07429484254225627\n",
      "Iteration 8811 Loss: 0.07429470415671553\n",
      "Iteration 8812 Loss: 0.07429456577912508\n",
      "Iteration 8813 Loss: 0.07429442740948458\n",
      "Iteration 8814 Loss: 0.07429428904779344\n",
      "Iteration 8815 Loss: 0.07429415069405125\n",
      "Iteration 8816 Loss: 0.07429401234825753\n",
      "Iteration 8817 Loss: 0.07429387401041185\n",
      "Iteration 8818 Loss: 0.07429373568051376\n",
      "Iteration 8819 Loss: 0.07429359735856268\n",
      "Iteration 8820 Loss: 0.07429345904455831\n",
      "Iteration 8821 Loss: 0.07429332073850003\n",
      "Iteration 8822 Loss: 0.07429318244038748\n",
      "Iteration 8823 Loss: 0.07429304415022009\n",
      "Iteration 8824 Loss: 0.07429290586799756\n",
      "Iteration 8825 Loss: 0.07429276759371922\n",
      "Iteration 8826 Loss: 0.07429262932738476\n",
      "Iteration 8827 Loss: 0.07429249106899366\n",
      "Iteration 8828 Loss: 0.07429235281854547\n",
      "Iteration 8829 Loss: 0.0742922145760397\n",
      "Iteration 8830 Loss: 0.07429207634147592\n",
      "Iteration 8831 Loss: 0.07429193811485366\n",
      "Iteration 8832 Loss: 0.07429179989617243\n",
      "Iteration 8833 Loss: 0.07429166168543178\n",
      "Iteration 8834 Loss: 0.07429152348263121\n",
      "Iteration 8835 Loss: 0.07429138528777031\n",
      "Iteration 8836 Loss: 0.0742912471008486\n",
      "Iteration 8837 Loss: 0.07429110892186555\n",
      "Iteration 8838 Loss: 0.07429097075082086\n",
      "Iteration 8839 Loss: 0.07429083258771385\n",
      "Iteration 8840 Loss: 0.07429069443254424\n",
      "Iteration 8841 Loss: 0.07429055628531148\n",
      "Iteration 8842 Loss: 0.07429041814601514\n",
      "Iteration 8843 Loss: 0.0742902800146547\n",
      "Iteration 8844 Loss: 0.0742901418912297\n",
      "Iteration 8845 Loss: 0.07429000377573976\n",
      "Iteration 8846 Loss: 0.07428986566818432\n",
      "Iteration 8847 Loss: 0.07428972756856296\n",
      "Iteration 8848 Loss: 0.07428958947687525\n",
      "Iteration 8849 Loss: 0.07428945139312067\n",
      "Iteration 8850 Loss: 0.07428931331729878\n",
      "Iteration 8851 Loss: 0.07428917524940908\n",
      "Iteration 8852 Loss: 0.07428903718945117\n",
      "Iteration 8853 Loss: 0.07428889913742456\n",
      "Iteration 8854 Loss: 0.07428876109332874\n",
      "Iteration 8855 Loss: 0.07428862305716329\n",
      "Iteration 8856 Loss: 0.0742884850289278\n",
      "Iteration 8857 Loss: 0.0742883470086217\n",
      "Iteration 8858 Loss: 0.0742882089962446\n",
      "Iteration 8859 Loss: 0.07428807099179598\n",
      "Iteration 8860 Loss: 0.07428793299527539\n",
      "Iteration 8861 Loss: 0.07428779500668248\n",
      "Iteration 8862 Loss: 0.07428765702601661\n",
      "Iteration 8863 Loss: 0.07428751905327745\n",
      "Iteration 8864 Loss: 0.07428738108846449\n",
      "Iteration 8865 Loss: 0.0742872431315772\n",
      "Iteration 8866 Loss: 0.07428710518261523\n",
      "Iteration 8867 Loss: 0.07428696724157804\n",
      "Iteration 8868 Loss: 0.0742868293084652\n",
      "Iteration 8869 Loss: 0.07428669138327622\n",
      "Iteration 8870 Loss: 0.07428655346601072\n",
      "Iteration 8871 Loss: 0.07428641555666811\n",
      "Iteration 8872 Loss: 0.07428627765524802\n",
      "Iteration 8873 Loss: 0.07428613976174998\n",
      "Iteration 8874 Loss: 0.07428600187617344\n",
      "Iteration 8875 Loss: 0.07428586399851808\n",
      "Iteration 8876 Loss: 0.0742857261287833\n",
      "Iteration 8877 Loss: 0.07428558826696874\n",
      "Iteration 8878 Loss: 0.07428545041307388\n",
      "Iteration 8879 Loss: 0.07428531256709828\n",
      "Iteration 8880 Loss: 0.07428517472904146\n",
      "Iteration 8881 Loss: 0.07428503689890298\n",
      "Iteration 8882 Loss: 0.07428489907668234\n",
      "Iteration 8883 Loss: 0.07428476126237915\n",
      "Iteration 8884 Loss: 0.07428462345599283\n",
      "Iteration 8885 Loss: 0.07428448565752305\n",
      "Iteration 8886 Loss: 0.07428434786696926\n",
      "Iteration 8887 Loss: 0.07428421008433102\n",
      "Iteration 8888 Loss: 0.07428407230960789\n",
      "Iteration 8889 Loss: 0.07428393454279936\n",
      "Iteration 8890 Loss: 0.07428379678390504\n",
      "Iteration 8891 Loss: 0.07428365903292439\n",
      "Iteration 8892 Loss: 0.07428352128985699\n",
      "Iteration 8893 Loss: 0.07428338355470239\n",
      "Iteration 8894 Loss: 0.07428324582746008\n",
      "Iteration 8895 Loss: 0.07428310810812969\n",
      "Iteration 8896 Loss: 0.07428297039671065\n",
      "Iteration 8897 Loss: 0.07428283269320252\n",
      "Iteration 8898 Loss: 0.07428269499760487\n",
      "Iteration 8899 Loss: 0.07428255730991729\n",
      "Iteration 8900 Loss: 0.07428241963013918\n",
      "Iteration 8901 Loss: 0.07428228195827022\n",
      "Iteration 8902 Loss: 0.07428214429430986\n",
      "Iteration 8903 Loss: 0.07428200663825767\n",
      "Iteration 8904 Loss: 0.07428186899011316\n",
      "Iteration 8905 Loss: 0.07428173134987595\n",
      "Iteration 8906 Loss: 0.07428159371754547\n",
      "Iteration 8907 Loss: 0.07428145609312133\n",
      "Iteration 8908 Loss: 0.07428131847660298\n",
      "Iteration 8909 Loss: 0.07428118086799011\n",
      "Iteration 8910 Loss: 0.0742810432672822\n",
      "Iteration 8911 Loss: 0.07428090567447872\n",
      "Iteration 8912 Loss: 0.07428076808957922\n",
      "Iteration 8913 Loss: 0.07428063051258331\n",
      "Iteration 8914 Loss: 0.07428049294349051\n",
      "Iteration 8915 Loss: 0.0742803553823003\n",
      "Iteration 8916 Loss: 0.07428021782901228\n",
      "Iteration 8917 Loss: 0.07428008028362597\n",
      "Iteration 8918 Loss: 0.07427994274614087\n",
      "Iteration 8919 Loss: 0.07427980521655661\n",
      "Iteration 8920 Loss: 0.07427966769487263\n",
      "Iteration 8921 Loss: 0.0742795301810886\n",
      "Iteration 8922 Loss: 0.07427939267520386\n",
      "Iteration 8923 Loss: 0.07427925517721812\n",
      "Iteration 8924 Loss: 0.07427911768713087\n",
      "Iteration 8925 Loss: 0.0742789802049416\n",
      "Iteration 8926 Loss: 0.0742788427306499\n",
      "Iteration 8927 Loss: 0.07427870526425535\n",
      "Iteration 8928 Loss: 0.07427856780575745\n",
      "Iteration 8929 Loss: 0.0742784303551557\n",
      "Iteration 8930 Loss: 0.07427829291244967\n",
      "Iteration 8931 Loss: 0.07427815547763887\n",
      "Iteration 8932 Loss: 0.0742780180507229\n",
      "Iteration 8933 Loss: 0.07427788063170128\n",
      "Iteration 8934 Loss: 0.07427774322057354\n",
      "Iteration 8935 Loss: 0.07427760581733917\n",
      "Iteration 8936 Loss: 0.07427746842199781\n",
      "Iteration 8937 Loss: 0.07427733103454898\n",
      "Iteration 8938 Loss: 0.07427719365499212\n",
      "Iteration 8939 Loss: 0.07427705628332688\n",
      "Iteration 8940 Loss: 0.07427691891955275\n",
      "Iteration 8941 Loss: 0.07427678156366924\n",
      "Iteration 8942 Loss: 0.07427664421567595\n",
      "Iteration 8943 Loss: 0.07427650687557245\n",
      "Iteration 8944 Loss: 0.07427636954335817\n",
      "Iteration 8945 Loss: 0.07427623221903276\n",
      "Iteration 8946 Loss: 0.07427609490259568\n",
      "Iteration 8947 Loss: 0.07427595759404652\n",
      "Iteration 8948 Loss: 0.07427582029338477\n",
      "Iteration 8949 Loss: 0.07427568300061005\n",
      "Iteration 8950 Loss: 0.07427554571572179\n",
      "Iteration 8951 Loss: 0.07427540843871967\n",
      "Iteration 8952 Loss: 0.07427527116960311\n",
      "Iteration 8953 Loss: 0.07427513390837166\n",
      "Iteration 8954 Loss: 0.07427499665502493\n",
      "Iteration 8955 Loss: 0.07427485940956244\n",
      "Iteration 8956 Loss: 0.07427472217198373\n",
      "Iteration 8957 Loss: 0.07427458494228827\n",
      "Iteration 8958 Loss: 0.07427444772047571\n",
      "Iteration 8959 Loss: 0.07427431050654554\n",
      "Iteration 8960 Loss: 0.07427417330049726\n",
      "Iteration 8961 Loss: 0.07427403610233048\n",
      "Iteration 8962 Loss: 0.07427389891204468\n",
      "Iteration 8963 Loss: 0.07427376172963943\n",
      "Iteration 8964 Loss: 0.0742736245551143\n",
      "Iteration 8965 Loss: 0.07427348738846883\n",
      "Iteration 8966 Loss: 0.07427335022970248\n",
      "Iteration 8967 Loss: 0.07427321307881488\n",
      "Iteration 8968 Loss: 0.07427307593580554\n",
      "Iteration 8969 Loss: 0.07427293880067401\n",
      "Iteration 8970 Loss: 0.07427280167341979\n",
      "Iteration 8971 Loss: 0.07427266455404247\n",
      "Iteration 8972 Loss: 0.07427252744254158\n",
      "Iteration 8973 Loss: 0.07427239033891668\n",
      "Iteration 8974 Loss: 0.07427225324316725\n",
      "Iteration 8975 Loss: 0.07427211615529286\n",
      "Iteration 8976 Loss: 0.07427197907529308\n",
      "Iteration 8977 Loss: 0.07427184200316742\n",
      "Iteration 8978 Loss: 0.07427170493891545\n",
      "Iteration 8979 Loss: 0.07427156788253668\n",
      "Iteration 8980 Loss: 0.07427143083403069\n",
      "Iteration 8981 Loss: 0.07427129379339696\n",
      "Iteration 8982 Loss: 0.07427115676063507\n",
      "Iteration 8983 Loss: 0.07427101973574464\n",
      "Iteration 8984 Loss: 0.07427088271872505\n",
      "Iteration 8985 Loss: 0.07427074570957595\n",
      "Iteration 8986 Loss: 0.07427060870829685\n",
      "Iteration 8987 Loss: 0.07427047171488732\n",
      "Iteration 8988 Loss: 0.07427033472934688\n",
      "Iteration 8989 Loss: 0.07427019775167505\n",
      "Iteration 8990 Loss: 0.0742700607818714\n",
      "Iteration 8991 Loss: 0.0742699238199355\n",
      "Iteration 8992 Loss: 0.07426978686586685\n",
      "Iteration 8993 Loss: 0.074269649919665\n",
      "Iteration 8994 Loss: 0.07426951298132951\n",
      "Iteration 8995 Loss: 0.0742693760508599\n",
      "Iteration 8996 Loss: 0.07426923912825567\n",
      "Iteration 8997 Loss: 0.0742691022135165\n",
      "Iteration 8998 Loss: 0.0742689653066418\n",
      "Iteration 8999 Loss: 0.07426882840763113\n",
      "Iteration 9000 Loss: 0.07426869151648408\n",
      "Iteration 9001 Loss: 0.07426855463320019\n",
      "Iteration 9002 Loss: 0.07426841775777898\n",
      "Iteration 9003 Loss: 0.07426828089022\n",
      "Iteration 9004 Loss: 0.07426814403052277\n",
      "Iteration 9005 Loss: 0.07426800717868683\n",
      "Iteration 9006 Loss: 0.07426787033471183\n",
      "Iteration 9007 Loss: 0.0742677334985972\n",
      "Iteration 9008 Loss: 0.07426759667034247\n",
      "Iteration 9009 Loss: 0.07426745984994723\n",
      "Iteration 9010 Loss: 0.07426732303741107\n",
      "Iteration 9011 Loss: 0.07426718623273339\n",
      "Iteration 9012 Loss: 0.0742670494359139\n",
      "Iteration 9013 Loss: 0.0742669126469521\n",
      "Iteration 9014 Loss: 0.0742667758658474\n",
      "Iteration 9015 Loss: 0.07426663909259944\n",
      "Iteration 9016 Loss: 0.07426650232720781\n",
      "Iteration 9017 Loss: 0.07426636556967207\n",
      "Iteration 9018 Loss: 0.07426622881999163\n",
      "Iteration 9019 Loss: 0.07426609207816612\n",
      "Iteration 9020 Loss: 0.07426595534419508\n",
      "Iteration 9021 Loss: 0.07426581861807802\n",
      "Iteration 9022 Loss: 0.0742656818998145\n",
      "Iteration 9023 Loss: 0.07426554518940412\n",
      "Iteration 9024 Loss: 0.07426540848684632\n",
      "Iteration 9025 Loss: 0.07426527179214071\n",
      "Iteration 9026 Loss: 0.07426513510528684\n",
      "Iteration 9027 Loss: 0.07426499842628421\n",
      "Iteration 9028 Loss: 0.0742648617551324\n",
      "Iteration 9029 Loss: 0.07426472509183095\n",
      "Iteration 9030 Loss: 0.07426458843637937\n",
      "Iteration 9031 Loss: 0.07426445178877723\n",
      "Iteration 9032 Loss: 0.07426431514902405\n",
      "Iteration 9033 Loss: 0.07426417851711944\n",
      "Iteration 9034 Loss: 0.0742640418930629\n",
      "Iteration 9035 Loss: 0.07426390527685393\n",
      "Iteration 9036 Loss: 0.0742637686684922\n",
      "Iteration 9037 Loss: 0.07426363206797712\n",
      "Iteration 9038 Loss: 0.07426349547530828\n",
      "Iteration 9039 Loss: 0.07426335889048521\n",
      "Iteration 9040 Loss: 0.07426322231350753\n",
      "Iteration 9041 Loss: 0.07426308574437468\n",
      "Iteration 9042 Loss: 0.07426294918308626\n",
      "Iteration 9043 Loss: 0.07426281262964182\n",
      "Iteration 9044 Loss: 0.0742626760840409\n",
      "Iteration 9045 Loss: 0.07426253954628301\n",
      "Iteration 9046 Loss: 0.0742624030163677\n",
      "Iteration 9047 Loss: 0.07426226649429456\n",
      "Iteration 9048 Loss: 0.07426212998006312\n",
      "Iteration 9049 Loss: 0.0742619934736729\n",
      "Iteration 9050 Loss: 0.07426185697512351\n",
      "Iteration 9051 Loss: 0.07426172048441439\n",
      "Iteration 9052 Loss: 0.07426158400154512\n",
      "Iteration 9053 Loss: 0.0742614475265153\n",
      "Iteration 9054 Loss: 0.07426131105932436\n",
      "Iteration 9055 Loss: 0.074261174599972\n",
      "Iteration 9056 Loss: 0.07426103814845768\n",
      "Iteration 9057 Loss: 0.07426090170478095\n",
      "Iteration 9058 Loss: 0.07426076526894133\n",
      "Iteration 9059 Loss: 0.07426062884093836\n",
      "Iteration 9060 Loss: 0.07426049242077164\n",
      "Iteration 9061 Loss: 0.07426035600844072\n",
      "Iteration 9062 Loss: 0.07426021960394504\n",
      "Iteration 9063 Loss: 0.0742600832072843\n",
      "Iteration 9064 Loss: 0.0742599468184579\n",
      "Iteration 9065 Loss: 0.07425981043746546\n",
      "Iteration 9066 Loss: 0.07425967406430659\n",
      "Iteration 9067 Loss: 0.07425953769898067\n",
      "Iteration 9068 Loss: 0.07425940134148737\n",
      "Iteration 9069 Loss: 0.07425926499182622\n",
      "Iteration 9070 Loss: 0.0742591286499967\n",
      "Iteration 9071 Loss: 0.07425899231599842\n",
      "Iteration 9072 Loss: 0.07425885598983091\n",
      "Iteration 9073 Loss: 0.0742587196714937\n",
      "Iteration 9074 Loss: 0.07425858336098637\n",
      "Iteration 9075 Loss: 0.07425844705830845\n",
      "Iteration 9076 Loss: 0.07425831076345943\n",
      "Iteration 9077 Loss: 0.0742581744764389\n",
      "Iteration 9078 Loss: 0.07425803819724641\n",
      "Iteration 9079 Loss: 0.0742579019258815\n",
      "Iteration 9080 Loss: 0.0742577656623438\n",
      "Iteration 9081 Loss: 0.07425762940663269\n",
      "Iteration 9082 Loss: 0.07425749315874786\n",
      "Iteration 9083 Loss: 0.07425735691868873\n",
      "Iteration 9084 Loss: 0.07425722068645493\n",
      "Iteration 9085 Loss: 0.07425708446204606\n",
      "Iteration 9086 Loss: 0.0742569482454615\n",
      "Iteration 9087 Loss: 0.07425681203670093\n",
      "Iteration 9088 Loss: 0.07425667583576388\n",
      "Iteration 9089 Loss: 0.07425653964264985\n",
      "Iteration 9090 Loss: 0.07425640345735839\n",
      "Iteration 9091 Loss: 0.0742562672798891\n",
      "Iteration 9092 Loss: 0.07425613111024143\n",
      "Iteration 9093 Loss: 0.07425599494841503\n",
      "Iteration 9094 Loss: 0.07425585879440941\n",
      "Iteration 9095 Loss: 0.0742557226482241\n",
      "Iteration 9096 Loss: 0.07425558650985861\n",
      "Iteration 9097 Loss: 0.07425545037931258\n",
      "Iteration 9098 Loss: 0.07425531425658552\n",
      "Iteration 9099 Loss: 0.07425517814167688\n",
      "Iteration 9100 Loss: 0.0742550420345864\n",
      "Iteration 9101 Loss: 0.07425490593531343\n",
      "Iteration 9102 Loss: 0.07425476984385762\n",
      "Iteration 9103 Loss: 0.07425463376021854\n",
      "Iteration 9104 Loss: 0.07425449768439565\n",
      "Iteration 9105 Loss: 0.07425436161638857\n",
      "Iteration 9106 Loss: 0.07425422555619683\n",
      "Iteration 9107 Loss: 0.07425408950381995\n",
      "Iteration 9108 Loss: 0.07425395345925742\n",
      "Iteration 9109 Loss: 0.07425381742250899\n",
      "Iteration 9110 Loss: 0.07425368139357398\n",
      "Iteration 9111 Loss: 0.07425354537245206\n",
      "Iteration 9112 Loss: 0.07425340935914274\n",
      "Iteration 9113 Loss: 0.07425327335364561\n",
      "Iteration 9114 Loss: 0.07425313735596017\n",
      "Iteration 9115 Loss: 0.07425300136608595\n",
      "Iteration 9116 Loss: 0.07425286538402256\n",
      "Iteration 9117 Loss: 0.07425272940976946\n",
      "Iteration 9118 Loss: 0.07425259344332631\n",
      "Iteration 9119 Loss: 0.07425245748469256\n",
      "Iteration 9120 Loss: 0.07425232153386781\n",
      "Iteration 9121 Loss: 0.07425218559085159\n",
      "Iteration 9122 Loss: 0.07425204965564342\n",
      "Iteration 9123 Loss: 0.07425191372824294\n",
      "Iteration 9124 Loss: 0.07425177780864958\n",
      "Iteration 9125 Loss: 0.07425164189686298\n",
      "Iteration 9126 Loss: 0.07425150599288259\n",
      "Iteration 9127 Loss: 0.07425137009670806\n",
      "Iteration 9128 Loss: 0.07425123420833887\n",
      "Iteration 9129 Loss: 0.07425109832777463\n",
      "Iteration 9130 Loss: 0.07425096245501484\n",
      "Iteration 9131 Loss: 0.07425082659005898\n",
      "Iteration 9132 Loss: 0.07425069073290673\n",
      "Iteration 9133 Loss: 0.07425055488355761\n",
      "Iteration 9134 Loss: 0.0742504190420111\n",
      "Iteration 9135 Loss: 0.07425028320826678\n",
      "Iteration 9136 Loss: 0.07425014738232424\n",
      "Iteration 9137 Loss: 0.07425001156418296\n",
      "Iteration 9138 Loss: 0.07424987575384252\n",
      "Iteration 9139 Loss: 0.07424973995130242\n",
      "Iteration 9140 Loss: 0.0742496041565623\n",
      "Iteration 9141 Loss: 0.0742494683696217\n",
      "Iteration 9142 Loss: 0.07424933259048004\n",
      "Iteration 9143 Loss: 0.07424919681913704\n",
      "Iteration 9144 Loss: 0.07424906105559216\n",
      "Iteration 9145 Loss: 0.07424892529984493\n",
      "Iteration 9146 Loss: 0.0742487895518949\n",
      "Iteration 9147 Loss: 0.0742486538117417\n",
      "Iteration 9148 Loss: 0.07424851807938475\n",
      "Iteration 9149 Loss: 0.07424838235482371\n",
      "Iteration 9150 Loss: 0.07424824663805807\n",
      "Iteration 9151 Loss: 0.07424811092908738\n",
      "Iteration 9152 Loss: 0.07424797522791118\n",
      "Iteration 9153 Loss: 0.0742478395345291\n",
      "Iteration 9154 Loss: 0.07424770384894057\n",
      "Iteration 9155 Loss: 0.07424756817114528\n",
      "Iteration 9156 Loss: 0.0742474325011426\n",
      "Iteration 9157 Loss: 0.0742472968389322\n",
      "Iteration 9158 Loss: 0.07424716118451358\n",
      "Iteration 9159 Loss: 0.07424702553788638\n",
      "Iteration 9160 Loss: 0.07424688989905008\n",
      "Iteration 9161 Loss: 0.07424675426800417\n",
      "Iteration 9162 Loss: 0.07424661864474827\n",
      "Iteration 9163 Loss: 0.07424648302928191\n",
      "Iteration 9164 Loss: 0.0742463474216046\n",
      "Iteration 9165 Loss: 0.07424621182171601\n",
      "Iteration 9166 Loss: 0.07424607622961557\n",
      "Iteration 9167 Loss: 0.0742459406453029\n",
      "Iteration 9168 Loss: 0.0742458050687775\n",
      "Iteration 9169 Loss: 0.07424566950003889\n",
      "Iteration 9170 Loss: 0.07424553393908669\n",
      "Iteration 9171 Loss: 0.07424539838592044\n",
      "Iteration 9172 Loss: 0.0742452628405397\n",
      "Iteration 9173 Loss: 0.07424512730294394\n",
      "Iteration 9174 Loss: 0.07424499177313278\n",
      "Iteration 9175 Loss: 0.07424485625110577\n",
      "Iteration 9176 Loss: 0.07424472073686239\n",
      "Iteration 9177 Loss: 0.07424458523040227\n",
      "Iteration 9178 Loss: 0.07424444973172495\n",
      "Iteration 9179 Loss: 0.07424431424082996\n",
      "Iteration 9180 Loss: 0.07424417875771674\n",
      "Iteration 9181 Loss: 0.07424404328238507\n",
      "Iteration 9182 Loss: 0.07424390781483434\n",
      "Iteration 9183 Loss: 0.07424377235506412\n",
      "Iteration 9184 Loss: 0.07424363690307398\n",
      "Iteration 9185 Loss: 0.07424350145886341\n",
      "Iteration 9186 Loss: 0.07424336602243208\n",
      "Iteration 9187 Loss: 0.07424323059377944\n",
      "Iteration 9188 Loss: 0.07424309517290507\n",
      "Iteration 9189 Loss: 0.07424295975980857\n",
      "Iteration 9190 Loss: 0.07424282435448935\n",
      "Iteration 9191 Loss: 0.0742426889569471\n",
      "Iteration 9192 Loss: 0.0742425535671813\n",
      "Iteration 9193 Loss: 0.07424241818519157\n",
      "Iteration 9194 Loss: 0.07424228281097738\n",
      "Iteration 9195 Loss: 0.0742421474445383\n",
      "Iteration 9196 Loss: 0.07424201208587387\n",
      "Iteration 9197 Loss: 0.0742418767349837\n",
      "Iteration 9198 Loss: 0.0742417413918673\n",
      "Iteration 9199 Loss: 0.07424160605652419\n",
      "Iteration 9200 Loss: 0.07424147072895393\n",
      "Iteration 9201 Loss: 0.07424133540915612\n",
      "Iteration 9202 Loss: 0.07424120009713031\n",
      "Iteration 9203 Loss: 0.0742410647928759\n",
      "Iteration 9204 Loss: 0.07424092949639267\n",
      "Iteration 9205 Loss: 0.07424079420768\n",
      "Iteration 9206 Loss: 0.07424065892673752\n",
      "Iteration 9207 Loss: 0.07424052365356477\n",
      "Iteration 9208 Loss: 0.0742403883881613\n",
      "Iteration 9209 Loss: 0.07424025313052662\n",
      "Iteration 9210 Loss: 0.07424011788066029\n",
      "Iteration 9211 Loss: 0.07423998263856192\n",
      "Iteration 9212 Loss: 0.07423984740423102\n",
      "Iteration 9213 Loss: 0.07423971217766707\n",
      "Iteration 9214 Loss: 0.07423957695886974\n",
      "Iteration 9215 Loss: 0.07423944174783859\n",
      "Iteration 9216 Loss: 0.074239306544573\n",
      "Iteration 9217 Loss: 0.07423917134907268\n",
      "Iteration 9218 Loss: 0.07423903616133713\n",
      "Iteration 9219 Loss: 0.07423890098136593\n",
      "Iteration 9220 Loss: 0.07423876580915859\n",
      "Iteration 9221 Loss: 0.07423863064471466\n",
      "Iteration 9222 Loss: 0.0742384954880337\n",
      "Iteration 9223 Loss: 0.07423836033911528\n",
      "Iteration 9224 Loss: 0.07423822519795897\n",
      "Iteration 9225 Loss: 0.07423809006456425\n",
      "Iteration 9226 Loss: 0.07423795493893068\n",
      "Iteration 9227 Loss: 0.07423781982105784\n",
      "Iteration 9228 Loss: 0.07423768471094536\n",
      "Iteration 9229 Loss: 0.0742375496085926\n",
      "Iteration 9230 Loss: 0.0742374145139993\n",
      "Iteration 9231 Loss: 0.07423727942716488\n",
      "Iteration 9232 Loss: 0.074237144348089\n",
      "Iteration 9233 Loss: 0.07423700927677108\n",
      "Iteration 9234 Loss: 0.07423687421321082\n",
      "Iteration 9235 Loss: 0.07423673915740767\n",
      "Iteration 9236 Loss: 0.07423660410936124\n",
      "Iteration 9237 Loss: 0.07423646906907097\n",
      "Iteration 9238 Loss: 0.07423633403653654\n",
      "Iteration 9239 Loss: 0.07423619901175744\n",
      "Iteration 9240 Loss: 0.07423606399473323\n",
      "Iteration 9241 Loss: 0.07423592898546347\n",
      "Iteration 9242 Loss: 0.07423579398394764\n",
      "Iteration 9243 Loss: 0.0742356589901855\n",
      "Iteration 9244 Loss: 0.07423552400417636\n",
      "Iteration 9245 Loss: 0.07423538902591989\n",
      "Iteration 9246 Loss: 0.07423525405541562\n",
      "Iteration 9247 Loss: 0.07423511909266312\n",
      "Iteration 9248 Loss: 0.07423498413766191\n",
      "Iteration 9249 Loss: 0.07423484919041148\n",
      "Iteration 9250 Loss: 0.07423471425091162\n",
      "Iteration 9251 Loss: 0.0742345793191616\n",
      "Iteration 9252 Loss: 0.07423444439516112\n",
      "Iteration 9253 Loss: 0.0742343094789097\n",
      "Iteration 9254 Loss: 0.07423417457040686\n",
      "Iteration 9255 Loss: 0.07423403966965222\n",
      "Iteration 9256 Loss: 0.0742339047766453\n",
      "Iteration 9257 Loss: 0.07423376989138565\n",
      "Iteration 9258 Loss: 0.07423363501387281\n",
      "Iteration 9259 Loss: 0.07423350014410635\n",
      "Iteration 9260 Loss: 0.07423336528208578\n",
      "Iteration 9261 Loss: 0.07423323042781076\n",
      "Iteration 9262 Loss: 0.07423309558128074\n",
      "Iteration 9263 Loss: 0.0742329607424953\n",
      "Iteration 9264 Loss: 0.07423282591145394\n",
      "Iteration 9265 Loss: 0.07423269108815637\n",
      "Iteration 9266 Loss: 0.07423255627260193\n",
      "Iteration 9267 Loss: 0.07423242146479032\n",
      "Iteration 9268 Loss: 0.0742322866647211\n",
      "Iteration 9269 Loss: 0.07423215187239371\n",
      "Iteration 9270 Loss: 0.07423201708780783\n",
      "Iteration 9271 Loss: 0.07423188231096288\n",
      "Iteration 9272 Loss: 0.07423174754185846\n",
      "Iteration 9273 Loss: 0.07423161278049425\n",
      "Iteration 9274 Loss: 0.07423147802686961\n",
      "Iteration 9275 Loss: 0.0742313432809842\n",
      "Iteration 9276 Loss: 0.0742312085428376\n",
      "Iteration 9277 Loss: 0.07423107381242926\n",
      "Iteration 9278 Loss: 0.07423093908975878\n",
      "Iteration 9279 Loss: 0.07423080437482574\n",
      "Iteration 9280 Loss: 0.0742306696676297\n",
      "Iteration 9281 Loss: 0.07423053496817014\n",
      "Iteration 9282 Loss: 0.07423040027644667\n",
      "Iteration 9283 Loss: 0.07423026559245882\n",
      "Iteration 9284 Loss: 0.07423013091620614\n",
      "Iteration 9285 Loss: 0.07422999624768822\n",
      "Iteration 9286 Loss: 0.07422986158690456\n",
      "Iteration 9287 Loss: 0.07422972693385477\n",
      "Iteration 9288 Loss: 0.07422959228853836\n",
      "Iteration 9289 Loss: 0.07422945765095491\n",
      "Iteration 9290 Loss: 0.07422932302110397\n",
      "Iteration 9291 Loss: 0.07422918839898504\n",
      "Iteration 9292 Loss: 0.07422905378459774\n",
      "Iteration 9293 Loss: 0.07422891917794161\n",
      "Iteration 9294 Loss: 0.07422878457901619\n",
      "Iteration 9295 Loss: 0.07422864998782097\n",
      "Iteration 9296 Loss: 0.07422851540435567\n",
      "Iteration 9297 Loss: 0.07422838082861961\n",
      "Iteration 9298 Loss: 0.07422824626061257\n",
      "Iteration 9299 Loss: 0.07422811170033397\n",
      "Iteration 9300 Loss: 0.07422797714778342\n",
      "Iteration 9301 Loss: 0.07422784260296048\n",
      "Iteration 9302 Loss: 0.07422770806586458\n",
      "Iteration 9303 Loss: 0.07422757353649549\n",
      "Iteration 9304 Loss: 0.07422743901485256\n",
      "Iteration 9305 Loss: 0.0742273045009355\n",
      "Iteration 9306 Loss: 0.07422716999474373\n",
      "Iteration 9307 Loss: 0.07422703549627685\n",
      "Iteration 9308 Loss: 0.07422690100553446\n",
      "Iteration 9309 Loss: 0.07422676652251606\n",
      "Iteration 9310 Loss: 0.07422663204722126\n",
      "Iteration 9311 Loss: 0.07422649757964955\n",
      "Iteration 9312 Loss: 0.07422636311980058\n",
      "Iteration 9313 Loss: 0.07422622866767378\n",
      "Iteration 9314 Loss: 0.07422609422326869\n",
      "Iteration 9315 Loss: 0.07422595978658503\n",
      "Iteration 9316 Loss: 0.07422582535762223\n",
      "Iteration 9317 Loss: 0.07422569093637985\n",
      "Iteration 9318 Loss: 0.07422555652285749\n",
      "Iteration 9319 Loss: 0.0742254221170547\n",
      "Iteration 9320 Loss: 0.07422528771897095\n",
      "Iteration 9321 Loss: 0.07422515332860591\n",
      "Iteration 9322 Loss: 0.07422501894595904\n",
      "Iteration 9323 Loss: 0.07422488457102994\n",
      "Iteration 9324 Loss: 0.07422475020381812\n",
      "Iteration 9325 Loss: 0.07422461584432329\n",
      "Iteration 9326 Loss: 0.07422448149254479\n",
      "Iteration 9327 Loss: 0.07422434714848232\n",
      "Iteration 9328 Loss: 0.07422421281213538\n",
      "Iteration 9329 Loss: 0.07422407848350347\n",
      "Iteration 9330 Loss: 0.07422394416258625\n",
      "Iteration 9331 Loss: 0.07422380984938319\n",
      "Iteration 9332 Loss: 0.07422367554389393\n",
      "Iteration 9333 Loss: 0.07422354124611799\n",
      "Iteration 9334 Loss: 0.07422340695605485\n",
      "Iteration 9335 Loss: 0.07422327267370418\n",
      "Iteration 9336 Loss: 0.07422313839906547\n",
      "Iteration 9337 Loss: 0.07422300413213823\n",
      "Iteration 9338 Loss: 0.0742228698729221\n",
      "Iteration 9339 Loss: 0.0742227356214166\n",
      "Iteration 9340 Loss: 0.07422260137762128\n",
      "Iteration 9341 Loss: 0.07422246714153573\n",
      "Iteration 9342 Loss: 0.0742223329131595\n",
      "Iteration 9343 Loss: 0.07422219869249205\n",
      "Iteration 9344 Loss: 0.07422206447953301\n",
      "Iteration 9345 Loss: 0.07422193027428199\n",
      "Iteration 9346 Loss: 0.07422179607673846\n",
      "Iteration 9347 Loss: 0.07422166188690198\n",
      "Iteration 9348 Loss: 0.07422152770477211\n",
      "Iteration 9349 Loss: 0.07422139353034843\n",
      "Iteration 9350 Loss: 0.07422125936363055\n",
      "Iteration 9351 Loss: 0.07422112520461788\n",
      "Iteration 9352 Loss: 0.0742209910533101\n",
      "Iteration 9353 Loss: 0.0742208569097067\n",
      "Iteration 9354 Loss: 0.07422072277380723\n",
      "Iteration 9355 Loss: 0.07422058864561132\n",
      "Iteration 9356 Loss: 0.07422045452511845\n",
      "Iteration 9357 Loss: 0.07422032041232819\n",
      "Iteration 9358 Loss: 0.07422018630724012\n",
      "Iteration 9359 Loss: 0.07422005220985377\n",
      "Iteration 9360 Loss: 0.0742199181201687\n",
      "Iteration 9361 Loss: 0.07421978403818444\n",
      "Iteration 9362 Loss: 0.07421964996390061\n",
      "Iteration 9363 Loss: 0.07421951589731672\n",
      "Iteration 9364 Loss: 0.0742193818384324\n",
      "Iteration 9365 Loss: 0.07421924778724708\n",
      "Iteration 9366 Loss: 0.07421911374376033\n",
      "Iteration 9367 Loss: 0.07421897970797181\n",
      "Iteration 9368 Loss: 0.07421884567988105\n",
      "Iteration 9369 Loss: 0.07421871165948753\n",
      "Iteration 9370 Loss: 0.07421857764679085\n",
      "Iteration 9371 Loss: 0.07421844364179056\n",
      "Iteration 9372 Loss: 0.07421830964448622\n",
      "Iteration 9373 Loss: 0.0742181756548774\n",
      "Iteration 9374 Loss: 0.07421804167296364\n",
      "Iteration 9375 Loss: 0.07421790769874445\n",
      "Iteration 9376 Loss: 0.0742177737322195\n",
      "Iteration 9377 Loss: 0.07421763977338823\n",
      "Iteration 9378 Loss: 0.07421750582225026\n",
      "Iteration 9379 Loss: 0.07421737187880512\n",
      "Iteration 9380 Loss: 0.07421723794305243\n",
      "Iteration 9381 Loss: 0.07421710401499161\n",
      "Iteration 9382 Loss: 0.07421697009462232\n",
      "Iteration 9383 Loss: 0.0742168361819441\n",
      "Iteration 9384 Loss: 0.07421670227695645\n",
      "Iteration 9385 Loss: 0.07421656837965906\n",
      "Iteration 9386 Loss: 0.07421643449005137\n",
      "Iteration 9387 Loss: 0.07421630060813297\n",
      "Iteration 9388 Loss: 0.07421616673390338\n",
      "Iteration 9389 Loss: 0.07421603286736222\n",
      "Iteration 9390 Loss: 0.07421589900850897\n",
      "Iteration 9391 Loss: 0.07421576515734331\n",
      "Iteration 9392 Loss: 0.07421563131386465\n",
      "Iteration 9393 Loss: 0.07421549747807264\n",
      "Iteration 9394 Loss: 0.07421536364996681\n",
      "Iteration 9395 Loss: 0.07421522982954669\n",
      "Iteration 9396 Loss: 0.07421509601681188\n",
      "Iteration 9397 Loss: 0.07421496221176194\n",
      "Iteration 9398 Loss: 0.07421482841439639\n",
      "Iteration 9399 Loss: 0.07421469462471483\n",
      "Iteration 9400 Loss: 0.07421456084271669\n",
      "Iteration 9401 Loss: 0.07421442706840169\n",
      "Iteration 9402 Loss: 0.07421429330176935\n",
      "Iteration 9403 Loss: 0.07421415954281911\n",
      "Iteration 9404 Loss: 0.0742140257915507\n",
      "Iteration 9405 Loss: 0.07421389204796353\n",
      "Iteration 9406 Loss: 0.07421375831205726\n",
      "Iteration 9407 Loss: 0.07421362458383138\n",
      "Iteration 9408 Loss: 0.07421349086328546\n",
      "Iteration 9409 Loss: 0.07421335715041909\n",
      "Iteration 9410 Loss: 0.07421322344523182\n",
      "Iteration 9411 Loss: 0.07421308974772312\n",
      "Iteration 9412 Loss: 0.07421295605789273\n",
      "Iteration 9413 Loss: 0.07421282237573998\n",
      "Iteration 9414 Loss: 0.07421268870126456\n",
      "Iteration 9415 Loss: 0.07421255503446604\n",
      "Iteration 9416 Loss: 0.07421242137534394\n",
      "Iteration 9417 Loss: 0.07421228772389787\n",
      "Iteration 9418 Loss: 0.07421215408012727\n",
      "Iteration 9419 Loss: 0.07421202044403175\n",
      "Iteration 9420 Loss: 0.07421188681561092\n",
      "Iteration 9421 Loss: 0.07421175319486428\n",
      "Iteration 9422 Loss: 0.07421161958179141\n",
      "Iteration 9423 Loss: 0.0742114859763919\n",
      "Iteration 9424 Loss: 0.0742113523786652\n",
      "Iteration 9425 Loss: 0.074211218788611\n",
      "Iteration 9426 Loss: 0.07421108520622877\n",
      "Iteration 9427 Loss: 0.07421095163151811\n",
      "Iteration 9428 Loss: 0.07421081806447857\n",
      "Iteration 9429 Loss: 0.07421068450510968\n",
      "Iteration 9430 Loss: 0.07421055095341098\n",
      "Iteration 9431 Loss: 0.07421041740938213\n",
      "Iteration 9432 Loss: 0.07421028387302253\n",
      "Iteration 9433 Loss: 0.0742101503443319\n",
      "Iteration 9434 Loss: 0.0742100168233097\n",
      "Iteration 9435 Loss: 0.07420988330995548\n",
      "Iteration 9436 Loss: 0.07420974980426889\n",
      "Iteration 9437 Loss: 0.0742096163062494\n",
      "Iteration 9438 Loss: 0.0742094828158966\n",
      "Iteration 9439 Loss: 0.07420934933320998\n",
      "Iteration 9440 Loss: 0.07420921585818929\n",
      "Iteration 9441 Loss: 0.07420908239083386\n",
      "Iteration 9442 Loss: 0.0742089489311434\n",
      "Iteration 9443 Loss: 0.07420881547911735\n",
      "Iteration 9444 Loss: 0.07420868203475536\n",
      "Iteration 9445 Loss: 0.07420854859805696\n",
      "Iteration 9446 Loss: 0.07420841516902173\n",
      "Iteration 9447 Loss: 0.07420828174764911\n",
      "Iteration 9448 Loss: 0.07420814833393885\n",
      "Iteration 9449 Loss: 0.07420801492789035\n",
      "Iteration 9450 Loss: 0.07420788152950328\n",
      "Iteration 9451 Loss: 0.0742077481387771\n",
      "Iteration 9452 Loss: 0.07420761475571144\n",
      "Iteration 9453 Loss: 0.0742074813803058\n",
      "Iteration 9454 Loss: 0.07420734801255986\n",
      "Iteration 9455 Loss: 0.074207214652473\n",
      "Iteration 9456 Loss: 0.07420708130004487\n",
      "Iteration 9457 Loss: 0.07420694795527501\n",
      "Iteration 9458 Loss: 0.0742068146181631\n",
      "Iteration 9459 Loss: 0.0742066812887085\n",
      "Iteration 9460 Loss: 0.07420654796691087\n",
      "Iteration 9461 Loss: 0.0742064146527698\n",
      "Iteration 9462 Loss: 0.0742062813462847\n",
      "Iteration 9463 Loss: 0.07420614804745533\n",
      "Iteration 9464 Loss: 0.07420601475628116\n",
      "Iteration 9465 Loss: 0.07420588147276164\n",
      "Iteration 9466 Loss: 0.07420574819689651\n",
      "Iteration 9467 Loss: 0.07420561492868527\n",
      "Iteration 9468 Loss: 0.07420548166812742\n",
      "Iteration 9469 Loss: 0.07420534841522257\n",
      "Iteration 9470 Loss: 0.07420521516997022\n",
      "Iteration 9471 Loss: 0.07420508193236997\n",
      "Iteration 9472 Loss: 0.07420494870242147\n",
      "Iteration 9473 Loss: 0.07420481548012409\n",
      "Iteration 9474 Loss: 0.0742046822654775\n",
      "Iteration 9475 Loss: 0.07420454905848131\n",
      "Iteration 9476 Loss: 0.07420441585913493\n",
      "Iteration 9477 Loss: 0.07420428266743803\n",
      "Iteration 9478 Loss: 0.0742041494833902\n",
      "Iteration 9479 Loss: 0.07420401630699092\n",
      "Iteration 9480 Loss: 0.0742038831382397\n",
      "Iteration 9481 Loss: 0.07420374997713623\n",
      "Iteration 9482 Loss: 0.07420361682368003\n",
      "Iteration 9483 Loss: 0.0742034836778706\n",
      "Iteration 9484 Loss: 0.0742033505397075\n",
      "Iteration 9485 Loss: 0.07420321740919038\n",
      "Iteration 9486 Loss: 0.07420308428631871\n",
      "Iteration 9487 Loss: 0.07420295117109213\n",
      "Iteration 9488 Loss: 0.07420281806351012\n",
      "Iteration 9489 Loss: 0.07420268496357227\n",
      "Iteration 9490 Loss: 0.07420255187127818\n",
      "Iteration 9491 Loss: 0.07420241878662732\n",
      "Iteration 9492 Loss: 0.07420228570961929\n",
      "Iteration 9493 Loss: 0.07420215264025369\n",
      "Iteration 9494 Loss: 0.07420201957853008\n",
      "Iteration 9495 Loss: 0.0742018865244479\n",
      "Iteration 9496 Loss: 0.07420175347800687\n",
      "Iteration 9497 Loss: 0.07420162043920644\n",
      "Iteration 9498 Loss: 0.07420148740804625\n",
      "Iteration 9499 Loss: 0.07420135438452573\n",
      "Iteration 9500 Loss: 0.07420122136864457\n",
      "Iteration 9501 Loss: 0.07420108836040225\n",
      "Iteration 9502 Loss: 0.07420095535979843\n",
      "Iteration 9503 Loss: 0.07420082236683254\n",
      "Iteration 9504 Loss: 0.07420068938150429\n",
      "Iteration 9505 Loss: 0.07420055640381304\n",
      "Iteration 9506 Loss: 0.0742004234337585\n",
      "Iteration 9507 Loss: 0.07420029047134022\n",
      "Iteration 9508 Loss: 0.07420015751655776\n",
      "Iteration 9509 Loss: 0.07420002456941054\n",
      "Iteration 9510 Loss: 0.07419989162989829\n",
      "Iteration 9511 Loss: 0.07419975869802051\n",
      "Iteration 9512 Loss: 0.07419962577377678\n",
      "Iteration 9513 Loss: 0.0741994928571666\n",
      "Iteration 9514 Loss: 0.07419935994818957\n",
      "Iteration 9515 Loss: 0.07419922704684527\n",
      "Iteration 9516 Loss: 0.07419909415313322\n",
      "Iteration 9517 Loss: 0.07419896126705301\n",
      "Iteration 9518 Loss: 0.07419882838860421\n",
      "Iteration 9519 Loss: 0.07419869551778634\n",
      "Iteration 9520 Loss: 0.07419856265459902\n",
      "Iteration 9521 Loss: 0.0741984297990417\n",
      "Iteration 9522 Loss: 0.07419829695111403\n",
      "Iteration 9523 Loss: 0.07419816411081555\n",
      "Iteration 9524 Loss: 0.07419803127814582\n",
      "Iteration 9525 Loss: 0.07419789845310439\n",
      "Iteration 9526 Loss: 0.07419776563569083\n",
      "Iteration 9527 Loss: 0.07419763282590472\n",
      "Iteration 9528 Loss: 0.07419750002374562\n",
      "Iteration 9529 Loss: 0.074197367229213\n",
      "Iteration 9530 Loss: 0.07419723444230653\n",
      "Iteration 9531 Loss: 0.07419710166302576\n",
      "Iteration 9532 Loss: 0.07419696889137017\n",
      "Iteration 9533 Loss: 0.07419683612733938\n",
      "Iteration 9534 Loss: 0.07419670337093297\n",
      "Iteration 9535 Loss: 0.07419657062215043\n",
      "Iteration 9536 Loss: 0.07419643788099146\n",
      "Iteration 9537 Loss: 0.07419630514745543\n",
      "Iteration 9538 Loss: 0.074196172421542\n",
      "Iteration 9539 Loss: 0.07419603970325077\n",
      "Iteration 9540 Loss: 0.07419590699258119\n",
      "Iteration 9541 Loss: 0.07419577428953297\n",
      "Iteration 9542 Loss: 0.07419564159410552\n",
      "Iteration 9543 Loss: 0.07419550890629849\n",
      "Iteration 9544 Loss: 0.07419537622611141\n",
      "Iteration 9545 Loss: 0.07419524355354386\n",
      "Iteration 9546 Loss: 0.0741951108885954\n",
      "Iteration 9547 Loss: 0.07419497823126557\n",
      "Iteration 9548 Loss: 0.07419484558155394\n",
      "Iteration 9549 Loss: 0.07419471293946009\n",
      "Iteration 9550 Loss: 0.07419458030498353\n",
      "Iteration 9551 Loss: 0.07419444767812386\n",
      "Iteration 9552 Loss: 0.07419431505888066\n",
      "Iteration 9553 Loss: 0.0741941824472534\n",
      "Iteration 9554 Loss: 0.07419404984324182\n",
      "Iteration 9555 Loss: 0.07419391724684529\n",
      "Iteration 9556 Loss: 0.07419378465806344\n",
      "Iteration 9557 Loss: 0.0741936520768959\n",
      "Iteration 9558 Loss: 0.07419351950334209\n",
      "Iteration 9559 Loss: 0.07419338693740171\n",
      "Iteration 9560 Loss: 0.07419325437907424\n",
      "Iteration 9561 Loss: 0.07419312182835931\n",
      "Iteration 9562 Loss: 0.07419298928525639\n",
      "Iteration 9563 Loss: 0.07419285674976514\n",
      "Iteration 9564 Loss: 0.07419272422188501\n",
      "Iteration 9565 Loss: 0.07419259170161566\n",
      "Iteration 9566 Loss: 0.07419245918895659\n",
      "Iteration 9567 Loss: 0.07419232668390742\n",
      "Iteration 9568 Loss: 0.07419219418646758\n",
      "Iteration 9569 Loss: 0.0741920616966368\n",
      "Iteration 9570 Loss: 0.07419192921441456\n",
      "Iteration 9571 Loss: 0.0741917967398004\n",
      "Iteration 9572 Loss: 0.07419166427279396\n",
      "Iteration 9573 Loss: 0.07419153181339473\n",
      "Iteration 9574 Loss: 0.07419139936160228\n",
      "Iteration 9575 Loss: 0.07419126691741618\n",
      "Iteration 9576 Loss: 0.074191134480836\n",
      "Iteration 9577 Loss: 0.07419100205186133\n",
      "Iteration 9578 Loss: 0.07419086963049172\n",
      "Iteration 9579 Loss: 0.07419073721672667\n",
      "Iteration 9580 Loss: 0.07419060481056575\n",
      "Iteration 9581 Loss: 0.07419047241200862\n",
      "Iteration 9582 Loss: 0.07419034002105476\n",
      "Iteration 9583 Loss: 0.0741902076377037\n",
      "Iteration 9584 Loss: 0.07419007526195513\n",
      "Iteration 9585 Loss: 0.0741899428938085\n",
      "Iteration 9586 Loss: 0.07418981053326339\n",
      "Iteration 9587 Loss: 0.07418967818031938\n",
      "Iteration 9588 Loss: 0.07418954583497606\n",
      "Iteration 9589 Loss: 0.07418941349723289\n",
      "Iteration 9590 Loss: 0.07418928116708959\n",
      "Iteration 9591 Loss: 0.07418914884454564\n",
      "Iteration 9592 Loss: 0.0741890165296005\n",
      "Iteration 9593 Loss: 0.07418888422225389\n",
      "Iteration 9594 Loss: 0.07418875192250533\n",
      "Iteration 9595 Loss: 0.07418861963035432\n",
      "Iteration 9596 Loss: 0.07418848734580051\n",
      "Iteration 9597 Loss: 0.07418835506884339\n",
      "Iteration 9598 Loss: 0.07418822279948255\n",
      "Iteration 9599 Loss: 0.07418809053771758\n",
      "Iteration 9600 Loss: 0.07418795828354799\n",
      "Iteration 9601 Loss: 0.07418782603697335\n",
      "Iteration 9602 Loss: 0.07418769379799331\n",
      "Iteration 9603 Loss: 0.07418756156660726\n",
      "Iteration 9604 Loss: 0.07418742934281494\n",
      "Iteration 9605 Loss: 0.07418729712661583\n",
      "Iteration 9606 Loss: 0.07418716491800947\n",
      "Iteration 9607 Loss: 0.0741870327169955\n",
      "Iteration 9608 Loss: 0.07418690052357342\n",
      "Iteration 9609 Loss: 0.07418676833774278\n",
      "Iteration 9610 Loss: 0.07418663615950316\n",
      "Iteration 9611 Loss: 0.07418650398885421\n",
      "Iteration 9612 Loss: 0.07418637182579532\n",
      "Iteration 9613 Loss: 0.07418623967032621\n",
      "Iteration 9614 Loss: 0.07418610752244639\n",
      "Iteration 9615 Loss: 0.07418597538215538\n",
      "Iteration 9616 Loss: 0.0741858432494528\n",
      "Iteration 9617 Loss: 0.07418571112433814\n",
      "Iteration 9618 Loss: 0.07418557900681108\n",
      "Iteration 9619 Loss: 0.0741854468968711\n",
      "Iteration 9620 Loss: 0.07418531479451776\n",
      "Iteration 9621 Loss: 0.07418518269975063\n",
      "Iteration 9622 Loss: 0.07418505061256933\n",
      "Iteration 9623 Loss: 0.07418491853297333\n",
      "Iteration 9624 Loss: 0.07418478646096224\n",
      "Iteration 9625 Loss: 0.07418465439653565\n",
      "Iteration 9626 Loss: 0.07418452233969311\n",
      "Iteration 9627 Loss: 0.07418439029043415\n",
      "Iteration 9628 Loss: 0.07418425824875838\n",
      "Iteration 9629 Loss: 0.07418412621466527\n",
      "Iteration 9630 Loss: 0.07418399418815452\n",
      "Iteration 9631 Loss: 0.07418386216922553\n",
      "Iteration 9632 Loss: 0.07418373015787806\n",
      "Iteration 9633 Loss: 0.07418359815411155\n",
      "Iteration 9634 Loss: 0.07418346615792555\n",
      "Iteration 9635 Loss: 0.07418333416931963\n",
      "Iteration 9636 Loss: 0.07418320218829347\n",
      "Iteration 9637 Loss: 0.07418307021484646\n",
      "Iteration 9638 Loss: 0.07418293824897827\n",
      "Iteration 9639 Loss: 0.07418280629068845\n",
      "Iteration 9640 Loss: 0.07418267433997655\n",
      "Iteration 9641 Loss: 0.0741825423968421\n",
      "Iteration 9642 Loss: 0.07418241046128475\n",
      "Iteration 9643 Loss: 0.07418227853330397\n",
      "Iteration 9644 Loss: 0.07418214661289942\n",
      "Iteration 9645 Loss: 0.07418201470007055\n",
      "Iteration 9646 Loss: 0.07418188279481705\n",
      "Iteration 9647 Loss: 0.07418175089713833\n",
      "Iteration 9648 Loss: 0.07418161900703416\n",
      "Iteration 9649 Loss: 0.07418148712450387\n",
      "Iteration 9650 Loss: 0.0741813552495472\n",
      "Iteration 9651 Loss: 0.07418122338216368\n",
      "Iteration 9652 Loss: 0.07418109152235282\n",
      "Iteration 9653 Loss: 0.0741809596701142\n",
      "Iteration 9654 Loss: 0.07418082782544741\n",
      "Iteration 9655 Loss: 0.07418069598835197\n",
      "Iteration 9656 Loss: 0.07418056415882752\n",
      "Iteration 9657 Loss: 0.0741804323368735\n",
      "Iteration 9658 Loss: 0.07418030052248958\n",
      "Iteration 9659 Loss: 0.07418016871567532\n",
      "Iteration 9660 Loss: 0.07418003691643024\n",
      "Iteration 9661 Loss: 0.07417990512475392\n",
      "Iteration 9662 Loss: 0.07417977334064592\n",
      "Iteration 9663 Loss: 0.07417964156410585\n",
      "Iteration 9664 Loss: 0.07417950979513323\n",
      "Iteration 9665 Loss: 0.07417937803372758\n",
      "Iteration 9666 Loss: 0.07417924627988856\n",
      "Iteration 9667 Loss: 0.07417911453361571\n",
      "Iteration 9668 Loss: 0.07417898279490852\n",
      "Iteration 9669 Loss: 0.07417885106376662\n",
      "Iteration 9670 Loss: 0.07417871934018956\n",
      "Iteration 9671 Loss: 0.07417858762417688\n",
      "Iteration 9672 Loss: 0.07417845591572822\n",
      "Iteration 9673 Loss: 0.07417832421484306\n",
      "Iteration 9674 Loss: 0.07417819252152102\n",
      "Iteration 9675 Loss: 0.07417806083576164\n",
      "Iteration 9676 Loss: 0.07417792915756444\n",
      "Iteration 9677 Loss: 0.07417779748692908\n",
      "Iteration 9678 Loss: 0.07417766582385507\n",
      "Iteration 9679 Loss: 0.07417753416834197\n",
      "Iteration 9680 Loss: 0.07417740252038937\n",
      "Iteration 9681 Loss: 0.07417727087999683\n",
      "Iteration 9682 Loss: 0.07417713924716383\n",
      "Iteration 9683 Loss: 0.07417700762189008\n",
      "Iteration 9684 Loss: 0.07417687600417511\n",
      "Iteration 9685 Loss: 0.07417674439401838\n",
      "Iteration 9686 Loss: 0.0741766127914195\n",
      "Iteration 9687 Loss: 0.07417648119637812\n",
      "Iteration 9688 Loss: 0.07417634960889372\n",
      "Iteration 9689 Loss: 0.07417621802896593\n",
      "Iteration 9690 Loss: 0.07417608645659421\n",
      "Iteration 9691 Loss: 0.0741759548917782\n",
      "Iteration 9692 Loss: 0.07417582333451748\n",
      "Iteration 9693 Loss: 0.07417569178481158\n",
      "Iteration 9694 Loss: 0.07417556024266\n",
      "Iteration 9695 Loss: 0.07417542870806247\n",
      "Iteration 9696 Loss: 0.07417529718101848\n",
      "Iteration 9697 Loss: 0.07417516566152756\n",
      "Iteration 9698 Loss: 0.07417503414958923\n",
      "Iteration 9699 Loss: 0.07417490264520317\n",
      "Iteration 9700 Loss: 0.07417477114836883\n",
      "Iteration 9701 Loss: 0.07417463965908591\n",
      "Iteration 9702 Loss: 0.07417450817735385\n",
      "Iteration 9703 Loss: 0.07417437670317234\n",
      "Iteration 9704 Loss: 0.07417424523654084\n",
      "Iteration 9705 Loss: 0.0741741137774589\n",
      "Iteration 9706 Loss: 0.07417398232592617\n",
      "Iteration 9707 Loss: 0.0741738508819422\n",
      "Iteration 9708 Loss: 0.07417371944550656\n",
      "Iteration 9709 Loss: 0.07417358801661875\n",
      "Iteration 9710 Loss: 0.07417345659527837\n",
      "Iteration 9711 Loss: 0.074173325181485\n",
      "Iteration 9712 Loss: 0.07417319377523818\n",
      "Iteration 9713 Loss: 0.07417306237653751\n",
      "Iteration 9714 Loss: 0.07417293098538258\n",
      "Iteration 9715 Loss: 0.07417279960177285\n",
      "Iteration 9716 Loss: 0.07417266822570798\n",
      "Iteration 9717 Loss: 0.07417253685718747\n",
      "Iteration 9718 Loss: 0.07417240549621096\n",
      "Iteration 9719 Loss: 0.07417227414277801\n",
      "Iteration 9720 Loss: 0.07417214279688805\n",
      "Iteration 9721 Loss: 0.07417201145854087\n",
      "Iteration 9722 Loss: 0.07417188012773585\n",
      "Iteration 9723 Loss: 0.0741717488044726\n",
      "Iteration 9724 Loss: 0.07417161748875073\n",
      "Iteration 9725 Loss: 0.07417148618056978\n",
      "Iteration 9726 Loss: 0.0741713548799293\n",
      "Iteration 9727 Loss: 0.0741712235868289\n",
      "Iteration 9728 Loss: 0.07417109230126805\n",
      "Iteration 9729 Loss: 0.07417096102324647\n",
      "Iteration 9730 Loss: 0.07417082975276361\n",
      "Iteration 9731 Loss: 0.07417069848981903\n",
      "Iteration 9732 Loss: 0.07417056723441243\n",
      "Iteration 9733 Loss: 0.07417043598654319\n",
      "Iteration 9734 Loss: 0.07417030474621097\n",
      "Iteration 9735 Loss: 0.0741701735134154\n",
      "Iteration 9736 Loss: 0.07417004228815591\n",
      "Iteration 9737 Loss: 0.07416991107043218\n",
      "Iteration 9738 Loss: 0.07416977986024371\n",
      "Iteration 9739 Loss: 0.0741696486575901\n",
      "Iteration 9740 Loss: 0.07416951746247087\n",
      "Iteration 9741 Loss: 0.07416938627488562\n",
      "Iteration 9742 Loss: 0.07416925509483399\n",
      "Iteration 9743 Loss: 0.07416912392231537\n",
      "Iteration 9744 Loss: 0.07416899275732945\n",
      "Iteration 9745 Loss: 0.07416886159987582\n",
      "Iteration 9746 Loss: 0.07416873044995395\n",
      "Iteration 9747 Loss: 0.07416859930756345\n",
      "Iteration 9748 Loss: 0.07416846817270395\n",
      "Iteration 9749 Loss: 0.07416833704537495\n",
      "Iteration 9750 Loss: 0.074168205925576\n",
      "Iteration 9751 Loss: 0.0741680748133067\n",
      "Iteration 9752 Loss: 0.07416794370856661\n",
      "Iteration 9753 Loss: 0.07416781261135527\n",
      "Iteration 9754 Loss: 0.07416768152167232\n",
      "Iteration 9755 Loss: 0.07416755043951724\n",
      "Iteration 9756 Loss: 0.07416741936488967\n",
      "Iteration 9757 Loss: 0.07416728829778914\n",
      "Iteration 9758 Loss: 0.07416715723821522\n",
      "Iteration 9759 Loss: 0.07416702618616747\n",
      "Iteration 9760 Loss: 0.07416689514164536\n",
      "Iteration 9761 Loss: 0.0741667641046487\n",
      "Iteration 9762 Loss: 0.07416663307517693\n",
      "Iteration 9763 Loss: 0.07416650205322954\n",
      "Iteration 9764 Loss: 0.07416637103880615\n",
      "Iteration 9765 Loss: 0.07416624003190637\n",
      "Iteration 9766 Loss: 0.0741661090325297\n",
      "Iteration 9767 Loss: 0.07416597804067579\n",
      "Iteration 9768 Loss: 0.07416584705634408\n",
      "Iteration 9769 Loss: 0.07416571607953434\n",
      "Iteration 9770 Loss: 0.07416558511024593\n",
      "Iteration 9771 Loss: 0.0741654541484785\n",
      "Iteration 9772 Loss: 0.07416532319423161\n",
      "Iteration 9773 Loss: 0.07416519224750484\n",
      "Iteration 9774 Loss: 0.0741650613082978\n",
      "Iteration 9775 Loss: 0.07416493037660996\n",
      "Iteration 9776 Loss: 0.07416479945244095\n",
      "Iteration 9777 Loss: 0.07416466853579029\n",
      "Iteration 9778 Loss: 0.07416453762665762\n",
      "Iteration 9779 Loss: 0.07416440672504247\n",
      "Iteration 9780 Loss: 0.07416427583094434\n",
      "Iteration 9781 Loss: 0.07416414494436298\n",
      "Iteration 9782 Loss: 0.07416401406529778\n",
      "Iteration 9783 Loss: 0.07416388319374836\n",
      "Iteration 9784 Loss: 0.07416375232971431\n",
      "Iteration 9785 Loss: 0.07416362147319515\n",
      "Iteration 9786 Loss: 0.07416349062419053\n",
      "Iteration 9787 Loss: 0.07416335978269994\n",
      "Iteration 9788 Loss: 0.07416322894872292\n",
      "Iteration 9789 Loss: 0.07416309812225916\n",
      "Iteration 9790 Loss: 0.07416296730330811\n",
      "Iteration 9791 Loss: 0.07416283649186947\n",
      "Iteration 9792 Loss: 0.0741627056879427\n",
      "Iteration 9793 Loss: 0.07416257489152735\n",
      "Iteration 9794 Loss: 0.07416244410262303\n",
      "Iteration 9795 Loss: 0.07416231332122931\n",
      "Iteration 9796 Loss: 0.0741621825473458\n",
      "Iteration 9797 Loss: 0.07416205178097196\n",
      "Iteration 9798 Loss: 0.07416192102210753\n",
      "Iteration 9799 Loss: 0.07416179027075184\n",
      "Iteration 9800 Loss: 0.07416165952690472\n",
      "Iteration 9801 Loss: 0.0741615287905655\n",
      "Iteration 9802 Loss: 0.07416139806173391\n",
      "Iteration 9803 Loss: 0.07416126734040943\n",
      "Iteration 9804 Loss: 0.0741611366265917\n",
      "Iteration 9805 Loss: 0.07416100592028016\n",
      "Iteration 9806 Loss: 0.07416087522147455\n",
      "Iteration 9807 Loss: 0.07416074453017432\n",
      "Iteration 9808 Loss: 0.07416061384637909\n",
      "Iteration 9809 Loss: 0.07416048317008841\n",
      "Iteration 9810 Loss: 0.07416035250130186\n",
      "Iteration 9811 Loss: 0.074160221840019\n",
      "Iteration 9812 Loss: 0.07416009118623933\n",
      "Iteration 9813 Loss: 0.07415996053996252\n",
      "Iteration 9814 Loss: 0.07415982990118805\n",
      "Iteration 9815 Loss: 0.07415969926991564\n",
      "Iteration 9816 Loss: 0.07415956864614473\n",
      "Iteration 9817 Loss: 0.07415943802987487\n",
      "Iteration 9818 Loss: 0.0741593074211057\n",
      "Iteration 9819 Loss: 0.07415917681983678\n",
      "Iteration 9820 Loss: 0.07415904622606762\n",
      "Iteration 9821 Loss: 0.07415891563979785\n",
      "Iteration 9822 Loss: 0.07415878506102705\n",
      "Iteration 9823 Loss: 0.07415865448975474\n",
      "Iteration 9824 Loss: 0.07415852392598046\n",
      "Iteration 9825 Loss: 0.0741583933697039\n",
      "Iteration 9826 Loss: 0.0741582628209245\n",
      "Iteration 9827 Loss: 0.07415813227964185\n",
      "Iteration 9828 Loss: 0.07415800174585563\n",
      "Iteration 9829 Loss: 0.07415787121956521\n",
      "Iteration 9830 Loss: 0.07415774070077044\n",
      "Iteration 9831 Loss: 0.07415761018947063\n",
      "Iteration 9832 Loss: 0.0741574796856654\n",
      "Iteration 9833 Loss: 0.07415734918935449\n",
      "Iteration 9834 Loss: 0.0741572187005372\n",
      "Iteration 9835 Loss: 0.07415708821921337\n",
      "Iteration 9836 Loss: 0.07415695774538236\n",
      "Iteration 9837 Loss: 0.07415682727904385\n",
      "Iteration 9838 Loss: 0.07415669682019732\n",
      "Iteration 9839 Loss: 0.07415656636884249\n",
      "Iteration 9840 Loss: 0.07415643592497881\n",
      "Iteration 9841 Loss: 0.07415630548860581\n",
      "Iteration 9842 Loss: 0.0741561750597232\n",
      "Iteration 9843 Loss: 0.07415604463833045\n",
      "Iteration 9844 Loss: 0.07415591422442715\n",
      "Iteration 9845 Loss: 0.07415578381801284\n",
      "Iteration 9846 Loss: 0.07415565341908714\n",
      "Iteration 9847 Loss: 0.07415552302764963\n",
      "Iteration 9848 Loss: 0.0741553926436998\n",
      "Iteration 9849 Loss: 0.07415526226723729\n",
      "Iteration 9850 Loss: 0.07415513189826166\n",
      "Iteration 9851 Loss: 0.07415500153677244\n",
      "Iteration 9852 Loss: 0.07415487118276924\n",
      "Iteration 9853 Loss: 0.07415474083625155\n",
      "Iteration 9854 Loss: 0.07415461049721912\n",
      "Iteration 9855 Loss: 0.07415448016567132\n",
      "Iteration 9856 Loss: 0.07415434984160782\n",
      "Iteration 9857 Loss: 0.07415421952502818\n",
      "Iteration 9858 Loss: 0.074154089215932\n",
      "Iteration 9859 Loss: 0.07415395891431875\n",
      "Iteration 9860 Loss: 0.07415382862018809\n",
      "Iteration 9861 Loss: 0.0741536983335395\n",
      "Iteration 9862 Loss: 0.0741535680543727\n",
      "Iteration 9863 Loss: 0.07415343778268713\n",
      "Iteration 9864 Loss: 0.07415330751848237\n",
      "Iteration 9865 Loss: 0.0741531772617581\n",
      "Iteration 9866 Loss: 0.07415304701251374\n",
      "Iteration 9867 Loss: 0.07415291677074895\n",
      "Iteration 9868 Loss: 0.07415278653646326\n",
      "Iteration 9869 Loss: 0.07415265630965626\n",
      "Iteration 9870 Loss: 0.07415252609032752\n",
      "Iteration 9871 Loss: 0.07415239587847662\n",
      "Iteration 9872 Loss: 0.07415226567410312\n",
      "Iteration 9873 Loss: 0.07415213547720659\n",
      "Iteration 9874 Loss: 0.07415200528778654\n",
      "Iteration 9875 Loss: 0.07415187510584266\n",
      "Iteration 9876 Loss: 0.07415174493137439\n",
      "Iteration 9877 Loss: 0.07415161476438148\n",
      "Iteration 9878 Loss: 0.07415148460486327\n",
      "Iteration 9879 Loss: 0.07415135445281948\n",
      "Iteration 9880 Loss: 0.07415122430824968\n",
      "Iteration 9881 Loss: 0.0741510941711534\n",
      "Iteration 9882 Loss: 0.07415096404153018\n",
      "Iteration 9883 Loss: 0.07415083391937964\n",
      "Iteration 9884 Loss: 0.07415070380470132\n",
      "Iteration 9885 Loss: 0.07415057369749478\n",
      "Iteration 9886 Loss: 0.07415044359775971\n",
      "Iteration 9887 Loss: 0.07415031350549556\n",
      "Iteration 9888 Loss: 0.07415018342070187\n",
      "Iteration 9889 Loss: 0.07415005334337835\n",
      "Iteration 9890 Loss: 0.07414992327352443\n",
      "Iteration 9891 Loss: 0.07414979321113971\n",
      "Iteration 9892 Loss: 0.07414966315622386\n",
      "Iteration 9893 Loss: 0.07414953310877631\n",
      "Iteration 9894 Loss: 0.07414940306879675\n",
      "Iteration 9895 Loss: 0.07414927303628467\n",
      "Iteration 9896 Loss: 0.07414914301123968\n",
      "Iteration 9897 Loss: 0.07414901299366135\n",
      "Iteration 9898 Loss: 0.07414888298354923\n",
      "Iteration 9899 Loss: 0.07414875298090293\n",
      "Iteration 9900 Loss: 0.07414862298572196\n",
      "Iteration 9901 Loss: 0.07414849299800592\n",
      "Iteration 9902 Loss: 0.07414836301775439\n",
      "Iteration 9903 Loss: 0.07414823304496695\n",
      "Iteration 9904 Loss: 0.07414810307964313\n",
      "Iteration 9905 Loss: 0.07414797312178256\n",
      "Iteration 9906 Loss: 0.07414784317138473\n",
      "Iteration 9907 Loss: 0.07414771322844926\n",
      "Iteration 9908 Loss: 0.07414758329297574\n",
      "Iteration 9909 Loss: 0.07414745336496376\n",
      "Iteration 9910 Loss: 0.0741473234444128\n",
      "Iteration 9911 Loss: 0.07414719353132246\n",
      "Iteration 9912 Loss: 0.07414706362569236\n",
      "Iteration 9913 Loss: 0.07414693372752205\n",
      "Iteration 9914 Loss: 0.07414680383681108\n",
      "Iteration 9915 Loss: 0.07414667395355903\n",
      "Iteration 9916 Loss: 0.07414654407776547\n",
      "Iteration 9917 Loss: 0.07414641420942995\n",
      "Iteration 9918 Loss: 0.07414628434855212\n",
      "Iteration 9919 Loss: 0.07414615449513148\n",
      "Iteration 9920 Loss: 0.07414602464916761\n",
      "Iteration 9921 Loss: 0.07414589481066006\n",
      "Iteration 9922 Loss: 0.07414576497960848\n",
      "Iteration 9923 Loss: 0.0741456351560124\n",
      "Iteration 9924 Loss: 0.07414550533987135\n",
      "Iteration 9925 Loss: 0.07414537553118491\n",
      "Iteration 9926 Loss: 0.07414524572995276\n",
      "Iteration 9927 Loss: 0.07414511593617434\n",
      "Iteration 9928 Loss: 0.07414498614984925\n",
      "Iteration 9929 Loss: 0.07414485637097715\n",
      "Iteration 9930 Loss: 0.0741447265995574\n",
      "Iteration 9931 Loss: 0.07414459683558977\n",
      "Iteration 9932 Loss: 0.07414446707907386\n",
      "Iteration 9933 Loss: 0.07414433733000911\n",
      "Iteration 9934 Loss: 0.07414420758839509\n",
      "Iteration 9935 Loss: 0.0741440778542315\n",
      "Iteration 9936 Loss: 0.07414394812751776\n",
      "Iteration 9937 Loss: 0.0741438184082535\n",
      "Iteration 9938 Loss: 0.07414368869643836\n",
      "Iteration 9939 Loss: 0.07414355899207185\n",
      "Iteration 9940 Loss: 0.07414342929515352\n",
      "Iteration 9941 Loss: 0.07414329960568297\n",
      "Iteration 9942 Loss: 0.0741431699236598\n",
      "Iteration 9943 Loss: 0.07414304024908348\n",
      "Iteration 9944 Loss: 0.0741429105819537\n",
      "Iteration 9945 Loss: 0.07414278092226996\n",
      "Iteration 9946 Loss: 0.0741426512700319\n",
      "Iteration 9947 Loss: 0.07414252162523903\n",
      "Iteration 9948 Loss: 0.07414239198789094\n",
      "Iteration 9949 Loss: 0.07414226235798717\n",
      "Iteration 9950 Loss: 0.07414213273552736\n",
      "Iteration 9951 Loss: 0.07414200312051104\n",
      "Iteration 9952 Loss: 0.07414187351293777\n",
      "Iteration 9953 Loss: 0.07414174391280717\n",
      "Iteration 9954 Loss: 0.07414161432011875\n",
      "Iteration 9955 Loss: 0.0741414847348721\n",
      "Iteration 9956 Loss: 0.0741413551570668\n",
      "Iteration 9957 Loss: 0.07414122558670251\n",
      "Iteration 9958 Loss: 0.07414109602377865\n",
      "Iteration 9959 Loss: 0.07414096646829485\n",
      "Iteration 9960 Loss: 0.07414083692025072\n",
      "Iteration 9961 Loss: 0.07414070737964581\n",
      "Iteration 9962 Loss: 0.07414057784647968\n",
      "Iteration 9963 Loss: 0.07414044832075195\n",
      "Iteration 9964 Loss: 0.07414031880246211\n",
      "Iteration 9965 Loss: 0.07414018929160977\n",
      "Iteration 9966 Loss: 0.07414005978819448\n",
      "Iteration 9967 Loss: 0.07413993029221586\n",
      "Iteration 9968 Loss: 0.07413980080367351\n",
      "Iteration 9969 Loss: 0.07413967132256694\n",
      "Iteration 9970 Loss: 0.07413954184889568\n",
      "Iteration 9971 Loss: 0.07413941238265943\n",
      "Iteration 9972 Loss: 0.07413928292385764\n",
      "Iteration 9973 Loss: 0.0741391534724899\n",
      "Iteration 9974 Loss: 0.0741390240285559\n",
      "Iteration 9975 Loss: 0.07413889459205507\n",
      "Iteration 9976 Loss: 0.07413876516298709\n",
      "Iteration 9977 Loss: 0.07413863574135147\n",
      "Iteration 9978 Loss: 0.07413850632714775\n",
      "Iteration 9979 Loss: 0.07413837692037564\n",
      "Iteration 9980 Loss: 0.07413824752103448\n",
      "Iteration 9981 Loss: 0.0741381181291241\n",
      "Iteration 9982 Loss: 0.07413798874464395\n",
      "Iteration 9983 Loss: 0.07413785936759358\n",
      "Iteration 9984 Loss: 0.0741377299979726\n",
      "Iteration 9985 Loss: 0.07413760063578057\n",
      "Iteration 9986 Loss: 0.07413747128101707\n",
      "Iteration 9987 Loss: 0.07413734193368164\n",
      "Iteration 9988 Loss: 0.07413721259377393\n",
      "Iteration 9989 Loss: 0.07413708326129347\n",
      "Iteration 9990 Loss: 0.0741369539362398\n",
      "Iteration 9991 Loss: 0.07413682461861255\n",
      "Iteration 9992 Loss: 0.07413669530841123\n",
      "Iteration 9993 Loss: 0.07413656600563544\n",
      "Iteration 9994 Loss: 0.0741364367102848\n",
      "Iteration 9995 Loss: 0.07413630742235884\n",
      "Iteration 9996 Loss: 0.07413617814185712\n",
      "Iteration 9997 Loss: 0.07413604886877925\n",
      "Iteration 9998 Loss: 0.0741359196031248\n",
      "Iteration 9999 Loss: 0.07413579034489323\n"
     ]
    }
   ],
   "source": [
    "model = PolynomialRegression(degrees=[2], lr=0.01, n_iters=10000)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06558837",
   "metadata": {},
   "source": [
    "### Find The Mean Squared Error to check how good our model is performing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "784e73c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.08273212786237422\n"
     ]
    }
   ],
   "source": [
    "def mse(y_test, predictions):\n",
    "        return np.mean((y_test - predictions)**2)\n",
    "\n",
    "mse = mse(y_test, predictions)\n",
    "print(\"Mean Squared Error:\",mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ca4eb",
   "metadata": {},
   "source": [
    "**This model is preforming well as the MSE value is closer to 0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "21f767ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_train, y_train, color='blue', marker='o', s = 10)\n",
    "plt.scatter(X_test, predictions, color='red', marker='o', s = 10)\n",
    "plt.legend([\"Data\", \"Polynomial predictions\"])\n",
    "plt.xlabel('X (Input)')\n",
    "plt.ylabel('y (target) ')\n",
    "plt.title('Polynomial Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8f80f2",
   "metadata": {},
   "source": [
    "### Calculate R2 Score to check how good is our fitting the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e2af9676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.9726705321657484\n"
     ]
    }
   ],
   "source": [
    "r2 = model.r2_score(y_test, predictions)\n",
    "print(f'R2 Score: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2959ad21",
   "metadata": {},
   "source": [
    "**An R2 score of 0.97 indicates that 97% of the variance in our dependent variable is explained by our independent variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097ef309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
