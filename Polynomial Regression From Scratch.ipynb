{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ef090f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a59af1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X = np.random.rand(100,1)\n",
    "y = 2 * (4 ** X) + np.random.rand(100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "36c8fed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X, y, s=10, color='blue', marker='o')\n",
    "plt.title('Non-linear Data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a342f69",
   "metadata": {},
   "source": [
    "### Training the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8ebb2d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss Function (MSE)\n",
    "def loss(y,y_pred):\n",
    "    return np.mean((y_pred - y)**2)\n",
    "\n",
    "#Calculating Gradients\n",
    "def gradients(X, y, y_pred):\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    dw = (1/n_samples)*np.dot(X.T,(y_pred - y))\n",
    "    db = (1/n_samples)*np.sum(y_pred - y)\n",
    "    \n",
    "    return dw,db\n",
    "\n",
    "#Copy the values from X to t and as per the degree provided in the list find it exponent\n",
    "#example: degree=[3] then X -> X^2 -> X^3\n",
    "def x_transform(X, degrees):\n",
    "    t = X.copy()\n",
    "    \n",
    "    for i in degrees:\n",
    "        X = np.append(X, t**i, axis=1)\n",
    "    return X\n",
    "\n",
    "#Fit the model\n",
    "def fit(X, y, degrees, n_iters, lr):\n",
    "    x = x_transform(X, degrees)\n",
    "    n_samples, n_features = x.shape\n",
    "    \n",
    "    w = np.zeros((n_features,1))\n",
    "    b = 0\n",
    "    \n",
    "    #Reshaping y to have one column and as many rows as needed\n",
    "    y = y.reshape(n_samples,1)\n",
    "    \n",
    "    #Storing the losses\n",
    "    losses = [] \n",
    "    \n",
    "    for n_iter in range(n_iters):\n",
    "        y_pred = np.dot(x, w) + b\n",
    "        \n",
    "        dw, db = gradients(x, y, y_pred)\n",
    "        \n",
    "        w = w - lr*dw\n",
    "        b = b - lr*db\n",
    "        \n",
    "        l = loss(y, y_pred)\n",
    "        losses.append(l)\n",
    "        print('Iteration',n_iter,'Loss:',l)\n",
    "        \n",
    "    return w,b,losses\n",
    "\n",
    "def predict(X, w, b, degrees):\n",
    "    x1 = x_transform(X, degrees)\n",
    "    y_pred = np.dot(x1, w) + b\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771caeb6",
   "metadata": {},
   "source": [
    "### Fitting the model and calculating the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f9807967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 Loss: 24.84311984444589\n",
      "Iteration 1 Loss: 24.188446436532473\n",
      "Iteration 2 Loss: 23.551413590854782\n",
      "Iteration 3 Loss: 22.931545139271403\n",
      "Iteration 4 Loss: 22.328377768784616\n",
      "Iteration 5 Loss: 21.741460674484248\n",
      "Iteration 6 Loss: 21.170355221861087\n",
      "Iteration 7 Loss: 20.61463461823721\n",
      "Iteration 8 Loss: 20.073883593066604\n",
      "Iteration 9 Loss: 19.547698086867083\n",
      "Iteration 10 Loss: 19.035684948550113\n",
      "Iteration 11 Loss: 18.537461640922075\n",
      "Iteration 12 Loss: 18.052655954136167\n",
      "Iteration 13 Loss: 17.58090572688045\n",
      "Iteration 14 Loss: 17.121858575093093\n",
      "Iteration 15 Loss: 16.675171628001625\n",
      "Iteration 16 Loss: 16.240511271288455\n",
      "Iteration 17 Loss: 15.81755289719029\n",
      "Iteration 18 Loss: 15.405980661344223\n",
      "Iteration 19 Loss: 15.00548724619843\n",
      "Iteration 20 Loss: 14.615773630810063\n",
      "Iteration 21 Loss: 14.236548866858135\n",
      "Iteration 22 Loss: 13.867529860703442\n",
      "Iteration 23 Loss: 13.508441161332325\n",
      "Iteration 24 Loss: 13.159014754025424\n",
      "Iteration 25 Loss: 12.818989859596844\n",
      "Iteration 26 Loss: 12.488112739053381\n",
      "Iteration 27 Loss: 12.16613650352742\n",
      "Iteration 28 Loss: 11.852820929341231\n",
      "Iteration 29 Loss: 11.54793227806399\n",
      "Iteration 30 Loss: 11.251243121426855\n",
      "Iteration 31 Loss: 10.962532170964877\n",
      "Iteration 32 Loss: 10.681584112258177\n",
      "Iteration 33 Loss: 10.40818944364813\n",
      "Iteration 34 Loss: 10.1421443193079\n",
      "Iteration 35 Loss: 9.88325039654959\n",
      "Iteration 36 Loss: 9.631314687253761\n",
      "Iteration 37 Loss: 9.386149413309976\n",
      "Iteration 38 Loss: 9.147571865960067\n",
      "Iteration 39 Loss: 8.915404268938806\n",
      "Iteration 40 Loss: 8.689473645309404\n",
      "Iteration 41 Loss: 8.469611687894146\n",
      "Iteration 42 Loss: 8.255654633203042\n",
      "Iteration 43 Loss: 8.047443138766129\n",
      "Iteration 44 Loss: 7.844822163777435\n",
      "Iteration 45 Loss: 7.64764085296132\n",
      "Iteration 46 Loss: 7.455752423574073\n",
      "Iteration 47 Loss: 7.269014055456223\n",
      "Iteration 48 Loss: 7.087286784053146\n",
      "Iteration 49 Loss: 6.910435396323846\n",
      "Iteration 50 Loss: 6.738328329459931\n",
      "Iteration 51 Loss: 6.570837572338944\n",
      "Iteration 52 Loss: 6.407838569638171\n",
      "Iteration 53 Loss: 6.249210128537165\n",
      "Iteration 54 Loss: 6.094834327939053\n",
      "Iteration 55 Loss: 5.944596430142647\n",
      "Iteration 56 Loss: 5.798384794899164\n",
      "Iteration 57 Loss: 5.6560907957892335\n",
      "Iteration 58 Loss: 5.5176087388574775\n",
      "Iteration 59 Loss: 5.382835783443783\n",
      "Iteration 60 Loss: 5.251671865151905\n",
      "Iteration 61 Loss: 5.124019620897745\n",
      "Iteration 62 Loss: 4.999784315981121\n",
      "Iteration 63 Loss: 4.878873773126425\n",
      "Iteration 64 Loss: 4.761198303438992\n",
      "Iteration 65 Loss: 4.646670639225494\n",
      "Iteration 66 Loss: 4.5352058686279895\n",
      "Iteration 67 Loss: 4.426721372022705\n",
      "Iteration 68 Loss: 4.321136760135883\n",
      "Iteration 69 Loss: 4.218373813830346\n",
      "Iteration 70 Loss: 4.118356425517675\n",
      "Iteration 71 Loss: 4.021010542152103\n",
      "Iteration 72 Loss: 3.9262641097634354\n",
      "Iteration 73 Loss: 3.834047019487447\n",
      "Iteration 74 Loss: 3.7442910550533046\n",
      "Iteration 75 Loss: 3.656929841688718\n",
      "Iteration 76 Loss: 3.571898796404503\n",
      "Iteration 77 Loss: 3.4891350796213483\n",
      "Iteration 78 Loss: 3.408577548102536\n",
      "Iteration 79 Loss: 3.330166709157351\n",
      "Iteration 80 Loss: 3.2538446760808966\n",
      "Iteration 81 Loss: 3.179555124796902\n",
      "Iteration 82 Loss: 3.107243251671092\n",
      "Iteration 83 Loss: 3.036855732463459\n",
      "Iteration 84 Loss: 2.968340682388743\n",
      "Iteration 85 Loss: 2.9016476172551684\n",
      "Iteration 86 Loss: 2.836727415652329\n",
      "Iteration 87 Loss: 2.7735322821599215\n",
      "Iteration 88 Loss: 2.7120157115497325\n",
      "Iteration 89 Loss: 2.6521324539540934\n",
      "Iteration 90 Loss: 2.5938384809746924\n",
      "Iteration 91 Loss: 2.5370909527063663\n",
      "Iteration 92 Loss: 2.481848185651163\n",
      "Iteration 93 Loss: 2.4280696214986417\n",
      "Iteration 94 Loss: 2.3757157967490308\n",
      "Iteration 95 Loss: 2.3247483131564817\n",
      "Iteration 96 Loss: 2.275129808970278\n",
      "Iteration 97 Loss: 2.226823930952456\n",
      "Iteration 98 Loss: 2.1797953071508873\n",
      "Iteration 99 Loss: 2.13400952040741\n",
      "Iteration 100 Loss: 2.089433082581179\n",
      "Iteration 101 Loss: 2.046033409467914\n",
      "Iteration 102 Loss: 2.003778796396273\n",
      "Iteration 103 Loss: 1.962638394483049\n",
      "Iteration 104 Loss: 1.9225821875294336\n",
      "Iteration 105 Loss: 1.8835809695409995\n",
      "Iteration 106 Loss: 1.8456063228546105\n",
      "Iteration 107 Loss: 1.8086305968558287\n",
      "Iteration 108 Loss: 1.7726268872709106\n",
      "Iteration 109 Loss: 1.7375690160178605\n",
      "Iteration 110 Loss: 1.7034315116014551\n",
      "Iteration 111 Loss: 1.6701895900375563\n",
      "Iteration 112 Loss: 1.6378191362924088\n",
      "Iteration 113 Loss: 1.6062966862230397\n",
      "Iteration 114 Loss: 1.5755994090052154\n",
      "Iteration 115 Loss: 1.5457050900358027\n",
      "Iteration 116 Loss: 1.5165921142967154\n",
      "Iteration 117 Loss: 1.4882394501679985\n",
      "Iteration 118 Loss: 1.4606266336779177\n",
      "Iteration 119 Loss: 1.4337337531782484\n",
      "Iteration 120 Loss: 1.4075414344333013\n",
      "Iteration 121 Loss: 1.3820308261115017\n",
      "Iteration 122 Loss: 1.3571835856686605\n",
      "Iteration 123 Loss: 1.3329818656123564\n",
      "Iteration 124 Loss: 1.3094083001371528\n",
      "Iteration 125 Loss: 1.286445992120617\n",
      "Iteration 126 Loss: 1.2640785004704305\n",
      "Iteration 127 Loss: 1.2422898278130747\n",
      "Iteration 128 Loss: 1.221064408514909\n",
      "Iteration 129 Loss: 1.2003870970266342\n",
      "Iteration 130 Loss: 1.1802431565424363\n",
      "Iteration 131 Loss: 1.1606182479652942\n",
      "Iteration 132 Loss: 1.141498419170204\n",
      "Iteration 133 Loss: 1.122870094557265\n",
      "Iteration 134 Loss: 1.104720064886804\n",
      "Iteration 135 Loss: 1.0870354773889264\n",
      "Iteration 136 Loss: 1.0698038261400833\n",
      "Iteration 137 Loss: 1.0530129426994481\n",
      "Iteration 138 Loss: 1.0366509869980811\n",
      "Iteration 139 Loss: 1.020706438474072\n",
      "Iteration 140 Loss: 1.0051680874470024\n",
      "Iteration 141 Loss: 0.9900250267252748\n",
      "Iteration 142 Loss: 0.9752666434400332\n",
      "Iteration 143 Loss: 0.9608826110995315\n",
      "Iteration 144 Loss: 0.9468628818580227\n",
      "Iteration 145 Loss: 0.9331976789933656\n",
      "Iteration 146 Loss: 0.9198774895877151\n",
      "Iteration 147 Loss: 0.906893057405817\n",
      "Iteration 148 Loss: 0.8942353759655657\n",
      "Iteration 149 Loss: 0.881895681795646\n",
      "Iteration 150 Loss: 0.8698654478751929\n",
      "Iteration 151 Loss: 0.8581363772505692\n",
      "Iteration 152 Loss: 0.8467003968244691\n",
      "Iteration 153 Loss: 0.8355496513127026\n",
      "Iteration 154 Loss: 0.8246764973641296\n",
      "Iteration 155 Loss: 0.8140734978393387\n",
      "Iteration 156 Loss: 0.8037334162437898\n",
      "Iteration 157 Loss: 0.7936492113112438\n",
      "Iteration 158 Loss: 0.7838140317334275\n",
      "Iteration 159 Loss: 0.7742212110319828\n",
      "Iteration 160 Loss: 0.7648642625688576\n",
      "Iteration 161 Loss: 0.7557368746914076\n",
      "Iteration 162 Loss: 0.7468329060085623\n",
      "Iteration 163 Loss: 0.7381463807945248\n",
      "Iteration 164 Loss: 0.7296714845165575\n",
      "Iteration 165 Loss: 0.7214025594835082\n",
      "Iteration 166 Loss: 0.7133341006118111\n",
      "Iteration 167 Loss: 0.7054607513057972\n",
      "Iteration 168 Loss: 0.6977772994492238\n",
      "Iteration 169 Loss: 0.69027867350502\n",
      "Iteration 170 Loss: 0.6829599387203333\n",
      "Iteration 171 Loss: 0.6758162934340248\n",
      "Iteration 172 Loss: 0.6688430654838531\n",
      "Iteration 173 Loss: 0.6620357087106585\n",
      "Iteration 174 Loss: 0.6553897995569186\n",
      "Iteration 175 Loss: 0.6489010337571364\n",
      "Iteration 176 Loss: 0.6425652231175806\n",
      "Iteration 177 Loss: 0.6363782923829606\n",
      "Iteration 178 Loss: 0.6303362761876953\n",
      "Iteration 179 Loss: 0.6244353160894869\n",
      "Iteration 180 Loss: 0.6186716576829826\n",
      "Iteration 181 Loss: 0.6130416477913571\n",
      "Iteration 182 Loss: 0.6075417317337165\n",
      "Iteration 183 Loss: 0.6021684506662727\n",
      "Iteration 184 Loss: 0.5969184389953006\n",
      "Iteration 185 Loss: 0.5917884218599356\n",
      "Iteration 186 Loss: 0.5867752126829315\n",
      "Iteration 187 Loss: 0.5818757107875403\n",
      "Iteration 188 Loss: 0.5770868990787259\n",
      "Iteration 189 Loss: 0.5724058417869852\n",
      "Iteration 190 Loss: 0.5678296822730723\n",
      "Iteration 191 Loss: 0.5633556408919945\n",
      "Iteration 192 Loss: 0.5589810129146714\n",
      "Iteration 193 Loss: 0.5547031665057038\n",
      "Iteration 194 Loss: 0.5505195407557383\n",
      "Iteration 195 Loss: 0.5464276437669489\n",
      "Iteration 196 Loss: 0.5424250507902095\n",
      "Iteration 197 Loss: 0.5385094024125562\n",
      "Iteration 198 Loss: 0.5346784027935776\n",
      "Iteration 199 Loss: 0.5309298179494255\n",
      "Iteration 200 Loss: 0.5272614740831473\n",
      "Iteration 201 Loss: 0.5236712559600946\n",
      "Iteration 202 Loss: 0.5201571053271928\n",
      "Iteration 203 Loss: 0.5167170193748851\n",
      "Iteration 204 Loss: 0.5133490492405989\n",
      "Iteration 205 Loss: 0.5100512985526133\n",
      "Iteration 206 Loss: 0.5068219220132406\n",
      "Iteration 207 Loss: 0.5036591240202548\n",
      "Iteration 208 Loss: 0.500561157325538\n",
      "Iteration 209 Loss: 0.49752632172994127\n",
      "Iteration 210 Loss: 0.4945529628133769\n",
      "Iteration 211 Loss: 0.4916394706991969\n",
      "Iteration 212 Loss: 0.48878427885192927\n",
      "Iteration 213 Loss: 0.4859858629074687\n",
      "Iteration 214 Loss: 0.4832427395348532\n",
      "Iteration 215 Loss: 0.48055346532876514\n",
      "Iteration 216 Loss: 0.4779166357319336\n",
      "Iteration 217 Loss: 0.4753308839866272\n",
      "Iteration 218 Loss: 0.47279488011445386\n",
      "Iteration 219 Loss: 0.47030732992370283\n",
      "Iteration 220 Loss: 0.46786697404348465\n",
      "Iteration 221 Loss: 0.4654725869839468\n",
      "Iteration 222 Loss: 0.4631229762218604\n",
      "Iteration 223 Loss: 0.4608169813108923\n",
      "Iteration 224 Loss: 0.4585534730158986\n",
      "Iteration 225 Loss: 0.4563313524705868\n",
      "Iteration 226 Loss: 0.4541495503579211\n",
      "Iteration 227 Loss: 0.4520070261126518\n",
      "Iteration 228 Loss: 0.4499027671453748\n",
      "Iteration 229 Loss: 0.44783578808753893\n",
      "Iteration 230 Loss: 0.4458051300568343\n",
      "Iteration 231 Loss: 0.4438098599424141\n",
      "Iteration 232 Loss: 0.4418490697094128\n",
      "Iteration 233 Loss: 0.4399218757222383\n",
      "Iteration 234 Loss: 0.4380274180861345\n",
      "Iteration 235 Loss: 0.43616486000651883\n",
      "Iteration 236 Loss: 0.434333387165616\n",
      "Iteration 237 Loss: 0.4325322071159179\n",
      "Iteration 238 Loss: 0.43076054869002206\n",
      "Iteration 239 Loss: 0.42901766142639786\n",
      "Iteration 240 Loss: 0.4273028150106582\n",
      "Iteration 241 Loss: 0.42561529873191345\n",
      "Iteration 242 Loss: 0.4239544209538022\n",
      "Iteration 243 Loss: 0.4223195085998009\n",
      "Iteration 244 Loss: 0.42070990665242997\n",
      "Iteration 245 Loss: 0.41912497766597967\n",
      "Iteration 246 Loss: 0.41756410129238675\n",
      "Iteration 247 Loss: 0.4160266738199162\n",
      "Iteration 248 Loss: 0.41451210772429237\n",
      "Iteration 249 Loss: 0.413019831231949\n",
      "Iteration 250 Loss: 0.41154928789507145\n",
      "Iteration 251 Loss: 0.41009993617810786\n",
      "Iteration 252 Loss: 0.4086712490554458\n",
      "Iteration 253 Loss: 0.4072627136199485\n",
      "Iteration 254 Loss: 0.4058738307020589\n",
      "Iteration 255 Loss: 0.4045041144991869\n",
      "Iteration 256 Loss: 0.40315309221510115\n",
      "Iteration 257 Loss: 0.4018203037090561\n",
      "Iteration 258 Loss: 0.4005053011543904\n",
      "Iteration 259 Loss: 0.3992076487063428\n",
      "Iteration 260 Loss: 0.39792692217883363\n",
      "Iteration 261 Loss: 0.3966627087299742\n",
      "Iteration 262 Loss: 0.3954146065560642\n",
      "Iteration 263 Loss: 0.39418222459384905\n",
      "Iteration 264 Loss: 0.39296518223081905\n",
      "Iteration 265 Loss: 0.39176310902332323\n",
      "Iteration 266 Loss: 0.39057564442229803\n",
      "Iteration 267 Loss: 0.38940243750639586\n",
      "Iteration 268 Loss: 0.38824314672232063\n",
      "Iteration 269 Loss: 0.38709743963217264\n",
      "Iteration 270 Loss: 0.3859649926676121\n",
      "Iteration 271 Loss: 0.38484549089066067\n",
      "Iteration 272 Loss: 0.38373862776095985\n",
      "Iteration 273 Loss: 0.382644104909312\n",
      "Iteration 274 Loss: 0.381561631917334\n",
      "Iteration 275 Loss: 0.3804909261030604\n",
      "Iteration 276 Loss: 0.3794317123123326\n",
      "Iteration 277 Loss: 0.3783837227158208\n",
      "Iteration 278 Loss: 0.37734669661152415\n",
      "Iteration 279 Loss: 0.37632038023260306\n",
      "Iteration 280 Loss: 0.3753045265603976\n",
      "Iteration 281 Loss: 0.3742988951424961\n",
      "Iteration 282 Loss: 0.37330325191571134\n",
      "Iteration 283 Loss: 0.3723173690338379\n",
      "Iteration 284 Loss: 0.3713410247000582\n",
      "Iteration 285 Loss: 0.37037400300387174\n",
      "Iteration 286 Loss: 0.36941609376242773\n",
      "Iteration 287 Loss: 0.36846709236613995\n",
      "Iteration 288 Loss: 0.36752679962846907\n",
      "Iteration 289 Loss: 0.3665950216397588\n",
      "Iteration 290 Loss: 0.3656715696250204\n",
      "Iteration 291 Loss: 0.3647562598055532\n",
      "Iteration 292 Loss: 0.36384891326430113\n",
      "Iteration 293 Loss: 0.3629493558148447\n",
      "Iteration 294 Loss: 0.3620574178739281\n",
      "Iteration 295 Loss: 0.36117293433742836\n",
      "Iteration 296 Loss: 0.3602957444596708\n",
      "Iteration 297 Loss: 0.3594256917360034\n",
      "Iteration 298 Loss: 0.3585626237885407\n",
      "Iteration 299 Loss: 0.35770639225499046\n",
      "Iteration 300 Loss: 0.3568568526804835\n",
      "Iteration 301 Loss: 0.3560138644123208\n",
      "Iteration 302 Loss: 0.3551772904975625\n",
      "Iteration 303 Loss: 0.35434699758338256\n",
      "Iteration 304 Loss: 0.3535228558201105\n",
      "Iteration 305 Loss: 0.35270473876689173\n",
      "Iteration 306 Loss: 0.35189252329989457\n",
      "Iteration 307 Loss: 0.35108608952299397\n",
      "Iteration 308 Loss: 0.35028532068086554\n",
      "Iteration 309 Loss: 0.3494901030744273\n",
      "Iteration 310 Loss: 0.34870032597856115\n",
      "Iteration 311 Loss: 0.3479158815620574\n",
      "Iteration 312 Loss: 0.34713666480971844\n",
      "Iteration 313 Loss: 0.34636257344656485\n",
      "Iteration 314 Loss: 0.34559350786408893\n",
      "Iteration 315 Loss: 0.3448293710484966\n",
      "Iteration 316 Loss: 0.3440700685108871\n",
      "Iteration 317 Loss: 0.3433155082193168\n",
      "Iteration 318 Loss: 0.34256560053269586\n",
      "Iteration 319 Loss: 0.3418202581364719\n",
      "Iteration 320 Loss: 0.34107939598004444\n",
      "Iteration 321 Loss: 0.34034293121587383\n",
      "Iteration 322 Loss: 0.33961078314022886\n",
      "Iteration 323 Loss: 0.33888287313553744\n",
      "Iteration 324 Loss: 0.3381591246142899\n",
      "Iteration 325 Loss: 0.33743946296445854\n",
      "Iteration 326 Loss: 0.33672381549638886\n",
      "Iteration 327 Loss: 0.33601211139112436\n",
      "Iteration 328 Loss: 0.33530428165012593\n",
      "Iteration 329 Loss: 0.33460025904634855\n",
      "Iteration 330 Loss: 0.3338999780766375\n",
      "Iteration 331 Loss: 0.33320337491540963\n",
      "Iteration 332 Loss: 0.3325103873695843\n",
      "Iteration 333 Loss: 0.3318209548347319\n",
      "Iteration 334 Loss: 0.33113501825240277\n",
      "Iteration 335 Loss: 0.3304525200686109\n",
      "Iteration 336 Loss: 0.32977340419343565\n",
      "Iteration 337 Loss: 0.3290976159617148\n",
      "Iteration 338 Loss: 0.32842510209479747\n",
      "Iteration 339 Loss: 0.32775581066332987\n",
      "Iteration 340 Loss: 0.32708969105104424\n",
      "Iteration 341 Loss: 0.32642669391952545\n",
      "Iteration 342 Loss: 0.3257667711739289\n",
      "Iteration 343 Loss: 0.32510987592962254\n",
      "Iteration 344 Loss: 0.32445596247972813\n",
      "Iteration 345 Loss: 0.32380498626354104\n",
      "Iteration 346 Loss: 0.32315690383579904\n",
      "Iteration 347 Loss: 0.32251167283678206\n",
      "Iteration 348 Loss: 0.32186925196321853\n",
      "Iteration 349 Loss: 0.32122960093997555\n",
      "Iteration 350 Loss: 0.32059268049251477\n",
      "Iteration 351 Loss: 0.31995845232008874\n",
      "Iteration 352 Loss: 0.3193268790696631\n",
      "Iteration 353 Loss: 0.31869792431054034\n",
      "Iteration 354 Loss: 0.3180715525096691\n",
      "Iteration 355 Loss: 0.3174477290076193\n",
      "Iteration 356 Loss: 0.3168264199952054\n",
      "Iteration 357 Loss: 0.31620759249073993\n",
      "Iteration 358 Loss: 0.3155912143179012\n",
      "Iteration 359 Loss: 0.31497725408419763\n",
      "Iteration 360 Loss: 0.314365681160013\n",
      "Iteration 361 Loss: 0.3137564656582175\n",
      "Iteration 362 Loss: 0.31314957841432733\n",
      "Iteration 363 Loss: 0.3125449909672009\n",
      "Iteration 364 Loss: 0.3119426755402552\n",
      "Iteration 365 Loss: 0.3113426050231885\n",
      "Iteration 366 Loss: 0.31074475295419623\n",
      "Iteration 367 Loss: 0.3101490935026665\n",
      "Iteration 368 Loss: 0.3095556014523422\n",
      "Iteration 369 Loss: 0.3089642521849371\n",
      "Iteration 370 Loss: 0.30837502166419467\n",
      "Iteration 371 Loss: 0.30778788642037475\n",
      "Iteration 372 Loss: 0.3072028235351611\n",
      "Iteration 373 Loss: 0.30661981062697385\n",
      "Iteration 374 Loss: 0.3060388258366793\n",
      "Iteration 375 Loss: 0.30545984781368407\n",
      "Iteration 376 Loss: 0.30488285570240486\n",
      "Iteration 377 Loss: 0.30430782912910287\n",
      "Iteration 378 Loss: 0.3037347481890733\n",
      "Iteration 379 Loss: 0.3031635934341793\n",
      "Iteration 380 Loss: 0.3025943458607235\n",
      "Iteration 381 Loss: 0.3020269868976451\n",
      "Iteration 382 Loss: 0.3014614983950364\n",
      "Iteration 383 Loss: 0.30089786261296725\n",
      "Iteration 384 Loss: 0.3003360622106129\n",
      "Iteration 385 Loss: 0.2997760802356722\n",
      "Iteration 386 Loss: 0.2992179001140746\n",
      "Iteration 387 Loss: 0.2986615056399601\n",
      "Iteration 388 Loss: 0.2981068809659333\n",
      "Iteration 389 Loss: 0.2975540105935775\n",
      "Iteration 390 Loss: 0.29700287936422604\n",
      "Iteration 391 Loss: 0.2964534724499812\n",
      "Iteration 392 Loss: 0.29590577534497536\n",
      "Iteration 393 Loss: 0.29535977385686807\n",
      "Iteration 394 Loss: 0.29481545409857185\n",
      "Iteration 395 Loss: 0.29427280248020116\n",
      "Iteration 396 Loss: 0.293731805701237\n",
      "Iteration 397 Loss: 0.293192450742906\n",
      "Iteration 398 Loss: 0.2926547248607598\n",
      "Iteration 399 Loss: 0.29211861557745844\n",
      "Iteration 400 Loss: 0.29158411067574663\n",
      "Iteration 401 Loss: 0.29105119819161884\n",
      "Iteration 402 Loss: 0.29051986640766897\n",
      "Iteration 403 Loss: 0.2899901038466189\n",
      "Iteration 404 Loss: 0.28946189926502125\n",
      "Iteration 405 Loss: 0.28893524164713213\n",
      "Iteration 406 Loss: 0.28841012019894857\n",
      "Iteration 407 Loss: 0.2878865243424059\n",
      "Iteration 408 Loss: 0.2873644437097323\n",
      "Iteration 409 Loss: 0.2868438681379549\n",
      "Iteration 410 Loss: 0.2863247876635539\n",
      "Iteration 411 Loss: 0.2858071925172597\n",
      "Iteration 412 Loss: 0.2852910731189916\n",
      "Iteration 413 Loss: 0.28477642007293136\n",
      "Iteration 414 Loss: 0.28426322416273025\n",
      "Iteration 415 Loss: 0.2837514763468441\n",
      "Iteration 416 Loss: 0.2832411677539947\n",
      "Iteration 417 Loss: 0.2827322896787534\n",
      "Iteration 418 Loss: 0.2822248335772417\n",
      "Iteration 419 Loss: 0.28171879106295056\n",
      "Iteration 420 Loss: 0.28121415390266935\n",
      "Iteration 421 Loss: 0.2807109140125255\n",
      "Iteration 422 Loss: 0.2802090634541311\n",
      "Iteration 423 Loss: 0.27970859443083157\n",
      "Iteration 424 Loss: 0.27920949928405664\n",
      "Iteration 425 Loss: 0.2787117704897691\n",
      "Iteration 426 Loss: 0.27821540065500827\n",
      "Iteration 427 Loss: 0.27772038251452674\n",
      "Iteration 428 Loss: 0.2772267089275184\n",
      "Iteration 429 Loss: 0.27673437287443287\n",
      "Iteration 430 Loss: 0.2762433674538763\n",
      "Iteration 431 Loss: 0.27575368587959553\n",
      "Iteration 432 Loss: 0.2752653214775431\n",
      "Iteration 433 Loss: 0.2747782676830206\n",
      "Iteration 434 Loss: 0.27429251803789967\n",
      "Iteration 435 Loss: 0.2738080661879164\n",
      "Iteration 436 Loss: 0.2733249058800398\n",
      "Iteration 437 Loss: 0.2728430309599098\n",
      "Iteration 438 Loss: 0.272362435369344\n",
      "Iteration 439 Loss: 0.27188311314391217\n",
      "Iteration 440 Loss: 0.27140505841057494\n",
      "Iteration 441 Loss: 0.270928265385386\n",
      "Iteration 442 Loss: 0.2704527283712571\n",
      "Iteration 443 Loss: 0.26997844175578034\n",
      "Iteration 444 Loss: 0.2695054000091119\n",
      "Iteration 445 Loss: 0.26903359768191043\n",
      "Iteration 446 Loss: 0.2685630294033314\n",
      "Iteration 447 Loss: 0.26809368987907506\n",
      "Iteration 448 Loss: 0.26762557388948693\n",
      "Iteration 449 Loss: 0.2671586762877086\n",
      "Iteration 450 Loss: 0.26669299199787905\n",
      "Iteration 451 Loss: 0.26622851601338293\n",
      "Iteration 452 Loss: 0.26576524339514657\n",
      "Iteration 453 Loss: 0.26530316926997904\n",
      "Iteration 454 Loss: 0.26484228882895844\n",
      "Iteration 455 Loss: 0.26438259732586056\n",
      "Iteration 456 Loss: 0.26392409007562895\n",
      "Iteration 457 Loss: 0.26346676245288847\n",
      "Iteration 458 Loss: 0.263010609890495\n",
      "Iteration 459 Loss: 0.26255562787812725\n",
      "Iteration 460 Loss: 0.2621018119609134\n",
      "Iteration 461 Loss: 0.26164915773809705\n",
      "Iteration 462 Loss: 0.26119766086173657\n",
      "Iteration 463 Loss: 0.2607473170354397\n",
      "Iteration 464 Loss: 0.2602981220131336\n",
      "Iteration 465 Loss: 0.2598500715978654\n",
      "Iteration 466 Loss: 0.2594031616406357\n",
      "Iteration 467 Loss: 0.25895738803926377\n",
      "Iteration 468 Loss: 0.2585127467372822\n",
      "Iteration 469 Loss: 0.25806923372286095\n",
      "Iteration 470 Loss: 0.25762684502776095\n",
      "Iteration 471 Loss: 0.257185576726314\n",
      "Iteration 472 Loss: 0.25674542493443214\n",
      "Iteration 473 Loss: 0.25630638580864107\n",
      "Iteration 474 Loss: 0.25586845554514026\n",
      "Iteration 475 Loss: 0.25543163037888855\n",
      "Iteration 476 Loss: 0.25499590658271315\n",
      "Iteration 477 Loss: 0.25456128046644294\n",
      "Iteration 478 Loss: 0.2541277483760643\n",
      "Iteration 479 Loss: 0.2536953066928999\n",
      "Iteration 480 Loss: 0.253263951832809\n",
      "Iteration 481 Loss: 0.25283368024540864\n",
      "Iteration 482 Loss: 0.25240448841331603\n",
      "Iteration 483 Loss: 0.25197637285141083\n",
      "Iteration 484 Loss: 0.25154933010611713\n",
      "Iteration 485 Loss: 0.25112335675470343\n",
      "Iteration 486 Loss: 0.2506984494046031\n",
      "Iteration 487 Loss: 0.250274604692751\n",
      "Iteration 488 Loss: 0.24985181928493808\n",
      "Iteration 489 Loss: 0.24943008987518409\n",
      "Iteration 490 Loss: 0.24900941318512526\n",
      "Iteration 491 Loss: 0.24858978596341966\n",
      "Iteration 492 Loss: 0.24817120498516668\n",
      "Iteration 493 Loss: 0.24775366705134347\n",
      "Iteration 494 Loss: 0.24733716898825495\n",
      "Iteration 495 Loss: 0.24692170764699864\n",
      "Iteration 496 Loss: 0.24650727990294416\n",
      "Iteration 497 Loss: 0.24609388265522583\n",
      "Iteration 498 Loss: 0.2456815128262484\n",
      "Iteration 499 Loss: 0.2452701673612065\n",
      "Iteration 500 Loss: 0.24485984322761617\n",
      "Iteration 501 Loss: 0.2444505374148588\n",
      "Iteration 502 Loss: 0.24404224693373722\n",
      "Iteration 503 Loss: 0.24363496881604288\n",
      "Iteration 504 Loss: 0.24322870011413505\n",
      "Iteration 505 Loss: 0.24282343790053054\n",
      "Iteration 506 Loss: 0.24241917926750464\n",
      "Iteration 507 Loss: 0.24201592132670144\n",
      "Iteration 508 Loss: 0.24161366120875585\n",
      "Iteration 509 Loss: 0.24121239606292377\n",
      "Iteration 510 Loss: 0.24081212305672386\n",
      "Iteration 511 Loss: 0.24041283937558655\n",
      "Iteration 512 Loss: 0.2400145422225136\n",
      "Iteration 513 Loss: 0.23961722881774608\n",
      "Iteration 514 Loss: 0.23922089639844085\n",
      "Iteration 515 Loss: 0.23882554221835584\n",
      "Iteration 516 Loss: 0.2384311635475423\n",
      "Iteration 517 Loss: 0.2380377576720475\n",
      "Iteration 518 Loss: 0.23764532189362148\n",
      "Iteration 519 Loss: 0.23725385352943515\n",
      "Iteration 520 Loss: 0.23686334991180338\n",
      "Iteration 521 Loss: 0.23647380838791485\n",
      "Iteration 522 Loss: 0.23608522631957096\n",
      "Iteration 523 Loss: 0.23569760108292986\n",
      "Iteration 524 Loss: 0.23531093006825723\n",
      "Iteration 525 Loss: 0.2349252106796837\n",
      "Iteration 526 Loss: 0.2345404403349687\n",
      "Iteration 527 Loss: 0.23415661646526986\n",
      "Iteration 528 Loss: 0.2337737365149186\n",
      "Iteration 529 Loss: 0.2333917979412012\n",
      "Iteration 530 Loss: 0.23301079821414622\n",
      "Iteration 531 Loss: 0.2326307348163159\n",
      "Iteration 532 Loss: 0.23225160524260424\n",
      "Iteration 533 Loss: 0.23187340700003956\n",
      "Iteration 534 Loss: 0.2314961376075922\n",
      "Iteration 535 Loss: 0.2311197945959865\n",
      "Iteration 536 Loss: 0.23074437550751903\n",
      "Iteration 537 Loss: 0.23036987789587932\n",
      "Iteration 538 Loss: 0.22999629932597718\n",
      "Iteration 539 Loss: 0.22962363737377278\n",
      "Iteration 540 Loss: 0.22925188962611187\n",
      "Iteration 541 Loss: 0.2288810536805647\n",
      "Iteration 542 Loss: 0.22851112714526933\n",
      "Iteration 543 Loss: 0.22814210763877812\n",
      "Iteration 544 Loss: 0.22777399278990956\n",
      "Iteration 545 Loss: 0.22740678023760147\n",
      "Iteration 546 Loss: 0.22704046763077027\n",
      "Iteration 547 Loss: 0.22667505262817209\n",
      "Iteration 548 Loss: 0.22631053289826744\n",
      "Iteration 549 Loss: 0.22594690611909077\n",
      "Iteration 550 Loss: 0.22558416997812059\n",
      "Iteration 551 Loss: 0.22522232217215563\n",
      "Iteration 552 Loss: 0.22486136040719168\n",
      "Iteration 553 Loss: 0.2245012823983028\n",
      "Iteration 554 Loss: 0.2241420858695252\n",
      "Iteration 555 Loss: 0.2237837685537432\n",
      "Iteration 556 Loss: 0.2234263281925793\n",
      "Iteration 557 Loss: 0.22306976253628524\n",
      "Iteration 558 Loss: 0.22271406934363738\n",
      "Iteration 559 Loss: 0.22235924638183352\n",
      "Iteration 560 Loss: 0.2220052914263923\n",
      "Iteration 561 Loss: 0.22165220226105536\n",
      "Iteration 562 Loss: 0.22129997667769175\n",
      "Iteration 563 Loss: 0.22094861247620426\n",
      "Iteration 564 Loss: 0.22059810746443836\n",
      "Iteration 565 Loss: 0.22024845945809304\n",
      "Iteration 566 Loss: 0.21989966628063431\n",
      "Iteration 567 Loss: 0.2195517257632095\n",
      "Iteration 568 Loss: 0.2192046357445652\n",
      "Iteration 569 Loss: 0.21885839407096586\n",
      "Iteration 570 Loss: 0.21851299859611456\n",
      "Iteration 571 Loss: 0.21816844718107598\n",
      "Iteration 572 Loss: 0.21782473769420094\n",
      "Iteration 573 Loss: 0.21748186801105274\n",
      "Iteration 574 Loss: 0.21713983601433454\n",
      "Iteration 575 Loss: 0.21679863959381998\n",
      "Iteration 576 Loss: 0.21645827664628312\n",
      "Iteration 577 Loss: 0.2161187450754325\n",
      "Iteration 578 Loss: 0.2157800427918445\n",
      "Iteration 579 Loss: 0.2154421677128994\n",
      "Iteration 580 Loss: 0.21510511776271868\n",
      "Iteration 581 Loss: 0.21476889087210327\n",
      "Iteration 582 Loss: 0.21443348497847417\n",
      "Iteration 583 Loss: 0.21409889802581283\n",
      "Iteration 584 Loss: 0.21376512796460476\n",
      "Iteration 585 Loss: 0.2134321727517823\n",
      "Iteration 586 Loss: 0.21310003035067063\n",
      "Iteration 587 Loss: 0.21276869873093346\n",
      "Iteration 588 Loss: 0.21243817586852085\n",
      "Iteration 589 Loss: 0.2121084597456171\n",
      "Iteration 590 Loss: 0.21177954835059112\n",
      "Iteration 591 Loss: 0.21145143967794663\n",
      "Iteration 592 Loss: 0.21112413172827477\n",
      "Iteration 593 Loss: 0.21079762250820536\n",
      "Iteration 594 Loss: 0.21047191003036242\n",
      "Iteration 595 Loss: 0.2101469923133176\n",
      "Iteration 596 Loss: 0.2098228673815465\n",
      "Iteration 597 Loss: 0.20949953326538484\n",
      "Iteration 598 Loss: 0.2091769880009867\n",
      "Iteration 599 Loss: 0.20885522963028172\n",
      "Iteration 600 Loss: 0.20853425620093533\n",
      "Iteration 601 Loss: 0.20821406576630824\n",
      "Iteration 602 Loss: 0.20789465638541726\n",
      "Iteration 603 Loss: 0.20757602612289702\n",
      "Iteration 604 Loss: 0.20725817304896255\n",
      "Iteration 605 Loss: 0.20694109523937143\n",
      "Iteration 606 Loss: 0.2066247907753889\n",
      "Iteration 607 Loss: 0.20630925774375122\n",
      "Iteration 608 Loss: 0.20599449423663174\n",
      "Iteration 609 Loss: 0.20568049835160612\n",
      "Iteration 610 Loss: 0.20536726819161896\n",
      "Iteration 611 Loss: 0.2050548018649513\n",
      "Iteration 612 Loss: 0.20474309748518807\n",
      "Iteration 613 Loss: 0.20443215317118624\n",
      "Iteration 614 Loss: 0.20412196704704416\n",
      "Iteration 615 Loss: 0.20381253724207057\n",
      "Iteration 616 Loss: 0.20350386189075528\n",
      "Iteration 617 Loss: 0.20319593913273937\n",
      "Iteration 618 Loss: 0.20288876711278636\n",
      "Iteration 619 Loss: 0.2025823439807532\n",
      "Iteration 620 Loss: 0.20227666789156365\n",
      "Iteration 621 Loss: 0.20197173700518026\n",
      "Iteration 622 Loss: 0.2016675494865769\n",
      "Iteration 623 Loss: 0.2013641035057133\n",
      "Iteration 624 Loss: 0.20106139723750832\n",
      "Iteration 625 Loss: 0.2007594288618147\n",
      "Iteration 626 Loss: 0.2004581965633937\n",
      "Iteration 627 Loss: 0.20015769853189058\n",
      "Iteration 628 Loss: 0.1998579329618103\n",
      "Iteration 629 Loss: 0.19955889805249297\n",
      "Iteration 630 Loss: 0.19926059200809154\n",
      "Iteration 631 Loss: 0.19896301303754746\n",
      "Iteration 632 Loss: 0.19866615935456833\n",
      "Iteration 633 Loss: 0.19837002917760518\n",
      "Iteration 634 Loss: 0.19807462072983117\n",
      "Iteration 635 Loss: 0.1977799322391189\n",
      "Iteration 636 Loss: 0.19748596193801937\n",
      "Iteration 637 Loss: 0.19719270806374084\n",
      "Iteration 638 Loss: 0.1969001688581281\n",
      "Iteration 639 Loss: 0.19660834256764168\n",
      "Iteration 640 Loss: 0.1963172274433382\n",
      "Iteration 641 Loss: 0.19602682174084982\n",
      "Iteration 642 Loss: 0.19573712372036517\n",
      "Iteration 643 Loss: 0.19544813164660962\n",
      "Iteration 644 Loss: 0.19515984378882625\n",
      "Iteration 645 Loss: 0.1948722584207577\n",
      "Iteration 646 Loss: 0.19458537382062635\n",
      "Iteration 647 Loss: 0.19429918827111708\n",
      "Iteration 648 Loss: 0.19401370005935878\n",
      "Iteration 649 Loss: 0.1937289074769067\n",
      "Iteration 650 Loss: 0.19344480881972462\n",
      "Iteration 651 Loss: 0.1931614023881675\n",
      "Iteration 652 Loss: 0.19287868648696477\n",
      "Iteration 653 Loss: 0.1925966594252028\n",
      "Iteration 654 Loss: 0.19231531951630865\n",
      "Iteration 655 Loss: 0.19203466507803346\n",
      "Iteration 656 Loss: 0.19175469443243556\n",
      "Iteration 657 Loss: 0.19147540590586562\n",
      "Iteration 658 Loss: 0.1911967978289492\n",
      "Iteration 659 Loss: 0.1909188685365722\n",
      "Iteration 660 Loss: 0.19064161636786495\n",
      "Iteration 661 Loss: 0.19036503966618626\n",
      "Iteration 662 Loss: 0.19008913677910916\n",
      "Iteration 663 Loss: 0.18981390605840517\n",
      "Iteration 664 Loss: 0.18953934586002993\n",
      "Iteration 665 Loss: 0.18926545454410776\n",
      "Iteration 666 Loss: 0.18899223047491812\n",
      "Iteration 667 Loss: 0.18871967202088022\n",
      "Iteration 668 Loss: 0.18844777755453954\n",
      "Iteration 669 Loss: 0.18817654545255322\n",
      "Iteration 670 Loss: 0.18790597409567614\n",
      "Iteration 671 Loss: 0.1876360618687478\n",
      "Iteration 672 Loss: 0.18736680716067736\n",
      "Iteration 673 Loss: 0.18709820836443158\n",
      "Iteration 674 Loss: 0.18683026387702012\n",
      "Iteration 675 Loss: 0.18656297209948286\n",
      "Iteration 676 Loss: 0.18629633143687702\n",
      "Iteration 677 Loss: 0.18603034029826332\n",
      "Iteration 678 Loss: 0.18576499709669367\n",
      "Iteration 679 Loss: 0.18550030024919845\n",
      "Iteration 680 Loss: 0.18523624817677317\n",
      "Iteration 681 Loss: 0.18497283930436667\n",
      "Iteration 682 Loss: 0.18471007206086804\n",
      "Iteration 683 Loss: 0.18444794487909444\n",
      "Iteration 684 Loss: 0.18418645619577909\n",
      "Iteration 685 Loss: 0.1839256044515591\n",
      "Iteration 686 Loss: 0.1836653880909627\n",
      "Iteration 687 Loss: 0.18340580556239838\n",
      "Iteration 688 Loss: 0.1831468553181421\n",
      "Iteration 689 Loss: 0.18288853581432607\n",
      "Iteration 690 Loss: 0.18263084551092704\n",
      "Iteration 691 Loss: 0.1823737828717545\n",
      "Iteration 692 Loss: 0.18211734636443874\n",
      "Iteration 693 Loss: 0.18186153446042108\n",
      "Iteration 694 Loss: 0.18160634563494035\n",
      "Iteration 695 Loss: 0.18135177836702351\n",
      "Iteration 696 Loss: 0.1810978311394732\n",
      "Iteration 697 Loss: 0.1808445024388574\n",
      "Iteration 698 Loss: 0.18059179075549806\n",
      "Iteration 699 Loss: 0.1803396945834605\n",
      "Iteration 700 Loss: 0.18008821242054232\n",
      "Iteration 701 Loss: 0.1798373427682624\n",
      "Iteration 702 Loss: 0.17958708413185095\n",
      "Iteration 703 Loss: 0.17933743502023775\n",
      "Iteration 704 Loss: 0.17908839394604265\n",
      "Iteration 705 Loss: 0.1788399594255645\n",
      "Iteration 706 Loss: 0.17859212997877097\n",
      "Iteration 707 Loss: 0.17834490412928752\n",
      "Iteration 708 Loss: 0.17809828040438844\n",
      "Iteration 709 Loss: 0.17785225733498514\n",
      "Iteration 710 Loss: 0.17760683345561687\n",
      "Iteration 711 Loss: 0.1773620073044403\n",
      "Iteration 712 Loss: 0.17711777742321927\n",
      "Iteration 713 Loss: 0.17687414235731558\n",
      "Iteration 714 Loss: 0.176631100655678\n",
      "Iteration 715 Loss: 0.17638865087083308\n",
      "Iteration 716 Loss: 0.1761467915588748\n",
      "Iteration 717 Loss: 0.17590552127945538\n",
      "Iteration 718 Loss: 0.17566483859577528\n",
      "Iteration 719 Loss: 0.17542474207457315\n",
      "Iteration 720 Loss: 0.17518523028611682\n",
      "Iteration 721 Loss: 0.1749463018041935\n",
      "Iteration 722 Loss: 0.17470795520609989\n",
      "Iteration 723 Loss: 0.17447018907263345\n",
      "Iteration 724 Loss: 0.17423300198808225\n",
      "Iteration 725 Loss: 0.1739963925402158\n",
      "Iteration 726 Loss: 0.1737603593202761\n",
      "Iteration 727 Loss: 0.1735249009229679\n",
      "Iteration 728 Loss: 0.1732900159464495\n",
      "Iteration 729 Loss: 0.17305570299232403\n",
      "Iteration 730 Loss: 0.17282196066562924\n",
      "Iteration 731 Loss: 0.17258878757482976\n",
      "Iteration 732 Loss: 0.17235618233180702\n",
      "Iteration 733 Loss: 0.17212414355185057\n",
      "Iteration 734 Loss: 0.17189266985364896\n",
      "Iteration 735 Loss: 0.1716617598592811\n",
      "Iteration 736 Loss: 0.17143141219420702\n",
      "Iteration 737 Loss: 0.17120162548725898\n",
      "Iteration 738 Loss: 0.17097239837063313\n",
      "Iteration 739 Loss: 0.17074372947988015\n",
      "Iteration 740 Loss: 0.17051561745389654\n",
      "Iteration 741 Loss: 0.17028806093491664\n",
      "Iteration 742 Loss: 0.17006105856850273\n",
      "Iteration 743 Loss: 0.16983460900353747\n",
      "Iteration 744 Loss: 0.16960871089221466\n",
      "Iteration 745 Loss: 0.1693833628900311\n",
      "Iteration 746 Loss: 0.1691585636557775\n",
      "Iteration 747 Loss: 0.16893431185153024\n",
      "Iteration 748 Loss: 0.16871060614264344\n",
      "Iteration 749 Loss: 0.16848744519773967\n",
      "Iteration 750 Loss: 0.16826482768870185\n",
      "Iteration 751 Loss: 0.16804275229066512\n",
      "Iteration 752 Loss: 0.1678212176820082\n",
      "Iteration 753 Loss: 0.1676002225443452\n",
      "Iteration 754 Loss: 0.16737976556251735\n",
      "Iteration 755 Loss: 0.16715984542458456\n",
      "Iteration 756 Loss: 0.1669404608218178\n",
      "Iteration 757 Loss: 0.1667216104486898\n",
      "Iteration 758 Loss: 0.16650329300286806\n",
      "Iteration 759 Loss: 0.1662855071852062\n",
      "Iteration 760 Loss: 0.16606825169973585\n",
      "Iteration 761 Loss: 0.16585152525365834\n",
      "Iteration 762 Loss: 0.16563532655733745\n",
      "Iteration 763 Loss: 0.1654196543242904\n",
      "Iteration 764 Loss: 0.16520450727118052\n",
      "Iteration 765 Loss: 0.16498988411780946\n",
      "Iteration 766 Loss: 0.16477578358710818\n",
      "Iteration 767 Loss: 0.16456220440513059\n",
      "Iteration 768 Loss: 0.16434914530104422\n",
      "Iteration 769 Loss: 0.16413660500712332\n",
      "Iteration 770 Loss: 0.16392458225874085\n",
      "Iteration 771 Loss: 0.16371307579436004\n",
      "Iteration 772 Loss: 0.16350208435552782\n",
      "Iteration 773 Loss: 0.16329160668686588\n",
      "Iteration 774 Loss: 0.16308164153606353\n",
      "Iteration 775 Loss: 0.16287218765387032\n",
      "Iteration 776 Loss: 0.16266324379408725\n",
      "Iteration 777 Loss: 0.16245480871356055\n",
      "Iteration 778 Loss: 0.1622468811721729\n",
      "Iteration 779 Loss: 0.16203945993283658\n",
      "Iteration 780 Loss: 0.16183254376148531\n",
      "Iteration 781 Loss: 0.16162613142706717\n",
      "Iteration 782 Loss: 0.16142022170153697\n",
      "Iteration 783 Loss: 0.16121481335984839\n",
      "Iteration 784 Loss: 0.1610099051799468\n",
      "Iteration 785 Loss: 0.160805495942762\n",
      "Iteration 786 Loss: 0.1606015844322003\n",
      "Iteration 787 Loss: 0.16039816943513752\n",
      "Iteration 788 Loss: 0.16019524974141125\n",
      "Iteration 789 Loss: 0.15999282414381397\n",
      "Iteration 790 Loss: 0.159790891438085\n",
      "Iteration 791 Loss: 0.15958945042290382\n",
      "Iteration 792 Loss: 0.15938849989988252\n",
      "Iteration 793 Loss: 0.15918803867355844\n",
      "Iteration 794 Loss: 0.15898806555138692\n",
      "Iteration 795 Loss: 0.15878857934373422\n",
      "Iteration 796 Loss: 0.15858957886387015\n",
      "Iteration 797 Loss: 0.15839106292796087\n",
      "Iteration 798 Loss: 0.158193030355062\n",
      "Iteration 799 Loss: 0.15799547996711097\n",
      "Iteration 800 Loss: 0.15779841058892025\n",
      "Iteration 801 Loss: 0.15760182104817003\n",
      "Iteration 802 Loss: 0.15740571017540148\n",
      "Iteration 803 Loss: 0.1572100768040091\n",
      "Iteration 804 Loss: 0.15701491977023424\n",
      "Iteration 805 Loss: 0.1568202379131577\n",
      "Iteration 806 Loss: 0.15662603007469272\n",
      "Iteration 807 Loss: 0.15643229509957834\n",
      "Iteration 808 Loss: 0.15623903183537194\n",
      "Iteration 809 Loss: 0.15604623913244262\n",
      "Iteration 810 Loss: 0.15585391584396413\n",
      "Iteration 811 Loss: 0.1556620608259081\n",
      "Iteration 812 Loss: 0.1554706729370368\n",
      "Iteration 813 Loss: 0.1552797510388967\n",
      "Iteration 814 Loss: 0.15508929399581128\n",
      "Iteration 815 Loss: 0.15489930067487434\n",
      "Iteration 816 Loss: 0.15470976994594307\n",
      "Iteration 817 Loss: 0.1545207006816313\n",
      "Iteration 818 Loss: 0.15433209175730284\n",
      "Iteration 819 Loss: 0.15414394205106452\n",
      "Iteration 820 Loss: 0.15395625044375966\n",
      "Iteration 821 Loss: 0.15376901581896094\n",
      "Iteration 822 Loss: 0.15358223706296403\n",
      "Iteration 823 Loss: 0.15339591306478095\n",
      "Iteration 824 Loss: 0.1532100427161332\n",
      "Iteration 825 Loss: 0.15302462491144492\n",
      "Iteration 826 Loss: 0.15283965854783682\n",
      "Iteration 827 Loss: 0.15265514252511905\n",
      "Iteration 828 Loss: 0.15247107574578464\n",
      "Iteration 829 Loss: 0.15228745711500333\n",
      "Iteration 830 Loss: 0.1521042855406144\n",
      "Iteration 831 Loss: 0.15192155993312054\n",
      "Iteration 832 Loss: 0.15173927920568114\n",
      "Iteration 833 Loss: 0.151557442274106\n",
      "Iteration 834 Loss: 0.15137604805684834\n",
      "Iteration 835 Loss: 0.15119509547499893\n",
      "Iteration 836 Loss: 0.1510145834522789\n",
      "Iteration 837 Loss: 0.15083451091503414\n",
      "Iteration 838 Loss: 0.15065487679222822\n",
      "Iteration 839 Loss: 0.15047568001543613\n",
      "Iteration 840 Loss: 0.150296919518838\n",
      "Iteration 841 Loss: 0.15011859423921256\n",
      "Iteration 842 Loss: 0.14994070311593094\n",
      "Iteration 843 Loss: 0.14976324509095018\n",
      "Iteration 844 Loss: 0.14958621910880693\n",
      "Iteration 845 Loss: 0.14940962411661118\n",
      "Iteration 846 Loss: 0.14923345906403995\n",
      "Iteration 847 Loss: 0.14905772290333086\n",
      "Iteration 848 Loss: 0.14888241458927604\n",
      "Iteration 849 Loss: 0.14870753307921572\n",
      "Iteration 850 Loss: 0.1485330773330323\n",
      "Iteration 851 Loss: 0.1483590463131436\n",
      "Iteration 852 Loss: 0.14818543898449726\n",
      "Iteration 853 Loss: 0.14801225431456413\n",
      "Iteration 854 Loss: 0.1478394912733321\n",
      "Iteration 855 Loss: 0.1476671488333001\n",
      "Iteration 856 Loss: 0.14749522596947212\n",
      "Iteration 857 Loss: 0.14732372165935068\n",
      "Iteration 858 Loss: 0.1471526348829308\n",
      "Iteration 859 Loss: 0.1469819646226944\n",
      "Iteration 860 Loss: 0.14681170986360342\n",
      "Iteration 861 Loss: 0.14664186959309455\n",
      "Iteration 862 Loss: 0.14647244280107238\n",
      "Iteration 863 Loss: 0.14630342847990407\n",
      "Iteration 864 Loss: 0.14613482562441296\n",
      "Iteration 865 Loss: 0.14596663323187267\n",
      "Iteration 866 Loss: 0.14579885030200118\n",
      "Iteration 867 Loss: 0.14563147583695446\n",
      "Iteration 868 Loss: 0.14546450884132114\n",
      "Iteration 869 Loss: 0.14529794832211615\n",
      "Iteration 870 Loss: 0.14513179328877474\n",
      "Iteration 871 Loss: 0.14496604275314684\n",
      "Iteration 872 Loss: 0.14480069572949106\n",
      "Iteration 873 Loss: 0.1446357512344686\n",
      "Iteration 874 Loss: 0.14447120828713778\n",
      "Iteration 875 Loss: 0.14430706590894768\n",
      "Iteration 876 Loss: 0.14414332312373282\n",
      "Iteration 877 Loss: 0.14397997895770706\n",
      "Iteration 878 Loss: 0.14381703243945773\n",
      "Iteration 879 Loss: 0.1436544825999398\n",
      "Iteration 880 Loss: 0.14349232847247045\n",
      "Iteration 881 Loss: 0.14333056909272293\n",
      "Iteration 882 Loss: 0.14316920349872103\n",
      "Iteration 883 Loss: 0.1430082307308333\n",
      "Iteration 884 Loss: 0.14284764983176698\n",
      "Iteration 885 Loss: 0.14268745984656295\n",
      "Iteration 886 Loss: 0.14252765982258947\n",
      "Iteration 887 Loss: 0.14236824880953677\n",
      "Iteration 888 Loss: 0.14220922585941131\n",
      "Iteration 889 Loss: 0.14205059002653003\n",
      "Iteration 890 Loss: 0.14189234036751483\n",
      "Iteration 891 Loss: 0.14173447594128719\n",
      "Iteration 892 Loss: 0.14157699580906194\n",
      "Iteration 893 Loss: 0.14141989903434213\n",
      "Iteration 894 Loss: 0.14126318468291335\n",
      "Iteration 895 Loss: 0.1411068518228381\n",
      "Iteration 896 Loss: 0.14095089952445028\n",
      "Iteration 897 Loss: 0.14079532686034965\n",
      "Iteration 898 Loss: 0.1406401329053962\n",
      "Iteration 899 Loss: 0.14048531673670492\n",
      "Iteration 900 Loss: 0.14033087743363992\n",
      "Iteration 901 Loss: 0.1401768140778091\n",
      "Iteration 902 Loss: 0.1400231257530587\n",
      "Iteration 903 Loss: 0.1398698115454681\n",
      "Iteration 904 Loss: 0.13971687054334356\n",
      "Iteration 905 Loss: 0.13956430183721394\n",
      "Iteration 906 Loss: 0.13941210451982433\n",
      "Iteration 907 Loss: 0.13926027768613086\n",
      "Iteration 908 Loss: 0.13910882043329584\n",
      "Iteration 909 Loss: 0.13895773186068155\n",
      "Iteration 910 Loss: 0.13880701106984547\n",
      "Iteration 911 Loss: 0.13865665716453485\n",
      "Iteration 912 Loss: 0.13850666925068103\n",
      "Iteration 913 Loss: 0.13835704643639443\n",
      "Iteration 914 Loss: 0.13820778783195928\n",
      "Iteration 915 Loss: 0.13805889254982803\n",
      "Iteration 916 Loss: 0.13791035970461615\n",
      "Iteration 917 Loss: 0.13776218841309726\n",
      "Iteration 918 Loss: 0.1376143777941971\n",
      "Iteration 919 Loss: 0.137466926968989\n",
      "Iteration 920 Loss: 0.13731983506068823\n",
      "Iteration 921 Loss: 0.13717310119464693\n",
      "Iteration 922 Loss: 0.13702672449834893\n",
      "Iteration 923 Loss: 0.13688070410140427\n",
      "Iteration 924 Loss: 0.1367350391355446\n",
      "Iteration 925 Loss: 0.1365897287346173\n",
      "Iteration 926 Loss: 0.13644477203458064\n",
      "Iteration 927 Loss: 0.13630016817349894\n",
      "Iteration 928 Loss: 0.136155916291537\n",
      "Iteration 929 Loss: 0.136012015530955\n",
      "Iteration 930 Loss: 0.1358684650361037\n",
      "Iteration 931 Loss: 0.13572526395341905\n",
      "Iteration 932 Loss: 0.13558241143141717\n",
      "Iteration 933 Loss: 0.1354399066206895\n",
      "Iteration 934 Loss: 0.1352977486738975\n",
      "Iteration 935 Loss: 0.13515593674576734\n",
      "Iteration 936 Loss: 0.13501446999308572\n",
      "Iteration 937 Loss: 0.1348733475746939\n",
      "Iteration 938 Loss: 0.13473256865148336\n",
      "Iteration 939 Loss: 0.1345921323863903\n",
      "Iteration 940 Loss: 0.134452037944391\n",
      "Iteration 941 Loss: 0.13431228449249683\n",
      "Iteration 942 Loss: 0.13417287119974924\n",
      "Iteration 943 Loss: 0.1340337972372146\n",
      "Iteration 944 Loss: 0.1338950617779794\n",
      "Iteration 945 Loss: 0.13375666399714561\n",
      "Iteration 946 Loss: 0.13361860307182546\n",
      "Iteration 947 Loss: 0.13348087818113627\n",
      "Iteration 948 Loss: 0.13334348850619637\n",
      "Iteration 949 Loss: 0.13320643323011944\n",
      "Iteration 950 Loss: 0.13306971153800984\n",
      "Iteration 951 Loss: 0.13293332261695823\n",
      "Iteration 952 Loss: 0.13279726565603614\n",
      "Iteration 953 Loss: 0.13266153984629134\n",
      "Iteration 954 Loss: 0.132526144380743\n",
      "Iteration 955 Loss: 0.13239107845437723\n",
      "Iteration 956 Loss: 0.1322563412641416\n",
      "Iteration 957 Loss: 0.132121932008941\n",
      "Iteration 958 Loss: 0.1319878498896325\n",
      "Iteration 959 Loss: 0.13185409410902074\n",
      "Iteration 960 Loss: 0.13172066387185302\n",
      "Iteration 961 Loss: 0.13158755838481498\n",
      "Iteration 962 Loss: 0.1314547768565253\n",
      "Iteration 963 Loss: 0.13132231849753123\n",
      "Iteration 964 Loss: 0.13119018252030426\n",
      "Iteration 965 Loss: 0.13105836813923488\n",
      "Iteration 966 Loss: 0.13092687457062813\n",
      "Iteration 967 Loss: 0.13079570103269883\n",
      "Iteration 968 Loss: 0.13066484674556736\n",
      "Iteration 969 Loss: 0.13053431093125428\n",
      "Iteration 970 Loss: 0.1304040928136762\n",
      "Iteration 971 Loss: 0.13027419161864134\n",
      "Iteration 972 Loss: 0.13014460657384425\n",
      "Iteration 973 Loss: 0.1300153369088618\n",
      "Iteration 974 Loss: 0.12988638185514823\n",
      "Iteration 975 Loss: 0.129757740646031\n",
      "Iteration 976 Loss: 0.12962941251670557\n",
      "Iteration 977 Loss: 0.12950139670423166\n",
      "Iteration 978 Loss: 0.12937369244752786\n",
      "Iteration 979 Loss: 0.12924629898736786\n",
      "Iteration 980 Loss: 0.12911921556637534\n",
      "Iteration 981 Loss: 0.12899244142901986\n",
      "Iteration 982 Loss: 0.12886597582161208\n",
      "Iteration 983 Loss: 0.1287398179922996\n",
      "Iteration 984 Loss: 0.12861396719106194\n",
      "Iteration 985 Loss: 0.12848842266970673\n",
      "Iteration 986 Loss: 0.12836318368186483\n",
      "Iteration 987 Loss: 0.12823824948298593\n",
      "Iteration 988 Loss: 0.1281136193303343\n",
      "Iteration 989 Loss: 0.12798929248298424\n",
      "Iteration 990 Loss: 0.1278652682018155\n",
      "Iteration 991 Loss: 0.12774154574950927\n",
      "Iteration 992 Loss: 0.1276181243905435\n",
      "Iteration 993 Loss: 0.12749500339118858\n",
      "Iteration 994 Loss: 0.12737218201950298\n",
      "Iteration 995 Loss: 0.12724965954532905\n",
      "Iteration 996 Loss: 0.1271274352402881\n",
      "Iteration 997 Loss: 0.12700550837777716\n",
      "Iteration 998 Loss: 0.12688387823296327\n",
      "Iteration 999 Loss: 0.12676254408278048\n",
      "Iteration 1000 Loss: 0.1266415052059245\n",
      "Iteration 1001 Loss: 0.12652076088284905\n",
      "Iteration 1002 Loss: 0.1264003103957614\n",
      "Iteration 1003 Loss: 0.1262801530286179\n",
      "Iteration 1004 Loss: 0.12616028806712012\n",
      "Iteration 1005 Loss: 0.12604071479871015\n",
      "Iteration 1006 Loss: 0.1259214325125666\n",
      "Iteration 1007 Loss: 0.12580244049960057\n",
      "Iteration 1008 Loss: 0.12568373805245078\n",
      "Iteration 1009 Loss: 0.12556532446548013\n",
      "Iteration 1010 Loss: 0.12544719903477103\n",
      "Iteration 1011 Loss: 0.12532936105812126\n",
      "Iteration 1012 Loss: 0.12521180983503993\n",
      "Iteration 1013 Loss: 0.1250945446667432\n",
      "Iteration 1014 Loss: 0.12497756485615015\n",
      "Iteration 1015 Loss: 0.12486086970787866\n",
      "Iteration 1016 Loss: 0.12474445852824109\n",
      "Iteration 1017 Loss: 0.12462833062524048\n",
      "Iteration 1018 Loss: 0.12451248530856614\n",
      "Iteration 1019 Loss: 0.12439692188958988\n",
      "Iteration 1020 Loss: 0.12428163968136124\n",
      "Iteration 1021 Loss: 0.1241666379986042\n",
      "Iteration 1022 Loss: 0.12405191615771262\n",
      "Iteration 1023 Loss: 0.12393747347674637\n",
      "Iteration 1024 Loss: 0.12382330927542702\n",
      "Iteration 1025 Loss: 0.12370942287513412\n",
      "Iteration 1026 Loss: 0.1235958135989008\n",
      "Iteration 1027 Loss: 0.12348248077141032\n",
      "Iteration 1028 Loss: 0.12336942371899128\n",
      "Iteration 1029 Loss: 0.12325664176961407\n",
      "Iteration 1030 Loss: 0.12314413425288699\n",
      "Iteration 1031 Loss: 0.12303190050005185\n",
      "Iteration 1032 Loss: 0.12291993984398038\n",
      "Iteration 1033 Loss: 0.12280825161917001\n",
      "Iteration 1034 Loss: 0.12269683516173986\n",
      "Iteration 1035 Loss: 0.12258568980942709\n",
      "Iteration 1036 Loss: 0.12247481490158264\n",
      "Iteration 1037 Loss: 0.12236420977916755\n",
      "Iteration 1038 Loss: 0.12225387378474886\n",
      "Iteration 1039 Loss: 0.12214380626249563\n",
      "Iteration 1040 Loss: 0.1220340065581754\n",
      "Iteration 1041 Loss: 0.12192447401914991\n",
      "Iteration 1042 Loss: 0.12181520799437129\n",
      "Iteration 1043 Loss: 0.12170620783437851\n",
      "Iteration 1044 Loss: 0.12159747289129294\n",
      "Iteration 1045 Loss: 0.12148900251881503\n",
      "Iteration 1046 Loss: 0.1213807960722201\n",
      "Iteration 1047 Loss: 0.12127285290835471\n",
      "Iteration 1048 Loss: 0.12116517238563289\n",
      "Iteration 1049 Loss: 0.12105775386403185\n",
      "Iteration 1050 Loss: 0.12095059670508881\n",
      "Iteration 1051 Loss: 0.12084370027189698\n",
      "Iteration 1052 Loss: 0.12073706392910122\n",
      "Iteration 1053 Loss: 0.12063068704289533\n",
      "Iteration 1054 Loss: 0.12052456898101721\n",
      "Iteration 1055 Loss: 0.12041870911274588\n",
      "Iteration 1056 Loss: 0.12031310680889709\n",
      "Iteration 1057 Loss: 0.12020776144182012\n",
      "Iteration 1058 Loss: 0.12010267238539382\n",
      "Iteration 1059 Loss: 0.11999783901502264\n",
      "Iteration 1060 Loss: 0.11989326070763351\n",
      "Iteration 1061 Loss: 0.11978893684167152\n",
      "Iteration 1062 Loss: 0.11968486679709643\n",
      "Iteration 1063 Loss: 0.11958104995537915\n",
      "Iteration 1064 Loss: 0.1194774856994978\n",
      "Iteration 1065 Loss: 0.11937417341393435\n",
      "Iteration 1066 Loss: 0.11927111248467068\n",
      "Iteration 1067 Loss: 0.11916830229918475\n",
      "Iteration 1068 Loss: 0.1190657422464476\n",
      "Iteration 1069 Loss: 0.1189634317169193\n",
      "Iteration 1070 Loss: 0.11886137010254498\n",
      "Iteration 1071 Loss: 0.11875955679675183\n",
      "Iteration 1072 Loss: 0.11865799119444535\n",
      "Iteration 1073 Loss: 0.1185566726920054\n",
      "Iteration 1074 Loss: 0.11845560068728302\n",
      "Iteration 1075 Loss: 0.11835477457959655\n",
      "Iteration 1076 Loss: 0.11825419376972816\n",
      "Iteration 1077 Loss: 0.11815385765992033\n",
      "Iteration 1078 Loss: 0.11805376565387232\n",
      "Iteration 1079 Loss: 0.1179539171567365\n",
      "Iteration 1080 Loss: 0.11785431157511474\n",
      "Iteration 1081 Loss: 0.11775494831705523\n",
      "Iteration 1082 Loss: 0.11765582679204849\n",
      "Iteration 1083 Loss: 0.11755694641102427\n",
      "Iteration 1084 Loss: 0.11745830658634782\n",
      "Iteration 1085 Loss: 0.11735990673181643\n",
      "Iteration 1086 Loss: 0.11726174626265586\n",
      "Iteration 1087 Loss: 0.1171638245955169\n",
      "Iteration 1088 Loss: 0.11706614114847227\n",
      "Iteration 1089 Loss: 0.11696869534101241\n",
      "Iteration 1090 Loss: 0.1168714865940427\n",
      "Iteration 1091 Loss: 0.11677451432987977\n",
      "Iteration 1092 Loss: 0.11667777797224774\n",
      "Iteration 1093 Loss: 0.11658127694627543\n",
      "Iteration 1094 Loss: 0.11648501067849246\n",
      "Iteration 1095 Loss: 0.11638897859682597\n",
      "Iteration 1096 Loss: 0.11629318013059732\n",
      "Iteration 1097 Loss: 0.11619761471051833\n",
      "Iteration 1098 Loss: 0.11610228176868853\n",
      "Iteration 1099 Loss: 0.11600718073859102\n",
      "Iteration 1100 Loss: 0.11591231105508981\n",
      "Iteration 1101 Loss: 0.1158176721544259\n",
      "Iteration 1102 Loss: 0.11572326347421413\n",
      "Iteration 1103 Loss: 0.11562908445344011\n",
      "Iteration 1104 Loss: 0.1155351345324563\n",
      "Iteration 1105 Loss: 0.11544141315297925\n",
      "Iteration 1106 Loss: 0.11534791975808599\n",
      "Iteration 1107 Loss: 0.11525465379221059\n",
      "Iteration 1108 Loss: 0.11516161470114111\n",
      "Iteration 1109 Loss: 0.11506880193201625\n",
      "Iteration 1110 Loss: 0.11497621493332198\n",
      "Iteration 1111 Loss: 0.1148838531548882\n",
      "Iteration 1112 Loss: 0.11479171604788557\n",
      "Iteration 1113 Loss: 0.11469980306482241\n",
      "Iteration 1114 Loss: 0.114608113659541\n",
      "Iteration 1115 Loss: 0.11451664728721482\n",
      "Iteration 1116 Loss: 0.11442540340434494\n",
      "Iteration 1117 Loss: 0.11433438146875691\n",
      "Iteration 1118 Loss: 0.1142435809395975\n",
      "Iteration 1119 Loss: 0.1141530012773317\n",
      "Iteration 1120 Loss: 0.11406264194373908\n",
      "Iteration 1121 Loss: 0.11397250240191097\n",
      "Iteration 1122 Loss: 0.11388258211624729\n",
      "Iteration 1123 Loss: 0.1137928805524528\n",
      "Iteration 1124 Loss: 0.1137033971775347\n",
      "Iteration 1125 Loss: 0.1136141314597987\n",
      "Iteration 1126 Loss: 0.11352508286884672\n",
      "Iteration 1127 Loss: 0.11343625087557258\n",
      "Iteration 1128 Loss: 0.11334763495216021\n",
      "Iteration 1129 Loss: 0.11325923457207933\n",
      "Iteration 1130 Loss: 0.11317104921008292\n",
      "Iteration 1131 Loss: 0.11308307834220405\n",
      "Iteration 1132 Loss: 0.11299532144575258\n",
      "Iteration 1133 Loss: 0.11290777799931219\n",
      "Iteration 1134 Loss: 0.11282044748273722\n",
      "Iteration 1135 Loss: 0.11273332937714957\n",
      "Iteration 1136 Loss: 0.11264642316493567\n",
      "Iteration 1137 Loss: 0.11255972832974326\n",
      "Iteration 1138 Loss: 0.11247324435647854\n",
      "Iteration 1139 Loss: 0.11238697073130284\n",
      "Iteration 1140 Loss: 0.11230090694162975\n",
      "Iteration 1141 Loss: 0.11221505247612212\n",
      "Iteration 1142 Loss: 0.11212940682468872\n",
      "Iteration 1143 Loss: 0.1120439694784815\n",
      "Iteration 1144 Loss: 0.11195873992989236\n",
      "Iteration 1145 Loss: 0.1118737176725504\n",
      "Iteration 1146 Loss: 0.11178890220131853\n",
      "Iteration 1147 Loss: 0.11170429301229086\n",
      "Iteration 1148 Loss: 0.11161988960278921\n",
      "Iteration 1149 Loss: 0.11153569147136073\n",
      "Iteration 1150 Loss: 0.11145169811777433\n",
      "Iteration 1151 Loss: 0.11136790904301817\n",
      "Iteration 1152 Loss: 0.11128432374929637\n",
      "Iteration 1153 Loss: 0.11120094174002614\n",
      "Iteration 1154 Loss: 0.11111776251983477\n",
      "Iteration 1155 Loss: 0.11103478559455698\n",
      "Iteration 1156 Loss: 0.1109520104712316\n",
      "Iteration 1157 Loss: 0.11086943665809881\n",
      "Iteration 1158 Loss: 0.11078706366459722\n",
      "Iteration 1159 Loss: 0.11070489100136076\n",
      "Iteration 1160 Loss: 0.11062291818021629\n",
      "Iteration 1161 Loss: 0.11054114471417975\n",
      "Iteration 1162 Loss: 0.11045957011745455\n",
      "Iteration 1163 Loss: 0.11037819390542739\n",
      "Iteration 1164 Loss: 0.1102970155946662\n",
      "Iteration 1165 Loss: 0.11021603470291698\n",
      "Iteration 1166 Loss: 0.11013525074910095\n",
      "Iteration 1167 Loss: 0.11005466325331169\n",
      "Iteration 1168 Loss: 0.10997427173681232\n",
      "Iteration 1169 Loss: 0.10989407572203262\n",
      "Iteration 1170 Loss: 0.10981407473256616\n",
      "Iteration 1171 Loss: 0.10973426829316761\n",
      "Iteration 1172 Loss: 0.10965465592974959\n",
      "Iteration 1173 Loss: 0.10957523716938017\n",
      "Iteration 1174 Loss: 0.10949601154027996\n",
      "Iteration 1175 Loss: 0.10941697857181916\n",
      "Iteration 1176 Loss: 0.10933813779451512\n",
      "Iteration 1177 Loss: 0.10925948874002894\n",
      "Iteration 1178 Loss: 0.10918103094116334\n",
      "Iteration 1179 Loss: 0.10910276393185951\n",
      "Iteration 1180 Loss: 0.10902468724719432\n",
      "Iteration 1181 Loss: 0.10894680042337773\n",
      "Iteration 1182 Loss: 0.10886910299774982\n",
      "Iteration 1183 Loss: 0.10879159450877836\n",
      "Iteration 1184 Loss: 0.10871427449605556\n",
      "Iteration 1185 Loss: 0.1086371425002959\n",
      "Iteration 1186 Loss: 0.10856019806333299\n",
      "Iteration 1187 Loss: 0.10848344072811708\n",
      "Iteration 1188 Loss: 0.10840687003871206\n",
      "Iteration 1189 Loss: 0.10833048554029295\n",
      "Iteration 1190 Loss: 0.10825428677914331\n",
      "Iteration 1191 Loss: 0.10817827330265226\n",
      "Iteration 1192 Loss: 0.10810244465931187\n",
      "Iteration 1193 Loss: 0.10802680039871478\n",
      "Iteration 1194 Loss: 0.10795134007155084\n",
      "Iteration 1195 Loss: 0.10787606322960532\n",
      "Iteration 1196 Loss: 0.1078009694257553\n",
      "Iteration 1197 Loss: 0.10772605821396777\n",
      "Iteration 1198 Loss: 0.10765132914929673\n",
      "Iteration 1199 Loss: 0.10757678178788027\n",
      "Iteration 1200 Loss: 0.1075024156869383\n",
      "Iteration 1201 Loss: 0.10742823040476969\n",
      "Iteration 1202 Loss: 0.1073542255007499\n",
      "Iteration 1203 Loss: 0.10728040053532785\n",
      "Iteration 1204 Loss: 0.10720675507002399\n",
      "Iteration 1205 Loss: 0.107133288667427\n",
      "Iteration 1206 Loss: 0.10706000089119172\n",
      "Iteration 1207 Loss: 0.10698689130603635\n",
      "Iteration 1208 Loss: 0.10691395947773977\n",
      "Iteration 1209 Loss: 0.10684120497313893\n",
      "Iteration 1210 Loss: 0.10676862736012663\n",
      "Iteration 1211 Loss: 0.10669622620764857\n",
      "Iteration 1212 Loss: 0.10662400108570087\n",
      "Iteration 1213 Loss: 0.10655195156532773\n",
      "Iteration 1214 Loss: 0.10648007721861835\n",
      "Iteration 1215 Loss: 0.10640837761870518\n",
      "Iteration 1216 Loss: 0.10633685233976052\n",
      "Iteration 1217 Loss: 0.10626550095699472\n",
      "Iteration 1218 Loss: 0.106194323046653\n",
      "Iteration 1219 Loss: 0.10612331818601356\n",
      "Iteration 1220 Loss: 0.10605248595338455\n",
      "Iteration 1221 Loss: 0.10598182592810196\n",
      "Iteration 1222 Loss: 0.10591133769052664\n",
      "Iteration 1223 Loss: 0.10584102082204236\n",
      "Iteration 1224 Loss: 0.10577087490505285\n",
      "Iteration 1225 Loss: 0.10570089952297962\n",
      "Iteration 1226 Loss: 0.10563109426025924\n",
      "Iteration 1227 Loss: 0.10556145870234125\n",
      "Iteration 1228 Loss: 0.1054919924356852\n",
      "Iteration 1229 Loss: 0.10542269504775828\n",
      "Iteration 1230 Loss: 0.1053535661270334\n",
      "Iteration 1231 Loss: 0.10528460526298611\n",
      "Iteration 1232 Loss: 0.10521581204609237\n",
      "Iteration 1233 Loss: 0.10514718606782626\n",
      "Iteration 1234 Loss: 0.10507872692065724\n",
      "Iteration 1235 Loss: 0.10501043419804816\n",
      "Iteration 1236 Loss: 0.10494230749445235\n",
      "Iteration 1237 Loss: 0.10487434640531156\n",
      "Iteration 1238 Loss: 0.10480655052705332\n",
      "Iteration 1239 Loss: 0.10473891945708885\n",
      "Iteration 1240 Loss: 0.1046714527938103\n",
      "Iteration 1241 Loss: 0.10460415013658853\n",
      "Iteration 1242 Loss: 0.10453701108577083\n",
      "Iteration 1243 Loss: 0.10447003524267819\n",
      "Iteration 1244 Loss: 0.10440322220960345\n",
      "Iteration 1245 Loss: 0.10433657158980836\n",
      "Iteration 1246 Loss: 0.10427008298752194\n",
      "Iteration 1247 Loss: 0.1042037560079372\n",
      "Iteration 1248 Loss: 0.10413759025720948\n",
      "Iteration 1249 Loss: 0.10407158534245398\n",
      "Iteration 1250 Loss: 0.10400574087174333\n",
      "Iteration 1251 Loss: 0.103940056454105\n",
      "Iteration 1252 Loss: 0.10387453169951963\n",
      "Iteration 1253 Loss: 0.10380916621891809\n",
      "Iteration 1254 Loss: 0.10374395962417954\n",
      "Iteration 1255 Loss: 0.10367891152812866\n",
      "Iteration 1256 Loss: 0.10361402154453411\n",
      "Iteration 1257 Loss: 0.10354928928810553\n",
      "Iteration 1258 Loss: 0.10348471437449143\n",
      "Iteration 1259 Loss: 0.10342029642027704\n",
      "Iteration 1260 Loss: 0.10335603504298206\n",
      "Iteration 1261 Loss: 0.10329192986105827\n",
      "Iteration 1262 Loss: 0.10322798049388691\n",
      "Iteration 1263 Loss: 0.10316418656177737\n",
      "Iteration 1264 Loss: 0.1031005476859638\n",
      "Iteration 1265 Loss: 0.10303706348860374\n",
      "Iteration 1266 Loss: 0.10297373359277531\n",
      "Iteration 1267 Loss: 0.10291055762247515\n",
      "Iteration 1268 Loss: 0.1028475352026164\n",
      "Iteration 1269 Loss: 0.10278466595902612\n",
      "Iteration 1270 Loss: 0.10272194951844325\n",
      "Iteration 1271 Loss: 0.10265938550851646\n",
      "Iteration 1272 Loss: 0.10259697355780155\n",
      "Iteration 1273 Loss: 0.10253471329575983\n",
      "Iteration 1274 Loss: 0.10247260435275535\n",
      "Iteration 1275 Loss: 0.1024106463600532\n",
      "Iteration 1276 Loss: 0.10234883894981667\n",
      "Iteration 1277 Loss: 0.10228718175510597\n",
      "Iteration 1278 Loss: 0.10222567440987505\n",
      "Iteration 1279 Loss: 0.10216431654897024\n",
      "Iteration 1280 Loss: 0.10210310780812745\n",
      "Iteration 1281 Loss: 0.10204204782397053\n",
      "Iteration 1282 Loss: 0.10198113623400867\n",
      "Iteration 1283 Loss: 0.10192037267663466\n",
      "Iteration 1284 Loss: 0.10185975679112227\n",
      "Iteration 1285 Loss: 0.10179928821762449\n",
      "Iteration 1286 Loss: 0.10173896659717122\n",
      "Iteration 1287 Loss: 0.10167879157166708\n",
      "Iteration 1288 Loss: 0.10161876278388948\n",
      "Iteration 1289 Loss: 0.10155887987748624\n",
      "Iteration 1290 Loss: 0.10149914249697378\n",
      "Iteration 1291 Loss: 0.10143955028773449\n",
      "Iteration 1292 Loss: 0.10138010289601525\n",
      "Iteration 1293 Loss: 0.10132079996892494\n",
      "Iteration 1294 Loss: 0.1012616411544325\n",
      "Iteration 1295 Loss: 0.10120262610136449\n",
      "Iteration 1296 Loss: 0.10114375445940343\n",
      "Iteration 1297 Loss: 0.10108502587908567\n",
      "Iteration 1298 Loss: 0.10102644001179902\n",
      "Iteration 1299 Loss: 0.10096799650978079\n",
      "Iteration 1300 Loss: 0.10090969502611598\n",
      "Iteration 1301 Loss: 0.10085153521473496\n",
      "Iteration 1302 Loss: 0.10079351673041127\n",
      "Iteration 1303 Loss: 0.10073563922875998\n",
      "Iteration 1304 Loss: 0.10067790236623524\n",
      "Iteration 1305 Loss: 0.10062030580012844\n",
      "Iteration 1306 Loss: 0.10056284918856634\n",
      "Iteration 1307 Loss: 0.10050553219050845\n",
      "Iteration 1308 Loss: 0.1004483544657458\n",
      "Iteration 1309 Loss: 0.1003913156748982\n",
      "Iteration 1310 Loss: 0.10033441547941263\n",
      "Iteration 1311 Loss: 0.10027765354156108\n",
      "Iteration 1312 Loss: 0.10022102952443848\n",
      "Iteration 1313 Loss: 0.10016454309196096\n",
      "Iteration 1314 Loss: 0.10010819390886366\n",
      "Iteration 1315 Loss: 0.10005198164069855\n",
      "Iteration 1316 Loss: 0.09999590595383281\n",
      "Iteration 1317 Loss: 0.09993996651544666\n",
      "Iteration 1318 Loss: 0.09988416299353126\n",
      "Iteration 1319 Loss: 0.09982849505688708\n",
      "Iteration 1320 Loss: 0.09977296237512133\n",
      "Iteration 1321 Loss: 0.09971756461864693\n",
      "Iteration 1322 Loss: 0.09966230145867952\n",
      "Iteration 1323 Loss: 0.09960717256723614\n",
      "Iteration 1324 Loss: 0.09955217761713314\n",
      "Iteration 1325 Loss: 0.0994973162819841\n",
      "Iteration 1326 Loss: 0.09944258823619817\n",
      "Iteration 1327 Loss: 0.09938799315497773\n",
      "Iteration 1328 Loss: 0.0993335307143168\n",
      "Iteration 1329 Loss: 0.099279200590999\n",
      "Iteration 1330 Loss: 0.09922500246259558\n",
      "Iteration 1331 Loss: 0.0991709360074636\n",
      "Iteration 1332 Loss: 0.09911700090474387\n",
      "Iteration 1333 Loss: 0.0990631968343591\n",
      "Iteration 1334 Loss: 0.099009523477012\n",
      "Iteration 1335 Loss: 0.09895598051418356\n",
      "Iteration 1336 Loss: 0.09890256762813086\n",
      "Iteration 1337 Loss: 0.09884928450188533\n",
      "Iteration 1338 Loss: 0.09879613081925084\n",
      "Iteration 1339 Loss: 0.09874310626480184\n",
      "Iteration 1340 Loss: 0.09869021052388127\n",
      "Iteration 1341 Loss: 0.09863744328259916\n",
      "Iteration 1342 Loss: 0.0985848042278304\n",
      "Iteration 1343 Loss: 0.09853229304721278\n",
      "Iteration 1344 Loss: 0.09847990942914546\n",
      "Iteration 1345 Loss: 0.09842765306278711\n",
      "Iteration 1346 Loss: 0.09837552363805342\n",
      "Iteration 1347 Loss: 0.09832352084561626\n",
      "Iteration 1348 Loss: 0.098271644376901\n",
      "Iteration 1349 Loss: 0.09821989392408514\n",
      "Iteration 1350 Loss: 0.09816826918009623\n",
      "Iteration 1351 Loss: 0.09811676983861033\n",
      "Iteration 1352 Loss: 0.0980653955940497\n",
      "Iteration 1353 Loss: 0.09801414614158159\n",
      "Iteration 1354 Loss: 0.09796302117711583\n",
      "Iteration 1355 Loss: 0.09791202039730357\n",
      "Iteration 1356 Loss: 0.09786114349953501\n",
      "Iteration 1357 Loss: 0.0978103901819379\n",
      "Iteration 1358 Loss: 0.09775976014337562\n",
      "Iteration 1359 Loss: 0.09770925308344537\n",
      "Iteration 1360 Loss: 0.09765886870247653\n",
      "Iteration 1361 Loss: 0.09760860670152854\n",
      "Iteration 1362 Loss: 0.09755846678238961\n",
      "Iteration 1363 Loss: 0.0975084486475744\n",
      "Iteration 1364 Loss: 0.0974585520003229\n",
      "Iteration 1365 Loss: 0.09740877654459792\n",
      "Iteration 1366 Loss: 0.09735912198508392\n",
      "Iteration 1367 Loss: 0.09730958802718492\n",
      "Iteration 1368 Loss: 0.09726017437702289\n",
      "Iteration 1369 Loss: 0.09721088074143606\n",
      "Iteration 1370 Loss: 0.09716170682797699\n",
      "Iteration 1371 Loss: 0.0971126523449108\n",
      "Iteration 1372 Loss: 0.09706371700121386\n",
      "Iteration 1373 Loss: 0.09701490050657148\n",
      "Iteration 1374 Loss: 0.09696620257137666\n",
      "Iteration 1375 Loss: 0.09691762290672798\n",
      "Iteration 1376 Loss: 0.09686916122442811\n",
      "Iteration 1377 Loss: 0.09682081723698208\n",
      "Iteration 1378 Loss: 0.09677259065759561\n",
      "Iteration 1379 Loss: 0.09672448120017321\n",
      "Iteration 1380 Loss: 0.09667648857931659\n",
      "Iteration 1381 Loss: 0.09662861251032301\n",
      "Iteration 1382 Loss: 0.09658085270918351\n",
      "Iteration 1383 Loss: 0.09653320889258125\n",
      "Iteration 1384 Loss: 0.09648568077788988\n",
      "Iteration 1385 Loss: 0.09643826808317173\n",
      "Iteration 1386 Loss: 0.09639097052717624\n",
      "Iteration 1387 Loss: 0.09634378782933803\n",
      "Iteration 1388 Loss: 0.09629671970977602\n",
      "Iteration 1389 Loss: 0.09624976588929039\n",
      "Iteration 1390 Loss: 0.09620292608936254\n",
      "Iteration 1391 Loss: 0.09615620003215192\n",
      "Iteration 1392 Loss: 0.09610958744049539\n",
      "Iteration 1393 Loss: 0.09606308803790535\n",
      "Iteration 1394 Loss: 0.09601670154856766\n",
      "Iteration 1395 Loss: 0.09597042769734065\n",
      "Iteration 1396 Loss: 0.09592426620975292\n",
      "Iteration 1397 Loss: 0.09587821681200208\n",
      "Iteration 1398 Loss: 0.095832279230953\n",
      "Iteration 1399 Loss: 0.095786453194136\n",
      "Iteration 1400 Loss: 0.09574073842974559\n",
      "Iteration 1401 Loss: 0.09569513466663858\n",
      "Iteration 1402 Loss: 0.09564964163433254\n",
      "Iteration 1403 Loss: 0.09560425906300422\n",
      "Iteration 1404 Loss: 0.09555898668348778\n",
      "Iteration 1405 Loss: 0.09551382422727357\n",
      "Iteration 1406 Loss: 0.09546877142650607\n",
      "Iteration 1407 Loss: 0.09542382801398272\n",
      "Iteration 1408 Loss: 0.09537899372315188\n",
      "Iteration 1409 Loss: 0.09533426828811166\n",
      "Iteration 1410 Loss: 0.09528965144360806\n",
      "Iteration 1411 Loss: 0.09524514292503358\n",
      "Iteration 1412 Loss: 0.09520074246842554\n",
      "Iteration 1413 Loss: 0.09515644981046448\n",
      "Iteration 1414 Loss: 0.09511226468847264\n",
      "Iteration 1415 Loss: 0.09506818684041245\n",
      "Iteration 1416 Loss: 0.09502421600488478\n",
      "Iteration 1417 Loss: 0.09498035192112768\n",
      "Iteration 1418 Loss: 0.09493659432901469\n",
      "Iteration 1419 Loss: 0.09489294296905301\n",
      "Iteration 1420 Loss: 0.09484939758238255\n",
      "Iteration 1421 Loss: 0.09480595791077372\n",
      "Iteration 1422 Loss: 0.09476262369662647\n",
      "Iteration 1423 Loss: 0.09471939468296836\n",
      "Iteration 1424 Loss: 0.09467627061345345\n",
      "Iteration 1425 Loss: 0.09463325123236005\n",
      "Iteration 1426 Loss: 0.09459033628459002\n",
      "Iteration 1427 Loss: 0.09454752551566672\n",
      "Iteration 1428 Loss: 0.09450481867173378\n",
      "Iteration 1429 Loss: 0.09446221549955332\n",
      "Iteration 1430 Loss: 0.0944197157465047\n",
      "Iteration 1431 Loss: 0.09437731916058283\n",
      "Iteration 1432 Loss: 0.09433502549039678\n",
      "Iteration 1433 Loss: 0.09429283448516823\n",
      "Iteration 1434 Loss: 0.09425074589473015\n",
      "Iteration 1435 Loss: 0.09420875946952485\n",
      "Iteration 1436 Loss: 0.09416687496060305\n",
      "Iteration 1437 Loss: 0.09412509211962213\n",
      "Iteration 1438 Loss: 0.09408341069884446\n",
      "Iteration 1439 Loss: 0.0940418304511364\n",
      "Iteration 1440 Loss: 0.09400035112996637\n",
      "Iteration 1441 Loss: 0.0939589724894038\n",
      "Iteration 1442 Loss: 0.09391769428411711\n",
      "Iteration 1443 Loss: 0.09387651626937289\n",
      "Iteration 1444 Loss: 0.09383543820103403\n",
      "Iteration 1445 Loss: 0.09379445983555829\n",
      "Iteration 1446 Loss: 0.09375358092999697\n",
      "Iteration 1447 Loss: 0.09371280124199359\n",
      "Iteration 1448 Loss: 0.09367212052978195\n",
      "Iteration 1449 Loss: 0.09363153855218542\n",
      "Iteration 1450 Loss: 0.09359105506861484\n",
      "Iteration 1451 Loss: 0.09355066983906728\n",
      "Iteration 1452 Loss: 0.09351038262412498\n",
      "Iteration 1453 Loss: 0.09347019318495349\n",
      "Iteration 1454 Loss: 0.09343010128330033\n",
      "Iteration 1455 Loss: 0.0933901066814936\n",
      "Iteration 1456 Loss: 0.09335020914244081\n",
      "Iteration 1457 Loss: 0.09331040842962704\n",
      "Iteration 1458 Loss: 0.09327070430711379\n",
      "Iteration 1459 Loss: 0.09323109653953765\n",
      "Iteration 1460 Loss: 0.09319158489210867\n",
      "Iteration 1461 Loss: 0.09315216913060906\n",
      "Iteration 1462 Loss: 0.0931128490213919\n",
      "Iteration 1463 Loss: 0.09307362433137957\n",
      "Iteration 1464 Loss: 0.09303449482806245\n",
      "Iteration 1465 Loss: 0.09299546027949766\n",
      "Iteration 1466 Loss: 0.09295652045430743\n",
      "Iteration 1467 Loss: 0.09291767512167783\n",
      "Iteration 1468 Loss: 0.09287892405135745\n",
      "Iteration 1469 Loss: 0.09284026701365605\n",
      "Iteration 1470 Loss: 0.09280170377944313\n",
      "Iteration 1471 Loss: 0.09276323412014649\n",
      "Iteration 1472 Loss: 0.092724857807751\n",
      "Iteration 1473 Loss: 0.09268657461479714\n",
      "Iteration 1474 Loss: 0.09264838431437987\n",
      "Iteration 1475 Loss: 0.09261028668014683\n",
      "Iteration 1476 Loss: 0.0925722814862975\n",
      "Iteration 1477 Loss: 0.09253436850758151\n",
      "Iteration 1478 Loss: 0.09249654751929753\n",
      "Iteration 1479 Loss: 0.09245881829729159\n",
      "Iteration 1480 Loss: 0.09242118061795633\n",
      "Iteration 1481 Loss: 0.09238363425822892\n",
      "Iteration 1482 Loss: 0.0923461789955905\n",
      "Iteration 1483 Loss: 0.092308814608064\n",
      "Iteration 1484 Loss: 0.0922715408742138\n",
      "Iteration 1485 Loss: 0.09223435757314366\n",
      "Iteration 1486 Loss: 0.09219726448449568\n",
      "Iteration 1487 Loss: 0.09216026138844899\n",
      "Iteration 1488 Loss: 0.09212334806571834\n",
      "Iteration 1489 Loss: 0.09208652429755299\n",
      "Iteration 1490 Loss: 0.09204978986573518\n",
      "Iteration 1491 Loss: 0.0920131445525789\n",
      "Iteration 1492 Loss: 0.09197658814092882\n",
      "Iteration 1493 Loss: 0.09194012041415865\n",
      "Iteration 1494 Loss: 0.09190374115617002\n",
      "Iteration 1495 Loss: 0.09186745015139128\n",
      "Iteration 1496 Loss: 0.09183124718477599\n",
      "Iteration 1497 Loss: 0.09179513204180191\n",
      "Iteration 1498 Loss: 0.09175910450846946\n",
      "Iteration 1499 Loss: 0.09172316437130053\n",
      "Iteration 1500 Loss: 0.09168731141733746\n",
      "Iteration 1501 Loss: 0.09165154543414139\n",
      "Iteration 1502 Loss: 0.09161586620979117\n",
      "Iteration 1503 Loss: 0.09158027353288203\n",
      "Iteration 1504 Loss: 0.09154476719252463\n",
      "Iteration 1505 Loss: 0.0915093469783433\n",
      "Iteration 1506 Loss: 0.09147401268047524\n",
      "Iteration 1507 Loss: 0.09143876408956884\n",
      "Iteration 1508 Loss: 0.09140360099678296\n",
      "Iteration 1509 Loss: 0.09136852319378523\n",
      "Iteration 1510 Loss: 0.09133353047275092\n",
      "Iteration 1511 Loss: 0.09129862262636189\n",
      "Iteration 1512 Loss: 0.0912637994478052\n",
      "Iteration 1513 Loss: 0.09122906073077179\n",
      "Iteration 1514 Loss: 0.09119440626945552\n",
      "Iteration 1515 Loss: 0.09115983585855167\n",
      "Iteration 1516 Loss: 0.091125349293256\n",
      "Iteration 1517 Loss: 0.09109094636926311\n",
      "Iteration 1518 Loss: 0.09105662688276572\n",
      "Iteration 1519 Loss: 0.09102239063045311\n",
      "Iteration 1520 Loss: 0.0909882374095101\n",
      "Iteration 1521 Loss: 0.09095416701761555\n",
      "Iteration 1522 Loss: 0.09092017925294167\n",
      "Iteration 1523 Loss: 0.09088627391415235\n",
      "Iteration 1524 Loss: 0.09085245080040207\n",
      "Iteration 1525 Loss: 0.09081870971133482\n",
      "Iteration 1526 Loss: 0.09078505044708297\n",
      "Iteration 1527 Loss: 0.09075147280826573\n",
      "Iteration 1528 Loss: 0.09071797659598842\n",
      "Iteration 1529 Loss: 0.09068456161184085\n",
      "Iteration 1530 Loss: 0.09065122765789653\n",
      "Iteration 1531 Loss: 0.09061797453671115\n",
      "Iteration 1532 Loss: 0.09058480205132166\n",
      "Iteration 1533 Loss: 0.09055171000524499\n",
      "Iteration 1534 Loss: 0.09051869820247693\n",
      "Iteration 1535 Loss: 0.09048576644749078\n",
      "Iteration 1536 Loss: 0.09045291454523641\n",
      "Iteration 1537 Loss: 0.0904201423011391\n",
      "Iteration 1538 Loss: 0.0903874495210982\n",
      "Iteration 1539 Loss: 0.09035483601148606\n",
      "Iteration 1540 Loss: 0.090322301579147\n",
      "Iteration 1541 Loss: 0.0902898460313959\n",
      "Iteration 1542 Loss: 0.09025746917601739\n",
      "Iteration 1543 Loss: 0.0902251708212644\n",
      "Iteration 1544 Loss: 0.09019295077585712\n",
      "Iteration 1545 Loss: 0.0901608088489819\n",
      "Iteration 1546 Loss: 0.09012874485029018\n",
      "Iteration 1547 Loss: 0.09009675858989716\n",
      "Iteration 1548 Loss: 0.09006484987838084\n",
      "Iteration 1549 Loss: 0.09003301852678079\n",
      "Iteration 1550 Loss: 0.09000126434659711\n",
      "Iteration 1551 Loss: 0.08996958714978931\n",
      "Iteration 1552 Loss: 0.08993798674877496\n",
      "Iteration 1553 Loss: 0.08990646295642898\n",
      "Iteration 1554 Loss: 0.08987501558608221\n",
      "Iteration 1555 Loss: 0.08984364445152034\n",
      "Iteration 1556 Loss: 0.089812349366983\n",
      "Iteration 1557 Loss: 0.08978113014716231\n",
      "Iteration 1558 Loss: 0.08974998660720218\n",
      "Iteration 1559 Loss: 0.08971891856269693\n",
      "Iteration 1560 Loss: 0.08968792582969008\n",
      "Iteration 1561 Loss: 0.08965700822467385\n",
      "Iteration 1562 Loss: 0.08962616556458727\n",
      "Iteration 1563 Loss: 0.0895953976668157\n",
      "Iteration 1564 Loss: 0.08956470434918938\n",
      "Iteration 1565 Loss: 0.08953408542998269\n",
      "Iteration 1566 Loss: 0.08950354072791264\n",
      "Iteration 1567 Loss: 0.08947307006213824\n",
      "Iteration 1568 Loss: 0.08944267325225899\n",
      "Iteration 1569 Loss: 0.0894123501183141\n",
      "Iteration 1570 Loss: 0.08938210048078142\n",
      "Iteration 1571 Loss: 0.08935192416057616\n",
      "Iteration 1572 Loss: 0.08932182097905007\n",
      "Iteration 1573 Loss: 0.08929179075799012\n",
      "Iteration 1574 Loss: 0.08926183331961766\n",
      "Iteration 1575 Loss: 0.08923194848658735\n",
      "Iteration 1576 Loss: 0.08920213608198589\n",
      "Iteration 1577 Loss: 0.08917239592933124\n",
      "Iteration 1578 Loss: 0.08914272785257127\n",
      "Iteration 1579 Loss: 0.08911313167608303\n",
      "Iteration 1580 Loss: 0.08908360722467154\n",
      "Iteration 1581 Loss: 0.08905415432356861\n",
      "Iteration 1582 Loss: 0.08902477279843227\n",
      "Iteration 1583 Loss: 0.08899546247534505\n",
      "Iteration 1584 Loss: 0.08896622318081356\n",
      "Iteration 1585 Loss: 0.08893705474176705\n",
      "Iteration 1586 Loss: 0.08890795698555663\n",
      "Iteration 1587 Loss: 0.08887892973995414\n",
      "Iteration 1588 Loss: 0.0888499728331511\n",
      "Iteration 1589 Loss: 0.08882108609375777\n",
      "Iteration 1590 Loss: 0.08879226935080195\n",
      "Iteration 1591 Loss: 0.08876352243372837\n",
      "Iteration 1592 Loss: 0.08873484517239696\n",
      "Iteration 1593 Loss: 0.08870623739708267\n",
      "Iteration 1594 Loss: 0.08867769893847392\n",
      "Iteration 1595 Loss: 0.08864922962767162\n",
      "Iteration 1596 Loss: 0.08862082929618852\n",
      "Iteration 1597 Loss: 0.08859249777594776\n",
      "Iteration 1598 Loss: 0.08856423489928215\n",
      "Iteration 1599 Loss: 0.0885360404989331\n",
      "Iteration 1600 Loss: 0.0885079144080496\n",
      "Iteration 1601 Loss: 0.08847985646018738\n",
      "Iteration 1602 Loss: 0.08845186648930758\n",
      "Iteration 1603 Loss: 0.08842394432977596\n",
      "Iteration 1604 Loss: 0.0883960898163622\n",
      "Iteration 1605 Loss: 0.08836830278423831\n",
      "Iteration 1606 Loss: 0.08834058306897828\n",
      "Iteration 1607 Loss: 0.08831293050655656\n",
      "Iteration 1608 Loss: 0.08828534493334743\n",
      "Iteration 1609 Loss: 0.08825782618612386\n",
      "Iteration 1610 Loss: 0.08823037410205671\n",
      "Iteration 1611 Loss: 0.08820298851871358\n",
      "Iteration 1612 Loss: 0.0881756692740578\n",
      "Iteration 1613 Loss: 0.0881484162064477\n",
      "Iteration 1614 Loss: 0.08812122915463558\n",
      "Iteration 1615 Loss: 0.08809410795776661\n",
      "Iteration 1616 Loss: 0.0880670524553779\n",
      "Iteration 1617 Loss: 0.08804006248739764\n",
      "Iteration 1618 Loss: 0.08801313789414422\n",
      "Iteration 1619 Loss: 0.08798627851632507\n",
      "Iteration 1620 Loss: 0.08795948419503576\n",
      "Iteration 1621 Loss: 0.08793275477175913\n",
      "Iteration 1622 Loss: 0.08790609008836442\n",
      "Iteration 1623 Loss: 0.08787948998710611\n",
      "Iteration 1624 Loss: 0.08785295431062315\n",
      "Iteration 1625 Loss: 0.08782648290193798\n",
      "Iteration 1626 Loss: 0.08780007560445556\n",
      "Iteration 1627 Loss: 0.08777373226196246\n",
      "Iteration 1628 Loss: 0.08774745271862593\n",
      "Iteration 1629 Loss: 0.08772123681899303\n",
      "Iteration 1630 Loss: 0.08769508440798963\n",
      "Iteration 1631 Loss: 0.08766899533091946\n",
      "Iteration 1632 Loss: 0.08764296943346331\n",
      "Iteration 1633 Loss: 0.08761700656167794\n",
      "Iteration 1634 Loss: 0.08759110656199538\n",
      "Iteration 1635 Loss: 0.08756526928122184\n",
      "Iteration 1636 Loss: 0.08753949456653685\n",
      "Iteration 1637 Loss: 0.08751378226549228\n",
      "Iteration 1638 Loss: 0.08748813222601176\n",
      "Iteration 1639 Loss: 0.08746254429638921\n",
      "Iteration 1640 Loss: 0.08743701832528847\n",
      "Iteration 1641 Loss: 0.08741155416174218\n",
      "Iteration 1642 Loss: 0.08738615165515061\n",
      "Iteration 1643 Loss: 0.08736081065528134\n",
      "Iteration 1644 Loss: 0.08733553101226793\n",
      "Iteration 1645 Loss: 0.08731031257660929\n",
      "Iteration 1646 Loss: 0.08728515519916835\n",
      "Iteration 1647 Loss: 0.08726005873117183\n",
      "Iteration 1648 Loss: 0.08723502302420882\n",
      "Iteration 1649 Loss: 0.08721004793022999\n",
      "Iteration 1650 Loss: 0.08718513330154706\n",
      "Iteration 1651 Loss: 0.08716027899083141\n",
      "Iteration 1652 Loss: 0.08713548485111361\n",
      "Iteration 1653 Loss: 0.08711075073578234\n",
      "Iteration 1654 Loss: 0.08708607649858358\n",
      "Iteration 1655 Loss: 0.08706146199361964\n",
      "Iteration 1656 Loss: 0.0870369070753485\n",
      "Iteration 1657 Loss: 0.08701241159858279\n",
      "Iteration 1658 Loss: 0.08698797541848881\n",
      "Iteration 1659 Loss: 0.08696359839058601\n",
      "Iteration 1660 Loss: 0.08693928037074582\n",
      "Iteration 1661 Loss: 0.08691502121519093\n",
      "Iteration 1662 Loss: 0.08689082078049451\n",
      "Iteration 1663 Loss: 0.086866678923579\n",
      "Iteration 1664 Loss: 0.08684259550171572\n",
      "Iteration 1665 Loss: 0.08681857037252383\n",
      "Iteration 1666 Loss: 0.08679460339396929\n",
      "Iteration 1667 Loss: 0.08677069442436439\n",
      "Iteration 1668 Loss: 0.08674684332236664\n",
      "Iteration 1669 Loss: 0.08672304994697785\n",
      "Iteration 1670 Loss: 0.08669931415754364\n",
      "Iteration 1671 Loss: 0.08667563581375232\n",
      "Iteration 1672 Loss: 0.08665201477563404\n",
      "Iteration 1673 Loss: 0.08662845090356021\n",
      "Iteration 1674 Loss: 0.08660494405824247\n",
      "Iteration 1675 Loss: 0.08658149410073174\n",
      "Iteration 1676 Loss: 0.08655810089241772\n",
      "Iteration 1677 Loss: 0.08653476429502797\n",
      "Iteration 1678 Loss: 0.08651148417062672\n",
      "Iteration 1679 Loss: 0.08648826038161457\n",
      "Iteration 1680 Loss: 0.08646509279072748\n",
      "Iteration 1681 Loss: 0.08644198126103575\n",
      "Iteration 1682 Loss: 0.08641892565594343\n",
      "Iteration 1683 Loss: 0.08639592583918754\n",
      "Iteration 1684 Loss: 0.08637298167483701\n",
      "Iteration 1685 Loss: 0.08635009302729219\n",
      "Iteration 1686 Loss: 0.08632725976128369\n",
      "Iteration 1687 Loss: 0.08630448174187196\n",
      "Iteration 1688 Loss: 0.08628175883444623\n",
      "Iteration 1689 Loss: 0.08625909090472365\n",
      "Iteration 1690 Loss: 0.08623647781874874\n",
      "Iteration 1691 Loss: 0.08621391944289242\n",
      "Iteration 1692 Loss: 0.08619141564385133\n",
      "Iteration 1693 Loss: 0.0861689662886468\n",
      "Iteration 1694 Loss: 0.0861465712446244\n",
      "Iteration 1695 Loss: 0.08612423037945283\n",
      "Iteration 1696 Loss: 0.08610194356112345\n",
      "Iteration 1697 Loss: 0.0860797106579491\n",
      "Iteration 1698 Loss: 0.08605753153856371\n",
      "Iteration 1699 Loss: 0.08603540607192123\n",
      "Iteration 1700 Loss: 0.086013334127295\n",
      "Iteration 1701 Loss: 0.08599131557427699\n",
      "Iteration 1702 Loss: 0.08596935028277691\n",
      "Iteration 1703 Loss: 0.08594743812302146\n",
      "Iteration 1704 Loss: 0.08592557896555374\n",
      "Iteration 1705 Loss: 0.08590377268123212\n",
      "Iteration 1706 Loss: 0.08588201914122974\n",
      "Iteration 1707 Loss: 0.08586031821703388\n",
      "Iteration 1708 Loss: 0.08583866978044478\n",
      "Iteration 1709 Loss: 0.08581707370357519\n",
      "Iteration 1710 Loss: 0.08579552985884947\n",
      "Iteration 1711 Loss: 0.0857740381190029\n",
      "Iteration 1712 Loss: 0.08575259835708092\n",
      "Iteration 1713 Loss: 0.08573121044643832\n",
      "Iteration 1714 Loss: 0.0857098742607385\n",
      "Iteration 1715 Loss: 0.08568858967395285\n",
      "Iteration 1716 Loss: 0.08566735656035959\n",
      "Iteration 1717 Loss: 0.08564617479454378\n",
      "Iteration 1718 Loss: 0.08562504425139568\n",
      "Iteration 1719 Loss: 0.08560396480611071\n",
      "Iteration 1720 Loss: 0.08558293633418837\n",
      "Iteration 1721 Loss: 0.08556195871143141\n",
      "Iteration 1722 Loss: 0.08554103181394536\n",
      "Iteration 1723 Loss: 0.08552015551813781\n",
      "Iteration 1724 Loss: 0.08549932970071725\n",
      "Iteration 1725 Loss: 0.08547855423869288\n",
      "Iteration 1726 Loss: 0.0854578290093735\n",
      "Iteration 1727 Loss: 0.0854371538903669\n",
      "Iteration 1728 Loss: 0.08541652875957911\n",
      "Iteration 1729 Loss: 0.08539595349521382\n",
      "Iteration 1730 Loss: 0.08537542797577145\n",
      "Iteration 1731 Loss: 0.08535495208004852\n",
      "Iteration 1732 Loss: 0.08533452568713684\n",
      "Iteration 1733 Loss: 0.08531414867642298\n",
      "Iteration 1734 Loss: 0.08529382092758744\n",
      "Iteration 1735 Loss: 0.08527354232060387\n",
      "Iteration 1736 Loss: 0.08525331273573836\n",
      "Iteration 1737 Loss: 0.08523313205354896\n",
      "Iteration 1738 Loss: 0.08521300015488462\n",
      "Iteration 1739 Loss: 0.08519291692088479\n",
      "Iteration 1740 Loss: 0.08517288223297853\n",
      "Iteration 1741 Loss: 0.08515289597288386\n",
      "Iteration 1742 Loss: 0.08513295802260684\n",
      "Iteration 1743 Loss: 0.08511306826444154\n",
      "Iteration 1744 Loss: 0.08509322658096836\n",
      "Iteration 1745 Loss: 0.08507343285505416\n",
      "Iteration 1746 Loss: 0.08505368696985129\n",
      "Iteration 1747 Loss: 0.08503398880879658\n",
      "Iteration 1748 Loss: 0.08501433825561115\n",
      "Iteration 1749 Loss: 0.08499473519429937\n",
      "Iteration 1750 Loss: 0.08497517950914828\n",
      "Iteration 1751 Loss: 0.0849556710847271\n",
      "Iteration 1752 Loss: 0.08493620980588604\n",
      "Iteration 1753 Loss: 0.08491679555775619\n",
      "Iteration 1754 Loss: 0.08489742822574836\n",
      "Iteration 1755 Loss: 0.08487810769555285\n",
      "Iteration 1756 Loss: 0.08485883385313821\n",
      "Iteration 1757 Loss: 0.08483960658475123\n",
      "Iteration 1758 Loss: 0.08482042577691562\n",
      "Iteration 1759 Loss: 0.08480129131643181\n",
      "Iteration 1760 Loss: 0.08478220309037594\n",
      "Iteration 1761 Loss: 0.08476316098609953\n",
      "Iteration 1762 Loss: 0.08474416489122842\n",
      "Iteration 1763 Loss: 0.08472521469366245\n",
      "Iteration 1764 Loss: 0.08470631028157458\n",
      "Iteration 1765 Loss: 0.08468745154341027\n",
      "Iteration 1766 Loss: 0.08466863836788685\n",
      "Iteration 1767 Loss: 0.08464987064399286\n",
      "Iteration 1768 Loss: 0.08463114826098739\n",
      "Iteration 1769 Loss: 0.08461247110839927\n",
      "Iteration 1770 Loss: 0.08459383907602679\n",
      "Iteration 1771 Loss: 0.08457525205393646\n",
      "Iteration 1772 Loss: 0.08455670993246298\n",
      "Iteration 1773 Loss: 0.08453821260220824\n",
      "Iteration 1774 Loss: 0.08451975995404061\n",
      "Iteration 1775 Loss: 0.08450135187909444\n",
      "Iteration 1776 Loss: 0.0844829882687695\n",
      "Iteration 1777 Loss: 0.0844646690147301\n",
      "Iteration 1778 Loss: 0.08444639400890462\n",
      "Iteration 1779 Loss: 0.08442816314348472\n",
      "Iteration 1780 Loss: 0.08440997631092494\n",
      "Iteration 1781 Loss: 0.08439183340394171\n",
      "Iteration 1782 Loss: 0.08437373431551297\n",
      "Iteration 1783 Loss: 0.08435567893887763\n",
      "Iteration 1784 Loss: 0.08433766716753452\n",
      "Iteration 1785 Loss: 0.08431969889524217\n",
      "Iteration 1786 Loss: 0.08430177401601795\n",
      "Iteration 1787 Loss: 0.08428389242413756\n",
      "Iteration 1788 Loss: 0.08426605401413426\n",
      "Iteration 1789 Loss: 0.0842482586807984\n",
      "Iteration 1790 Loss: 0.08423050631917665\n",
      "Iteration 1791 Loss: 0.08421279682457167\n",
      "Iteration 1792 Loss: 0.08419513009254086\n",
      "Iteration 1793 Loss: 0.08417750601889645\n",
      "Iteration 1794 Loss: 0.08415992449970446\n",
      "Iteration 1795 Loss: 0.08414238543128434\n",
      "Iteration 1796 Loss: 0.08412488871020792\n",
      "Iteration 1797 Loss: 0.08410743423329936\n",
      "Iteration 1798 Loss: 0.08409002189763408\n",
      "Iteration 1799 Loss: 0.0840726516005383\n",
      "Iteration 1800 Loss: 0.08405532323958873\n",
      "Iteration 1801 Loss: 0.08403803671261127\n",
      "Iteration 1802 Loss: 0.08402079191768116\n",
      "Iteration 1803 Loss: 0.08400358875312179\n",
      "Iteration 1804 Loss: 0.08398642711750445\n",
      "Iteration 1805 Loss: 0.08396930690964759\n",
      "Iteration 1806 Loss: 0.08395222802861628\n",
      "Iteration 1807 Loss: 0.08393519037372146\n",
      "Iteration 1808 Loss: 0.08391819384451951\n",
      "Iteration 1809 Loss: 0.08390123834081153\n",
      "Iteration 1810 Loss: 0.08388432376264293\n",
      "Iteration 1811 Loss: 0.0838674500103025\n",
      "Iteration 1812 Loss: 0.08385061698432227\n",
      "Iteration 1813 Loss: 0.08383382458547638\n",
      "Iteration 1814 Loss: 0.08381707271478103\n",
      "Iteration 1815 Loss: 0.08380036127349351\n",
      "Iteration 1816 Loss: 0.08378369016311177\n",
      "Iteration 1817 Loss: 0.08376705928537381\n",
      "Iteration 1818 Loss: 0.08375046854225712\n",
      "Iteration 1819 Loss: 0.08373391783597801\n",
      "Iteration 1820 Loss: 0.08371740706899096\n",
      "Iteration 1821 Loss: 0.08370093614398857\n",
      "Iteration 1822 Loss: 0.08368450496390013\n",
      "Iteration 1823 Loss: 0.08366811343189175\n",
      "Iteration 1824 Loss: 0.08365176145136535\n",
      "Iteration 1825 Loss: 0.08363544892595844\n",
      "Iteration 1826 Loss: 0.08361917575954314\n",
      "Iteration 1827 Loss: 0.08360294185622606\n",
      "Iteration 1828 Loss: 0.08358674712034737\n",
      "Iteration 1829 Loss: 0.08357059145648028\n",
      "Iteration 1830 Loss: 0.08355447476943079\n",
      "Iteration 1831 Loss: 0.08353839696423669\n",
      "Iteration 1832 Loss: 0.08352235794616714\n",
      "Iteration 1833 Loss: 0.0835063576207223\n",
      "Iteration 1834 Loss: 0.0834903958936326\n",
      "Iteration 1835 Loss: 0.08347447267085814\n",
      "Iteration 1836 Loss: 0.08345858785858827\n",
      "Iteration 1837 Loss: 0.08344274136324087\n",
      "Iteration 1838 Loss: 0.0834269330914619\n",
      "Iteration 1839 Loss: 0.08341116295012486\n",
      "Iteration 1840 Loss: 0.08339543084633011\n",
      "Iteration 1841 Loss: 0.08337973668740445\n",
      "Iteration 1842 Loss: 0.08336408038090054\n",
      "Iteration 1843 Loss: 0.08334846183459632\n",
      "Iteration 1844 Loss: 0.08333288095649447\n",
      "Iteration 1845 Loss: 0.08331733765482179\n",
      "Iteration 1846 Loss: 0.08330183183802893\n",
      "Iteration 1847 Loss: 0.0832863634147893\n",
      "Iteration 1848 Loss: 0.08327093229399926\n",
      "Iteration 1849 Loss: 0.08325553838477681\n",
      "Iteration 1850 Loss: 0.08324018159646183\n",
      "Iteration 1851 Loss: 0.0832248618386147\n",
      "Iteration 1852 Loss: 0.08320957902101651\n",
      "Iteration 1853 Loss: 0.08319433305366809\n",
      "Iteration 1854 Loss: 0.08317912384678967\n",
      "Iteration 1855 Loss: 0.08316395131082008\n",
      "Iteration 1856 Loss: 0.08314881535641672\n",
      "Iteration 1857 Loss: 0.08313371589445431\n",
      "Iteration 1858 Loss: 0.08311865283602513\n",
      "Iteration 1859 Loss: 0.08310362609243788\n",
      "Iteration 1860 Loss: 0.08308863557521756\n",
      "Iteration 1861 Loss: 0.08307368119610471\n",
      "Iteration 1862 Loss: 0.08305876286705495\n",
      "Iteration 1863 Loss: 0.08304388050023842\n",
      "Iteration 1864 Loss: 0.08302903400803936\n",
      "Iteration 1865 Loss: 0.08301422330305551\n",
      "Iteration 1866 Loss: 0.08299944829809759\n",
      "Iteration 1867 Loss: 0.08298470890618886\n",
      "Iteration 1868 Loss: 0.0829700050405644\n",
      "Iteration 1869 Loss: 0.0829553366146708\n",
      "Iteration 1870 Loss: 0.08294070354216572\n",
      "Iteration 1871 Loss: 0.08292610573691694\n",
      "Iteration 1872 Loss: 0.08291154311300243\n",
      "Iteration 1873 Loss: 0.08289701558470933\n",
      "Iteration 1874 Loss: 0.08288252306653383\n",
      "Iteration 1875 Loss: 0.08286806547318024\n",
      "Iteration 1876 Loss: 0.08285364271956112\n",
      "Iteration 1877 Loss: 0.08283925472079606\n",
      "Iteration 1878 Loss: 0.08282490139221144\n",
      "Iteration 1879 Loss: 0.08281058264934042\n",
      "Iteration 1880 Loss: 0.0827962984079215\n",
      "Iteration 1881 Loss: 0.08278204858389887\n",
      "Iteration 1882 Loss: 0.0827678330934214\n",
      "Iteration 1883 Loss: 0.08275365185284231\n",
      "Iteration 1884 Loss: 0.08273950477871866\n",
      "Iteration 1885 Loss: 0.08272539178781098\n",
      "Iteration 1886 Loss: 0.08271131279708253\n",
      "Iteration 1887 Loss: 0.0826972677236988\n",
      "Iteration 1888 Loss: 0.08268325648502749\n",
      "Iteration 1889 Loss: 0.08266927899863738\n",
      "Iteration 1890 Loss: 0.08265533518229821\n",
      "Iteration 1891 Loss: 0.08264142495398012\n",
      "Iteration 1892 Loss: 0.08262754823185324\n",
      "Iteration 1893 Loss: 0.08261370493428694\n",
      "Iteration 1894 Loss: 0.08259989497984963\n",
      "Iteration 1895 Loss: 0.08258611828730816\n",
      "Iteration 1896 Loss: 0.08257237477562734\n",
      "Iteration 1897 Loss: 0.08255866436396936\n",
      "Iteration 1898 Loss: 0.0825449869716936\n",
      "Iteration 1899 Loss: 0.0825313425183558\n",
      "Iteration 1900 Loss: 0.08251773092370775\n",
      "Iteration 1901 Loss: 0.08250415210769688\n",
      "Iteration 1902 Loss: 0.08249060599046562\n",
      "Iteration 1903 Loss: 0.08247709249235115\n",
      "Iteration 1904 Loss: 0.08246361153388453\n",
      "Iteration 1905 Loss: 0.0824501630357907\n",
      "Iteration 1906 Loss: 0.08243674691898781\n",
      "Iteration 1907 Loss: 0.08242336310458648\n",
      "Iteration 1908 Loss: 0.0824100115138898\n",
      "Iteration 1909 Loss: 0.08239669206839269\n",
      "Iteration 1910 Loss: 0.08238340468978109\n",
      "Iteration 1911 Loss: 0.08237014929993215\n",
      "Iteration 1912 Loss: 0.08235692582091302\n",
      "Iteration 1913 Loss: 0.08234373417498114\n",
      "Iteration 1914 Loss: 0.08233057428458318\n",
      "Iteration 1915 Loss: 0.08231744607235486\n",
      "Iteration 1916 Loss: 0.08230434946112031\n",
      "Iteration 1917 Loss: 0.08229128437389205\n",
      "Iteration 1918 Loss: 0.08227825073386998\n",
      "Iteration 1919 Loss: 0.08226524846444097\n",
      "Iteration 1920 Loss: 0.08225227748917904\n",
      "Iteration 1921 Loss: 0.0822393377318442\n",
      "Iteration 1922 Loss: 0.0822264291163822\n",
      "Iteration 1923 Loss: 0.08221355156692431\n",
      "Iteration 1924 Loss: 0.08220070500778645\n",
      "Iteration 1925 Loss: 0.08218788936346932\n",
      "Iteration 1926 Loss: 0.08217510455865722\n",
      "Iteration 1927 Loss: 0.08216235051821823\n",
      "Iteration 1928 Loss: 0.08214962716720355\n",
      "Iteration 1929 Loss: 0.0821369344308469\n",
      "Iteration 1930 Loss: 0.08212427223456417\n",
      "Iteration 1931 Loss: 0.08211164050395321\n",
      "Iteration 1932 Loss: 0.08209903916479294\n",
      "Iteration 1933 Loss: 0.08208646814304338\n",
      "Iteration 1934 Loss: 0.0820739273648448\n",
      "Iteration 1935 Loss: 0.08206141675651751\n",
      "Iteration 1936 Loss: 0.08204893624456144\n",
      "Iteration 1937 Loss: 0.08203648575565561\n",
      "Iteration 1938 Loss: 0.08202406521665762\n",
      "Iteration 1939 Loss: 0.08201167455460345\n",
      "Iteration 1940 Loss: 0.08199931369670686\n",
      "Iteration 1941 Loss: 0.08198698257035893\n",
      "Iteration 1942 Loss: 0.08197468110312775\n",
      "Iteration 1943 Loss: 0.08196240922275784\n",
      "Iteration 1944 Loss: 0.08195016685716978\n",
      "Iteration 1945 Loss: 0.08193795393446\n",
      "Iteration 1946 Loss: 0.08192577038290004\n",
      "Iteration 1947 Loss: 0.08191361613093612\n",
      "Iteration 1948 Loss: 0.08190149110718893\n",
      "Iteration 1949 Loss: 0.08188939524045319\n",
      "Iteration 1950 Loss: 0.08187732845969706\n",
      "Iteration 1951 Loss: 0.0818652906940618\n",
      "Iteration 1952 Loss: 0.08185328187286134\n",
      "Iteration 1953 Loss: 0.08184130192558195\n",
      "Iteration 1954 Loss: 0.08182935078188149\n",
      "Iteration 1955 Loss: 0.08181742837158959\n",
      "Iteration 1956 Loss: 0.08180553462470669\n",
      "Iteration 1957 Loss: 0.08179366947140383\n",
      "Iteration 1958 Loss: 0.08178183284202227\n",
      "Iteration 1959 Loss: 0.08177002466707284\n",
      "Iteration 1960 Loss: 0.08175824487723593\n",
      "Iteration 1961 Loss: 0.0817464934033609\n",
      "Iteration 1962 Loss: 0.08173477017646533\n",
      "Iteration 1963 Loss: 0.08172307512773518\n",
      "Iteration 1964 Loss: 0.08171140818852397\n",
      "Iteration 1965 Loss: 0.0816997692903526\n",
      "Iteration 1966 Loss: 0.08168815836490874\n",
      "Iteration 1967 Loss: 0.0816765753440466\n",
      "Iteration 1968 Loss: 0.08166502015978647\n",
      "Iteration 1969 Loss: 0.08165349274431427\n",
      "Iteration 1970 Loss: 0.0816419930299812\n",
      "Iteration 1971 Loss: 0.08163052094930327\n",
      "Iteration 1972 Loss: 0.08161907643496098\n",
      "Iteration 1973 Loss: 0.08160765941979893\n",
      "Iteration 1974 Loss: 0.08159626983682527\n",
      "Iteration 1975 Loss: 0.08158490761921147\n",
      "Iteration 1976 Loss: 0.08157357270029193\n",
      "Iteration 1977 Loss: 0.08156226501356338\n",
      "Iteration 1978 Loss: 0.0815509844926846\n",
      "Iteration 1979 Loss: 0.08153973107147622\n",
      "Iteration 1980 Loss: 0.08152850468391987\n",
      "Iteration 1981 Loss: 0.08151730526415844\n",
      "Iteration 1982 Loss: 0.08150613274649489\n",
      "Iteration 1983 Loss: 0.08149498706539263\n",
      "Iteration 1984 Loss: 0.08148386815547448\n",
      "Iteration 1985 Loss: 0.08147277595152287\n",
      "Iteration 1986 Loss: 0.08146171038847896\n",
      "Iteration 1987 Loss: 0.08145067140144252\n",
      "Iteration 1988 Loss: 0.08143965892567147\n",
      "Iteration 1989 Loss: 0.08142867289658152\n",
      "Iteration 1990 Loss: 0.08141771324974574\n",
      "Iteration 1991 Loss: 0.08140677992089423\n",
      "Iteration 1992 Loss: 0.0813958728459137\n",
      "Iteration 1993 Loss: 0.08138499196084718\n",
      "Iteration 1994 Loss: 0.08137413720189332\n",
      "Iteration 1995 Loss: 0.08136330850540657\n",
      "Iteration 1996 Loss: 0.0813525058078962\n",
      "Iteration 1997 Loss: 0.0813417290460264\n",
      "Iteration 1998 Loss: 0.08133097815661557\n",
      "Iteration 1999 Loss: 0.08132025307663623\n",
      "Iteration 2000 Loss: 0.0813095537432143\n",
      "Iteration 2001 Loss: 0.08129888009362919\n",
      "Iteration 2002 Loss: 0.08128823206531274\n",
      "Iteration 2003 Loss: 0.08127760959584977\n",
      "Iteration 2004 Loss: 0.08126701262297682\n",
      "Iteration 2005 Loss: 0.08125644108458227\n",
      "Iteration 2006 Loss: 0.08124589491870596\n",
      "Iteration 2007 Loss: 0.08123537406353856\n",
      "Iteration 2008 Loss: 0.08122487845742157\n",
      "Iteration 2009 Loss: 0.08121440803884653\n",
      "Iteration 2010 Loss: 0.08120396274645497\n",
      "Iteration 2011 Loss: 0.08119354251903806\n",
      "Iteration 2012 Loss: 0.08118314729553588\n",
      "Iteration 2013 Loss: 0.0811727770150376\n",
      "Iteration 2014 Loss: 0.0811624316167805\n",
      "Iteration 2015 Loss: 0.08115211104015024\n",
      "Iteration 2016 Loss: 0.08114181522468007\n",
      "Iteration 2017 Loss: 0.08113154411005052\n",
      "Iteration 2018 Loss: 0.08112129763608923\n",
      "Iteration 2019 Loss: 0.08111107574277039\n",
      "Iteration 2020 Loss: 0.0811008783702146\n",
      "Iteration 2021 Loss: 0.0810907054586882\n",
      "Iteration 2022 Loss: 0.08108055694860321\n",
      "Iteration 2023 Loss: 0.08107043278051686\n",
      "Iteration 2024 Loss: 0.0810603328951312\n",
      "Iteration 2025 Loss: 0.08105025723329286\n",
      "Iteration 2026 Loss: 0.08104020573599252\n",
      "Iteration 2027 Loss: 0.08103017834436473\n",
      "Iteration 2028 Loss: 0.08102017499968746\n",
      "Iteration 2029 Loss: 0.08101019564338183\n",
      "Iteration 2030 Loss: 0.08100024021701159\n",
      "Iteration 2031 Loss: 0.08099030866228307\n",
      "Iteration 2032 Loss: 0.0809804009210446\n",
      "Iteration 2033 Loss: 0.08097051693528606\n",
      "Iteration 2034 Loss: 0.08096065664713896\n",
      "Iteration 2035 Loss: 0.08095081999887561\n",
      "Iteration 2036 Loss: 0.08094100693290923\n",
      "Iteration 2037 Loss: 0.08093121739179306\n",
      "Iteration 2038 Loss: 0.08092145131822061\n",
      "Iteration 2039 Loss: 0.08091170865502492\n",
      "Iteration 2040 Loss: 0.0809019893451784\n",
      "Iteration 2041 Loss: 0.08089229333179239\n",
      "Iteration 2042 Loss: 0.08088262055811675\n",
      "Iteration 2043 Loss: 0.08087297096753997\n",
      "Iteration 2044 Loss: 0.08086334450358816\n",
      "Iteration 2045 Loss: 0.08085374110992527\n",
      "Iteration 2046 Loss: 0.08084416073035237\n",
      "Iteration 2047 Loss: 0.08083460330880751\n",
      "Iteration 2048 Loss: 0.08082506878936548\n",
      "Iteration 2049 Loss: 0.08081555711623732\n",
      "Iteration 2050 Loss: 0.08080606823376998\n",
      "Iteration 2051 Loss: 0.08079660208644601\n",
      "Iteration 2052 Loss: 0.08078715861888335\n",
      "Iteration 2053 Loss: 0.08077773777583472\n",
      "Iteration 2054 Loss: 0.08076833950218776\n",
      "Iteration 2055 Loss: 0.08075896374296411\n",
      "Iteration 2056 Loss: 0.08074961044331962\n",
      "Iteration 2057 Loss: 0.08074027954854367\n",
      "Iteration 2058 Loss: 0.08073097100405895\n",
      "Iteration 2059 Loss: 0.08072168475542126\n",
      "Iteration 2060 Loss: 0.08071242074831898\n",
      "Iteration 2061 Loss: 0.08070317892857296\n",
      "Iteration 2062 Loss: 0.08069395924213589\n",
      "Iteration 2063 Loss: 0.08068476163509232\n",
      "Iteration 2064 Loss: 0.08067558605365824\n",
      "Iteration 2065 Loss: 0.08066643244418045\n",
      "Iteration 2066 Loss: 0.0806573007531368\n",
      "Iteration 2067 Loss: 0.08064819092713536\n",
      "Iteration 2068 Loss: 0.08063910291291446\n",
      "Iteration 2069 Loss: 0.08063003665734214\n",
      "Iteration 2070 Loss: 0.08062099210741586\n",
      "Iteration 2071 Loss: 0.08061196921026237\n",
      "Iteration 2072 Loss: 0.08060296791313729\n",
      "Iteration 2073 Loss: 0.08059398816342457\n",
      "Iteration 2074 Loss: 0.08058502990863663\n",
      "Iteration 2075 Loss: 0.08057609309641361\n",
      "Iteration 2076 Loss: 0.08056717767452332\n",
      "Iteration 2077 Loss: 0.08055828359086088\n",
      "Iteration 2078 Loss: 0.08054941079344838\n",
      "Iteration 2079 Loss: 0.08054055923043454\n",
      "Iteration 2080 Loss: 0.08053172885009445\n",
      "Iteration 2081 Loss: 0.08052291960082927\n",
      "Iteration 2082 Loss: 0.08051413143116591\n",
      "Iteration 2083 Loss: 0.08050536428975667\n",
      "Iteration 2084 Loss: 0.080496618125379\n",
      "Iteration 2085 Loss: 0.08048789288693524\n",
      "Iteration 2086 Loss: 0.08047918852345211\n",
      "Iteration 2087 Loss: 0.0804705049840807\n",
      "Iteration 2088 Loss: 0.080461842218096\n",
      "Iteration 2089 Loss: 0.08045320017489649\n",
      "Iteration 2090 Loss: 0.080444578804004\n",
      "Iteration 2091 Loss: 0.0804359780550635\n",
      "Iteration 2092 Loss: 0.08042739787784255\n",
      "Iteration 2093 Loss: 0.08041883822223113\n",
      "Iteration 2094 Loss: 0.08041029903824143\n",
      "Iteration 2095 Loss: 0.08040178027600729\n",
      "Iteration 2096 Loss: 0.08039328188578422\n",
      "Iteration 2097 Loss: 0.08038480381794891\n",
      "Iteration 2098 Loss: 0.08037634602299894\n",
      "Iteration 2099 Loss: 0.08036790845155256\n",
      "Iteration 2100 Loss: 0.08035949105434842\n",
      "Iteration 2101 Loss: 0.08035109378224506\n",
      "Iteration 2102 Loss: 0.08034271658622087\n",
      "Iteration 2103 Loss: 0.08033435941737378\n",
      "Iteration 2104 Loss: 0.08032602222692081\n",
      "Iteration 2105 Loss: 0.08031770496619771\n",
      "Iteration 2106 Loss: 0.08030940758665911\n",
      "Iteration 2107 Loss: 0.08030113003987777\n",
      "Iteration 2108 Loss: 0.08029287227754453\n",
      "Iteration 2109 Loss: 0.0802846342514679\n",
      "Iteration 2110 Loss: 0.0802764159135738\n",
      "Iteration 2111 Loss: 0.08026821721590541\n",
      "Iteration 2112 Loss: 0.08026003811062271\n",
      "Iteration 2113 Loss: 0.08025187855000229\n",
      "Iteration 2114 Loss: 0.080243738486437\n",
      "Iteration 2115 Loss: 0.0802356178724358\n",
      "Iteration 2116 Loss: 0.08022751666062324\n",
      "Iteration 2117 Loss: 0.08021943480373946\n",
      "Iteration 2118 Loss: 0.0802113722546396\n",
      "Iteration 2119 Loss: 0.08020332896629391\n",
      "Iteration 2120 Loss: 0.08019530489178703\n",
      "Iteration 2121 Loss: 0.0801872999843181\n",
      "Iteration 2122 Loss: 0.08017931419720029\n",
      "Iteration 2123 Loss: 0.08017134748386036\n",
      "Iteration 2124 Loss: 0.0801633997978388\n",
      "Iteration 2125 Loss: 0.08015547109278919\n",
      "Iteration 2126 Loss: 0.08014756132247806\n",
      "Iteration 2127 Loss: 0.08013967044078464\n",
      "Iteration 2128 Loss: 0.08013179840170061\n",
      "Iteration 2129 Loss: 0.08012394515932961\n",
      "Iteration 2130 Loss: 0.08011611066788736\n",
      "Iteration 2131 Loss: 0.08010829488170076\n",
      "Iteration 2132 Loss: 0.08010049775520849\n",
      "Iteration 2133 Loss: 0.08009271924295995\n",
      "Iteration 2134 Loss: 0.08008495929961539\n",
      "Iteration 2135 Loss: 0.08007721787994555\n",
      "Iteration 2136 Loss: 0.08006949493883138\n",
      "Iteration 2137 Loss: 0.0800617904312638\n",
      "Iteration 2138 Loss: 0.08005410431234339\n",
      "Iteration 2139 Loss: 0.08004643653728014\n",
      "Iteration 2140 Loss: 0.08003878706139325\n",
      "Iteration 2141 Loss: 0.08003115584011074\n",
      "Iteration 2142 Loss: 0.08002354282896924\n",
      "Iteration 2143 Loss: 0.08001594798361385\n",
      "Iteration 2144 Loss: 0.08000837125979761\n",
      "Iteration 2145 Loss: 0.08000081261338143\n",
      "Iteration 2146 Loss: 0.07999327200033382\n",
      "Iteration 2147 Loss: 0.07998574937673066\n",
      "Iteration 2148 Loss: 0.07997824469875471\n",
      "Iteration 2149 Loss: 0.07997075792269563\n",
      "Iteration 2150 Loss: 0.07996328900494944\n",
      "Iteration 2151 Loss: 0.0799558379020188\n",
      "Iteration 2152 Loss: 0.0799484045705118\n",
      "Iteration 2153 Loss: 0.07994098896714268\n",
      "Iteration 2154 Loss: 0.07993359104873118\n",
      "Iteration 2155 Loss: 0.07992621077220204\n",
      "Iteration 2156 Loss: 0.07991884809458516\n",
      "Iteration 2157 Loss: 0.07991150297301497\n",
      "Iteration 2158 Loss: 0.07990417536473055\n",
      "Iteration 2159 Loss: 0.07989686522707512\n",
      "Iteration 2160 Loss: 0.07988957251749575\n",
      "Iteration 2161 Loss: 0.07988229719354335\n",
      "Iteration 2162 Loss: 0.07987503921287226\n",
      "Iteration 2163 Loss: 0.0798677985332399\n",
      "Iteration 2164 Loss: 0.07986057511250669\n",
      "Iteration 2165 Loss: 0.07985336890863584\n",
      "Iteration 2166 Loss: 0.07984617987969289\n",
      "Iteration 2167 Loss: 0.07983900798384556\n",
      "Iteration 2168 Loss: 0.07983185317936356\n",
      "Iteration 2169 Loss: 0.07982471542461834\n",
      "Iteration 2170 Loss: 0.07981759467808267\n",
      "Iteration 2171 Loss: 0.07981049089833062\n",
      "Iteration 2172 Loss: 0.0798034040440372\n",
      "Iteration 2173 Loss: 0.07979633407397808\n",
      "Iteration 2174 Loss: 0.07978928094702945\n",
      "Iteration 2175 Loss: 0.07978224462216765\n",
      "Iteration 2176 Loss: 0.07977522505846907\n",
      "Iteration 2177 Loss: 0.07976822221510964\n",
      "Iteration 2178 Loss: 0.0797612360513651\n",
      "Iteration 2179 Loss: 0.0797542665266102\n",
      "Iteration 2180 Loss: 0.07974731360031873\n",
      "Iteration 2181 Loss: 0.07974037723206323\n",
      "Iteration 2182 Loss: 0.07973345738151481\n",
      "Iteration 2183 Loss: 0.07972655400844281\n",
      "Iteration 2184 Loss: 0.0797196670727147\n",
      "Iteration 2185 Loss: 0.0797127965342955\n",
      "Iteration 2186 Loss: 0.07970594235324818\n",
      "Iteration 2187 Loss: 0.0796991044897327\n",
      "Iteration 2188 Loss: 0.07969228290400628\n",
      "Iteration 2189 Loss: 0.07968547755642294\n",
      "Iteration 2190 Loss: 0.0796786884074333\n",
      "Iteration 2191 Loss: 0.07967191541758434\n",
      "Iteration 2192 Loss: 0.0796651585475193\n",
      "Iteration 2193 Loss: 0.07965841775797719\n",
      "Iteration 2194 Loss: 0.07965169300979281\n",
      "Iteration 2195 Loss: 0.07964498426389635\n",
      "Iteration 2196 Loss: 0.07963829148131311\n",
      "Iteration 2197 Loss: 0.07963161462316355\n",
      "Iteration 2198 Loss: 0.07962495365066279\n",
      "Iteration 2199 Loss: 0.07961830852512047\n",
      "Iteration 2200 Loss: 0.0796116792079405\n",
      "Iteration 2201 Loss: 0.07960506566062087\n",
      "Iteration 2202 Loss: 0.07959846784475338\n",
      "Iteration 2203 Loss: 0.07959188572202348\n",
      "Iteration 2204 Loss: 0.07958531925420988\n",
      "Iteration 2205 Loss: 0.07957876840318452\n",
      "Iteration 2206 Loss: 0.07957223313091227\n",
      "Iteration 2207 Loss: 0.0795657133994506\n",
      "Iteration 2208 Loss: 0.0795592091709495\n",
      "Iteration 2209 Loss: 0.07955272040765128\n",
      "Iteration 2210 Loss: 0.07954624707189006\n",
      "Iteration 2211 Loss: 0.07953978912609189\n",
      "Iteration 2212 Loss: 0.0795333465327745\n",
      "Iteration 2213 Loss: 0.07952691925454664\n",
      "Iteration 2214 Loss: 0.0795205072541084\n",
      "Iteration 2215 Loss: 0.0795141104942508\n",
      "Iteration 2216 Loss: 0.07950772893785545\n",
      "Iteration 2217 Loss: 0.07950136254789439\n",
      "Iteration 2218 Loss: 0.07949501128742992\n",
      "Iteration 2219 Loss: 0.07948867511961447\n",
      "Iteration 2220 Loss: 0.07948235400769006\n",
      "Iteration 2221 Loss: 0.07947604791498834\n",
      "Iteration 2222 Loss: 0.07946975680493056\n",
      "Iteration 2223 Loss: 0.07946348064102673\n",
      "Iteration 2224 Loss: 0.07945721938687612\n",
      "Iteration 2225 Loss: 0.07945097300616641\n",
      "Iteration 2226 Loss: 0.07944474146267395\n",
      "Iteration 2227 Loss: 0.07943852472026347\n",
      "Iteration 2228 Loss: 0.07943232274288752\n",
      "Iteration 2229 Loss: 0.07942613549458655\n",
      "Iteration 2230 Loss: 0.07941996293948876\n",
      "Iteration 2231 Loss: 0.07941380504180966\n",
      "Iteration 2232 Loss: 0.07940766176585196\n",
      "Iteration 2233 Loss: 0.0794015330760055\n",
      "Iteration 2234 Loss: 0.07939541893674669\n",
      "Iteration 2235 Loss: 0.07938931931263875\n",
      "Iteration 2236 Loss: 0.07938323416833099\n",
      "Iteration 2237 Loss: 0.079377163468559\n",
      "Iteration 2238 Loss: 0.07937110717814441\n",
      "Iteration 2239 Loss: 0.07936506526199436\n",
      "Iteration 2240 Loss: 0.07935903768510173\n",
      "Iteration 2241 Loss: 0.0793530244125446\n",
      "Iteration 2242 Loss: 0.07934702540948603\n",
      "Iteration 2243 Loss: 0.0793410406411742\n",
      "Iteration 2244 Loss: 0.07933507007294184\n",
      "Iteration 2245 Loss: 0.07932911367020626\n",
      "Iteration 2246 Loss: 0.0793231713984689\n",
      "Iteration 2247 Loss: 0.07931724322331536\n",
      "Iteration 2248 Loss: 0.07931132911041519\n",
      "Iteration 2249 Loss: 0.07930542902552139\n",
      "Iteration 2250 Loss: 0.07929954293447058\n",
      "Iteration 2251 Loss: 0.07929367080318267\n",
      "Iteration 2252 Loss: 0.07928781259766045\n",
      "Iteration 2253 Loss: 0.07928196828398969\n",
      "Iteration 2254 Loss: 0.07927613782833877\n",
      "Iteration 2255 Loss: 0.07927032119695848\n",
      "Iteration 2256 Loss: 0.0792645183561819\n",
      "Iteration 2257 Loss: 0.07925872927242408\n",
      "Iteration 2258 Loss: 0.07925295391218211\n",
      "Iteration 2259 Loss: 0.07924719224203448\n",
      "Iteration 2260 Loss: 0.07924144422864127\n",
      "Iteration 2261 Loss: 0.07923570983874381\n",
      "Iteration 2262 Loss: 0.07922998903916431\n",
      "Iteration 2263 Loss: 0.07922428179680603\n",
      "Iteration 2264 Loss: 0.07921858807865291\n",
      "Iteration 2265 Loss: 0.07921290785176913\n",
      "Iteration 2266 Loss: 0.0792072410832993\n",
      "Iteration 2267 Loss: 0.07920158774046801\n",
      "Iteration 2268 Loss: 0.07919594779057978\n",
      "Iteration 2269 Loss: 0.07919032120101885\n",
      "Iteration 2270 Loss: 0.07918470793924881\n",
      "Iteration 2271 Loss: 0.07917910797281265\n",
      "Iteration 2272 Loss: 0.07917352126933234\n",
      "Iteration 2273 Loss: 0.07916794779650893\n",
      "Iteration 2274 Loss: 0.07916238752212201\n",
      "Iteration 2275 Loss: 0.07915684041402977\n",
      "Iteration 2276 Loss: 0.07915130644016866\n",
      "Iteration 2277 Loss: 0.07914578556855344\n",
      "Iteration 2278 Loss: 0.07914027776727658\n",
      "Iteration 2279 Loss: 0.07913478300450848\n",
      "Iteration 2280 Loss: 0.07912930124849699\n",
      "Iteration 2281 Loss: 0.0791238324675675\n",
      "Iteration 2282 Loss: 0.07911837663012238\n",
      "Iteration 2283 Loss: 0.07911293370464106\n",
      "Iteration 2284 Loss: 0.07910750365967999\n",
      "Iteration 2285 Loss: 0.07910208646387197\n",
      "Iteration 2286 Loss: 0.07909668208592638\n",
      "Iteration 2287 Loss: 0.07909129049462883\n",
      "Iteration 2288 Loss: 0.07908591165884087\n",
      "Iteration 2289 Loss: 0.07908054554750031\n",
      "Iteration 2290 Loss: 0.07907519212962016\n",
      "Iteration 2291 Loss: 0.07906985137428929\n",
      "Iteration 2292 Loss: 0.07906452325067176\n",
      "Iteration 2293 Loss: 0.07905920772800679\n",
      "Iteration 2294 Loss: 0.07905390477560865\n",
      "Iteration 2295 Loss: 0.07904861436286625\n",
      "Iteration 2296 Loss: 0.07904333645924316\n",
      "Iteration 2297 Loss: 0.07903807103427737\n",
      "Iteration 2298 Loss: 0.07903281805758111\n",
      "Iteration 2299 Loss: 0.07902757749884066\n",
      "Iteration 2300 Loss: 0.07902234932781613\n",
      "Iteration 2301 Loss: 0.07901713351434142\n",
      "Iteration 2302 Loss: 0.07901193002832382\n",
      "Iteration 2303 Loss: 0.07900673883974399\n",
      "Iteration 2304 Loss: 0.07900155991865584\n",
      "Iteration 2305 Loss: 0.0789963932351861\n",
      "Iteration 2306 Loss: 0.0789912387595345\n",
      "Iteration 2307 Loss: 0.07898609646197319\n",
      "Iteration 2308 Loss: 0.07898096631284682\n",
      "Iteration 2309 Loss: 0.07897584828257238\n",
      "Iteration 2310 Loss: 0.0789707423416389\n",
      "Iteration 2311 Loss: 0.07896564846060729\n",
      "Iteration 2312 Loss: 0.07896056661011037\n",
      "Iteration 2313 Loss: 0.0789554967608522\n",
      "Iteration 2314 Loss: 0.07895043888360859\n",
      "Iteration 2315 Loss: 0.07894539294922635\n",
      "Iteration 2316 Loss: 0.07894035892862336\n",
      "Iteration 2317 Loss: 0.07893533679278847\n",
      "Iteration 2318 Loss: 0.07893032651278112\n",
      "Iteration 2319 Loss: 0.0789253280597313\n",
      "Iteration 2320 Loss: 0.07892034140483936\n",
      "Iteration 2321 Loss: 0.07891536651937592\n",
      "Iteration 2322 Loss: 0.07891040337468143\n",
      "Iteration 2323 Loss: 0.0789054519421663\n",
      "Iteration 2324 Loss: 0.07890051219331062\n",
      "Iteration 2325 Loss: 0.07889558409966399\n",
      "Iteration 2326 Loss: 0.07889066763284525\n",
      "Iteration 2327 Loss: 0.07888576276454248\n",
      "Iteration 2328 Loss: 0.07888086946651274\n",
      "Iteration 2329 Loss: 0.07887598771058184\n",
      "Iteration 2330 Loss: 0.07887111746864443\n",
      "Iteration 2331 Loss: 0.07886625871266338\n",
      "Iteration 2332 Loss: 0.07886141141467021\n",
      "Iteration 2333 Loss: 0.0788565755467643\n",
      "Iteration 2334 Loss: 0.07885175108111322\n",
      "Iteration 2335 Loss: 0.07884693798995224\n",
      "Iteration 2336 Loss: 0.0788421362455844\n",
      "Iteration 2337 Loss: 0.07883734582038007\n",
      "Iteration 2338 Loss: 0.07883256668677723\n",
      "Iteration 2339 Loss: 0.0788277988172807\n",
      "Iteration 2340 Loss: 0.07882304218446254\n",
      "Iteration 2341 Loss: 0.07881829676096155\n",
      "Iteration 2342 Loss: 0.0788135625194832\n",
      "Iteration 2343 Loss: 0.07880883943279954\n",
      "Iteration 2344 Loss: 0.07880412747374894\n",
      "Iteration 2345 Loss: 0.0787994266152359\n",
      "Iteration 2346 Loss: 0.07879473683023099\n",
      "Iteration 2347 Loss: 0.07879005809177078\n",
      "Iteration 2348 Loss: 0.07878539037295733\n",
      "Iteration 2349 Loss: 0.07878073364695834\n",
      "Iteration 2350 Loss: 0.07877608788700687\n",
      "Iteration 2351 Loss: 0.07877145306640133\n",
      "Iteration 2352 Loss: 0.07876682915850512\n",
      "Iteration 2353 Loss: 0.0787622161367465\n",
      "Iteration 2354 Loss: 0.07875761397461849\n",
      "Iteration 2355 Loss: 0.07875302264567881\n",
      "Iteration 2356 Loss: 0.0787484421235495\n",
      "Iteration 2357 Loss: 0.07874387238191698\n",
      "Iteration 2358 Loss: 0.07873931339453166\n",
      "Iteration 2359 Loss: 0.07873476513520813\n",
      "Iteration 2360 Loss: 0.0787302275778245\n",
      "Iteration 2361 Loss: 0.07872570069632284\n",
      "Iteration 2362 Loss: 0.07872118446470845\n",
      "Iteration 2363 Loss: 0.07871667885705025\n",
      "Iteration 2364 Loss: 0.07871218384748016\n",
      "Iteration 2365 Loss: 0.07870769941019312\n",
      "Iteration 2366 Loss: 0.07870322551944713\n",
      "Iteration 2367 Loss: 0.07869876214956274\n",
      "Iteration 2368 Loss: 0.07869430927492309\n",
      "Iteration 2369 Loss: 0.07868986686997395\n",
      "Iteration 2370 Loss: 0.07868543490922318\n",
      "Iteration 2371 Loss: 0.07868101336724072\n",
      "Iteration 2372 Loss: 0.07867660221865863\n",
      "Iteration 2373 Loss: 0.07867220143817077\n",
      "Iteration 2374 Loss: 0.0786678110005325\n",
      "Iteration 2375 Loss: 0.07866343088056095\n",
      "Iteration 2376 Loss: 0.07865906105313435\n",
      "Iteration 2377 Loss: 0.07865470149319241\n",
      "Iteration 2378 Loss: 0.07865035217573574\n",
      "Iteration 2379 Loss: 0.07864601307582593\n",
      "Iteration 2380 Loss: 0.07864168416858534\n",
      "Iteration 2381 Loss: 0.07863736542919689\n",
      "Iteration 2382 Loss: 0.07863305683290407\n",
      "Iteration 2383 Loss: 0.07862875835501074\n",
      "Iteration 2384 Loss: 0.07862446997088077\n",
      "Iteration 2385 Loss: 0.07862019165593813\n",
      "Iteration 2386 Loss: 0.07861592338566688\n",
      "Iteration 2387 Loss: 0.07861166513561042\n",
      "Iteration 2388 Loss: 0.07860741688137217\n",
      "Iteration 2389 Loss: 0.07860317859861472\n",
      "Iteration 2390 Loss: 0.07859895026306009\n",
      "Iteration 2391 Loss: 0.07859473185048946\n",
      "Iteration 2392 Loss: 0.07859052333674303\n",
      "Iteration 2393 Loss: 0.0785863246977197\n",
      "Iteration 2394 Loss: 0.07858213590937742\n",
      "Iteration 2395 Loss: 0.07857795694773252\n",
      "Iteration 2396 Loss: 0.07857378778885972\n",
      "Iteration 2397 Loss: 0.07856962840889227\n",
      "Iteration 2398 Loss: 0.07856547878402141\n",
      "Iteration 2399 Loss: 0.07856133889049643\n",
      "Iteration 2400 Loss: 0.07855720870462447\n",
      "Iteration 2401 Loss: 0.07855308820277053\n",
      "Iteration 2402 Loss: 0.07854897736135708\n",
      "Iteration 2403 Loss: 0.07854487615686404\n",
      "Iteration 2404 Loss: 0.07854078456582876\n",
      "Iteration 2405 Loss: 0.07853670256484571\n",
      "Iteration 2406 Loss: 0.07853263013056636\n",
      "Iteration 2407 Loss: 0.07852856723969906\n",
      "Iteration 2408 Loss: 0.07852451386900909\n",
      "Iteration 2409 Loss: 0.07852046999531814\n",
      "Iteration 2410 Loss: 0.07851643559550447\n",
      "Iteration 2411 Loss: 0.07851241064650277\n",
      "Iteration 2412 Loss: 0.07850839512530372\n",
      "Iteration 2413 Loss: 0.07850438900895432\n",
      "Iteration 2414 Loss: 0.07850039227455728\n",
      "Iteration 2415 Loss: 0.07849640489927139\n",
      "Iteration 2416 Loss: 0.07849242686031077\n",
      "Iteration 2417 Loss: 0.07848845813494532\n",
      "Iteration 2418 Loss: 0.07848449870050014\n",
      "Iteration 2419 Loss: 0.07848054853435572\n",
      "Iteration 2420 Loss: 0.07847660761394773\n",
      "Iteration 2421 Loss: 0.07847267591676671\n",
      "Iteration 2422 Loss: 0.07846875342035799\n",
      "Iteration 2423 Loss: 0.07846484010232176\n",
      "Iteration 2424 Loss: 0.07846093594031274\n",
      "Iteration 2425 Loss: 0.07845704091204014\n",
      "Iteration 2426 Loss: 0.07845315499526741\n",
      "Iteration 2427 Loss: 0.07844927816781225\n",
      "Iteration 2428 Loss: 0.07844541040754635\n",
      "Iteration 2429 Loss: 0.07844155169239551\n",
      "Iteration 2430 Loss: 0.07843770200033906\n",
      "Iteration 2431 Loss: 0.07843386130941017\n",
      "Iteration 2432 Loss: 0.07843002959769552\n",
      "Iteration 2433 Loss: 0.07842620684333511\n",
      "Iteration 2434 Loss: 0.0784223930245223\n",
      "Iteration 2435 Loss: 0.07841858811950357\n",
      "Iteration 2436 Loss: 0.07841479210657833\n",
      "Iteration 2437 Loss: 0.07841100496409903\n",
      "Iteration 2438 Loss: 0.07840722667047066\n",
      "Iteration 2439 Loss: 0.07840345720415111\n",
      "Iteration 2440 Loss: 0.07839969654365048\n",
      "Iteration 2441 Loss: 0.07839594466753137\n",
      "Iteration 2442 Loss: 0.07839220155440874\n",
      "Iteration 2443 Loss: 0.07838846718294945\n",
      "Iteration 2444 Loss: 0.07838474153187247\n",
      "Iteration 2445 Loss: 0.0783810245799486\n",
      "Iteration 2446 Loss: 0.07837731630600034\n",
      "Iteration 2447 Loss: 0.0783736166889019\n",
      "Iteration 2448 Loss: 0.0783699257075789\n",
      "Iteration 2449 Loss: 0.0783662433410082\n",
      "Iteration 2450 Loss: 0.07836256956821816\n",
      "Iteration 2451 Loss: 0.078358904368288\n",
      "Iteration 2452 Loss: 0.0783552477203481\n",
      "Iteration 2453 Loss: 0.07835159960357951\n",
      "Iteration 2454 Loss: 0.07834795999721421\n",
      "Iteration 2455 Loss: 0.07834432888053473\n",
      "Iteration 2456 Loss: 0.07834070623287392\n",
      "Iteration 2457 Loss: 0.07833709203361522\n",
      "Iteration 2458 Loss: 0.07833348626219222\n",
      "Iteration 2459 Loss: 0.0783298888980886\n",
      "Iteration 2460 Loss: 0.07832629992083809\n",
      "Iteration 2461 Loss: 0.07832271931002426\n",
      "Iteration 2462 Loss: 0.07831914704528038\n",
      "Iteration 2463 Loss: 0.07831558310628951\n",
      "Iteration 2464 Loss: 0.07831202747278407\n",
      "Iteration 2465 Loss: 0.07830848012454598\n",
      "Iteration 2466 Loss: 0.0783049410414063\n",
      "Iteration 2467 Loss: 0.07830141020324537\n",
      "Iteration 2468 Loss: 0.07829788758999241\n",
      "Iteration 2469 Loss: 0.07829437318162574\n",
      "Iteration 2470 Loss: 0.07829086695817236\n",
      "Iteration 2471 Loss: 0.07828736889970794\n",
      "Iteration 2472 Loss: 0.07828387898635673\n",
      "Iteration 2473 Loss: 0.07828039719829136\n",
      "Iteration 2474 Loss: 0.07827692351573282\n",
      "Iteration 2475 Loss: 0.07827345791895031\n",
      "Iteration 2476 Loss: 0.07827000038826118\n",
      "Iteration 2477 Loss: 0.07826655090403062\n",
      "Iteration 2478 Loss: 0.0782631094466717\n",
      "Iteration 2479 Loss: 0.07825967599664523\n",
      "Iteration 2480 Loss: 0.07825625053445964\n",
      "Iteration 2481 Loss: 0.07825283304067092\n",
      "Iteration 2482 Loss: 0.07824942349588229\n",
      "Iteration 2483 Loss: 0.07824602188074435\n",
      "Iteration 2484 Loss: 0.07824262817595494\n",
      "Iteration 2485 Loss: 0.07823924236225871\n",
      "Iteration 2486 Loss: 0.07823586442044748\n",
      "Iteration 2487 Loss: 0.07823249433135952\n",
      "Iteration 2488 Loss: 0.0782291320758803\n",
      "Iteration 2489 Loss: 0.07822577763494143\n",
      "Iteration 2490 Loss: 0.0782224309895212\n",
      "Iteration 2491 Loss: 0.0782190921206442\n",
      "Iteration 2492 Loss: 0.07821576100938137\n",
      "Iteration 2493 Loss: 0.07821243763684951\n",
      "Iteration 2494 Loss: 0.07820912198421172\n",
      "Iteration 2495 Loss: 0.07820581403267683\n",
      "Iteration 2496 Loss: 0.07820251376349957\n",
      "Iteration 2497 Loss: 0.07819922115798027\n",
      "Iteration 2498 Loss: 0.07819593619746486\n",
      "Iteration 2499 Loss: 0.07819265886334476\n",
      "Iteration 2500 Loss: 0.07818938913705663\n",
      "Iteration 2501 Loss: 0.07818612700008255\n",
      "Iteration 2502 Loss: 0.07818287243394954\n",
      "Iteration 2503 Loss: 0.07817962542022977\n",
      "Iteration 2504 Loss: 0.07817638594054017\n",
      "Iteration 2505 Loss: 0.07817315397654265\n",
      "Iteration 2506 Loss: 0.0781699295099437\n",
      "Iteration 2507 Loss: 0.07816671252249434\n",
      "Iteration 2508 Loss: 0.07816350299599022\n",
      "Iteration 2509 Loss: 0.07816030091227122\n",
      "Iteration 2510 Loss: 0.07815710625322146\n",
      "Iteration 2511 Loss: 0.07815391900076936\n",
      "Iteration 2512 Loss: 0.07815073913688723\n",
      "Iteration 2513 Loss: 0.07814756664359127\n",
      "Iteration 2514 Loss: 0.07814440150294179\n",
      "Iteration 2515 Loss: 0.07814124369704246\n",
      "Iteration 2516 Loss: 0.0781380932080408\n",
      "Iteration 2517 Loss: 0.07813495001812772\n",
      "Iteration 2518 Loss: 0.07813181410953761\n",
      "Iteration 2519 Loss: 0.07812868546454804\n",
      "Iteration 2520 Loss: 0.07812556406547984\n",
      "Iteration 2521 Loss: 0.07812244989469698\n",
      "Iteration 2522 Loss: 0.07811934293460635\n",
      "Iteration 2523 Loss: 0.07811624316765768\n",
      "Iteration 2524 Loss: 0.0781131505763435\n",
      "Iteration 2525 Loss: 0.07811006514319897\n",
      "Iteration 2526 Loss: 0.07810698685080186\n",
      "Iteration 2527 Loss: 0.07810391568177241\n",
      "Iteration 2528 Loss: 0.0781008516187731\n",
      "Iteration 2529 Loss: 0.0780977946445088\n",
      "Iteration 2530 Loss: 0.07809474474172648\n",
      "Iteration 2531 Loss: 0.0780917018932151\n",
      "Iteration 2532 Loss: 0.07808866608180559\n",
      "Iteration 2533 Loss: 0.07808563729037081\n",
      "Iteration 2534 Loss: 0.0780826155018251\n",
      "Iteration 2535 Loss: 0.0780796006991248\n",
      "Iteration 2536 Loss: 0.07807659286526751\n",
      "Iteration 2537 Loss: 0.07807359198329235\n",
      "Iteration 2538 Loss: 0.07807059803627976\n",
      "Iteration 2539 Loss: 0.07806761100735146\n",
      "Iteration 2540 Loss: 0.07806463087967021\n",
      "Iteration 2541 Loss: 0.07806165763643988\n",
      "Iteration 2542 Loss: 0.07805869126090527\n",
      "Iteration 2543 Loss: 0.07805573173635194\n",
      "Iteration 2544 Loss: 0.07805277904610622\n",
      "Iteration 2545 Loss: 0.0780498331735351\n",
      "Iteration 2546 Loss: 0.07804689410204609\n",
      "Iteration 2547 Loss: 0.07804396181508698\n",
      "Iteration 2548 Loss: 0.07804103629614631\n",
      "Iteration 2549 Loss: 0.07803811752875235\n",
      "Iteration 2550 Loss: 0.07803520549647383\n",
      "Iteration 2551 Loss: 0.07803230018291946\n",
      "Iteration 2552 Loss: 0.0780294015717378\n",
      "Iteration 2553 Loss: 0.0780265096466174\n",
      "Iteration 2554 Loss: 0.07802362439128642\n",
      "Iteration 2555 Loss: 0.07802074578951279\n",
      "Iteration 2556 Loss: 0.07801787382510388\n",
      "Iteration 2557 Loss: 0.0780150084819066\n",
      "Iteration 2558 Loss: 0.07801214974380731\n",
      "Iteration 2559 Loss: 0.07800929759473138\n",
      "Iteration 2560 Loss: 0.0780064520186436\n",
      "Iteration 2561 Loss: 0.0780036129995477\n",
      "Iteration 2562 Loss: 0.07800078052148643\n",
      "Iteration 2563 Loss: 0.07799795456854147\n",
      "Iteration 2564 Loss: 0.07799513512483319\n",
      "Iteration 2565 Loss: 0.07799232217452073\n",
      "Iteration 2566 Loss: 0.07798951570180185\n",
      "Iteration 2567 Loss: 0.07798671569091273\n",
      "Iteration 2568 Loss: 0.0779839221261281\n",
      "Iteration 2569 Loss: 0.07798113499176089\n",
      "Iteration 2570 Loss: 0.07797835427216224\n",
      "Iteration 2571 Loss: 0.07797557995172158\n",
      "Iteration 2572 Loss: 0.07797281201486621\n",
      "Iteration 2573 Loss: 0.07797005044606148\n",
      "Iteration 2574 Loss: 0.07796729522981055\n",
      "Iteration 2575 Loss: 0.07796454635065446\n",
      "Iteration 2576 Loss: 0.07796180379317166\n",
      "Iteration 2577 Loss: 0.07795906754197851\n",
      "Iteration 2578 Loss: 0.07795633758172853\n",
      "Iteration 2579 Loss: 0.07795361389711293\n",
      "Iteration 2580 Loss: 0.07795089647286005\n",
      "Iteration 2581 Loss: 0.07794818529373547\n",
      "Iteration 2582 Loss: 0.07794548034454195\n",
      "Iteration 2583 Loss: 0.07794278161011922\n",
      "Iteration 2584 Loss: 0.077940089075344\n",
      "Iteration 2585 Loss: 0.07793740272512985\n",
      "Iteration 2586 Loss: 0.07793472254442708\n",
      "Iteration 2587 Loss: 0.07793204851822268\n",
      "Iteration 2588 Loss: 0.07792938063154024\n",
      "Iteration 2589 Loss: 0.07792671886943986\n",
      "Iteration 2590 Loss: 0.07792406321701796\n",
      "Iteration 2591 Loss: 0.07792141365940736\n",
      "Iteration 2592 Loss: 0.07791877018177713\n",
      "Iteration 2593 Loss: 0.07791613276933239\n",
      "Iteration 2594 Loss: 0.07791350140731447\n",
      "Iteration 2595 Loss: 0.07791087608100035\n",
      "Iteration 2596 Loss: 0.07790825677570327\n",
      "Iteration 2597 Loss: 0.07790564347677204\n",
      "Iteration 2598 Loss: 0.07790303616959127\n",
      "Iteration 2599 Loss: 0.07790043483958102\n",
      "Iteration 2600 Loss: 0.07789783947219708\n",
      "Iteration 2601 Loss: 0.07789525005293058\n",
      "Iteration 2602 Loss: 0.07789266656730803\n",
      "Iteration 2603 Loss: 0.07789008900089119\n",
      "Iteration 2604 Loss: 0.077887517339277\n",
      "Iteration 2605 Loss: 0.07788495156809758\n",
      "Iteration 2606 Loss: 0.07788239167301993\n",
      "Iteration 2607 Loss: 0.07787983763974608\n",
      "Iteration 2608 Loss: 0.07787728945401291\n",
      "Iteration 2609 Loss: 0.0778747471015919\n",
      "Iteration 2610 Loss: 0.07787221056828945\n",
      "Iteration 2611 Loss: 0.0778696798399463\n",
      "Iteration 2612 Loss: 0.07786715490243795\n",
      "Iteration 2613 Loss: 0.07786463574167404\n",
      "Iteration 2614 Loss: 0.07786212234359878\n",
      "Iteration 2615 Loss: 0.07785961469419052\n",
      "Iteration 2616 Loss: 0.07785711277946178\n",
      "Iteration 2617 Loss: 0.07785461658545918\n",
      "Iteration 2618 Loss: 0.07785212609826343\n",
      "Iteration 2619 Loss: 0.07784964130398897\n",
      "Iteration 2620 Loss: 0.07784716218878421\n",
      "Iteration 2621 Loss: 0.0778446887388313\n",
      "Iteration 2622 Loss: 0.07784222094034608\n",
      "Iteration 2623 Loss: 0.07783975877957791\n",
      "Iteration 2624 Loss: 0.07783730224280974\n",
      "Iteration 2625 Loss: 0.07783485131635783\n",
      "Iteration 2626 Loss: 0.07783240598657198\n",
      "Iteration 2627 Loss: 0.07782996623983501\n",
      "Iteration 2628 Loss: 0.07782753206256315\n",
      "Iteration 2629 Loss: 0.07782510344120558\n",
      "Iteration 2630 Loss: 0.07782268036224456\n",
      "Iteration 2631 Loss: 0.0778202628121953\n",
      "Iteration 2632 Loss: 0.07781785077760581\n",
      "Iteration 2633 Loss: 0.077815444245057\n",
      "Iteration 2634 Loss: 0.07781304320116234\n",
      "Iteration 2635 Loss: 0.07781064763256798\n",
      "Iteration 2636 Loss: 0.0778082575259527\n",
      "Iteration 2637 Loss: 0.07780587286802756\n",
      "Iteration 2638 Loss: 0.07780349364553615\n",
      "Iteration 2639 Loss: 0.07780111984525431\n",
      "Iteration 2640 Loss: 0.07779875145399015\n",
      "Iteration 2641 Loss: 0.07779638845858385\n",
      "Iteration 2642 Loss: 0.07779403084590769\n",
      "Iteration 2643 Loss: 0.07779167860286597\n",
      "Iteration 2644 Loss: 0.07778933171639489\n",
      "Iteration 2645 Loss: 0.07778699017346248\n",
      "Iteration 2646 Loss: 0.07778465396106847\n",
      "Iteration 2647 Loss: 0.07778232306624437\n",
      "Iteration 2648 Loss: 0.07777999747605324\n",
      "Iteration 2649 Loss: 0.07777767717758968\n",
      "Iteration 2650 Loss: 0.0777753621579797\n",
      "Iteration 2651 Loss: 0.07777305240438072\n",
      "Iteration 2652 Loss: 0.0777707479039815\n",
      "Iteration 2653 Loss: 0.07776844864400184\n",
      "Iteration 2654 Loss: 0.077766154611693\n",
      "Iteration 2655 Loss: 0.07776386579433697\n",
      "Iteration 2656 Loss: 0.07776158217924695\n",
      "Iteration 2657 Loss: 0.0777593037537669\n",
      "Iteration 2658 Loss: 0.07775703050527183\n",
      "Iteration 2659 Loss: 0.07775476242116726\n",
      "Iteration 2660 Loss: 0.07775249948888961\n",
      "Iteration 2661 Loss: 0.07775024169590584\n",
      "Iteration 2662 Loss: 0.07774798902971351\n",
      "Iteration 2663 Loss: 0.0777457414778404\n",
      "Iteration 2664 Loss: 0.07774349902784503\n",
      "Iteration 2665 Loss: 0.07774126166731606\n",
      "Iteration 2666 Loss: 0.07773902938387235\n",
      "Iteration 2667 Loss: 0.07773680216516304\n",
      "Iteration 2668 Loss: 0.0777345799988673\n",
      "Iteration 2669 Loss: 0.07773236287269435\n",
      "Iteration 2670 Loss: 0.07773015077438336\n",
      "Iteration 2671 Loss: 0.07772794369170334\n",
      "Iteration 2672 Loss: 0.07772574161245326\n",
      "Iteration 2673 Loss: 0.07772354452446155\n",
      "Iteration 2674 Loss: 0.07772135241558653\n",
      "Iteration 2675 Loss: 0.077719165273716\n",
      "Iteration 2676 Loss: 0.07771698308676729\n",
      "Iteration 2677 Loss: 0.07771480584268728\n",
      "Iteration 2678 Loss: 0.077712633529452\n",
      "Iteration 2679 Loss: 0.07771046613506705\n",
      "Iteration 2680 Loss: 0.07770830364756703\n",
      "Iteration 2681 Loss: 0.07770614605501577\n",
      "Iteration 2682 Loss: 0.07770399334550628\n",
      "Iteration 2683 Loss: 0.07770184550716042\n",
      "Iteration 2684 Loss: 0.07769970252812912\n",
      "Iteration 2685 Loss: 0.07769756439659224\n",
      "Iteration 2686 Loss: 0.07769543110075827\n",
      "Iteration 2687 Loss: 0.07769330262886448\n",
      "Iteration 2688 Loss: 0.07769117896917686\n",
      "Iteration 2689 Loss: 0.07768906010999009\n",
      "Iteration 2690 Loss: 0.07768694603962714\n",
      "Iteration 2691 Loss: 0.07768483674643954\n",
      "Iteration 2692 Loss: 0.07768273221880734\n",
      "Iteration 2693 Loss: 0.07768063244513869\n",
      "Iteration 2694 Loss: 0.0776785374138701\n",
      "Iteration 2695 Loss: 0.07767644711346629\n",
      "Iteration 2696 Loss: 0.0776743615324199\n",
      "Iteration 2697 Loss: 0.07767228065925195\n",
      "Iteration 2698 Loss: 0.07767020448251115\n",
      "Iteration 2699 Loss: 0.07766813299077416\n",
      "Iteration 2700 Loss: 0.07766606617264557\n",
      "Iteration 2701 Loss: 0.07766400401675773\n",
      "Iteration 2702 Loss: 0.07766194651177054\n",
      "Iteration 2703 Loss: 0.07765989364637174\n",
      "Iteration 2704 Loss: 0.0776578454092765\n",
      "Iteration 2705 Loss: 0.0776558017892275\n",
      "Iteration 2706 Loss: 0.07765376277499492\n",
      "Iteration 2707 Loss: 0.07765172835537629\n",
      "Iteration 2708 Loss: 0.07764969851919633\n",
      "Iteration 2709 Loss: 0.07764767325530716\n",
      "Iteration 2710 Loss: 0.07764565255258801\n",
      "Iteration 2711 Loss: 0.07764363639994502\n",
      "Iteration 2712 Loss: 0.07764162478631172\n",
      "Iteration 2713 Loss: 0.07763961770064831\n",
      "Iteration 2714 Loss: 0.07763761513194206\n",
      "Iteration 2715 Loss: 0.07763561706920694\n",
      "Iteration 2716 Loss: 0.0776336235014838\n",
      "Iteration 2717 Loss: 0.07763163441784017\n",
      "Iteration 2718 Loss: 0.07762964980737018\n",
      "Iteration 2719 Loss: 0.07762766965919461\n",
      "Iteration 2720 Loss: 0.07762569396246069\n",
      "Iteration 2721 Loss: 0.07762372270634212\n",
      "Iteration 2722 Loss: 0.0776217558800389\n",
      "Iteration 2723 Loss: 0.07761979347277753\n",
      "Iteration 2724 Loss: 0.07761783547381064\n",
      "Iteration 2725 Loss: 0.07761588187241707\n",
      "Iteration 2726 Loss: 0.07761393265790176\n",
      "Iteration 2727 Loss: 0.07761198781959568\n",
      "Iteration 2728 Loss: 0.07761004734685592\n",
      "Iteration 2729 Loss: 0.07760811122906551\n",
      "Iteration 2730 Loss: 0.07760617945563315\n",
      "Iteration 2731 Loss: 0.07760425201599358\n",
      "Iteration 2732 Loss: 0.07760232889960705\n",
      "Iteration 2733 Loss: 0.07760041009595979\n",
      "Iteration 2734 Loss: 0.07759849559456339\n",
      "Iteration 2735 Loss: 0.07759658538495519\n",
      "Iteration 2736 Loss: 0.07759467945669786\n",
      "Iteration 2737 Loss: 0.07759277779937954\n",
      "Iteration 2738 Loss: 0.07759088040261386\n",
      "Iteration 2739 Loss: 0.07758898725603969\n",
      "Iteration 2740 Loss: 0.07758709834932113\n",
      "Iteration 2741 Loss: 0.07758521367214741\n",
      "Iteration 2742 Loss: 0.07758333321423304\n",
      "Iteration 2743 Loss: 0.07758145696531744\n",
      "Iteration 2744 Loss: 0.07757958491516509\n",
      "Iteration 2745 Loss: 0.0775777170535655\n",
      "Iteration 2746 Loss: 0.07757585337033293\n",
      "Iteration 2747 Loss: 0.07757399385530656\n",
      "Iteration 2748 Loss: 0.07757213849835021\n",
      "Iteration 2749 Loss: 0.07757028728935254\n",
      "Iteration 2750 Loss: 0.07756844021822673\n",
      "Iteration 2751 Loss: 0.07756659727491061\n",
      "Iteration 2752 Loss: 0.07756475844936661\n",
      "Iteration 2753 Loss: 0.07756292373158143\n",
      "Iteration 2754 Loss: 0.0775610931115662\n",
      "Iteration 2755 Loss: 0.07755926657935654\n",
      "Iteration 2756 Loss: 0.07755744412501234\n",
      "Iteration 2757 Loss: 0.07755562573861749\n",
      "Iteration 2758 Loss: 0.07755381141028032\n",
      "Iteration 2759 Loss: 0.07755200113013305\n",
      "Iteration 2760 Loss: 0.07755019488833209\n",
      "Iteration 2761 Loss: 0.07754839267505777\n",
      "Iteration 2762 Loss: 0.07754659448051429\n",
      "Iteration 2763 Loss: 0.07754480029492994\n",
      "Iteration 2764 Loss: 0.07754301010855653\n",
      "Iteration 2765 Loss: 0.07754122391166987\n",
      "Iteration 2766 Loss: 0.07753944169456928\n",
      "Iteration 2767 Loss: 0.07753766344757787\n",
      "Iteration 2768 Loss: 0.07753588916104225\n",
      "Iteration 2769 Loss: 0.07753411882533254\n",
      "Iteration 2770 Loss: 0.07753235243084239\n",
      "Iteration 2771 Loss: 0.0775305899679888\n",
      "Iteration 2772 Loss: 0.07752883142721215\n",
      "Iteration 2773 Loss: 0.07752707679897611\n",
      "Iteration 2774 Loss: 0.07752532607376762\n",
      "Iteration 2775 Loss: 0.07752357924209669\n",
      "Iteration 2776 Loss: 0.07752183629449663\n",
      "Iteration 2777 Loss: 0.07752009722152367\n",
      "Iteration 2778 Loss: 0.0775183620137571\n",
      "Iteration 2779 Loss: 0.07751663066179919\n",
      "Iteration 2780 Loss: 0.0775149031562752\n",
      "Iteration 2781 Loss: 0.07751317948783291\n",
      "Iteration 2782 Loss: 0.07751145964714333\n",
      "Iteration 2783 Loss: 0.07750974362489985\n",
      "Iteration 2784 Loss: 0.07750803141181867\n",
      "Iteration 2785 Loss: 0.07750632299863872\n",
      "Iteration 2786 Loss: 0.07750461837612124\n",
      "Iteration 2787 Loss: 0.07750291753505026\n",
      "Iteration 2788 Loss: 0.07750122046623212\n",
      "Iteration 2789 Loss: 0.07749952716049553\n",
      "Iteration 2790 Loss: 0.07749783760869168\n",
      "Iteration 2791 Loss: 0.07749615180169389\n",
      "Iteration 2792 Loss: 0.07749446973039793\n",
      "Iteration 2793 Loss: 0.07749279138572153\n",
      "Iteration 2794 Loss: 0.07749111675860468\n",
      "Iteration 2795 Loss: 0.07748944584000945\n",
      "Iteration 2796 Loss: 0.07748777862091986\n",
      "Iteration 2797 Loss: 0.07748611509234209\n",
      "Iteration 2798 Loss: 0.07748445524530391\n",
      "Iteration 2799 Loss: 0.07748279907085523\n",
      "Iteration 2800 Loss: 0.07748114656006763\n",
      "Iteration 2801 Loss: 0.07747949770403458\n",
      "Iteration 2802 Loss: 0.07747785249387114\n",
      "Iteration 2803 Loss: 0.07747621092071401\n",
      "Iteration 2804 Loss: 0.07747457297572152\n",
      "Iteration 2805 Loss: 0.07747293865007356\n",
      "Iteration 2806 Loss: 0.07747130793497158\n",
      "Iteration 2807 Loss: 0.07746968082163831\n",
      "Iteration 2808 Loss: 0.07746805730131803\n",
      "Iteration 2809 Loss: 0.07746643736527621\n",
      "Iteration 2810 Loss: 0.07746482100479972\n",
      "Iteration 2811 Loss: 0.07746320821119662\n",
      "Iteration 2812 Loss: 0.0774615989757961\n",
      "Iteration 2813 Loss: 0.07745999328994856\n",
      "Iteration 2814 Loss: 0.07745839114502553\n",
      "Iteration 2815 Loss: 0.07745679253241937\n",
      "Iteration 2816 Loss: 0.07745519744354355\n",
      "Iteration 2817 Loss: 0.07745360586983244\n",
      "Iteration 2818 Loss: 0.07745201780274132\n",
      "Iteration 2819 Loss: 0.07745043323374622\n",
      "Iteration 2820 Loss: 0.077448852154344\n",
      "Iteration 2821 Loss: 0.0774472745560522\n",
      "Iteration 2822 Loss: 0.07744570043040908\n",
      "Iteration 2823 Loss: 0.0774441297689734\n",
      "Iteration 2824 Loss: 0.07744256256332471\n",
      "Iteration 2825 Loss: 0.07744099880506283\n",
      "Iteration 2826 Loss: 0.07743943848580823\n",
      "Iteration 2827 Loss: 0.07743788159720168\n",
      "Iteration 2828 Loss: 0.0774363281309045\n",
      "Iteration 2829 Loss: 0.07743477807859805\n",
      "Iteration 2830 Loss: 0.0774332314319842\n",
      "Iteration 2831 Loss: 0.07743168818278492\n",
      "Iteration 2832 Loss: 0.07743014832274243\n",
      "Iteration 2833 Loss: 0.07742861184361907\n",
      "Iteration 2834 Loss: 0.07742707873719708\n",
      "Iteration 2835 Loss: 0.07742554899527895\n",
      "Iteration 2836 Loss: 0.07742402260968707\n",
      "Iteration 2837 Loss: 0.07742249957226371\n",
      "Iteration 2838 Loss: 0.07742097987487105\n",
      "Iteration 2839 Loss: 0.07741946350939108\n",
      "Iteration 2840 Loss: 0.07741795046772572\n",
      "Iteration 2841 Loss: 0.07741644074179632\n",
      "Iteration 2842 Loss: 0.0774149343235442\n",
      "Iteration 2843 Loss: 0.07741343120493019\n",
      "Iteration 2844 Loss: 0.07741193137793469\n",
      "Iteration 2845 Loss: 0.07741043483455778\n",
      "Iteration 2846 Loss: 0.07740894156681888\n",
      "Iteration 2847 Loss: 0.0774074515667569\n",
      "Iteration 2848 Loss: 0.07740596482643022\n",
      "Iteration 2849 Loss: 0.07740448133791644\n",
      "Iteration 2850 Loss: 0.07740300109331255\n",
      "Iteration 2851 Loss: 0.07740152408473486\n",
      "Iteration 2852 Loss: 0.07740005030431879\n",
      "Iteration 2853 Loss: 0.07739857974421899\n",
      "Iteration 2854 Loss: 0.07739711239660908\n",
      "Iteration 2855 Loss: 0.07739564825368195\n",
      "Iteration 2856 Loss: 0.07739418730764942\n",
      "Iteration 2857 Loss: 0.07739272955074228\n",
      "Iteration 2858 Loss: 0.07739127497521037\n",
      "Iteration 2859 Loss: 0.07738982357332212\n",
      "Iteration 2860 Loss: 0.07738837533736509\n",
      "Iteration 2861 Loss: 0.0773869302596456\n",
      "Iteration 2862 Loss: 0.07738548833248861\n",
      "Iteration 2863 Loss: 0.07738404954823781\n",
      "Iteration 2864 Loss: 0.07738261389925558\n",
      "Iteration 2865 Loss: 0.07738118137792294\n",
      "Iteration 2866 Loss: 0.07737975197663932\n",
      "Iteration 2867 Loss: 0.07737832568782294\n",
      "Iteration 2868 Loss: 0.0773769025039102\n",
      "Iteration 2869 Loss: 0.07737548241735613\n",
      "Iteration 2870 Loss: 0.07737406542063413\n",
      "Iteration 2871 Loss: 0.0773726515062358\n",
      "Iteration 2872 Loss: 0.0773712406666712\n",
      "Iteration 2873 Loss: 0.07736983289446854\n",
      "Iteration 2874 Loss: 0.07736842818217435\n",
      "Iteration 2875 Loss: 0.07736702652235312\n",
      "Iteration 2876 Loss: 0.07736562790758762\n",
      "Iteration 2877 Loss: 0.0773642323304788\n",
      "Iteration 2878 Loss: 0.07736283978364533\n",
      "Iteration 2879 Loss: 0.07736145025972407\n",
      "Iteration 2880 Loss: 0.07736006375136982\n",
      "Iteration 2881 Loss: 0.07735868025125521\n",
      "Iteration 2882 Loss: 0.0773572997520708\n",
      "Iteration 2883 Loss: 0.07735592224652495\n",
      "Iteration 2884 Loss: 0.07735454772734367\n",
      "Iteration 2885 Loss: 0.07735317618727078\n",
      "Iteration 2886 Loss: 0.07735180761906796\n",
      "Iteration 2887 Loss: 0.07735044201551414\n",
      "Iteration 2888 Loss: 0.07734907936940615\n",
      "Iteration 2889 Loss: 0.07734771967355832\n",
      "Iteration 2890 Loss: 0.07734636292080248\n",
      "Iteration 2891 Loss: 0.07734500910398782\n",
      "Iteration 2892 Loss: 0.077343658215981\n",
      "Iteration 2893 Loss: 0.07734231024966624\n",
      "Iteration 2894 Loss: 0.0773409651979448\n",
      "Iteration 2895 Loss: 0.07733962305373555\n",
      "Iteration 2896 Loss: 0.0773382838099744\n",
      "Iteration 2897 Loss: 0.07733694745961447\n",
      "Iteration 2898 Loss: 0.07733561399562622\n",
      "Iteration 2899 Loss: 0.07733428341099705\n",
      "Iteration 2900 Loss: 0.07733295569873164\n",
      "Iteration 2901 Loss: 0.07733163085185149\n",
      "Iteration 2902 Loss: 0.07733030886339537\n",
      "Iteration 2903 Loss: 0.07732898972641872\n",
      "Iteration 2904 Loss: 0.07732767343399413\n",
      "Iteration 2905 Loss: 0.07732635997921099\n",
      "Iteration 2906 Loss: 0.0773250493551755\n",
      "Iteration 2907 Loss: 0.07732374155501075\n",
      "Iteration 2908 Loss: 0.07732243657185647\n",
      "Iteration 2909 Loss: 0.07732113439886917\n",
      "Iteration 2910 Loss: 0.07731983502922209\n",
      "Iteration 2911 Loss: 0.077318538456105\n",
      "Iteration 2912 Loss: 0.07731724467272429\n",
      "Iteration 2913 Loss: 0.07731595367230296\n",
      "Iteration 2914 Loss: 0.07731466544808051\n",
      "Iteration 2915 Loss: 0.07731337999331284\n",
      "Iteration 2916 Loss: 0.0773120973012724\n",
      "Iteration 2917 Loss: 0.07731081736524788\n",
      "Iteration 2918 Loss: 0.07730954017854455\n",
      "Iteration 2919 Loss: 0.07730826573448372\n",
      "Iteration 2920 Loss: 0.07730699402640316\n",
      "Iteration 2921 Loss: 0.0773057250476569\n",
      "Iteration 2922 Loss: 0.07730445879161502\n",
      "Iteration 2923 Loss: 0.07730319525166389\n",
      "Iteration 2924 Loss: 0.07730193442120584\n",
      "Iteration 2925 Loss: 0.07730067629365939\n",
      "Iteration 2926 Loss: 0.07729942086245917\n",
      "Iteration 2927 Loss: 0.07729816812105561\n",
      "Iteration 2928 Loss: 0.07729691806291529\n",
      "Iteration 2929 Loss: 0.07729567068152056\n",
      "Iteration 2930 Loss: 0.07729442597036976\n",
      "Iteration 2931 Loss: 0.07729318392297707\n",
      "Iteration 2932 Loss: 0.07729194453287229\n",
      "Iteration 2933 Loss: 0.07729070779360132\n",
      "Iteration 2934 Loss: 0.07728947369872546\n",
      "Iteration 2935 Loss: 0.0772882422418219\n",
      "Iteration 2936 Loss: 0.0772870134164834\n",
      "Iteration 2937 Loss: 0.07728578721631842\n",
      "Iteration 2938 Loss: 0.07728456363495084\n",
      "Iteration 2939 Loss: 0.07728334266602023\n",
      "Iteration 2940 Loss: 0.07728212430318154\n",
      "Iteration 2941 Loss: 0.07728090854010525\n",
      "Iteration 2942 Loss: 0.07727969537047731\n",
      "Iteration 2943 Loss: 0.07727848478799887\n",
      "Iteration 2944 Loss: 0.07727727678638666\n",
      "Iteration 2945 Loss: 0.07727607135937267\n",
      "Iteration 2946 Loss: 0.0772748685007039\n",
      "Iteration 2947 Loss: 0.07727366820414304\n",
      "Iteration 2948 Loss: 0.07727247046346751\n",
      "Iteration 2949 Loss: 0.07727127527247028\n",
      "Iteration 2950 Loss: 0.07727008262495919\n",
      "Iteration 2951 Loss: 0.0772688925147573\n",
      "Iteration 2952 Loss: 0.0772677049357027\n",
      "Iteration 2953 Loss: 0.07726651988164847\n",
      "Iteration 2954 Loss: 0.07726533734646267\n",
      "Iteration 2955 Loss: 0.07726415732402826\n",
      "Iteration 2956 Loss: 0.07726297980824316\n",
      "Iteration 2957 Loss: 0.0772618047930203\n",
      "Iteration 2958 Loss: 0.07726063227228713\n",
      "Iteration 2959 Loss: 0.07725946223998614\n",
      "Iteration 2960 Loss: 0.0772582946900745\n",
      "Iteration 2961 Loss: 0.07725712961652408\n",
      "Iteration 2962 Loss: 0.07725596701332146\n",
      "Iteration 2963 Loss: 0.07725480687446788\n",
      "Iteration 2964 Loss: 0.07725364919397933\n",
      "Iteration 2965 Loss: 0.07725249396588608\n",
      "Iteration 2966 Loss: 0.07725134118423319\n",
      "Iteration 2967 Loss: 0.07725019084308014\n",
      "Iteration 2968 Loss: 0.07724904293650098\n",
      "Iteration 2969 Loss: 0.07724789745858397\n",
      "Iteration 2970 Loss: 0.07724675440343208\n",
      "Iteration 2971 Loss: 0.07724561376516245\n",
      "Iteration 2972 Loss: 0.07724447553790656\n",
      "Iteration 2973 Loss: 0.07724333971581027\n",
      "Iteration 2974 Loss: 0.07724220629303367\n",
      "Iteration 2975 Loss: 0.07724107526375108\n",
      "Iteration 2976 Loss: 0.07723994662215108\n",
      "Iteration 2977 Loss: 0.07723882036243615\n",
      "Iteration 2978 Loss: 0.0772376964788233\n",
      "Iteration 2979 Loss: 0.07723657496554333\n",
      "Iteration 2980 Loss: 0.07723545581684121\n",
      "Iteration 2981 Loss: 0.0772343390269759\n",
      "Iteration 2982 Loss: 0.07723322459022045\n",
      "Iteration 2983 Loss: 0.07723211250086165\n",
      "Iteration 2984 Loss: 0.07723100275320044\n",
      "Iteration 2985 Loss: 0.07722989534155152\n",
      "Iteration 2986 Loss: 0.07722879026024346\n",
      "Iteration 2987 Loss: 0.07722768750361869\n",
      "Iteration 2988 Loss: 0.07722658706603334\n",
      "Iteration 2989 Loss: 0.0772254889418574\n",
      "Iteration 2990 Loss: 0.07722439312547463\n",
      "Iteration 2991 Loss: 0.0772232996112822\n",
      "Iteration 2992 Loss: 0.07722220839369112\n",
      "Iteration 2993 Loss: 0.07722111946712619\n",
      "Iteration 2994 Loss: 0.07722003282602546\n",
      "Iteration 2995 Loss: 0.07721894846484069\n",
      "Iteration 2996 Loss: 0.0772178663780372\n",
      "Iteration 2997 Loss: 0.07721678656009374\n",
      "Iteration 2998 Loss: 0.07721570900550255\n",
      "Iteration 2999 Loss: 0.07721463370876924\n",
      "Iteration 3000 Loss: 0.07721356066441284\n",
      "Iteration 3001 Loss: 0.07721248986696573\n",
      "Iteration 3002 Loss: 0.07721142131097353\n",
      "Iteration 3003 Loss: 0.07721035499099545\n",
      "Iteration 3004 Loss: 0.0772092909016036\n",
      "Iteration 3005 Loss: 0.0772082290373834\n",
      "Iteration 3006 Loss: 0.07720716939293365\n",
      "Iteration 3007 Loss: 0.07720611196286613\n",
      "Iteration 3008 Loss: 0.0772050567418058\n",
      "Iteration 3009 Loss: 0.07720400372439074\n",
      "Iteration 3010 Loss: 0.077202952905272\n",
      "Iteration 3011 Loss: 0.07720190427911387\n",
      "Iteration 3012 Loss: 0.07720085784059347\n",
      "Iteration 3013 Loss: 0.07719981358440084\n",
      "Iteration 3014 Loss: 0.0771987715052392\n",
      "Iteration 3015 Loss: 0.07719773159782442\n",
      "Iteration 3016 Loss: 0.07719669385688539\n",
      "Iteration 3017 Loss: 0.07719565827716383\n",
      "Iteration 3018 Loss: 0.0771946248534142\n",
      "Iteration 3019 Loss: 0.07719359358040388\n",
      "Iteration 3020 Loss: 0.0771925644529128\n",
      "Iteration 3021 Loss: 0.07719153746573379\n",
      "Iteration 3022 Loss: 0.07719051261367234\n",
      "Iteration 3023 Loss: 0.07718948989154648\n",
      "Iteration 3024 Loss: 0.07718846929418699\n",
      "Iteration 3025 Loss: 0.07718745081643724\n",
      "Iteration 3026 Loss: 0.07718643445315307\n",
      "Iteration 3027 Loss: 0.07718542019920294\n",
      "Iteration 3028 Loss: 0.0771844080494678\n",
      "Iteration 3029 Loss: 0.07718339799884101\n",
      "Iteration 3030 Loss: 0.07718239004222846\n",
      "Iteration 3031 Loss: 0.07718138417454841\n",
      "Iteration 3032 Loss: 0.07718038039073151\n",
      "Iteration 3033 Loss: 0.0771793786857208\n",
      "Iteration 3034 Loss: 0.07717837905447159\n",
      "Iteration 3035 Loss: 0.07717738149195144\n",
      "Iteration 3036 Loss: 0.0771763859931403\n",
      "Iteration 3037 Loss: 0.07717539255303033\n",
      "Iteration 3038 Loss: 0.07717440116662569\n",
      "Iteration 3039 Loss: 0.07717341182894297\n",
      "Iteration 3040 Loss: 0.07717242453501078\n",
      "Iteration 3041 Loss: 0.07717143927986986\n",
      "Iteration 3042 Loss: 0.0771704560585731\n",
      "Iteration 3043 Loss: 0.07716947486618532\n",
      "Iteration 3044 Loss: 0.07716849569778345\n",
      "Iteration 3045 Loss: 0.07716751854845634\n",
      "Iteration 3046 Loss: 0.07716654341330502\n",
      "Iteration 3047 Loss: 0.07716557028744217\n",
      "Iteration 3048 Loss: 0.07716459916599253\n",
      "Iteration 3049 Loss: 0.0771636300440927\n",
      "Iteration 3050 Loss: 0.0771626629168912\n",
      "Iteration 3051 Loss: 0.07716169777954819\n",
      "Iteration 3052 Loss: 0.07716073462723584\n",
      "Iteration 3053 Loss: 0.07715977345513785\n",
      "Iteration 3054 Loss: 0.0771588142584499\n",
      "Iteration 3055 Loss: 0.07715785703237929\n",
      "Iteration 3056 Loss: 0.0771569017721449\n",
      "Iteration 3057 Loss: 0.07715594847297727\n",
      "Iteration 3058 Loss: 0.07715499713011878\n",
      "Iteration 3059 Loss: 0.0771540477388231\n",
      "Iteration 3060 Loss: 0.07715310029435571\n",
      "Iteration 3061 Loss: 0.07715215479199346\n",
      "Iteration 3062 Loss: 0.0771512112270248\n",
      "Iteration 3063 Loss: 0.07715026959474969\n",
      "Iteration 3064 Loss: 0.07714932989047937\n",
      "Iteration 3065 Loss: 0.07714839210953674\n",
      "Iteration 3066 Loss: 0.07714745624725591\n",
      "Iteration 3067 Loss: 0.07714652229898247\n",
      "Iteration 3068 Loss: 0.07714559026007332\n",
      "Iteration 3069 Loss: 0.07714466012589667\n",
      "Iteration 3070 Loss: 0.07714373189183188\n",
      "Iteration 3071 Loss: 0.07714280555326995\n",
      "Iteration 3072 Loss: 0.0771418811056126\n",
      "Iteration 3073 Loss: 0.07714095854427319\n",
      "Iteration 3074 Loss: 0.07714003786467603\n",
      "Iteration 3075 Loss: 0.07713911906225666\n",
      "Iteration 3076 Loss: 0.07713820213246164\n",
      "Iteration 3077 Loss: 0.07713728707074881\n",
      "Iteration 3078 Loss: 0.07713637387258687\n",
      "Iteration 3079 Loss: 0.07713546253345566\n",
      "Iteration 3080 Loss: 0.07713455304884612\n",
      "Iteration 3081 Loss: 0.07713364541425995\n",
      "Iteration 3082 Loss: 0.07713273962521014\n",
      "Iteration 3083 Loss: 0.07713183567722026\n",
      "Iteration 3084 Loss: 0.0771309335658251\n",
      "Iteration 3085 Loss: 0.07713003328657006\n",
      "Iteration 3086 Loss: 0.07712913483501156\n",
      "Iteration 3087 Loss: 0.07712823820671677\n",
      "Iteration 3088 Loss: 0.07712734339726372\n",
      "Iteration 3089 Loss: 0.07712645040224123\n",
      "Iteration 3090 Loss: 0.07712555921724883\n",
      "Iteration 3091 Loss: 0.07712466983789669\n",
      "Iteration 3092 Loss: 0.07712378225980583\n",
      "Iteration 3093 Loss: 0.07712289647860787\n",
      "Iteration 3094 Loss: 0.077122012489945\n",
      "Iteration 3095 Loss: 0.07712113028947022\n",
      "Iteration 3096 Loss: 0.07712024987284688\n",
      "Iteration 3097 Loss: 0.07711937123574916\n",
      "Iteration 3098 Loss: 0.07711849437386153\n",
      "Iteration 3099 Loss: 0.07711761928287912\n",
      "Iteration 3100 Loss: 0.07711674595850748\n",
      "Iteration 3101 Loss: 0.07711587439646275\n",
      "Iteration 3102 Loss: 0.07711500459247138\n",
      "Iteration 3103 Loss: 0.07711413654227021\n",
      "Iteration 3104 Loss: 0.07711327024160662\n",
      "Iteration 3105 Loss: 0.07711240568623821\n",
      "Iteration 3106 Loss: 0.07711154287193306\n",
      "Iteration 3107 Loss: 0.07711068179446931\n",
      "Iteration 3108 Loss: 0.07710982244963568\n",
      "Iteration 3109 Loss: 0.07710896483323096\n",
      "Iteration 3110 Loss: 0.07710810894106429\n",
      "Iteration 3111 Loss: 0.07710725476895487\n",
      "Iteration 3112 Loss: 0.07710640231273236\n",
      "Iteration 3113 Loss: 0.07710555156823622\n",
      "Iteration 3114 Loss: 0.07710470253131635\n",
      "Iteration 3115 Loss: 0.07710385519783262\n",
      "Iteration 3116 Loss: 0.07710300956365503\n",
      "Iteration 3117 Loss: 0.07710216562466357\n",
      "Iteration 3118 Loss: 0.07710132337674845\n",
      "Iteration 3119 Loss: 0.07710048281580961\n",
      "Iteration 3120 Loss: 0.07709964393775731\n",
      "Iteration 3121 Loss: 0.07709880673851151\n",
      "Iteration 3122 Loss: 0.07709797121400228\n",
      "Iteration 3123 Loss: 0.0770971373601694\n",
      "Iteration 3124 Loss: 0.07709630517296284\n",
      "Iteration 3125 Loss: 0.07709547464834224\n",
      "Iteration 3126 Loss: 0.07709464578227701\n",
      "Iteration 3127 Loss: 0.07709381857074665\n",
      "Iteration 3128 Loss: 0.07709299300974026\n",
      "Iteration 3129 Loss: 0.0770921690952567\n",
      "Iteration 3130 Loss: 0.07709134682330468\n",
      "Iteration 3131 Loss: 0.07709052618990256\n",
      "Iteration 3132 Loss: 0.07708970719107848\n",
      "Iteration 3133 Loss: 0.07708888982287018\n",
      "Iteration 3134 Loss: 0.07708807408132506\n",
      "Iteration 3135 Loss: 0.07708725996250025\n",
      "Iteration 3136 Loss: 0.07708644746246235\n",
      "Iteration 3137 Loss: 0.07708563657728765\n",
      "Iteration 3138 Loss: 0.07708482730306189\n",
      "Iteration 3139 Loss: 0.07708401963588052\n",
      "Iteration 3140 Loss: 0.0770832135718483\n",
      "Iteration 3141 Loss: 0.07708240910707968\n",
      "Iteration 3142 Loss: 0.07708160623769839\n",
      "Iteration 3143 Loss: 0.0770808049598378\n",
      "Iteration 3144 Loss: 0.0770800052696404\n",
      "Iteration 3145 Loss: 0.07707920716325846\n",
      "Iteration 3146 Loss: 0.07707841063685343\n",
      "Iteration 3147 Loss: 0.07707761568659598\n",
      "Iteration 3148 Loss: 0.07707682230866643\n",
      "Iteration 3149 Loss: 0.07707603049925411\n",
      "Iteration 3150 Loss: 0.07707524025455786\n",
      "Iteration 3151 Loss: 0.0770744515707856\n",
      "Iteration 3152 Loss: 0.07707366444415462\n",
      "Iteration 3153 Loss: 0.07707287887089127\n",
      "Iteration 3154 Loss: 0.07707209484723132\n",
      "Iteration 3155 Loss: 0.07707131236941957\n",
      "Iteration 3156 Loss: 0.07707053143370998\n",
      "Iteration 3157 Loss: 0.07706975203636561\n",
      "Iteration 3158 Loss: 0.07706897417365878\n",
      "Iteration 3159 Loss: 0.07706819784187065\n",
      "Iteration 3160 Loss: 0.07706742303729165\n",
      "Iteration 3161 Loss: 0.07706664975622114\n",
      "Iteration 3162 Loss: 0.0770658779949675\n",
      "Iteration 3163 Loss: 0.07706510774984826\n",
      "Iteration 3164 Loss: 0.07706433901718959\n",
      "Iteration 3165 Loss: 0.07706357179332701\n",
      "Iteration 3166 Loss: 0.07706280607460465\n",
      "Iteration 3167 Loss: 0.07706204185737575\n",
      "Iteration 3168 Loss: 0.07706127913800231\n",
      "Iteration 3169 Loss: 0.07706051791285529\n",
      "Iteration 3170 Loss: 0.0770597581783144\n",
      "Iteration 3171 Loss: 0.07705899993076826\n",
      "Iteration 3172 Loss: 0.07705824316661421\n",
      "Iteration 3173 Loss: 0.07705748788225839\n",
      "Iteration 3174 Loss: 0.07705673407411578\n",
      "Iteration 3175 Loss: 0.07705598173861002\n",
      "Iteration 3176 Loss: 0.07705523087217342\n",
      "Iteration 3177 Loss: 0.07705448147124704\n",
      "Iteration 3178 Loss: 0.07705373353228066\n",
      "Iteration 3179 Loss: 0.07705298705173264\n",
      "Iteration 3180 Loss: 0.0770522420260699\n",
      "Iteration 3181 Loss: 0.0770514984517682\n",
      "Iteration 3182 Loss: 0.07705075632531168\n",
      "Iteration 3183 Loss: 0.07705001564319305\n",
      "Iteration 3184 Loss: 0.0770492764019137\n",
      "Iteration 3185 Loss: 0.0770485385979835\n",
      "Iteration 3186 Loss: 0.07704780222792068\n",
      "Iteration 3187 Loss: 0.07704706728825216\n",
      "Iteration 3188 Loss: 0.07704633377551322\n",
      "Iteration 3189 Loss: 0.07704560168624754\n",
      "Iteration 3190 Loss: 0.07704487101700734\n",
      "Iteration 3191 Loss: 0.07704414176435315\n",
      "Iteration 3192 Loss: 0.07704341392485388\n",
      "Iteration 3193 Loss: 0.07704268749508689\n",
      "Iteration 3194 Loss: 0.07704196247163775\n",
      "Iteration 3195 Loss: 0.07704123885110045\n",
      "Iteration 3196 Loss: 0.07704051663007724\n",
      "Iteration 3197 Loss: 0.0770397958051787\n",
      "Iteration 3198 Loss: 0.07703907637302351\n",
      "Iteration 3199 Loss: 0.0770383583302388\n",
      "Iteration 3200 Loss: 0.0770376416734598\n",
      "Iteration 3201 Loss: 0.07703692639932994\n",
      "Iteration 3202 Loss: 0.07703621250450086\n",
      "Iteration 3203 Loss: 0.07703549998563233\n",
      "Iteration 3204 Loss: 0.07703478883939227\n",
      "Iteration 3205 Loss: 0.07703407906245678\n",
      "Iteration 3206 Loss: 0.07703337065150992\n",
      "Iteration 3207 Loss: 0.07703266360324394\n",
      "Iteration 3208 Loss: 0.07703195791435918\n",
      "Iteration 3209 Loss: 0.0770312535815639\n",
      "Iteration 3210 Loss: 0.07703055060157445\n",
      "Iteration 3211 Loss: 0.07702984897111521\n",
      "Iteration 3212 Loss: 0.07702914868691843\n",
      "Iteration 3213 Loss: 0.07702844974572448\n",
      "Iteration 3214 Loss: 0.07702775214428154\n",
      "Iteration 3215 Loss: 0.07702705587934583\n",
      "Iteration 3216 Loss: 0.07702636094768128\n",
      "Iteration 3217 Loss: 0.07702566734605988\n",
      "Iteration 3218 Loss: 0.07702497507126153\n",
      "Iteration 3219 Loss: 0.07702428412007375\n",
      "Iteration 3220 Loss: 0.07702359448929211\n",
      "Iteration 3221 Loss: 0.07702290617571987\n",
      "Iteration 3222 Loss: 0.0770222191761681\n",
      "Iteration 3223 Loss: 0.07702153348745563\n",
      "Iteration 3224 Loss: 0.07702084910640912\n",
      "Iteration 3225 Loss: 0.07702016602986285\n",
      "Iteration 3226 Loss: 0.07701948425465885\n",
      "Iteration 3227 Loss: 0.07701880377764693\n",
      "Iteration 3228 Loss: 0.07701812459568438\n",
      "Iteration 3229 Loss: 0.07701744670563641\n",
      "Iteration 3230 Loss: 0.07701677010437565\n",
      "Iteration 3231 Loss: 0.07701609478878244\n",
      "Iteration 3232 Loss: 0.07701542075574469\n",
      "Iteration 3233 Loss: 0.07701474800215799\n",
      "Iteration 3234 Loss: 0.07701407652492534\n",
      "Iteration 3235 Loss: 0.07701340632095743\n",
      "Iteration 3236 Loss: 0.07701273738717232\n",
      "Iteration 3237 Loss: 0.07701206972049573\n",
      "Iteration 3238 Loss: 0.07701140331786087\n",
      "Iteration 3239 Loss: 0.07701073817620822\n",
      "Iteration 3240 Loss: 0.077010074292486\n",
      "Iteration 3241 Loss: 0.07700941166364964\n",
      "Iteration 3242 Loss: 0.07700875028666211\n",
      "Iteration 3243 Loss: 0.07700809015849369\n",
      "Iteration 3244 Loss: 0.07700743127612217\n",
      "Iteration 3245 Loss: 0.07700677363653256\n",
      "Iteration 3246 Loss: 0.07700611723671734\n",
      "Iteration 3247 Loss: 0.07700546207367623\n",
      "Iteration 3248 Loss: 0.07700480814441626\n",
      "Iteration 3249 Loss: 0.07700415544595185\n",
      "Iteration 3250 Loss: 0.07700350397530462\n",
      "Iteration 3251 Loss: 0.07700285372950344\n",
      "Iteration 3252 Loss: 0.07700220470558444\n",
      "Iteration 3253 Loss: 0.07700155690059098\n",
      "Iteration 3254 Loss: 0.07700091031157356\n",
      "Iteration 3255 Loss: 0.07700026493558998\n",
      "Iteration 3256 Loss: 0.0769996207697051\n",
      "Iteration 3257 Loss: 0.07699897781099102\n",
      "Iteration 3258 Loss: 0.07699833605652691\n",
      "Iteration 3259 Loss: 0.07699769550339906\n",
      "Iteration 3260 Loss: 0.07699705614870099\n",
      "Iteration 3261 Loss: 0.07699641798953301\n",
      "Iteration 3262 Loss: 0.07699578102300278\n",
      "Iteration 3263 Loss: 0.07699514524622489\n",
      "Iteration 3264 Loss: 0.07699451065632106\n",
      "Iteration 3265 Loss: 0.07699387725041978\n",
      "Iteration 3266 Loss: 0.07699324502565676\n",
      "Iteration 3267 Loss: 0.07699261397917466\n",
      "Iteration 3268 Loss: 0.076991984108123\n",
      "Iteration 3269 Loss: 0.07699135540965835\n",
      "Iteration 3270 Loss: 0.07699072788094413\n",
      "Iteration 3271 Loss: 0.07699010151915078\n",
      "Iteration 3272 Loss: 0.07698947632145556\n",
      "Iteration 3273 Loss: 0.07698885228504254\n",
      "Iteration 3274 Loss: 0.07698822940710272\n",
      "Iteration 3275 Loss: 0.076987607684834\n",
      "Iteration 3276 Loss: 0.07698698711544101\n",
      "Iteration 3277 Loss: 0.07698636769613529\n",
      "Iteration 3278 Loss: 0.07698574942413507\n",
      "Iteration 3279 Loss: 0.07698513229666543\n",
      "Iteration 3280 Loss: 0.07698451631095819\n",
      "Iteration 3281 Loss: 0.07698390146425188\n",
      "Iteration 3282 Loss: 0.07698328775379185\n",
      "Iteration 3283 Loss: 0.07698267517683005\n",
      "Iteration 3284 Loss: 0.07698206373062516\n",
      "Iteration 3285 Loss: 0.07698145341244267\n",
      "Iteration 3286 Loss: 0.07698084421955449\n",
      "Iteration 3287 Loss: 0.07698023614923934\n",
      "Iteration 3288 Loss: 0.07697962919878258\n",
      "Iteration 3289 Loss: 0.07697902336547618\n",
      "Iteration 3290 Loss: 0.07697841864661856\n",
      "Iteration 3291 Loss: 0.07697781503951487\n",
      "Iteration 3292 Loss: 0.0769772125414768\n",
      "Iteration 3293 Loss: 0.0769766111498226\n",
      "Iteration 3294 Loss: 0.07697601086187697\n",
      "Iteration 3295 Loss: 0.07697541167497132\n",
      "Iteration 3296 Loss: 0.07697481358644329\n",
      "Iteration 3297 Loss: 0.0769742165936372\n",
      "Iteration 3298 Loss: 0.0769736206939038\n",
      "Iteration 3299 Loss: 0.07697302588460031\n",
      "Iteration 3300 Loss: 0.07697243216309034\n",
      "Iteration 3301 Loss: 0.07697183952674391\n",
      "Iteration 3302 Loss: 0.07697124797293753\n",
      "Iteration 3303 Loss: 0.07697065749905409\n",
      "Iteration 3304 Loss: 0.07697006810248277\n",
      "Iteration 3305 Loss: 0.07696947978061916\n",
      "Iteration 3306 Loss: 0.07696889253086524\n",
      "Iteration 3307 Loss: 0.07696830635062925\n",
      "Iteration 3308 Loss: 0.0769677212373258\n",
      "Iteration 3309 Loss: 0.07696713718837579\n",
      "Iteration 3310 Loss: 0.07696655420120634\n",
      "Iteration 3311 Loss: 0.07696597227325092\n",
      "Iteration 3312 Loss: 0.0769653914019492\n",
      "Iteration 3313 Loss: 0.07696481158474713\n",
      "Iteration 3314 Loss: 0.07696423281909695\n",
      "Iteration 3315 Loss: 0.07696365510245678\n",
      "Iteration 3316 Loss: 0.07696307843229146\n",
      "Iteration 3317 Loss: 0.07696250280607156\n",
      "Iteration 3318 Loss: 0.07696192822127396\n",
      "Iteration 3319 Loss: 0.07696135467538173\n",
      "Iteration 3320 Loss: 0.07696078216588412\n",
      "Iteration 3321 Loss: 0.07696021069027631\n",
      "Iteration 3322 Loss: 0.07695964024605971\n",
      "Iteration 3323 Loss: 0.07695907083074194\n",
      "Iteration 3324 Loss: 0.07695850244183637\n",
      "Iteration 3325 Loss: 0.07695793507686269\n",
      "Iteration 3326 Loss: 0.07695736873334656\n",
      "Iteration 3327 Loss: 0.07695680340881966\n",
      "Iteration 3328 Loss: 0.0769562391008197\n",
      "Iteration 3329 Loss: 0.07695567580689039\n",
      "Iteration 3330 Loss: 0.07695511352458131\n",
      "Iteration 3331 Loss: 0.0769545522514483\n",
      "Iteration 3332 Loss: 0.07695399198505279\n",
      "Iteration 3333 Loss: 0.07695343272296243\n",
      "Iteration 3334 Loss: 0.07695287446275066\n",
      "Iteration 3335 Loss: 0.07695231720199687\n",
      "Iteration 3336 Loss: 0.07695176093828636\n",
      "Iteration 3337 Loss: 0.0769512056692103\n",
      "Iteration 3338 Loss: 0.07695065139236568\n",
      "Iteration 3339 Loss: 0.07695009810535548\n",
      "Iteration 3340 Loss: 0.07694954580578836\n",
      "Iteration 3341 Loss: 0.07694899449127893\n",
      "Iteration 3342 Loss: 0.07694844415944754\n",
      "Iteration 3343 Loss: 0.07694789480792033\n",
      "Iteration 3344 Loss: 0.07694734643432928\n",
      "Iteration 3345 Loss: 0.07694679903631217\n",
      "Iteration 3346 Loss: 0.07694625261151233\n",
      "Iteration 3347 Loss: 0.07694570715757915\n",
      "Iteration 3348 Loss: 0.07694516267216743\n",
      "Iteration 3349 Loss: 0.0769446191529379\n",
      "Iteration 3350 Loss: 0.07694407659755703\n",
      "Iteration 3351 Loss: 0.07694353500369668\n",
      "Iteration 3352 Loss: 0.07694299436903466\n",
      "Iteration 3353 Loss: 0.07694245469125433\n",
      "Iteration 3354 Loss: 0.07694191596804471\n",
      "Iteration 3355 Loss: 0.07694137819710047\n",
      "Iteration 3356 Loss: 0.07694084137612187\n",
      "Iteration 3357 Loss: 0.07694030550281472\n",
      "Iteration 3358 Loss: 0.07693977057489057\n",
      "Iteration 3359 Loss: 0.07693923659006646\n",
      "Iteration 3360 Loss: 0.07693870354606493\n",
      "Iteration 3361 Loss: 0.07693817144061414\n",
      "Iteration 3362 Loss: 0.07693764027144778\n",
      "Iteration 3363 Loss: 0.0769371100363051\n",
      "Iteration 3364 Loss: 0.07693658073293073\n",
      "Iteration 3365 Loss: 0.07693605235907494\n",
      "Iteration 3366 Loss: 0.07693552491249338\n",
      "Iteration 3367 Loss: 0.07693499839094722\n",
      "Iteration 3368 Loss: 0.07693447279220311\n",
      "Iteration 3369 Loss: 0.07693394811403308\n",
      "Iteration 3370 Loss: 0.07693342435421456\n",
      "Iteration 3371 Loss: 0.07693290151053049\n",
      "Iteration 3372 Loss: 0.0769323795807692\n",
      "Iteration 3373 Loss: 0.07693185856272435\n",
      "Iteration 3374 Loss: 0.07693133845419498\n",
      "Iteration 3375 Loss: 0.07693081925298552\n",
      "Iteration 3376 Loss: 0.0769303009569058\n",
      "Iteration 3377 Loss: 0.07692978356377086\n",
      "Iteration 3378 Loss: 0.07692926707140117\n",
      "Iteration 3379 Loss: 0.0769287514776225\n",
      "Iteration 3380 Loss: 0.0769282367802658\n",
      "Iteration 3381 Loss: 0.07692772297716749\n",
      "Iteration 3382 Loss: 0.07692721006616914\n",
      "Iteration 3383 Loss: 0.07692669804511755\n",
      "Iteration 3384 Loss: 0.07692618691186494\n",
      "Iteration 3385 Loss: 0.07692567666426851\n",
      "Iteration 3386 Loss: 0.07692516730019096\n",
      "Iteration 3387 Loss: 0.07692465881749994\n",
      "Iteration 3388 Loss: 0.07692415121406845\n",
      "Iteration 3389 Loss: 0.07692364448777461\n",
      "Iteration 3390 Loss: 0.07692313863650177\n",
      "Iteration 3391 Loss: 0.07692263365813838\n",
      "Iteration 3392 Loss: 0.07692212955057806\n",
      "Iteration 3393 Loss: 0.07692162631171945\n",
      "Iteration 3394 Loss: 0.07692112393946657\n",
      "Iteration 3395 Loss: 0.07692062243172837\n",
      "Iteration 3396 Loss: 0.07692012178641879\n",
      "Iteration 3397 Loss: 0.07691962200145706\n",
      "Iteration 3398 Loss: 0.0769191230747674\n",
      "Iteration 3399 Loss: 0.07691862500427912\n",
      "Iteration 3400 Loss: 0.07691812778792649\n",
      "Iteration 3401 Loss: 0.07691763142364885\n",
      "Iteration 3402 Loss: 0.07691713590939063\n",
      "Iteration 3403 Loss: 0.07691664124310113\n",
      "Iteration 3404 Loss: 0.07691614742273478\n",
      "Iteration 3405 Loss: 0.07691565444625091\n",
      "Iteration 3406 Loss: 0.07691516231161391\n",
      "Iteration 3407 Loss: 0.07691467101679304\n",
      "Iteration 3408 Loss: 0.07691418055976251\n",
      "Iteration 3409 Loss: 0.07691369093850155\n",
      "Iteration 3410 Loss: 0.07691320215099422\n",
      "Iteration 3411 Loss: 0.07691271419522949\n",
      "Iteration 3412 Loss: 0.07691222706920141\n",
      "Iteration 3413 Loss: 0.07691174077090865\n",
      "Iteration 3414 Loss: 0.07691125529835488\n",
      "Iteration 3415 Loss: 0.07691077064954871\n",
      "Iteration 3416 Loss: 0.0769102868225035\n",
      "Iteration 3417 Loss: 0.0769098038152375\n",
      "Iteration 3418 Loss: 0.07690932162577371\n",
      "Iteration 3419 Loss: 0.07690884025214004\n",
      "Iteration 3420 Loss: 0.07690835969236916\n",
      "Iteration 3421 Loss: 0.07690787994449858\n",
      "Iteration 3422 Loss: 0.07690740100657044\n",
      "Iteration 3423 Loss: 0.07690692287663195\n",
      "Iteration 3424 Loss: 0.0769064455527347\n",
      "Iteration 3425 Loss: 0.07690596903293531\n",
      "Iteration 3426 Loss: 0.07690549331529505\n",
      "Iteration 3427 Loss: 0.07690501839787987\n",
      "Iteration 3428 Loss: 0.0769045442787605\n",
      "Iteration 3429 Loss: 0.07690407095601232\n",
      "Iteration 3430 Loss: 0.07690359842771544\n",
      "Iteration 3431 Loss: 0.07690312669195458\n",
      "Iteration 3432 Loss: 0.07690265574681919\n",
      "Iteration 3433 Loss: 0.07690218559040338\n",
      "Iteration 3434 Loss: 0.07690171622080583\n",
      "Iteration 3435 Loss: 0.07690124763612995\n",
      "Iteration 3436 Loss: 0.07690077983448368\n",
      "Iteration 3437 Loss: 0.07690031281397966\n",
      "Iteration 3438 Loss: 0.07689984657273509\n",
      "Iteration 3439 Loss: 0.07689938110887161\n",
      "Iteration 3440 Loss: 0.07689891642051572\n",
      "Iteration 3441 Loss: 0.07689845250579827\n",
      "Iteration 3442 Loss: 0.07689798936285483\n",
      "Iteration 3443 Loss: 0.07689752698982526\n",
      "Iteration 3444 Loss: 0.07689706538485423\n",
      "Iteration 3445 Loss: 0.07689660454609074\n",
      "Iteration 3446 Loss: 0.07689614447168833\n",
      "Iteration 3447 Loss: 0.07689568515980519\n",
      "Iteration 3448 Loss: 0.07689522660860379\n",
      "Iteration 3449 Loss: 0.0768947688162512\n",
      "Iteration 3450 Loss: 0.0768943117809189\n",
      "Iteration 3451 Loss: 0.07689385550078287\n",
      "Iteration 3452 Loss: 0.07689339997402354\n",
      "Iteration 3453 Loss: 0.07689294519882568\n",
      "Iteration 3454 Loss: 0.07689249117337861\n",
      "Iteration 3455 Loss: 0.07689203789587593\n",
      "Iteration 3456 Loss: 0.07689158536451578\n",
      "Iteration 3457 Loss: 0.07689113357750055\n",
      "Iteration 3458 Loss: 0.07689068253303714\n",
      "Iteration 3459 Loss: 0.07689023222933676\n",
      "Iteration 3460 Loss: 0.07688978266461496\n",
      "Iteration 3461 Loss: 0.07688933383709158\n",
      "Iteration 3462 Loss: 0.07688888574499099\n",
      "Iteration 3463 Loss: 0.07688843838654166\n",
      "Iteration 3464 Loss: 0.07688799175997663\n",
      "Iteration 3465 Loss: 0.07688754586353289\n",
      "Iteration 3466 Loss: 0.0768871006954521\n",
      "Iteration 3467 Loss: 0.07688665625397999\n",
      "Iteration 3468 Loss: 0.07688621253736659\n",
      "Iteration 3469 Loss: 0.07688576954386618\n",
      "Iteration 3470 Loss: 0.0768853272717374\n",
      "Iteration 3471 Loss: 0.07688488571924304\n",
      "Iteration 3472 Loss: 0.07688444488465009\n",
      "Iteration 3473 Loss: 0.07688400476622985\n",
      "Iteration 3474 Loss: 0.0768835653622578\n",
      "Iteration 3475 Loss: 0.07688312667101362\n",
      "Iteration 3476 Loss: 0.0768826886907812\n",
      "Iteration 3477 Loss: 0.07688225141984849\n",
      "Iteration 3478 Loss: 0.07688181485650782\n",
      "Iteration 3479 Loss: 0.0768813789990555\n",
      "Iteration 3480 Loss: 0.07688094384579207\n",
      "Iteration 3481 Loss: 0.07688050939502226\n",
      "Iteration 3482 Loss: 0.07688007564505477\n",
      "Iteration 3483 Loss: 0.07687964259420259\n",
      "Iteration 3484 Loss: 0.07687921024078274\n",
      "Iteration 3485 Loss: 0.07687877858311636\n",
      "Iteration 3486 Loss: 0.07687834761952864\n",
      "Iteration 3487 Loss: 0.07687791734834892\n",
      "Iteration 3488 Loss: 0.07687748776791051\n",
      "Iteration 3489 Loss: 0.07687705887655089\n",
      "Iteration 3490 Loss: 0.07687663067261157\n",
      "Iteration 3491 Loss: 0.076876203154438\n",
      "Iteration 3492 Loss: 0.07687577632037977\n",
      "Iteration 3493 Loss: 0.07687535016879045\n",
      "Iteration 3494 Loss: 0.07687492469802765\n",
      "Iteration 3495 Loss: 0.07687449990645291\n",
      "Iteration 3496 Loss: 0.0768740757924319\n",
      "Iteration 3497 Loss: 0.07687365235433402\n",
      "Iteration 3498 Loss: 0.07687322959053298\n",
      "Iteration 3499 Loss: 0.07687280749940614\n",
      "Iteration 3500 Loss: 0.07687238607933501\n",
      "Iteration 3501 Loss: 0.07687196532870492\n",
      "Iteration 3502 Loss: 0.07687154524590531\n",
      "Iteration 3503 Loss: 0.07687112582932933\n",
      "Iteration 3504 Loss: 0.07687070707737423\n",
      "Iteration 3505 Loss: 0.07687028898844094\n",
      "Iteration 3506 Loss: 0.07686987156093447\n",
      "Iteration 3507 Loss: 0.0768694547932638\n",
      "Iteration 3508 Loss: 0.07686903868384154\n",
      "Iteration 3509 Loss: 0.07686862323108422\n",
      "Iteration 3510 Loss: 0.07686820843341245\n",
      "Iteration 3511 Loss: 0.0768677942892504\n",
      "Iteration 3512 Loss: 0.0768673807970263\n",
      "Iteration 3513 Loss: 0.076866967955172\n",
      "Iteration 3514 Loss: 0.07686655576212337\n",
      "Iteration 3515 Loss: 0.07686614421631999\n",
      "Iteration 3516 Loss: 0.07686573331620529\n",
      "Iteration 3517 Loss: 0.07686532306022632\n",
      "Iteration 3518 Loss: 0.07686491344683427\n",
      "Iteration 3519 Loss: 0.07686450447448372\n",
      "Iteration 3520 Loss: 0.07686409614163323\n",
      "Iteration 3521 Loss: 0.07686368844674508\n",
      "Iteration 3522 Loss: 0.07686328138828528\n",
      "Iteration 3523 Loss: 0.07686287496472356\n",
      "Iteration 3524 Loss: 0.07686246917453345\n",
      "Iteration 3525 Loss: 0.07686206401619206\n",
      "Iteration 3526 Loss: 0.07686165948818037\n",
      "Iteration 3527 Loss: 0.07686125558898294\n",
      "Iteration 3528 Loss: 0.0768608523170881\n",
      "Iteration 3529 Loss: 0.07686044967098779\n",
      "Iteration 3530 Loss: 0.0768600476491777\n",
      "Iteration 3531 Loss: 0.07685964625015719\n",
      "Iteration 3532 Loss: 0.0768592454724292\n",
      "Iteration 3533 Loss: 0.07685884531450025\n",
      "Iteration 3534 Loss: 0.07685844577488078\n",
      "Iteration 3535 Loss: 0.07685804685208457\n",
      "Iteration 3536 Loss: 0.07685764854462915\n",
      "Iteration 3537 Loss: 0.07685725085103566\n",
      "Iteration 3538 Loss: 0.0768568537698288\n",
      "Iteration 3539 Loss: 0.07685645729953688\n",
      "Iteration 3540 Loss: 0.0768560614386919\n",
      "Iteration 3541 Loss: 0.07685566618582922\n",
      "Iteration 3542 Loss: 0.07685527153948793\n",
      "Iteration 3543 Loss: 0.0768548774982107\n",
      "Iteration 3544 Loss: 0.07685448406054357\n",
      "Iteration 3545 Loss: 0.07685409122503634\n",
      "Iteration 3546 Loss: 0.07685369899024219\n",
      "Iteration 3547 Loss: 0.07685330735471789\n",
      "Iteration 3548 Loss: 0.07685291631702369\n",
      "Iteration 3549 Loss: 0.07685252587572344\n",
      "Iteration 3550 Loss: 0.07685213602938439\n",
      "Iteration 3551 Loss: 0.07685174677657726\n",
      "Iteration 3552 Loss: 0.07685135811587634\n",
      "Iteration 3553 Loss: 0.07685097004585932\n",
      "Iteration 3554 Loss: 0.07685058256510749\n",
      "Iteration 3555 Loss: 0.07685019567220537\n",
      "Iteration 3556 Loss: 0.07684980936574115\n",
      "Iteration 3557 Loss: 0.07684942364430632\n",
      "Iteration 3558 Loss: 0.07684903850649585\n",
      "Iteration 3559 Loss: 0.07684865395090809\n",
      "Iteration 3560 Loss: 0.076848269976145\n",
      "Iteration 3561 Loss: 0.07684788658081153\n",
      "Iteration 3562 Loss: 0.07684750376351651\n",
      "Iteration 3563 Loss: 0.07684712152287183\n",
      "Iteration 3564 Loss: 0.0768467398574929\n",
      "Iteration 3565 Loss: 0.07684635876599848\n",
      "Iteration 3566 Loss: 0.0768459782470106\n",
      "Iteration 3567 Loss: 0.07684559829915483\n",
      "Iteration 3568 Loss: 0.0768452189210599\n",
      "Iteration 3569 Loss: 0.07684484011135809\n",
      "Iteration 3570 Loss: 0.07684446186868482\n",
      "Iteration 3571 Loss: 0.07684408419167892\n",
      "Iteration 3572 Loss: 0.0768437070789825\n",
      "Iteration 3573 Loss: 0.07684333052924101\n",
      "Iteration 3574 Loss: 0.07684295454110321\n",
      "Iteration 3575 Loss: 0.07684257911322113\n",
      "Iteration 3576 Loss: 0.0768422042442501\n",
      "Iteration 3577 Loss: 0.07684182993284866\n",
      "Iteration 3578 Loss: 0.0768414561776787\n",
      "Iteration 3579 Loss: 0.07684108297740531\n",
      "Iteration 3580 Loss: 0.0768407103306969\n",
      "Iteration 3581 Loss: 0.07684033823622509\n",
      "Iteration 3582 Loss: 0.0768399666926647\n",
      "Iteration 3583 Loss: 0.07683959569869386\n",
      "Iteration 3584 Loss: 0.07683922525299375\n",
      "Iteration 3585 Loss: 0.07683885535424906\n",
      "Iteration 3586 Loss: 0.07683848600114733\n",
      "Iteration 3587 Loss: 0.07683811719237962\n",
      "Iteration 3588 Loss: 0.07683774892663994\n",
      "Iteration 3589 Loss: 0.0768373812026256\n",
      "Iteration 3590 Loss: 0.07683701401903709\n",
      "Iteration 3591 Loss: 0.07683664737457793\n",
      "Iteration 3592 Loss: 0.076836281267955\n",
      "Iteration 3593 Loss: 0.07683591569787813\n",
      "Iteration 3594 Loss: 0.07683555066306061\n",
      "Iteration 3595 Loss: 0.0768351861622184\n",
      "Iteration 3596 Loss: 0.07683482219407095\n",
      "Iteration 3597 Loss: 0.07683445875734074\n",
      "Iteration 3598 Loss: 0.07683409585075326\n",
      "Iteration 3599 Loss: 0.07683373347303726\n",
      "Iteration 3600 Loss: 0.07683337162292446\n",
      "Iteration 3601 Loss: 0.07683301029914968\n",
      "Iteration 3602 Loss: 0.07683264950045102\n",
      "Iteration 3603 Loss: 0.07683228922556931\n",
      "Iteration 3604 Loss: 0.07683192947324877\n",
      "Iteration 3605 Loss: 0.07683157024223643\n",
      "Iteration 3606 Loss: 0.07683121153128257\n",
      "Iteration 3607 Loss: 0.07683085333914039\n",
      "Iteration 3608 Loss: 0.0768304956645661\n",
      "Iteration 3609 Loss: 0.07683013850631917\n",
      "Iteration 3610 Loss: 0.07682978186316176\n",
      "Iteration 3611 Loss: 0.07682942573385936\n",
      "Iteration 3612 Loss: 0.07682907011718014\n",
      "Iteration 3613 Loss: 0.07682871501189564\n",
      "Iteration 3614 Loss: 0.07682836041678005\n",
      "Iteration 3615 Loss: 0.07682800633061078\n",
      "Iteration 3616 Loss: 0.07682765275216817\n",
      "Iteration 3617 Loss: 0.07682729968023543\n",
      "Iteration 3618 Loss: 0.0768269471135988\n",
      "Iteration 3619 Loss: 0.0768265950510475\n",
      "Iteration 3620 Loss: 0.07682624349137368\n",
      "Iteration 3621 Loss: 0.0768258924333724\n",
      "Iteration 3622 Loss: 0.07682554187584177\n",
      "Iteration 3623 Loss: 0.07682519181758267\n",
      "Iteration 3624 Loss: 0.07682484225739901\n",
      "Iteration 3625 Loss: 0.07682449319409748\n",
      "Iteration 3626 Loss: 0.07682414462648797\n",
      "Iteration 3627 Loss: 0.07682379655338285\n",
      "Iteration 3628 Loss: 0.0768234489735978\n",
      "Iteration 3629 Loss: 0.0768231018859511\n",
      "Iteration 3630 Loss: 0.07682275528926401\n",
      "Iteration 3631 Loss: 0.07682240918236063\n",
      "Iteration 3632 Loss: 0.076822063564068\n",
      "Iteration 3633 Loss: 0.07682171843321589\n",
      "Iteration 3634 Loss: 0.07682137378863707\n",
      "Iteration 3635 Loss: 0.07682102962916704\n",
      "Iteration 3636 Loss: 0.07682068595364416\n",
      "Iteration 3637 Loss: 0.07682034276090967\n",
      "Iteration 3638 Loss: 0.0768200000498076\n",
      "Iteration 3639 Loss: 0.07681965781918469\n",
      "Iteration 3640 Loss: 0.07681931606789079\n",
      "Iteration 3641 Loss: 0.07681897479477812\n",
      "Iteration 3642 Loss: 0.07681863399870212\n",
      "Iteration 3643 Loss: 0.07681829367852075\n",
      "Iteration 3644 Loss: 0.07681795383309482\n",
      "Iteration 3645 Loss: 0.07681761446128794\n",
      "Iteration 3646 Loss: 0.07681727556196648\n",
      "Iteration 3647 Loss: 0.07681693713399952\n",
      "Iteration 3648 Loss: 0.07681659917625899\n",
      "Iteration 3649 Loss: 0.07681626168761951\n",
      "Iteration 3650 Loss: 0.07681592466695845\n",
      "Iteration 3651 Loss: 0.07681558811315592\n",
      "Iteration 3652 Loss: 0.07681525202509466\n",
      "Iteration 3653 Loss: 0.07681491640166035\n",
      "Iteration 3654 Loss: 0.07681458124174119\n",
      "Iteration 3655 Loss: 0.07681424654422812\n",
      "Iteration 3656 Loss: 0.07681391230801489\n",
      "Iteration 3657 Loss: 0.07681357853199787\n",
      "Iteration 3658 Loss: 0.076813245215076\n",
      "Iteration 3659 Loss: 0.07681291235615123\n",
      "Iteration 3660 Loss: 0.07681257995412775\n",
      "Iteration 3661 Loss: 0.07681224800791282\n",
      "Iteration 3662 Loss: 0.07681191651641606\n",
      "Iteration 3663 Loss: 0.07681158547855\n",
      "Iteration 3664 Loss: 0.07681125489322958\n",
      "Iteration 3665 Loss: 0.07681092475937254\n",
      "Iteration 3666 Loss: 0.07681059507589923\n",
      "Iteration 3667 Loss: 0.07681026584173259\n",
      "Iteration 3668 Loss: 0.07680993705579818\n",
      "Iteration 3669 Loss: 0.07680960871702426\n",
      "Iteration 3670 Loss: 0.07680928082434157\n",
      "Iteration 3671 Loss: 0.07680895337668354\n",
      "Iteration 3672 Loss: 0.0768086263729863\n",
      "Iteration 3673 Loss: 0.07680829981218829\n",
      "Iteration 3674 Loss: 0.07680797369323081\n",
      "Iteration 3675 Loss: 0.07680764801505761\n",
      "Iteration 3676 Loss: 0.076807322776615\n",
      "Iteration 3677 Loss: 0.07680699797685199\n",
      "Iteration 3678 Loss: 0.07680667361471985\n",
      "Iteration 3679 Loss: 0.07680634968917278\n",
      "Iteration 3680 Loss: 0.07680602619916732\n",
      "Iteration 3681 Loss: 0.07680570314366253\n",
      "Iteration 3682 Loss: 0.07680538052162018\n",
      "Iteration 3683 Loss: 0.0768050583320043\n",
      "Iteration 3684 Loss: 0.07680473657378166\n",
      "Iteration 3685 Loss: 0.07680441524592146\n",
      "Iteration 3686 Loss: 0.07680409434739545\n",
      "Iteration 3687 Loss: 0.07680377387717789\n",
      "Iteration 3688 Loss: 0.07680345383424544\n",
      "Iteration 3689 Loss: 0.07680313421757742\n",
      "Iteration 3690 Loss: 0.07680281502615549\n",
      "Iteration 3691 Loss: 0.07680249625896378\n",
      "Iteration 3692 Loss: 0.07680217791498904\n",
      "Iteration 3693 Loss: 0.07680185999322038\n",
      "Iteration 3694 Loss: 0.07680154249264938\n",
      "Iteration 3695 Loss: 0.07680122541227016\n",
      "Iteration 3696 Loss: 0.07680090875107914\n",
      "Iteration 3697 Loss: 0.07680059250807529\n",
      "Iteration 3698 Loss: 0.07680027668226005\n",
      "Iteration 3699 Loss: 0.07679996127263719\n",
      "Iteration 3700 Loss: 0.07679964627821292\n",
      "Iteration 3701 Loss: 0.07679933169799595\n",
      "Iteration 3702 Loss: 0.07679901753099738\n",
      "Iteration 3703 Loss: 0.07679870377623069\n",
      "Iteration 3704 Loss: 0.07679839043271172\n",
      "Iteration 3705 Loss: 0.07679807749945876\n",
      "Iteration 3706 Loss: 0.0767977649754926\n",
      "Iteration 3707 Loss: 0.07679745285983616\n",
      "Iteration 3708 Loss: 0.07679714115151506\n",
      "Iteration 3709 Loss: 0.07679682984955694\n",
      "Iteration 3710 Loss: 0.07679651895299205\n",
      "Iteration 3711 Loss: 0.07679620846085299\n",
      "Iteration 3712 Loss: 0.07679589837217461\n",
      "Iteration 3713 Loss: 0.07679558868599418\n",
      "Iteration 3714 Loss: 0.07679527940135122\n",
      "Iteration 3715 Loss: 0.07679497051728786\n",
      "Iteration 3716 Loss: 0.07679466203284824\n",
      "Iteration 3717 Loss: 0.07679435394707897\n",
      "Iteration 3718 Loss: 0.076794046259029\n",
      "Iteration 3719 Loss: 0.07679373896774959\n",
      "Iteration 3720 Loss: 0.07679343207229421\n",
      "Iteration 3721 Loss: 0.07679312557171879\n",
      "Iteration 3722 Loss: 0.07679281946508146\n",
      "Iteration 3723 Loss: 0.0767925137514427\n",
      "Iteration 3724 Loss: 0.0767922084298652\n",
      "Iteration 3725 Loss: 0.07679190349941403\n",
      "Iteration 3726 Loss: 0.07679159895915638\n",
      "Iteration 3727 Loss: 0.07679129480816202\n",
      "Iteration 3728 Loss: 0.07679099104550265\n",
      "Iteration 3729 Loss: 0.07679068767025232\n",
      "Iteration 3730 Loss: 0.07679038468148751\n",
      "Iteration 3731 Loss: 0.07679008207828673\n",
      "Iteration 3732 Loss: 0.07678977985973093\n",
      "Iteration 3733 Loss: 0.07678947802490307\n",
      "Iteration 3734 Loss: 0.07678917657288857\n",
      "Iteration 3735 Loss: 0.07678887550277491\n",
      "Iteration 3736 Loss: 0.07678857481365187\n",
      "Iteration 3737 Loss: 0.07678827450461144\n",
      "Iteration 3738 Loss: 0.07678797457474787\n",
      "Iteration 3739 Loss: 0.07678767502315749\n",
      "Iteration 3740 Loss: 0.07678737584893894\n",
      "Iteration 3741 Loss: 0.07678707705119303\n",
      "Iteration 3742 Loss: 0.07678677862902272\n",
      "Iteration 3743 Loss: 0.0767864805815332\n",
      "Iteration 3744 Loss: 0.0767861829078318\n",
      "Iteration 3745 Loss: 0.07678588560702815\n",
      "Iteration 3746 Loss: 0.0767855886782338\n",
      "Iteration 3747 Loss: 0.07678529212056279\n",
      "Iteration 3748 Loss: 0.07678499593313094\n",
      "Iteration 3749 Loss: 0.07678470011505652\n",
      "Iteration 3750 Loss: 0.07678440466545992\n",
      "Iteration 3751 Loss: 0.07678410958346346\n",
      "Iteration 3752 Loss: 0.0767838148681918\n",
      "Iteration 3753 Loss: 0.07678352051877174\n",
      "Iteration 3754 Loss: 0.07678322653433203\n",
      "Iteration 3755 Loss: 0.07678293291400373\n",
      "Iteration 3756 Loss: 0.07678263965691992\n",
      "Iteration 3757 Loss: 0.07678234676221579\n",
      "Iteration 3758 Loss: 0.07678205422902869\n",
      "Iteration 3759 Loss: 0.07678176205649798\n",
      "Iteration 3760 Loss: 0.0767814702437652\n",
      "Iteration 3761 Loss: 0.07678117878997398\n",
      "Iteration 3762 Loss: 0.07678088769427\n",
      "Iteration 3763 Loss: 0.07678059695580103\n",
      "Iteration 3764 Loss: 0.07678030657371687\n",
      "Iteration 3765 Loss: 0.07678001654716944\n",
      "Iteration 3766 Loss: 0.07677972687531281\n",
      "Iteration 3767 Loss: 0.07677943755730295\n",
      "Iteration 3768 Loss: 0.076779148592298\n",
      "Iteration 3769 Loss: 0.076778859979458\n",
      "Iteration 3770 Loss: 0.07677857171794532\n",
      "Iteration 3771 Loss: 0.07677828380692406\n",
      "Iteration 3772 Loss: 0.07677799624556052\n",
      "Iteration 3773 Loss: 0.07677770903302301\n",
      "Iteration 3774 Loss: 0.07677742216848189\n",
      "Iteration 3775 Loss: 0.07677713565110943\n",
      "Iteration 3776 Loss: 0.07677684948008005\n",
      "Iteration 3777 Loss: 0.0767765636545701\n",
      "Iteration 3778 Loss: 0.07677627817375796\n",
      "Iteration 3779 Loss: 0.0767759930368241\n",
      "Iteration 3780 Loss: 0.07677570824295075\n",
      "Iteration 3781 Loss: 0.07677542379132234\n",
      "Iteration 3782 Loss: 0.07677513968112529\n",
      "Iteration 3783 Loss: 0.07677485591154783\n",
      "Iteration 3784 Loss: 0.0767745724817803\n",
      "Iteration 3785 Loss: 0.07677428939101504\n",
      "Iteration 3786 Loss: 0.0767740066384463\n",
      "Iteration 3787 Loss: 0.07677372422327018\n",
      "Iteration 3788 Loss: 0.07677344214468497\n",
      "Iteration 3789 Loss: 0.07677316040189076\n",
      "Iteration 3790 Loss: 0.07677287899408962\n",
      "Iteration 3791 Loss: 0.07677259792048559\n",
      "Iteration 3792 Loss: 0.07677231718028456\n",
      "Iteration 3793 Loss: 0.07677203677269451\n",
      "Iteration 3794 Loss: 0.07677175669692521\n",
      "Iteration 3795 Loss: 0.07677147695218839\n",
      "Iteration 3796 Loss: 0.0767711975376977\n",
      "Iteration 3797 Loss: 0.07677091845266876\n",
      "Iteration 3798 Loss: 0.07677063969631906\n",
      "Iteration 3799 Loss: 0.07677036126786796\n",
      "Iteration 3800 Loss: 0.07677008316653679\n",
      "Iteration 3801 Loss: 0.07676980539154869\n",
      "Iteration 3802 Loss: 0.07676952794212877\n",
      "Iteration 3803 Loss: 0.07676925081750402\n",
      "Iteration 3804 Loss: 0.07676897401690329\n",
      "Iteration 3805 Loss: 0.07676869753955734\n",
      "Iteration 3806 Loss: 0.0767684213846987\n",
      "Iteration 3807 Loss: 0.07676814555156185\n",
      "Iteration 3808 Loss: 0.07676787003938314\n",
      "Iteration 3809 Loss: 0.07676759484740084\n",
      "Iteration 3810 Loss: 0.0767673199748549\n",
      "Iteration 3811 Loss: 0.07676704542098732\n",
      "Iteration 3812 Loss: 0.07676677118504177\n",
      "Iteration 3813 Loss: 0.0767664972662639\n",
      "Iteration 3814 Loss: 0.07676622366390105\n",
      "Iteration 3815 Loss: 0.07676595037720263\n",
      "Iteration 3816 Loss: 0.0767656774054196\n",
      "Iteration 3817 Loss: 0.07676540474780491\n",
      "Iteration 3818 Loss: 0.07676513240361331\n",
      "Iteration 3819 Loss: 0.07676486037210135\n",
      "Iteration 3820 Loss: 0.07676458865252736\n",
      "Iteration 3821 Loss: 0.07676431724415156\n",
      "Iteration 3822 Loss: 0.07676404614623587\n",
      "Iteration 3823 Loss: 0.07676377535804403\n",
      "Iteration 3824 Loss: 0.0767635048788417\n",
      "Iteration 3825 Loss: 0.07676323470789614\n",
      "Iteration 3826 Loss: 0.07676296484447649\n",
      "Iteration 3827 Loss: 0.07676269528785369\n",
      "Iteration 3828 Loss: 0.07676242603730038\n",
      "Iteration 3829 Loss: 0.07676215709209107\n",
      "Iteration 3830 Loss: 0.07676188845150196\n",
      "Iteration 3831 Loss: 0.07676162011481105\n",
      "Iteration 3832 Loss: 0.07676135208129804\n",
      "Iteration 3833 Loss: 0.07676108435024448\n",
      "Iteration 3834 Loss: 0.07676081692093362\n",
      "Iteration 3835 Loss: 0.0767605497926504\n",
      "Iteration 3836 Loss: 0.07676028296468158\n",
      "Iteration 3837 Loss: 0.07676001643631572\n",
      "Iteration 3838 Loss: 0.0767597502068429\n",
      "Iteration 3839 Loss: 0.07675948427555511\n",
      "Iteration 3840 Loss: 0.07675921864174598\n",
      "Iteration 3841 Loss: 0.07675895330471091\n",
      "Iteration 3842 Loss: 0.07675868826374704\n",
      "Iteration 3843 Loss: 0.07675842351815308\n",
      "Iteration 3844 Loss: 0.07675815906722963\n",
      "Iteration 3845 Loss: 0.07675789491027887\n",
      "Iteration 3846 Loss: 0.07675763104660473\n",
      "Iteration 3847 Loss: 0.07675736747551282\n",
      "Iteration 3848 Loss: 0.07675710419631038\n",
      "Iteration 3849 Loss: 0.0767568412083065\n",
      "Iteration 3850 Loss: 0.07675657851081182\n",
      "Iteration 3851 Loss: 0.07675631610313867\n",
      "Iteration 3852 Loss: 0.07675605398460106\n",
      "Iteration 3853 Loss: 0.07675579215451472\n",
      "Iteration 3854 Loss: 0.076755530612197\n",
      "Iteration 3855 Loss: 0.07675526935696697\n",
      "Iteration 3856 Loss: 0.07675500838814525\n",
      "Iteration 3857 Loss: 0.07675474770505418\n",
      "Iteration 3858 Loss: 0.07675448730701781\n",
      "Iteration 3859 Loss: 0.07675422719336168\n",
      "Iteration 3860 Loss: 0.07675396736341314\n",
      "Iteration 3861 Loss: 0.07675370781650108\n",
      "Iteration 3862 Loss: 0.0767534485519561\n",
      "Iteration 3863 Loss: 0.07675318956911026\n",
      "Iteration 3864 Loss: 0.07675293086729751\n",
      "Iteration 3865 Loss: 0.07675267244585315\n",
      "Iteration 3866 Loss: 0.07675241430411432\n",
      "Iteration 3867 Loss: 0.07675215644141964\n",
      "Iteration 3868 Loss: 0.07675189885710938\n",
      "Iteration 3869 Loss: 0.07675164155052543\n",
      "Iteration 3870 Loss: 0.07675138452101124\n",
      "Iteration 3871 Loss: 0.07675112776791194\n",
      "Iteration 3872 Loss: 0.07675087129057412\n",
      "Iteration 3873 Loss: 0.07675061508834616\n",
      "Iteration 3874 Loss: 0.07675035916057783\n",
      "Iteration 3875 Loss: 0.07675010350662062\n",
      "Iteration 3876 Loss: 0.07674984812582747\n",
      "Iteration 3877 Loss: 0.07674959301755299\n",
      "Iteration 3878 Loss: 0.0767493381811534\n",
      "Iteration 3879 Loss: 0.07674908361598638\n",
      "Iteration 3880 Loss: 0.0767488293214113\n",
      "Iteration 3881 Loss: 0.07674857529678886\n",
      "Iteration 3882 Loss: 0.07674832154148159\n",
      "Iteration 3883 Loss: 0.07674806805485346\n",
      "Iteration 3884 Loss: 0.0767478148362699\n",
      "Iteration 3885 Loss: 0.07674756188509806\n",
      "Iteration 3886 Loss: 0.07674730920070645\n",
      "Iteration 3887 Loss: 0.07674705678246524\n",
      "Iteration 3888 Loss: 0.0767468046297461\n",
      "Iteration 3889 Loss: 0.07674655274192226\n",
      "Iteration 3890 Loss: 0.07674630111836843\n",
      "Iteration 3891 Loss: 0.07674604975846078\n",
      "Iteration 3892 Loss: 0.0767457986615772\n",
      "Iteration 3893 Loss: 0.07674554782709692\n",
      "Iteration 3894 Loss: 0.07674529725440073\n",
      "Iteration 3895 Loss: 0.07674504694287092\n",
      "Iteration 3896 Loss: 0.07674479689189131\n",
      "Iteration 3897 Loss: 0.0767445471008472\n",
      "Iteration 3898 Loss: 0.07674429756912537\n",
      "Iteration 3899 Loss: 0.07674404829611417\n",
      "Iteration 3900 Loss: 0.07674379928120334\n",
      "Iteration 3901 Loss: 0.0767435505237842\n",
      "Iteration 3902 Loss: 0.07674330202324946\n",
      "Iteration 3903 Loss: 0.07674305377899338\n",
      "Iteration 3904 Loss: 0.07674280579041161\n",
      "Iteration 3905 Loss: 0.0767425580569014\n",
      "Iteration 3906 Loss: 0.07674231057786135\n",
      "Iteration 3907 Loss: 0.07674206335269164\n",
      "Iteration 3908 Loss: 0.07674181638079373\n",
      "Iteration 3909 Loss: 0.07674156966157075\n",
      "Iteration 3910 Loss: 0.07674132319442714\n",
      "Iteration 3911 Loss: 0.07674107697876882\n",
      "Iteration 3912 Loss: 0.07674083101400317\n",
      "Iteration 3913 Loss: 0.07674058529953907\n",
      "Iteration 3914 Loss: 0.0767403398347867\n",
      "Iteration 3915 Loss: 0.07674009461915783\n",
      "Iteration 3916 Loss: 0.07673984965206557\n",
      "Iteration 3917 Loss: 0.07673960493292442\n",
      "Iteration 3918 Loss: 0.07673936046115047\n",
      "Iteration 3919 Loss: 0.07673911623616099\n",
      "Iteration 3920 Loss: 0.07673887225737493\n",
      "Iteration 3921 Loss: 0.07673862852421248\n",
      "Iteration 3922 Loss: 0.07673838503609531\n",
      "Iteration 3923 Loss: 0.07673814179244645\n",
      "Iteration 3924 Loss: 0.07673789879269041\n",
      "Iteration 3925 Loss: 0.076737656036253\n",
      "Iteration 3926 Loss: 0.07673741352256151\n",
      "Iteration 3927 Loss: 0.07673717125104464\n",
      "Iteration 3928 Loss: 0.07673692922113241\n",
      "Iteration 3929 Loss: 0.0767366874322562\n",
      "Iteration 3930 Loss: 0.07673644588384888\n",
      "Iteration 3931 Loss: 0.07673620457534472\n",
      "Iteration 3932 Loss: 0.07673596350617916\n",
      "Iteration 3933 Loss: 0.07673572267578929\n",
      "Iteration 3934 Loss: 0.07673548208361339\n",
      "Iteration 3935 Loss: 0.07673524172909116\n",
      "Iteration 3936 Loss: 0.07673500161166358\n",
      "Iteration 3937 Loss: 0.07673476173077316\n",
      "Iteration 3938 Loss: 0.07673452208586365\n",
      "Iteration 3939 Loss: 0.07673428267638019\n",
      "Iteration 3940 Loss: 0.07673404350176921\n",
      "Iteration 3941 Loss: 0.07673380456147866\n",
      "Iteration 3942 Loss: 0.0767335658549576\n",
      "Iteration 3943 Loss: 0.07673332738165663\n",
      "Iteration 3944 Loss: 0.07673308914102747\n",
      "Iteration 3945 Loss: 0.07673285113252346\n",
      "Iteration 3946 Loss: 0.07673261335559904\n",
      "Iteration 3947 Loss: 0.07673237580971005\n",
      "Iteration 3948 Loss: 0.0767321384943137\n",
      "Iteration 3949 Loss: 0.07673190140886849\n",
      "Iteration 3950 Loss: 0.07673166455283421\n",
      "Iteration 3951 Loss: 0.07673142792567195\n",
      "Iteration 3952 Loss: 0.07673119152684423\n",
      "Iteration 3953 Loss: 0.07673095535581476\n",
      "Iteration 3954 Loss: 0.07673071941204856\n",
      "Iteration 3955 Loss: 0.07673048369501204\n",
      "Iteration 3956 Loss: 0.07673024820417286\n",
      "Iteration 3957 Loss: 0.07673001293899989\n",
      "Iteration 3958 Loss: 0.07672977789896347\n",
      "Iteration 3959 Loss: 0.07672954308353509\n",
      "Iteration 3960 Loss: 0.0767293084921876\n",
      "Iteration 3961 Loss: 0.0767290741243951\n",
      "Iteration 3962 Loss: 0.07672883997963295\n",
      "Iteration 3963 Loss: 0.07672860605737782\n",
      "Iteration 3964 Loss: 0.07672837235710765\n",
      "Iteration 3965 Loss: 0.07672813887830167\n",
      "Iteration 3966 Loss: 0.0767279056204403\n",
      "Iteration 3967 Loss: 0.07672767258300532\n",
      "Iteration 3968 Loss: 0.07672743976547976\n",
      "Iteration 3969 Loss: 0.07672720716734782\n",
      "Iteration 3970 Loss: 0.07672697478809512\n",
      "Iteration 3971 Loss: 0.0767267426272083\n",
      "Iteration 3972 Loss: 0.07672651068417548\n",
      "Iteration 3973 Loss: 0.07672627895848587\n",
      "Iteration 3974 Loss: 0.07672604744962998\n",
      "Iteration 3975 Loss: 0.07672581615709965\n",
      "Iteration 3976 Loss: 0.07672558508038771\n",
      "Iteration 3977 Loss: 0.0767253542189885\n",
      "Iteration 3978 Loss: 0.07672512357239743\n",
      "Iteration 3979 Loss: 0.07672489314011122\n",
      "Iteration 3980 Loss: 0.0767246629216277\n",
      "Iteration 3981 Loss: 0.07672443291644611\n",
      "Iteration 3982 Loss: 0.0767242031240667\n",
      "Iteration 3983 Loss: 0.07672397354399106\n",
      "Iteration 3984 Loss: 0.07672374417572199\n",
      "Iteration 3985 Loss: 0.07672351501876347\n",
      "Iteration 3986 Loss: 0.07672328607262063\n",
      "Iteration 3987 Loss: 0.07672305733680011\n",
      "Iteration 3988 Loss: 0.07672282881080926\n",
      "Iteration 3989 Loss: 0.07672260049415693\n",
      "Iteration 3990 Loss: 0.0767223723863532\n",
      "Iteration 3991 Loss: 0.07672214448690916\n",
      "Iteration 3992 Loss: 0.07672191679533726\n",
      "Iteration 3993 Loss: 0.07672168931115107\n",
      "Iteration 3994 Loss: 0.07672146203386533\n",
      "Iteration 3995 Loss: 0.07672123496299595\n",
      "Iteration 3996 Loss: 0.07672100809806005\n",
      "Iteration 3997 Loss: 0.07672078143857597\n",
      "Iteration 3998 Loss: 0.07672055498406308\n",
      "Iteration 3999 Loss: 0.07672032873404212\n",
      "Iteration 4000 Loss: 0.07672010268803481\n",
      "Iteration 4001 Loss: 0.0767198768455641\n",
      "Iteration 4002 Loss: 0.07671965120615415\n",
      "Iteration 4003 Loss: 0.07671942576933026\n",
      "Iteration 4004 Loss: 0.07671920053461881\n",
      "Iteration 4005 Loss: 0.07671897550154742\n",
      "Iteration 4006 Loss: 0.07671875066964484\n",
      "Iteration 4007 Loss: 0.07671852603844093\n",
      "Iteration 4008 Loss: 0.0767183016074668\n",
      "Iteration 4009 Loss: 0.07671807737625455\n",
      "Iteration 4010 Loss: 0.07671785334433745\n",
      "Iteration 4011 Loss: 0.07671762951125005\n",
      "Iteration 4012 Loss: 0.07671740587652785\n",
      "Iteration 4013 Loss: 0.07671718243970761\n",
      "Iteration 4014 Loss: 0.07671695920032723\n",
      "Iteration 4015 Loss: 0.07671673615792556\n",
      "Iteration 4016 Loss: 0.07671651331204271\n",
      "Iteration 4017 Loss: 0.07671629066221995\n",
      "Iteration 4018 Loss: 0.07671606820799953\n",
      "Iteration 4019 Loss: 0.07671584594892497\n",
      "Iteration 4020 Loss: 0.0767156238845407\n",
      "Iteration 4021 Loss: 0.07671540201439254\n",
      "Iteration 4022 Loss: 0.07671518033802713\n",
      "Iteration 4023 Loss: 0.07671495885499241\n",
      "Iteration 4024 Loss: 0.0767147375648373\n",
      "Iteration 4025 Loss: 0.07671451646711186\n",
      "Iteration 4026 Loss: 0.07671429556136729\n",
      "Iteration 4027 Loss: 0.07671407484715581\n",
      "Iteration 4028 Loss: 0.07671385432403081\n",
      "Iteration 4029 Loss: 0.07671363399154671\n",
      "Iteration 4030 Loss: 0.07671341384925899\n",
      "Iteration 4031 Loss: 0.07671319389672429\n",
      "Iteration 4032 Loss: 0.07671297413350027\n",
      "Iteration 4033 Loss: 0.07671275455914567\n",
      "Iteration 4034 Loss: 0.07671253517322035\n",
      "Iteration 4035 Loss: 0.0767123159752852\n",
      "Iteration 4036 Loss: 0.07671209696490217\n",
      "Iteration 4037 Loss: 0.07671187814163431\n",
      "Iteration 4038 Loss: 0.07671165950504574\n",
      "Iteration 4039 Loss: 0.07671144105470161\n",
      "Iteration 4040 Loss: 0.07671122279016811\n",
      "Iteration 4041 Loss: 0.07671100471101253\n",
      "Iteration 4042 Loss: 0.0767107868168032\n",
      "Iteration 4043 Loss: 0.07671056910710954\n",
      "Iteration 4044 Loss: 0.07671035158150188\n",
      "Iteration 4045 Loss: 0.07671013423955182\n",
      "Iteration 4046 Loss: 0.0767099170808318\n",
      "Iteration 4047 Loss: 0.07670970010491535\n",
      "Iteration 4048 Loss: 0.07670948331137714\n",
      "Iteration 4049 Loss: 0.07670926669979275\n",
      "Iteration 4050 Loss: 0.0767090502697388\n",
      "Iteration 4051 Loss: 0.07670883402079313\n",
      "Iteration 4052 Loss: 0.07670861795253434\n",
      "Iteration 4053 Loss: 0.07670840206454221\n",
      "Iteration 4054 Loss: 0.07670818635639749\n",
      "Iteration 4055 Loss: 0.07670797082768202\n",
      "Iteration 4056 Loss: 0.07670775547797859\n",
      "Iteration 4057 Loss: 0.07670754030687102\n",
      "Iteration 4058 Loss: 0.07670732531394409\n",
      "Iteration 4059 Loss: 0.07670711049878368\n",
      "Iteration 4060 Loss: 0.07670689586097673\n",
      "Iteration 4061 Loss: 0.07670668140011105\n",
      "Iteration 4062 Loss: 0.07670646711577545\n",
      "Iteration 4063 Loss: 0.0767062530075599\n",
      "Iteration 4064 Loss: 0.07670603907505516\n",
      "Iteration 4065 Loss: 0.07670582531785311\n",
      "Iteration 4066 Loss: 0.07670561173554669\n",
      "Iteration 4067 Loss: 0.0767053983277296\n",
      "Iteration 4068 Loss: 0.07670518509399679\n",
      "Iteration 4069 Loss: 0.07670497203394407\n",
      "Iteration 4070 Loss: 0.07670475914716819\n",
      "Iteration 4071 Loss: 0.07670454643326696\n",
      "Iteration 4072 Loss: 0.07670433389183917\n",
      "Iteration 4073 Loss: 0.07670412152248453\n",
      "Iteration 4074 Loss: 0.07670390932480373\n",
      "Iteration 4075 Loss: 0.07670369729839852\n",
      "Iteration 4076 Loss: 0.07670348544287148\n",
      "Iteration 4077 Loss: 0.07670327375782632\n",
      "Iteration 4078 Loss: 0.07670306224286762\n",
      "Iteration 4079 Loss: 0.0767028508976008\n",
      "Iteration 4080 Loss: 0.07670263972163252\n",
      "Iteration 4081 Loss: 0.07670242871457016\n",
      "Iteration 4082 Loss: 0.0767022178760222\n",
      "Iteration 4083 Loss: 0.07670200720559797\n",
      "Iteration 4084 Loss: 0.07670179670290785\n",
      "Iteration 4085 Loss: 0.07670158636756304\n",
      "Iteration 4086 Loss: 0.07670137619917583\n",
      "Iteration 4087 Loss: 0.07670116619735935\n",
      "Iteration 4088 Loss: 0.07670095636172776\n",
      "Iteration 4089 Loss: 0.07670074669189604\n",
      "Iteration 4090 Loss: 0.0767005371874802\n",
      "Iteration 4091 Loss: 0.07670032784809715\n",
      "Iteration 4092 Loss: 0.07670011867336474\n",
      "Iteration 4093 Loss: 0.07669990966290177\n",
      "Iteration 4094 Loss: 0.07669970081632795\n",
      "Iteration 4095 Loss: 0.07669949213326391\n",
      "Iteration 4096 Loss: 0.07669928361333116\n",
      "Iteration 4097 Loss: 0.07669907525615224\n",
      "Iteration 4098 Loss: 0.07669886706135053\n",
      "Iteration 4099 Loss: 0.07669865902855039\n",
      "Iteration 4100 Loss: 0.07669845115737692\n",
      "Iteration 4101 Loss: 0.07669824344745639\n",
      "Iteration 4102 Loss: 0.07669803589841578\n",
      "Iteration 4103 Loss: 0.07669782850988305\n",
      "Iteration 4104 Loss: 0.0766976212814871\n",
      "Iteration 4105 Loss: 0.0766974142128577\n",
      "Iteration 4106 Loss: 0.07669720730362546\n",
      "Iteration 4107 Loss: 0.07669700055342198\n",
      "Iteration 4108 Loss: 0.07669679396187976\n",
      "Iteration 4109 Loss: 0.0766965875286321\n",
      "Iteration 4110 Loss: 0.07669638125331328\n",
      "Iteration 4111 Loss: 0.07669617513555843\n",
      "Iteration 4112 Loss: 0.07669596917500363\n",
      "Iteration 4113 Loss: 0.0766957633712857\n",
      "Iteration 4114 Loss: 0.07669555772404252\n",
      "Iteration 4115 Loss: 0.07669535223291273\n",
      "Iteration 4116 Loss: 0.07669514689753586\n",
      "Iteration 4117 Loss: 0.07669494171755238\n",
      "Iteration 4118 Loss: 0.07669473669260361\n",
      "Iteration 4119 Loss: 0.07669453182233177\n",
      "Iteration 4120 Loss: 0.07669432710637986\n",
      "Iteration 4121 Loss: 0.07669412254439183\n",
      "Iteration 4122 Loss: 0.07669391813601244\n",
      "Iteration 4123 Loss: 0.07669371388088736\n",
      "Iteration 4124 Loss: 0.07669350977866311\n",
      "Iteration 4125 Loss: 0.07669330582898705\n",
      "Iteration 4126 Loss: 0.07669310203150748\n",
      "Iteration 4127 Loss: 0.07669289838587341\n",
      "Iteration 4128 Loss: 0.07669269489173483\n",
      "Iteration 4129 Loss: 0.07669249154874251\n",
      "Iteration 4130 Loss: 0.07669228835654808\n",
      "Iteration 4131 Loss: 0.07669208531480415\n",
      "Iteration 4132 Loss: 0.07669188242316397\n",
      "Iteration 4133 Loss: 0.07669167968128171\n",
      "Iteration 4134 Loss: 0.07669147708881241\n",
      "Iteration 4135 Loss: 0.07669127464541198\n",
      "Iteration 4136 Loss: 0.07669107235073704\n",
      "Iteration 4137 Loss: 0.07669087020444522\n",
      "Iteration 4138 Loss: 0.0766906682061949\n",
      "Iteration 4139 Loss: 0.07669046635564515\n",
      "Iteration 4140 Loss: 0.07669026465245612\n",
      "Iteration 4141 Loss: 0.07669006309628863\n",
      "Iteration 4142 Loss: 0.07668986168680435\n",
      "Iteration 4143 Loss: 0.07668966042366586\n",
      "Iteration 4144 Loss: 0.07668945930653642\n",
      "Iteration 4145 Loss: 0.0766892583350802\n",
      "Iteration 4146 Loss: 0.07668905750896216\n",
      "Iteration 4147 Loss: 0.07668885682784811\n",
      "Iteration 4148 Loss: 0.0766886562914046\n",
      "Iteration 4149 Loss: 0.07668845589929911\n",
      "Iteration 4150 Loss: 0.07668825565119974\n",
      "Iteration 4151 Loss: 0.07668805554677559\n",
      "Iteration 4152 Loss: 0.07668785558569656\n",
      "Iteration 4153 Loss: 0.0766876557676331\n",
      "Iteration 4154 Loss: 0.07668745609225684\n",
      "Iteration 4155 Loss: 0.07668725655923979\n",
      "Iteration 4156 Loss: 0.07668705716825523\n",
      "Iteration 4157 Loss: 0.0766868579189768\n",
      "Iteration 4158 Loss: 0.0766866588110792\n",
      "Iteration 4159 Loss: 0.07668645984423784\n",
      "Iteration 4160 Loss: 0.07668626101812887\n",
      "Iteration 4161 Loss: 0.07668606233242936\n",
      "Iteration 4162 Loss: 0.07668586378681701\n",
      "Iteration 4163 Loss: 0.07668566538097048\n",
      "Iteration 4164 Loss: 0.076685467114569\n",
      "Iteration 4165 Loss: 0.07668526898729272\n",
      "Iteration 4166 Loss: 0.07668507099882256\n",
      "Iteration 4167 Loss: 0.0766848731488402\n",
      "Iteration 4168 Loss: 0.07668467543702814\n",
      "Iteration 4169 Loss: 0.0766844778630695\n",
      "Iteration 4170 Loss: 0.07668428042664834\n",
      "Iteration 4171 Loss: 0.07668408312744945\n",
      "Iteration 4172 Loss: 0.0766838859651583\n",
      "Iteration 4173 Loss: 0.07668368893946113\n",
      "Iteration 4174 Loss: 0.07668349205004517\n",
      "Iteration 4175 Loss: 0.07668329529659813\n",
      "Iteration 4176 Loss: 0.0766830986788086\n",
      "Iteration 4177 Loss: 0.07668290219636598\n",
      "Iteration 4178 Loss: 0.07668270584896024\n",
      "Iteration 4179 Loss: 0.07668250963628234\n",
      "Iteration 4180 Loss: 0.0766823135580238\n",
      "Iteration 4181 Loss: 0.07668211761387708\n",
      "Iteration 4182 Loss: 0.07668192180353517\n",
      "Iteration 4183 Loss: 0.076681726126692\n",
      "Iteration 4184 Loss: 0.07668153058304208\n",
      "Iteration 4185 Loss: 0.07668133517228083\n",
      "Iteration 4186 Loss: 0.07668113989410426\n",
      "Iteration 4187 Loss: 0.0766809447482092\n",
      "Iteration 4188 Loss: 0.07668074973429327\n",
      "Iteration 4189 Loss: 0.07668055485205462\n",
      "Iteration 4190 Loss: 0.07668036010119238\n",
      "Iteration 4191 Loss: 0.0766801654814063\n",
      "Iteration 4192 Loss: 0.07667997099239682\n",
      "Iteration 4193 Loss: 0.07667977663386516\n",
      "Iteration 4194 Loss: 0.07667958240551333\n",
      "Iteration 4195 Loss: 0.07667938830704393\n",
      "Iteration 4196 Loss: 0.07667919433816038\n",
      "Iteration 4197 Loss: 0.07667900049856673\n",
      "Iteration 4198 Loss: 0.07667880678796793\n",
      "Iteration 4199 Loss: 0.07667861320606942\n",
      "Iteration 4200 Loss: 0.07667841975257753\n",
      "Iteration 4201 Loss: 0.0766782264271992\n",
      "Iteration 4202 Loss: 0.07667803322964216\n",
      "Iteration 4203 Loss: 0.07667784015961478\n",
      "Iteration 4204 Loss: 0.07667764721682623\n",
      "Iteration 4205 Loss: 0.07667745440098626\n",
      "Iteration 4206 Loss: 0.07667726171180544\n",
      "Iteration 4207 Loss: 0.076677069148995\n",
      "Iteration 4208 Loss: 0.07667687671226685\n",
      "Iteration 4209 Loss: 0.07667668440133368\n",
      "Iteration 4210 Loss: 0.0766764922159088\n",
      "Iteration 4211 Loss: 0.07667630015570621\n",
      "Iteration 4212 Loss: 0.07667610822044066\n",
      "Iteration 4213 Loss: 0.07667591640982761\n",
      "Iteration 4214 Loss: 0.07667572472358312\n",
      "Iteration 4215 Loss: 0.07667553316142402\n",
      "Iteration 4216 Loss: 0.0766753417230678\n",
      "Iteration 4217 Loss: 0.07667515040823264\n",
      "Iteration 4218 Loss: 0.07667495921663746\n",
      "Iteration 4219 Loss: 0.0766747681480017\n",
      "Iteration 4220 Loss: 0.07667457720204567\n",
      "Iteration 4221 Loss: 0.07667438637849028\n",
      "Iteration 4222 Loss: 0.07667419567705705\n",
      "Iteration 4223 Loss: 0.0766740050974684\n",
      "Iteration 4224 Loss: 0.07667381463944717\n",
      "Iteration 4225 Loss: 0.07667362430271699\n",
      "Iteration 4226 Loss: 0.07667343408700217\n",
      "Iteration 4227 Loss: 0.07667324399202766\n",
      "Iteration 4228 Loss: 0.07667305401751913\n",
      "Iteration 4229 Loss: 0.07667286416320274\n",
      "Iteration 4230 Loss: 0.07667267442880563\n",
      "Iteration 4231 Loss: 0.0766724848140554\n",
      "Iteration 4232 Loss: 0.07667229531868026\n",
      "Iteration 4233 Loss: 0.07667210594240924\n",
      "Iteration 4234 Loss: 0.07667191668497188\n",
      "Iteration 4235 Loss: 0.07667172754609851\n",
      "Iteration 4236 Loss: 0.07667153852552007\n",
      "Iteration 4237 Loss: 0.07667134962296808\n",
      "Iteration 4238 Loss: 0.07667116083817482\n",
      "Iteration 4239 Loss: 0.07667097217087317\n",
      "Iteration 4240 Loss: 0.07667078362079663\n",
      "Iteration 4241 Loss: 0.07667059518767942\n",
      "Iteration 4242 Loss: 0.07667040687125634\n",
      "Iteration 4243 Loss: 0.07667021867126293\n",
      "Iteration 4244 Loss: 0.07667003058743528\n",
      "Iteration 4245 Loss: 0.07666984261951008\n",
      "Iteration 4246 Loss: 0.07666965476722483\n",
      "Iteration 4247 Loss: 0.07666946703031752\n",
      "Iteration 4248 Loss: 0.07666927940852682\n",
      "Iteration 4249 Loss: 0.07666909190159207\n",
      "Iteration 4250 Loss: 0.07666890450925318\n",
      "Iteration 4251 Loss: 0.07666871723125081\n",
      "Iteration 4252 Loss: 0.07666853006732605\n",
      "Iteration 4253 Loss: 0.07666834301722085\n",
      "Iteration 4254 Loss: 0.07666815608067759\n",
      "Iteration 4255 Loss: 0.07666796925743945\n",
      "Iteration 4256 Loss: 0.0766677825472501\n",
      "Iteration 4257 Loss: 0.07666759594985394\n",
      "Iteration 4258 Loss: 0.07666740946499577\n",
      "Iteration 4259 Loss: 0.0766672230924214\n",
      "Iteration 4260 Loss: 0.07666703683187685\n",
      "Iteration 4261 Loss: 0.07666685068310906\n",
      "Iteration 4262 Loss: 0.07666666464586541\n",
      "Iteration 4263 Loss: 0.07666647871989393\n",
      "Iteration 4264 Loss: 0.07666629290494338\n",
      "Iteration 4265 Loss: 0.07666610720076299\n",
      "Iteration 4266 Loss: 0.07666592160710256\n",
      "Iteration 4267 Loss: 0.07666573612371268\n",
      "Iteration 4268 Loss: 0.07666555075034444\n",
      "Iteration 4269 Loss: 0.07666536548674947\n",
      "Iteration 4270 Loss: 0.07666518033268026\n",
      "Iteration 4271 Loss: 0.07666499528788955\n",
      "Iteration 4272 Loss: 0.07666481035213088\n",
      "Iteration 4273 Loss: 0.07666462552515843\n",
      "Iteration 4274 Loss: 0.07666444080672682\n",
      "Iteration 4275 Loss: 0.07666425619659144\n",
      "Iteration 4276 Loss: 0.07666407169450822\n",
      "Iteration 4277 Loss: 0.07666388730023355\n",
      "Iteration 4278 Loss: 0.07666370301352454\n",
      "Iteration 4279 Loss: 0.07666351883413898\n",
      "Iteration 4280 Loss: 0.07666333476183496\n",
      "Iteration 4281 Loss: 0.07666315079637151\n",
      "Iteration 4282 Loss: 0.07666296693750797\n",
      "Iteration 4283 Loss: 0.07666278318500436\n",
      "Iteration 4284 Loss: 0.07666259953862138\n",
      "Iteration 4285 Loss: 0.07666241599812015\n",
      "Iteration 4286 Loss: 0.07666223256326252\n",
      "Iteration 4287 Loss: 0.07666204923381076\n",
      "Iteration 4288 Loss: 0.07666186600952782\n",
      "Iteration 4289 Loss: 0.0766616828901772\n",
      "Iteration 4290 Loss: 0.07666149987552312\n",
      "Iteration 4291 Loss: 0.07666131696533006\n",
      "Iteration 4292 Loss: 0.07666113415936335\n",
      "Iteration 4293 Loss: 0.07666095145738873\n",
      "Iteration 4294 Loss: 0.07666076885917264\n",
      "Iteration 4295 Loss: 0.0766605863644819\n",
      "Iteration 4296 Loss: 0.07666040397308418\n",
      "Iteration 4297 Loss: 0.07666022168474743\n",
      "Iteration 4298 Loss: 0.07666003949924038\n",
      "Iteration 4299 Loss: 0.07665985741633212\n",
      "Iteration 4300 Loss: 0.07665967543579245\n",
      "Iteration 4301 Loss: 0.07665949355739177\n",
      "Iteration 4302 Loss: 0.0766593117809008\n",
      "Iteration 4303 Loss: 0.07665913010609118\n",
      "Iteration 4304 Loss: 0.07665894853273472\n",
      "Iteration 4305 Loss: 0.07665876706060402\n",
      "Iteration 4306 Loss: 0.07665858568947226\n",
      "Iteration 4307 Loss: 0.07665840441911298\n",
      "Iteration 4308 Loss: 0.07665822324930041\n",
      "Iteration 4309 Loss: 0.07665804217980933\n",
      "Iteration 4310 Loss: 0.07665786121041504\n",
      "Iteration 4311 Loss: 0.07665768034089339\n",
      "Iteration 4312 Loss: 0.07665749957102072\n",
      "Iteration 4313 Loss: 0.07665731890057395\n",
      "Iteration 4314 Loss: 0.07665713832933067\n",
      "Iteration 4315 Loss: 0.07665695785706882\n",
      "Iteration 4316 Loss: 0.07665677748356693\n",
      "Iteration 4317 Loss: 0.0766565972086041\n",
      "Iteration 4318 Loss: 0.07665641703196008\n",
      "Iteration 4319 Loss: 0.07665623695341484\n",
      "Iteration 4320 Loss: 0.07665605697274923\n",
      "Iteration 4321 Loss: 0.07665587708974446\n",
      "Iteration 4322 Loss: 0.07665569730418233\n",
      "Iteration 4323 Loss: 0.07665551761584499\n",
      "Iteration 4324 Loss: 0.07665533802451543\n",
      "Iteration 4325 Loss: 0.07665515852997697\n",
      "Iteration 4326 Loss: 0.07665497913201343\n",
      "Iteration 4327 Loss: 0.07665479983040929\n",
      "Iteration 4328 Loss: 0.07665462062494942\n",
      "Iteration 4329 Loss: 0.07665444151541936\n",
      "Iteration 4330 Loss: 0.07665426250160504\n",
      "Iteration 4331 Loss: 0.0766540835832929\n",
      "Iteration 4332 Loss: 0.07665390476027004\n",
      "Iteration 4333 Loss: 0.07665372603232395\n",
      "Iteration 4334 Loss: 0.07665354739924274\n",
      "Iteration 4335 Loss: 0.0766533688608149\n",
      "Iteration 4336 Loss: 0.07665319041682957\n",
      "Iteration 4337 Loss: 0.07665301206707627\n",
      "Iteration 4338 Loss: 0.07665283381134518\n",
      "Iteration 4339 Loss: 0.07665265564942696\n",
      "Iteration 4340 Loss: 0.07665247758111259\n",
      "Iteration 4341 Loss: 0.07665229960619378\n",
      "Iteration 4342 Loss: 0.07665212172446269\n",
      "Iteration 4343 Loss: 0.07665194393571198\n",
      "Iteration 4344 Loss: 0.07665176623973476\n",
      "Iteration 4345 Loss: 0.07665158863632465\n",
      "Iteration 4346 Loss: 0.07665141112527585\n",
      "Iteration 4347 Loss: 0.07665123370638299\n",
      "Iteration 4348 Loss: 0.07665105637944127\n",
      "Iteration 4349 Loss: 0.07665087914424631\n",
      "Iteration 4350 Loss: 0.07665070200059426\n",
      "Iteration 4351 Loss: 0.07665052494828174\n",
      "Iteration 4352 Loss: 0.07665034798710593\n",
      "Iteration 4353 Loss: 0.07665017111686442\n",
      "Iteration 4354 Loss: 0.07664999433735538\n",
      "Iteration 4355 Loss: 0.0766498176483774\n",
      "Iteration 4356 Loss: 0.07664964104972961\n",
      "Iteration 4357 Loss: 0.07664946454121152\n",
      "Iteration 4358 Loss: 0.07664928812262331\n",
      "Iteration 4359 Loss: 0.0766491117937655\n",
      "Iteration 4360 Loss: 0.07664893555443913\n",
      "Iteration 4361 Loss: 0.07664875940444581\n",
      "Iteration 4362 Loss: 0.07664858334358743\n",
      "Iteration 4363 Loss: 0.07664840737166659\n",
      "Iteration 4364 Loss: 0.0766482314884862\n",
      "Iteration 4365 Loss: 0.07664805569384982\n",
      "Iteration 4366 Loss: 0.07664787998756123\n",
      "Iteration 4367 Loss: 0.07664770436942496\n",
      "Iteration 4368 Loss: 0.07664752883924592\n",
      "Iteration 4369 Loss: 0.07664735339682935\n",
      "Iteration 4370 Loss: 0.07664717804198116\n",
      "Iteration 4371 Loss: 0.07664700277450763\n",
      "Iteration 4372 Loss: 0.07664682759421562\n",
      "Iteration 4373 Loss: 0.07664665250091235\n",
      "Iteration 4374 Loss: 0.07664647749440542\n",
      "Iteration 4375 Loss: 0.07664630257450314\n",
      "Iteration 4376 Loss: 0.07664612774101412\n",
      "Iteration 4377 Loss: 0.07664595299374746\n",
      "Iteration 4378 Loss: 0.07664577833251275\n",
      "Iteration 4379 Loss: 0.07664560375712001\n",
      "Iteration 4380 Loss: 0.07664542926737981\n",
      "Iteration 4381 Loss: 0.076645254863103\n",
      "Iteration 4382 Loss: 0.07664508054410116\n",
      "Iteration 4383 Loss: 0.07664490631018606\n",
      "Iteration 4384 Loss: 0.07664473216117001\n",
      "Iteration 4385 Loss: 0.07664455809686596\n",
      "Iteration 4386 Loss: 0.076644384117087\n",
      "Iteration 4387 Loss: 0.07664421022164693\n",
      "Iteration 4388 Loss: 0.07664403641035986\n",
      "Iteration 4389 Loss: 0.07664386268304041\n",
      "Iteration 4390 Loss: 0.07664368903950372\n",
      "Iteration 4391 Loss: 0.07664351547956519\n",
      "Iteration 4392 Loss: 0.07664334200304075\n",
      "Iteration 4393 Loss: 0.07664316860974696\n",
      "Iteration 4394 Loss: 0.07664299529950051\n",
      "Iteration 4395 Loss: 0.07664282207211884\n",
      "Iteration 4396 Loss: 0.0766426489274196\n",
      "Iteration 4397 Loss: 0.076642475865221\n",
      "Iteration 4398 Loss: 0.07664230288534161\n",
      "Iteration 4399 Loss: 0.07664212998760059\n",
      "Iteration 4400 Loss: 0.07664195717181743\n",
      "Iteration 4401 Loss: 0.07664178443781201\n",
      "Iteration 4402 Loss: 0.07664161178540475\n",
      "Iteration 4403 Loss: 0.07664143921441653\n",
      "Iteration 4404 Loss: 0.07664126672466852\n",
      "Iteration 4405 Loss: 0.0766410943159824\n",
      "Iteration 4406 Loss: 0.07664092198818039\n",
      "Iteration 4407 Loss: 0.07664074974108495\n",
      "Iteration 4408 Loss: 0.07664057757451913\n",
      "Iteration 4409 Loss: 0.07664040548830633\n",
      "Iteration 4410 Loss: 0.07664023348227036\n",
      "Iteration 4411 Loss: 0.07664006155623554\n",
      "Iteration 4412 Loss: 0.07663988971002654\n",
      "Iteration 4413 Loss: 0.07663971794346852\n",
      "Iteration 4414 Loss: 0.07663954625638703\n",
      "Iteration 4415 Loss: 0.07663937464860796\n",
      "Iteration 4416 Loss: 0.07663920311995782\n",
      "Iteration 4417 Loss: 0.07663903167026341\n",
      "Iteration 4418 Loss: 0.07663886029935192\n",
      "Iteration 4419 Loss: 0.07663868900705108\n",
      "Iteration 4420 Loss: 0.0766385177931889\n",
      "Iteration 4421 Loss: 0.07663834665759393\n",
      "Iteration 4422 Loss: 0.07663817560009507\n",
      "Iteration 4423 Loss: 0.07663800462052164\n",
      "Iteration 4424 Loss: 0.07663783371870343\n",
      "Iteration 4425 Loss: 0.07663766289447055\n",
      "Iteration 4426 Loss: 0.07663749214765354\n",
      "Iteration 4427 Loss: 0.07663732147808357\n",
      "Iteration 4428 Loss: 0.07663715088559177\n",
      "Iteration 4429 Loss: 0.0766369803700101\n",
      "Iteration 4430 Loss: 0.07663680993117081\n",
      "Iteration 4431 Loss: 0.07663663956890639\n",
      "Iteration 4432 Loss: 0.07663646928304998\n",
      "Iteration 4433 Loss: 0.07663629907343492\n",
      "Iteration 4434 Loss: 0.07663612893989513\n",
      "Iteration 4435 Loss: 0.07663595888226485\n",
      "Iteration 4436 Loss: 0.07663578890037867\n",
      "Iteration 4437 Loss: 0.07663561899407165\n",
      "Iteration 4438 Loss: 0.07663544916317926\n",
      "Iteration 4439 Loss: 0.07663527940753735\n",
      "Iteration 4440 Loss: 0.07663510972698216\n",
      "Iteration 4441 Loss: 0.07663494012135032\n",
      "Iteration 4442 Loss: 0.0766347705904789\n",
      "Iteration 4443 Loss: 0.07663460113420528\n",
      "Iteration 4444 Loss: 0.07663443175236737\n",
      "Iteration 4445 Loss: 0.07663426244480334\n",
      "Iteration 4446 Loss: 0.0766340932113519\n",
      "Iteration 4447 Loss: 0.07663392405185192\n",
      "Iteration 4448 Loss: 0.07663375496614287\n",
      "Iteration 4449 Loss: 0.07663358595406457\n",
      "Iteration 4450 Loss: 0.07663341701545714\n",
      "Iteration 4451 Loss: 0.07663324815016125\n",
      "Iteration 4452 Loss: 0.07663307935801777\n",
      "Iteration 4453 Loss: 0.07663291063886807\n",
      "Iteration 4454 Loss: 0.07663274199255393\n",
      "Iteration 4455 Loss: 0.07663257341891735\n",
      "Iteration 4456 Loss: 0.07663240491780093\n",
      "Iteration 4457 Loss: 0.07663223648904754\n",
      "Iteration 4458 Loss: 0.0766320681325004\n",
      "Iteration 4459 Loss: 0.07663189984800317\n",
      "Iteration 4460 Loss: 0.07663173163539984\n",
      "Iteration 4461 Loss: 0.07663156349453494\n",
      "Iteration 4462 Loss: 0.07663139542525312\n",
      "Iteration 4463 Loss: 0.07663122742739956\n",
      "Iteration 4464 Loss: 0.07663105950081978\n",
      "Iteration 4465 Loss: 0.07663089164535983\n",
      "Iteration 4466 Loss: 0.07663072386086583\n",
      "Iteration 4467 Loss: 0.07663055614718445\n",
      "Iteration 4468 Loss: 0.07663038850416277\n",
      "Iteration 4469 Loss: 0.07663022093164824\n",
      "Iteration 4470 Loss: 0.07663005342948859\n",
      "Iteration 4471 Loss: 0.07662988599753189\n",
      "Iteration 4472 Loss: 0.0766297186356267\n",
      "Iteration 4473 Loss: 0.07662955134362197\n",
      "Iteration 4474 Loss: 0.07662938412136679\n",
      "Iteration 4475 Loss: 0.07662921696871097\n",
      "Iteration 4476 Loss: 0.07662904988550434\n",
      "Iteration 4477 Loss: 0.07662888287159726\n",
      "Iteration 4478 Loss: 0.07662871592684051\n",
      "Iteration 4479 Loss: 0.07662854905108507\n",
      "Iteration 4480 Loss: 0.0766283822441824\n",
      "Iteration 4481 Loss: 0.07662821550598432\n",
      "Iteration 4482 Loss: 0.07662804883634296\n",
      "Iteration 4483 Loss: 0.07662788223511087\n",
      "Iteration 4484 Loss: 0.07662771570214083\n",
      "Iteration 4485 Loss: 0.07662754923728608\n",
      "Iteration 4486 Loss: 0.07662738284040031\n",
      "Iteration 4487 Loss: 0.07662721651133736\n",
      "Iteration 4488 Loss: 0.07662705024995152\n",
      "Iteration 4489 Loss: 0.07662688405609751\n",
      "Iteration 4490 Loss: 0.07662671792963026\n",
      "Iteration 4491 Loss: 0.07662655187040515\n",
      "Iteration 4492 Loss: 0.0766263858782779\n",
      "Iteration 4493 Loss: 0.07662621995310452\n",
      "Iteration 4494 Loss: 0.07662605409474145\n",
      "Iteration 4495 Loss: 0.07662588830304547\n",
      "Iteration 4496 Loss: 0.0766257225778736\n",
      "Iteration 4497 Loss: 0.07662555691908335\n",
      "Iteration 4498 Loss: 0.07662539132653248\n",
      "Iteration 4499 Loss: 0.07662522580007916\n",
      "Iteration 4500 Loss: 0.07662506033958186\n",
      "Iteration 4501 Loss: 0.07662489494489938\n",
      "Iteration 4502 Loss: 0.07662472961589095\n",
      "Iteration 4503 Loss: 0.07662456435241599\n",
      "Iteration 4504 Loss: 0.07662439915433442\n",
      "Iteration 4505 Loss: 0.0766242340215065\n",
      "Iteration 4506 Loss: 0.07662406895379263\n",
      "Iteration 4507 Loss: 0.07662390395105373\n",
      "Iteration 4508 Loss: 0.07662373901315106\n",
      "Iteration 4509 Loss: 0.07662357413994612\n",
      "Iteration 4510 Loss: 0.07662340933130082\n",
      "Iteration 4511 Loss: 0.0766232445870773\n",
      "Iteration 4512 Loss: 0.07662307990713828\n",
      "Iteration 4513 Loss: 0.07662291529134646\n",
      "Iteration 4514 Loss: 0.07662275073956519\n",
      "Iteration 4515 Loss: 0.07662258625165792\n",
      "Iteration 4516 Loss: 0.07662242182748867\n",
      "Iteration 4517 Loss: 0.07662225746692151\n",
      "Iteration 4518 Loss: 0.07662209316982108\n",
      "Iteration 4519 Loss: 0.07662192893605223\n",
      "Iteration 4520 Loss: 0.07662176476548012\n",
      "Iteration 4521 Loss: 0.07662160065797037\n",
      "Iteration 4522 Loss: 0.0766214366133888\n",
      "Iteration 4523 Loss: 0.07662127263160151\n",
      "Iteration 4524 Loss: 0.0766211087124751\n",
      "Iteration 4525 Loss: 0.07662094485587637\n",
      "Iteration 4526 Loss: 0.0766207810616725\n",
      "Iteration 4527 Loss: 0.07662061732973087\n",
      "Iteration 4528 Loss: 0.07662045365991943\n",
      "Iteration 4529 Loss: 0.07662029005210616\n",
      "Iteration 4530 Loss: 0.07662012650615957\n",
      "Iteration 4531 Loss: 0.07661996302194841\n",
      "Iteration 4532 Loss: 0.07661979959934179\n",
      "Iteration 4533 Loss: 0.07661963623820904\n",
      "Iteration 4534 Loss: 0.07661947293841993\n",
      "Iteration 4535 Loss: 0.07661930969984443\n",
      "Iteration 4536 Loss: 0.0766191465223529\n",
      "Iteration 4537 Loss: 0.07661898340581606\n",
      "Iteration 4538 Loss: 0.07661882035010482\n",
      "Iteration 4539 Loss: 0.07661865735509056\n",
      "Iteration 4540 Loss: 0.07661849442064475\n",
      "Iteration 4541 Loss: 0.07661833154663934\n",
      "Iteration 4542 Loss: 0.07661816873294659\n",
      "Iteration 4543 Loss: 0.07661800597943905\n",
      "Iteration 4544 Loss: 0.07661784328598946\n",
      "Iteration 4545 Loss: 0.07661768065247108\n",
      "Iteration 4546 Loss: 0.07661751807875737\n",
      "Iteration 4547 Loss: 0.076617355564722\n",
      "Iteration 4548 Loss: 0.07661719311023908\n",
      "Iteration 4549 Loss: 0.07661703071518305\n",
      "Iteration 4550 Loss: 0.07661686837942855\n",
      "Iteration 4551 Loss: 0.07661670610285061\n",
      "Iteration 4552 Loss: 0.07661654388532436\n",
      "Iteration 4553 Loss: 0.0766163817267256\n",
      "Iteration 4554 Loss: 0.07661621962693005\n",
      "Iteration 4555 Loss: 0.07661605758581407\n",
      "Iteration 4556 Loss: 0.07661589560325409\n",
      "Iteration 4557 Loss: 0.07661573367912687\n",
      "Iteration 4558 Loss: 0.07661557181330955\n",
      "Iteration 4559 Loss: 0.07661541000567952\n",
      "Iteration 4560 Loss: 0.07661524825611447\n",
      "Iteration 4561 Loss: 0.07661508656449235\n",
      "Iteration 4562 Loss: 0.0766149249306915\n",
      "Iteration 4563 Loss: 0.0766147633545905\n",
      "Iteration 4564 Loss: 0.0766146018360682\n",
      "Iteration 4565 Loss: 0.07661444037500381\n",
      "Iteration 4566 Loss: 0.07661427897127672\n",
      "Iteration 4567 Loss: 0.07661411762476676\n",
      "Iteration 4568 Loss: 0.07661395633535391\n",
      "Iteration 4569 Loss: 0.07661379510291857\n",
      "Iteration 4570 Loss: 0.07661363392734136\n",
      "Iteration 4571 Loss: 0.07661347280850317\n",
      "Iteration 4572 Loss: 0.07661331174628525\n",
      "Iteration 4573 Loss: 0.07661315074056904\n",
      "Iteration 4574 Loss: 0.07661298979123635\n",
      "Iteration 4575 Loss: 0.07661282889816932\n",
      "Iteration 4576 Loss: 0.07661266806125021\n",
      "Iteration 4577 Loss: 0.07661250728036167\n",
      "Iteration 4578 Loss: 0.07661234655538669\n",
      "Iteration 4579 Loss: 0.07661218588620848\n",
      "Iteration 4580 Loss: 0.07661202527271044\n",
      "Iteration 4581 Loss: 0.07661186471477646\n",
      "Iteration 4582 Loss: 0.07661170421229059\n",
      "Iteration 4583 Loss: 0.07661154376513711\n",
      "Iteration 4584 Loss: 0.07661138337320067\n",
      "Iteration 4585 Loss: 0.07661122303636617\n",
      "Iteration 4586 Loss: 0.07661106275451879\n",
      "Iteration 4587 Loss: 0.07661090252754399\n",
      "Iteration 4588 Loss: 0.07661074235532757\n",
      "Iteration 4589 Loss: 0.07661058223775545\n",
      "Iteration 4590 Loss: 0.07661042217471398\n",
      "Iteration 4591 Loss: 0.07661026216608971\n",
      "Iteration 4592 Loss: 0.0766101022117695\n",
      "Iteration 4593 Loss: 0.07660994231164046\n",
      "Iteration 4594 Loss: 0.07660978246559\n",
      "Iteration 4595 Loss: 0.07660962267350574\n",
      "Iteration 4596 Loss: 0.07660946293527565\n",
      "Iteration 4597 Loss: 0.07660930325078798\n",
      "Iteration 4598 Loss: 0.07660914361993112\n",
      "Iteration 4599 Loss: 0.07660898404259388\n",
      "Iteration 4600 Loss: 0.07660882451866526\n",
      "Iteration 4601 Loss: 0.07660866504803461\n",
      "Iteration 4602 Loss: 0.07660850563059148\n",
      "Iteration 4603 Loss: 0.07660834626622555\n",
      "Iteration 4604 Loss: 0.07660818695482717\n",
      "Iteration 4605 Loss: 0.0766080276962865\n",
      "Iteration 4606 Loss: 0.07660786849049424\n",
      "Iteration 4607 Loss: 0.07660770933734128\n",
      "Iteration 4608 Loss: 0.07660755023671878\n",
      "Iteration 4609 Loss: 0.07660739118851818\n",
      "Iteration 4610 Loss: 0.07660723219263113\n",
      "Iteration 4611 Loss: 0.07660707324894958\n",
      "Iteration 4612 Loss: 0.07660691435736576\n",
      "Iteration 4613 Loss: 0.07660675551777218\n",
      "Iteration 4614 Loss: 0.07660659673006151\n",
      "Iteration 4615 Loss: 0.07660643799412677\n",
      "Iteration 4616 Loss: 0.07660627930986122\n",
      "Iteration 4617 Loss: 0.07660612067715837\n",
      "Iteration 4618 Loss: 0.076605962095912\n",
      "Iteration 4619 Loss: 0.07660580356601607\n",
      "Iteration 4620 Loss: 0.076605645087365\n",
      "Iteration 4621 Loss: 0.07660548665985323\n",
      "Iteration 4622 Loss: 0.07660532828337557\n",
      "Iteration 4623 Loss: 0.0766051699578271\n",
      "Iteration 4624 Loss: 0.07660501168310313\n",
      "Iteration 4625 Loss: 0.07660485345909923\n",
      "Iteration 4626 Loss: 0.07660469528571116\n",
      "Iteration 4627 Loss: 0.07660453716283501\n",
      "Iteration 4628 Loss: 0.07660437909036713\n",
      "Iteration 4629 Loss: 0.07660422106820412\n",
      "Iteration 4630 Loss: 0.07660406309624271\n",
      "Iteration 4631 Loss: 0.07660390517438005\n",
      "Iteration 4632 Loss: 0.07660374730251342\n",
      "Iteration 4633 Loss: 0.07660358948054047\n",
      "Iteration 4634 Loss: 0.07660343170835886\n",
      "Iteration 4635 Loss: 0.07660327398586685\n",
      "Iteration 4636 Loss: 0.0766031163129626\n",
      "Iteration 4637 Loss: 0.07660295868954478\n",
      "Iteration 4638 Loss: 0.07660280111551211\n",
      "Iteration 4639 Loss: 0.07660264359076371\n",
      "Iteration 4640 Loss: 0.07660248611519889\n",
      "Iteration 4641 Loss: 0.07660232868871719\n",
      "Iteration 4642 Loss: 0.07660217131121833\n",
      "Iteration 4643 Loss: 0.07660201398260233\n",
      "Iteration 4644 Loss: 0.07660185670276959\n",
      "Iteration 4645 Loss: 0.07660169947162056\n",
      "Iteration 4646 Loss: 0.07660154228905591\n",
      "Iteration 4647 Loss: 0.07660138515497678\n",
      "Iteration 4648 Loss: 0.07660122806928431\n",
      "Iteration 4649 Loss: 0.07660107103187996\n",
      "Iteration 4650 Loss: 0.07660091404266557\n",
      "Iteration 4651 Loss: 0.07660075710154292\n",
      "Iteration 4652 Loss: 0.07660060020841436\n",
      "Iteration 4653 Loss: 0.07660044336318225\n",
      "Iteration 4654 Loss: 0.07660028656574924\n",
      "Iteration 4655 Loss: 0.07660012981601827\n",
      "Iteration 4656 Loss: 0.07659997311389241\n",
      "Iteration 4657 Loss: 0.07659981645927512\n",
      "Iteration 4658 Loss: 0.07659965985206994\n",
      "Iteration 4659 Loss: 0.07659950329218078\n",
      "Iteration 4660 Loss: 0.07659934677951162\n",
      "Iteration 4661 Loss: 0.07659919031396681\n",
      "Iteration 4662 Loss: 0.07659903389545092\n",
      "Iteration 4663 Loss: 0.07659887752386871\n",
      "Iteration 4664 Loss: 0.07659872119912514\n",
      "Iteration 4665 Loss: 0.07659856492112539\n",
      "Iteration 4666 Loss: 0.0765984086897751\n",
      "Iteration 4667 Loss: 0.07659825250497981\n",
      "Iteration 4668 Loss: 0.0765980963666455\n",
      "Iteration 4669 Loss: 0.07659794027467828\n",
      "Iteration 4670 Loss: 0.07659778422898449\n",
      "Iteration 4671 Loss: 0.07659762822947087\n",
      "Iteration 4672 Loss: 0.07659747227604412\n",
      "Iteration 4673 Loss: 0.07659731636861136\n",
      "Iteration 4674 Loss: 0.07659716050707982\n",
      "Iteration 4675 Loss: 0.07659700469135704\n",
      "Iteration 4676 Loss: 0.07659684892135074\n",
      "Iteration 4677 Loss: 0.07659669319696887\n",
      "Iteration 4678 Loss: 0.07659653751811958\n",
      "Iteration 4679 Loss: 0.0765963818847113\n",
      "Iteration 4680 Loss: 0.07659622629665265\n",
      "Iteration 4681 Loss: 0.0765960707538525\n",
      "Iteration 4682 Loss: 0.07659591525621981\n",
      "Iteration 4683 Loss: 0.07659575980366394\n",
      "Iteration 4684 Loss: 0.07659560439609443\n",
      "Iteration 4685 Loss: 0.0765954490334209\n",
      "Iteration 4686 Loss: 0.07659529371555336\n",
      "Iteration 4687 Loss: 0.07659513844240197\n",
      "Iteration 4688 Loss: 0.07659498321387713\n",
      "Iteration 4689 Loss: 0.0765948280298893\n",
      "Iteration 4690 Loss: 0.0765946728903495\n",
      "Iteration 4691 Loss: 0.07659451779516856\n",
      "Iteration 4692 Loss: 0.07659436274425785\n",
      "Iteration 4693 Loss: 0.07659420773752876\n",
      "Iteration 4694 Loss: 0.07659405277489302\n",
      "Iteration 4695 Loss: 0.0765938978562625\n",
      "Iteration 4696 Loss: 0.07659374298154926\n",
      "Iteration 4697 Loss: 0.0765935881506657\n",
      "Iteration 4698 Loss: 0.0765934333635242\n",
      "Iteration 4699 Loss: 0.07659327862003766\n",
      "Iteration 4700 Loss: 0.07659312392011895\n",
      "Iteration 4701 Loss: 0.07659296926368127\n",
      "Iteration 4702 Loss: 0.0765928146506379\n",
      "Iteration 4703 Loss: 0.07659266008090254\n",
      "Iteration 4704 Loss: 0.07659250555438885\n",
      "Iteration 4705 Loss: 0.07659235107101098\n",
      "Iteration 4706 Loss: 0.07659219663068303\n",
      "Iteration 4707 Loss: 0.0765920422333195\n",
      "Iteration 4708 Loss: 0.07659188787883495\n",
      "Iteration 4709 Loss: 0.07659173356714417\n",
      "Iteration 4710 Loss: 0.07659157929816232\n",
      "Iteration 4711 Loss: 0.07659142507180451\n",
      "Iteration 4712 Loss: 0.07659127088798635\n",
      "Iteration 4713 Loss: 0.07659111674662333\n",
      "Iteration 4714 Loss: 0.07659096264763138\n",
      "Iteration 4715 Loss: 0.07659080859092661\n",
      "Iteration 4716 Loss: 0.07659065457642517\n",
      "Iteration 4717 Loss: 0.07659050060404364\n",
      "Iteration 4718 Loss: 0.07659034667369863\n",
      "Iteration 4719 Loss: 0.07659019278530703\n",
      "Iteration 4720 Loss: 0.0765900389387859\n",
      "Iteration 4721 Loss: 0.07658988513405257\n",
      "Iteration 4722 Loss: 0.07658973137102444\n",
      "Iteration 4723 Loss: 0.0765895776496192\n",
      "Iteration 4724 Loss: 0.0765894239697548\n",
      "Iteration 4725 Loss: 0.0765892703313492\n",
      "Iteration 4726 Loss: 0.07658911673432077\n",
      "Iteration 4727 Loss: 0.07658896317858795\n",
      "Iteration 4728 Loss: 0.07658880966406935\n",
      "Iteration 4729 Loss: 0.07658865619068396\n",
      "Iteration 4730 Loss: 0.07658850275835068\n",
      "Iteration 4731 Loss: 0.07658834936698891\n",
      "Iteration 4732 Loss: 0.07658819601651808\n",
      "Iteration 4733 Loss: 0.07658804270685772\n",
      "Iteration 4734 Loss: 0.0765878894379278\n",
      "Iteration 4735 Loss: 0.07658773620964837\n",
      "Iteration 4736 Loss: 0.07658758302193958\n",
      "Iteration 4737 Loss: 0.07658742987472186\n",
      "Iteration 4738 Loss: 0.07658727676791587\n",
      "Iteration 4739 Loss: 0.07658712370144242\n",
      "Iteration 4740 Loss: 0.07658697067522248\n",
      "Iteration 4741 Loss: 0.07658681768917727\n",
      "Iteration 4742 Loss: 0.0765866647432282\n",
      "Iteration 4743 Loss: 0.0765865118372968\n",
      "Iteration 4744 Loss: 0.07658635897130481\n",
      "Iteration 4745 Loss: 0.07658620614517422\n",
      "Iteration 4746 Loss: 0.07658605335882719\n",
      "Iteration 4747 Loss: 0.076585900612186\n",
      "Iteration 4748 Loss: 0.07658574790517325\n",
      "Iteration 4749 Loss: 0.07658559523771158\n",
      "Iteration 4750 Loss: 0.07658544260972384\n",
      "Iteration 4751 Loss: 0.0765852900211333\n",
      "Iteration 4752 Loss: 0.07658513747186303\n",
      "Iteration 4753 Loss: 0.0765849849618366\n",
      "Iteration 4754 Loss: 0.07658483249097756\n",
      "Iteration 4755 Loss: 0.07658468005920983\n",
      "Iteration 4756 Loss: 0.07658452766645737\n",
      "Iteration 4757 Loss: 0.07658437531264434\n",
      "Iteration 4758 Loss: 0.07658422299769521\n",
      "Iteration 4759 Loss: 0.07658407072153442\n",
      "Iteration 4760 Loss: 0.07658391848408684\n",
      "Iteration 4761 Loss: 0.07658376628527731\n",
      "Iteration 4762 Loss: 0.07658361412503094\n",
      "Iteration 4763 Loss: 0.07658346200327305\n",
      "Iteration 4764 Loss: 0.07658330991992908\n",
      "Iteration 4765 Loss: 0.07658315787492474\n",
      "Iteration 4766 Loss: 0.07658300586818573\n",
      "Iteration 4767 Loss: 0.07658285389963819\n",
      "Iteration 4768 Loss: 0.07658270196920827\n",
      "Iteration 4769 Loss: 0.07658255007682227\n",
      "Iteration 4770 Loss: 0.07658239822240688\n",
      "Iteration 4771 Loss: 0.07658224640588869\n",
      "Iteration 4772 Loss: 0.07658209462719463\n",
      "Iteration 4773 Loss: 0.07658194288625181\n",
      "Iteration 4774 Loss: 0.07658179118298751\n",
      "Iteration 4775 Loss: 0.07658163951732909\n",
      "Iteration 4776 Loss: 0.0765814878892042\n",
      "Iteration 4777 Loss: 0.0765813362985406\n",
      "Iteration 4778 Loss: 0.07658118474526622\n",
      "Iteration 4779 Loss: 0.07658103322930927\n",
      "Iteration 4780 Loss: 0.07658088175059799\n",
      "Iteration 4781 Loss: 0.07658073030906092\n",
      "Iteration 4782 Loss: 0.07658057890462666\n",
      "Iteration 4783 Loss: 0.076580427537224\n",
      "Iteration 4784 Loss: 0.07658027620678202\n",
      "Iteration 4785 Loss: 0.07658012491322987\n",
      "Iteration 4786 Loss: 0.07657997365649682\n",
      "Iteration 4787 Loss: 0.07657982243651247\n",
      "Iteration 4788 Loss: 0.07657967125320647\n",
      "Iteration 4789 Loss: 0.07657952010650869\n",
      "Iteration 4790 Loss: 0.07657936899634907\n",
      "Iteration 4791 Loss: 0.07657921792265791\n",
      "Iteration 4792 Loss: 0.0765790668853655\n",
      "Iteration 4793 Loss: 0.07657891588440237\n",
      "Iteration 4794 Loss: 0.07657876491969928\n",
      "Iteration 4795 Loss: 0.07657861399118704\n",
      "Iteration 4796 Loss: 0.0765784630987967\n",
      "Iteration 4797 Loss: 0.07657831224245942\n",
      "Iteration 4798 Loss: 0.07657816142210666\n",
      "Iteration 4799 Loss: 0.07657801063766988\n",
      "Iteration 4800 Loss: 0.07657785988908077\n",
      "Iteration 4801 Loss: 0.0765777091762712\n",
      "Iteration 4802 Loss: 0.07657755849917329\n",
      "Iteration 4803 Loss: 0.07657740785771909\n",
      "Iteration 4804 Loss: 0.07657725725184106\n",
      "Iteration 4805 Loss: 0.07657710668147164\n",
      "Iteration 4806 Loss: 0.07657695614654358\n",
      "Iteration 4807 Loss: 0.07657680564698972\n",
      "Iteration 4808 Loss: 0.07657665518274304\n",
      "Iteration 4809 Loss: 0.0765765047537367\n",
      "Iteration 4810 Loss: 0.0765763543599041\n",
      "Iteration 4811 Loss: 0.07657620400117866\n",
      "Iteration 4812 Loss: 0.07657605367749412\n",
      "Iteration 4813 Loss: 0.0765759033887842\n",
      "Iteration 4814 Loss: 0.07657575313498292\n",
      "Iteration 4815 Loss: 0.07657560291602444\n",
      "Iteration 4816 Loss: 0.076575452731843\n",
      "Iteration 4817 Loss: 0.07657530258237312\n",
      "Iteration 4818 Loss: 0.07657515246754935\n",
      "Iteration 4819 Loss: 0.07657500238730654\n",
      "Iteration 4820 Loss: 0.07657485234157954\n",
      "Iteration 4821 Loss: 0.07657470233030343\n",
      "Iteration 4822 Loss: 0.07657455235341352\n",
      "Iteration 4823 Loss: 0.07657440241084516\n",
      "Iteration 4824 Loss: 0.07657425250253397\n",
      "Iteration 4825 Loss: 0.07657410262841563\n",
      "Iteration 4826 Loss: 0.07657395278842598\n",
      "Iteration 4827 Loss: 0.07657380298250104\n",
      "Iteration 4828 Loss: 0.07657365321057705\n",
      "Iteration 4829 Loss: 0.07657350347259027\n",
      "Iteration 4830 Loss: 0.07657335376847722\n",
      "Iteration 4831 Loss: 0.07657320409817454\n",
      "Iteration 4832 Loss: 0.076573054461619\n",
      "Iteration 4833 Loss: 0.07657290485874758\n",
      "Iteration 4834 Loss: 0.07657275528949743\n",
      "Iteration 4835 Loss: 0.07657260575380569\n",
      "Iteration 4836 Loss: 0.07657245625160981\n",
      "Iteration 4837 Loss: 0.07657230678284736\n",
      "Iteration 4838 Loss: 0.07657215734745601\n",
      "Iteration 4839 Loss: 0.07657200794537367\n",
      "Iteration 4840 Loss: 0.07657185857653827\n",
      "Iteration 4841 Loss: 0.07657170924088808\n",
      "Iteration 4842 Loss: 0.07657155993836126\n",
      "Iteration 4843 Loss: 0.0765714106688964\n",
      "Iteration 4844 Loss: 0.07657126143243201\n",
      "Iteration 4845 Loss: 0.07657111222890693\n",
      "Iteration 4846 Loss: 0.07657096305825999\n",
      "Iteration 4847 Loss: 0.07657081392043025\n",
      "Iteration 4848 Loss: 0.07657066481535688\n",
      "Iteration 4849 Loss: 0.07657051574297934\n",
      "Iteration 4850 Loss: 0.07657036670323697\n",
      "Iteration 4851 Loss: 0.0765702176960695\n",
      "Iteration 4852 Loss: 0.07657006872141672\n",
      "Iteration 4853 Loss: 0.07656991977921848\n",
      "Iteration 4854 Loss: 0.07656977086941491\n",
      "Iteration 4855 Loss: 0.0765696219919462\n",
      "Iteration 4856 Loss: 0.07656947314675279\n",
      "Iteration 4857 Loss: 0.07656932433377504\n",
      "Iteration 4858 Loss: 0.07656917555295373\n",
      "Iteration 4859 Loss: 0.07656902680422963\n",
      "Iteration 4860 Loss: 0.07656887808754365\n",
      "Iteration 4861 Loss: 0.07656872940283682\n",
      "Iteration 4862 Loss: 0.07656858075005048\n",
      "Iteration 4863 Loss: 0.07656843212912591\n",
      "Iteration 4864 Loss: 0.07656828354000467\n",
      "Iteration 4865 Loss: 0.07656813498262834\n",
      "Iteration 4866 Loss: 0.07656798645693878\n",
      "Iteration 4867 Loss: 0.0765678379628779\n",
      "Iteration 4868 Loss: 0.0765676895003877\n",
      "Iteration 4869 Loss: 0.07656754106941051\n",
      "Iteration 4870 Loss: 0.07656739266988863\n",
      "Iteration 4871 Loss: 0.07656724430176451\n",
      "Iteration 4872 Loss: 0.07656709596498082\n",
      "Iteration 4873 Loss: 0.07656694765948027\n",
      "Iteration 4874 Loss: 0.07656679938520586\n",
      "Iteration 4875 Loss: 0.0765666511421006\n",
      "Iteration 4876 Loss: 0.07656650293010761\n",
      "Iteration 4877 Loss: 0.07656635474917022\n",
      "Iteration 4878 Loss: 0.076566206599232\n",
      "Iteration 4879 Loss: 0.07656605848023643\n",
      "Iteration 4880 Loss: 0.07656591039212728\n",
      "Iteration 4881 Loss: 0.07656576233484841\n",
      "Iteration 4882 Loss: 0.07656561430834373\n",
      "Iteration 4883 Loss: 0.0765654663125576\n",
      "Iteration 4884 Loss: 0.0765653183474341\n",
      "Iteration 4885 Loss: 0.07656517041291769\n",
      "Iteration 4886 Loss: 0.07656502250895286\n",
      "Iteration 4887 Loss: 0.07656487463548438\n",
      "Iteration 4888 Loss: 0.07656472679245699\n",
      "Iteration 4889 Loss: 0.07656457897981568\n",
      "Iteration 4890 Loss: 0.07656443119750546\n",
      "Iteration 4891 Loss: 0.07656428344547155\n",
      "Iteration 4892 Loss: 0.07656413572365939\n",
      "Iteration 4893 Loss: 0.07656398803201432\n",
      "Iteration 4894 Loss: 0.07656384037048199\n",
      "Iteration 4895 Loss: 0.07656369273900815\n",
      "Iteration 4896 Loss: 0.07656354513753863\n",
      "Iteration 4897 Loss: 0.07656339756601953\n",
      "Iteration 4898 Loss: 0.07656325002439685\n",
      "Iteration 4899 Loss: 0.07656310251261689\n",
      "Iteration 4900 Loss: 0.07656295503062602\n",
      "Iteration 4901 Loss: 0.0765628075783708\n",
      "Iteration 4902 Loss: 0.0765626601557979\n",
      "Iteration 4903 Loss: 0.07656251276285399\n",
      "Iteration 4904 Loss: 0.07656236539948608\n",
      "Iteration 4905 Loss: 0.0765622180656411\n",
      "Iteration 4906 Loss: 0.07656207076126631\n",
      "Iteration 4907 Loss: 0.07656192348630897\n",
      "Iteration 4908 Loss: 0.07656177624071643\n",
      "Iteration 4909 Loss: 0.07656162902443629\n",
      "Iteration 4910 Loss: 0.07656148183741625\n",
      "Iteration 4911 Loss: 0.07656133467960402\n",
      "Iteration 4912 Loss: 0.07656118755094758\n",
      "Iteration 4913 Loss: 0.07656104045139493\n",
      "Iteration 4914 Loss: 0.07656089338089431\n",
      "Iteration 4915 Loss: 0.07656074633939394\n",
      "Iteration 4916 Loss: 0.07656059932684231\n",
      "Iteration 4917 Loss: 0.07656045234318791\n",
      "Iteration 4918 Loss: 0.07656030538837948\n",
      "Iteration 4919 Loss: 0.07656015846236576\n",
      "Iteration 4920 Loss: 0.07656001156509566\n",
      "Iteration 4921 Loss: 0.07655986469651828\n",
      "Iteration 4922 Loss: 0.07655971785658272\n",
      "Iteration 4923 Loss: 0.0765595710452383\n",
      "Iteration 4924 Loss: 0.0765594242624345\n",
      "Iteration 4925 Loss: 0.07655927750812072\n",
      "Iteration 4926 Loss: 0.07655913078224669\n",
      "Iteration 4927 Loss: 0.07655898408476217\n",
      "Iteration 4928 Loss: 0.0765588374156171\n",
      "Iteration 4929 Loss: 0.07655869077476139\n",
      "Iteration 4930 Loss: 0.07655854416214526\n",
      "Iteration 4931 Loss: 0.07655839757771907\n",
      "Iteration 4932 Loss: 0.07655825102143296\n",
      "Iteration 4933 Loss: 0.07655810449323759\n",
      "Iteration 4934 Loss: 0.07655795799308356\n",
      "Iteration 4935 Loss: 0.0765578115209216\n",
      "Iteration 4936 Loss: 0.07655766507670256\n",
      "Iteration 4937 Loss: 0.07655751866037738\n",
      "Iteration 4938 Loss: 0.07655737227189727\n",
      "Iteration 4939 Loss: 0.07655722591121329\n",
      "Iteration 4940 Loss: 0.07655707957827688\n",
      "Iteration 4941 Loss: 0.07655693327303942\n",
      "Iteration 4942 Loss: 0.07655678699545257\n",
      "Iteration 4943 Loss: 0.0765566407454679\n",
      "Iteration 4944 Loss: 0.0765564945230373\n",
      "Iteration 4945 Loss: 0.0765563483281126\n",
      "Iteration 4946 Loss: 0.0765562021606459\n",
      "Iteration 4947 Loss: 0.07655605602058929\n",
      "Iteration 4948 Loss: 0.07655590990789507\n",
      "Iteration 4949 Loss: 0.07655576382251561\n",
      "Iteration 4950 Loss: 0.07655561776440344\n",
      "Iteration 4951 Loss: 0.07655547173351109\n",
      "Iteration 4952 Loss: 0.0765553257297913\n",
      "Iteration 4953 Loss: 0.07655517975319694\n",
      "Iteration 4954 Loss: 0.07655503380368096\n",
      "Iteration 4955 Loss: 0.07655488788119641\n",
      "Iteration 4956 Loss: 0.07655474198569646\n",
      "Iteration 4957 Loss: 0.07655459611713443\n",
      "Iteration 4958 Loss: 0.07655445027546365\n",
      "Iteration 4959 Loss: 0.0765543044606377\n",
      "Iteration 4960 Loss: 0.07655415867261017\n",
      "Iteration 4961 Loss: 0.07655401291133486\n",
      "Iteration 4962 Loss: 0.07655386717676552\n",
      "Iteration 4963 Loss: 0.07655372146885613\n",
      "Iteration 4964 Loss: 0.07655357578756092\n",
      "Iteration 4965 Loss: 0.07655343013283385\n",
      "Iteration 4966 Loss: 0.07655328450462934\n",
      "Iteration 4967 Loss: 0.07655313890290177\n",
      "Iteration 4968 Loss: 0.07655299332760566\n",
      "Iteration 4969 Loss: 0.0765528477786956\n",
      "Iteration 4970 Loss: 0.07655270225612643\n",
      "Iteration 4971 Loss: 0.07655255675985284\n",
      "Iteration 4972 Loss: 0.0765524112898299\n",
      "Iteration 4973 Loss: 0.07655226584601257\n",
      "Iteration 4974 Loss: 0.07655212042835614\n",
      "Iteration 4975 Loss: 0.0765519750368158\n",
      "Iteration 4976 Loss: 0.0765518296713469\n",
      "Iteration 4977 Loss: 0.07655168433190501\n",
      "Iteration 4978 Loss: 0.07655153901844575\n",
      "Iteration 4979 Loss: 0.07655139373092476\n",
      "Iteration 4980 Loss: 0.07655124846929787\n",
      "Iteration 4981 Loss: 0.07655110323352102\n",
      "Iteration 4982 Loss: 0.07655095802355025\n",
      "Iteration 4983 Loss: 0.07655081283934162\n",
      "Iteration 4984 Loss: 0.07655066768085145\n",
      "Iteration 4985 Loss: 0.07655052254803604\n",
      "Iteration 4986 Loss: 0.0765503774408518\n",
      "Iteration 4987 Loss: 0.0765502323592554\n",
      "Iteration 4988 Loss: 0.0765500873032034\n",
      "Iteration 4989 Loss: 0.07654994227265259\n",
      "Iteration 4990 Loss: 0.07654979726755987\n",
      "Iteration 4991 Loss: 0.07654965228788214\n",
      "Iteration 4992 Loss: 0.07654950733357657\n",
      "Iteration 4993 Loss: 0.07654936240460022\n",
      "Iteration 4994 Loss: 0.07654921750091051\n",
      "Iteration 4995 Loss: 0.07654907262246471\n",
      "Iteration 4996 Loss: 0.07654892776922038\n",
      "Iteration 4997 Loss: 0.07654878294113508\n",
      "Iteration 4998 Loss: 0.07654863813816655\n",
      "Iteration 4999 Loss: 0.0765484933602725\n",
      "Iteration 5000 Loss: 0.07654834860741086\n",
      "Iteration 5001 Loss: 0.07654820387953964\n",
      "Iteration 5002 Loss: 0.07654805917661697\n",
      "Iteration 5003 Loss: 0.076547914498601\n",
      "Iteration 5004 Loss: 0.07654776984545002\n",
      "Iteration 5005 Loss: 0.07654762521712245\n",
      "Iteration 5006 Loss: 0.07654748061357684\n",
      "Iteration 5007 Loss: 0.07654733603477176\n",
      "Iteration 5008 Loss: 0.07654719148066588\n",
      "Iteration 5009 Loss: 0.07654704695121806\n",
      "Iteration 5010 Loss: 0.07654690244638712\n",
      "Iteration 5011 Loss: 0.07654675796613214\n",
      "Iteration 5012 Loss: 0.07654661351041216\n",
      "Iteration 5013 Loss: 0.07654646907918641\n",
      "Iteration 5014 Loss: 0.07654632467241419\n",
      "Iteration 5015 Loss: 0.07654618029005483\n",
      "Iteration 5016 Loss: 0.07654603593206795\n",
      "Iteration 5017 Loss: 0.07654589159841305\n",
      "Iteration 5018 Loss: 0.07654574728904978\n",
      "Iteration 5019 Loss: 0.076545603003938\n",
      "Iteration 5020 Loss: 0.07654545874303752\n",
      "Iteration 5021 Loss: 0.07654531450630839\n",
      "Iteration 5022 Loss: 0.07654517029371066\n",
      "Iteration 5023 Loss: 0.07654502610520446\n",
      "Iteration 5024 Loss: 0.07654488194075011\n",
      "Iteration 5025 Loss: 0.07654473780030795\n",
      "Iteration 5026 Loss: 0.07654459368383842\n",
      "Iteration 5027 Loss: 0.07654444959130209\n",
      "Iteration 5028 Loss: 0.0765443055226596\n",
      "Iteration 5029 Loss: 0.07654416147787169\n",
      "Iteration 5030 Loss: 0.07654401745689922\n",
      "Iteration 5031 Loss: 0.07654387345970307\n",
      "Iteration 5032 Loss: 0.07654372948624433\n",
      "Iteration 5033 Loss: 0.07654358553648404\n",
      "Iteration 5034 Loss: 0.07654344161038347\n",
      "Iteration 5035 Loss: 0.07654329770790395\n",
      "Iteration 5036 Loss: 0.07654315382900682\n",
      "Iteration 5037 Loss: 0.0765430099736536\n",
      "Iteration 5038 Loss: 0.07654286614180587\n",
      "Iteration 5039 Loss: 0.07654272233342527\n",
      "Iteration 5040 Loss: 0.07654257854847372\n",
      "Iteration 5041 Loss: 0.07654243478691285\n",
      "Iteration 5042 Loss: 0.07654229104870479\n",
      "Iteration 5043 Loss: 0.07654214733381147\n",
      "Iteration 5044 Loss: 0.07654200364219513\n",
      "Iteration 5045 Loss: 0.07654185997381793\n",
      "Iteration 5046 Loss: 0.07654171632864223\n",
      "Iteration 5047 Loss: 0.07654157270663042\n",
      "Iteration 5048 Loss: 0.076541429107745\n",
      "Iteration 5049 Loss: 0.07654128553194854\n",
      "Iteration 5050 Loss: 0.07654114197920377\n",
      "Iteration 5051 Loss: 0.0765409984494734\n",
      "Iteration 5052 Loss: 0.07654085494272032\n",
      "Iteration 5053 Loss: 0.07654071145890745\n",
      "Iteration 5054 Loss: 0.07654056799799791\n",
      "Iteration 5055 Loss: 0.07654042455995479\n",
      "Iteration 5056 Loss: 0.07654028114474126\n",
      "Iteration 5057 Loss: 0.0765401377523206\n",
      "Iteration 5058 Loss: 0.07653999438265632\n",
      "Iteration 5059 Loss: 0.07653985103571183\n",
      "Iteration 5060 Loss: 0.07653970771145069\n",
      "Iteration 5061 Loss: 0.0765395644098366\n",
      "Iteration 5062 Loss: 0.07653942113083327\n",
      "Iteration 5063 Loss: 0.07653927787440452\n",
      "Iteration 5064 Loss: 0.0765391346405143\n",
      "Iteration 5065 Loss: 0.07653899142912662\n",
      "Iteration 5066 Loss: 0.07653884824020551\n",
      "Iteration 5067 Loss: 0.0765387050737152\n",
      "Iteration 5068 Loss: 0.07653856192961997\n",
      "Iteration 5069 Loss: 0.07653841880788416\n",
      "Iteration 5070 Loss: 0.0765382757084722\n",
      "Iteration 5071 Loss: 0.07653813263134855\n",
      "Iteration 5072 Loss: 0.07653798957647794\n",
      "Iteration 5073 Loss: 0.07653784654382495\n",
      "Iteration 5074 Loss: 0.07653770353335441\n",
      "Iteration 5075 Loss: 0.07653756054503116\n",
      "Iteration 5076 Loss: 0.07653741757882022\n",
      "Iteration 5077 Loss: 0.07653727463468649\n",
      "Iteration 5078 Loss: 0.07653713171259519\n",
      "Iteration 5079 Loss: 0.07653698881251149\n",
      "Iteration 5080 Loss: 0.07653684593440063\n",
      "Iteration 5081 Loss: 0.07653670307822807\n",
      "Iteration 5082 Loss: 0.07653656024395922\n",
      "Iteration 5083 Loss: 0.0765364174315595\n",
      "Iteration 5084 Loss: 0.07653627464099469\n",
      "Iteration 5085 Loss: 0.07653613187223038\n",
      "Iteration 5086 Loss: 0.07653598912523242\n",
      "Iteration 5087 Loss: 0.07653584639996663\n",
      "Iteration 5088 Loss: 0.07653570369639899\n",
      "Iteration 5089 Loss: 0.0765355610144955\n",
      "Iteration 5090 Loss: 0.07653541835422226\n",
      "Iteration 5091 Loss: 0.07653527571554547\n",
      "Iteration 5092 Loss: 0.07653513309843142\n",
      "Iteration 5093 Loss: 0.0765349905028464\n",
      "Iteration 5094 Loss: 0.07653484792875698\n",
      "Iteration 5095 Loss: 0.07653470537612955\n",
      "Iteration 5096 Loss: 0.07653456284493071\n",
      "Iteration 5097 Loss: 0.0765344203351272\n",
      "Iteration 5098 Loss: 0.07653427784668565\n",
      "Iteration 5099 Loss: 0.07653413537957304\n",
      "Iteration 5100 Loss: 0.07653399293375625\n",
      "Iteration 5101 Loss: 0.07653385050920226\n",
      "Iteration 5102 Loss: 0.07653370810587815\n",
      "Iteration 5103 Loss: 0.076533565723751\n",
      "Iteration 5104 Loss: 0.07653342336278814\n",
      "Iteration 5105 Loss: 0.07653328102295684\n",
      "Iteration 5106 Loss: 0.07653313870422447\n",
      "Iteration 5107 Loss: 0.07653299640655853\n",
      "Iteration 5108 Loss: 0.07653285412992655\n",
      "Iteration 5109 Loss: 0.07653271187429618\n",
      "Iteration 5110 Loss: 0.07653256963963512\n",
      "Iteration 5111 Loss: 0.07653242742591104\n",
      "Iteration 5112 Loss: 0.07653228523309194\n",
      "Iteration 5113 Loss: 0.07653214306114568\n",
      "Iteration 5114 Loss: 0.07653200091004031\n",
      "Iteration 5115 Loss: 0.07653185877974386\n",
      "Iteration 5116 Loss: 0.07653171667022457\n",
      "Iteration 5117 Loss: 0.07653157458145061\n",
      "Iteration 5118 Loss: 0.07653143251339031\n",
      "Iteration 5119 Loss: 0.07653129046601213\n",
      "Iteration 5120 Loss: 0.07653114843928446\n",
      "Iteration 5121 Loss: 0.07653100643317588\n",
      "Iteration 5122 Loss: 0.07653086444765504\n",
      "Iteration 5123 Loss: 0.07653072248269052\n",
      "Iteration 5124 Loss: 0.07653058053825125\n",
      "Iteration 5125 Loss: 0.07653043861430593\n",
      "Iteration 5126 Loss: 0.07653029671082362\n",
      "Iteration 5127 Loss: 0.07653015482777319\n",
      "Iteration 5128 Loss: 0.07653001296512377\n",
      "Iteration 5129 Loss: 0.07652987112284448\n",
      "Iteration 5130 Loss: 0.07652972930090461\n",
      "Iteration 5131 Loss: 0.07652958749927338\n",
      "Iteration 5132 Loss: 0.07652944571792014\n",
      "Iteration 5133 Loss: 0.07652930395681437\n",
      "Iteration 5134 Loss: 0.0765291622159256\n",
      "Iteration 5135 Loss: 0.07652902049522337\n",
      "Iteration 5136 Loss: 0.07652887879467742\n",
      "Iteration 5137 Loss: 0.07652873711425733\n",
      "Iteration 5138 Loss: 0.07652859545393302\n",
      "Iteration 5139 Loss: 0.07652845381367439\n",
      "Iteration 5140 Loss: 0.07652831219345134\n",
      "Iteration 5141 Loss: 0.07652817059323386\n",
      "Iteration 5142 Loss: 0.0765280290129921\n",
      "Iteration 5143 Loss: 0.07652788745269616\n",
      "Iteration 5144 Loss: 0.07652774591231636\n",
      "Iteration 5145 Loss: 0.076527604391823\n",
      "Iteration 5146 Loss: 0.07652746289118638\n",
      "Iteration 5147 Loss: 0.07652732141037705\n",
      "Iteration 5148 Loss: 0.07652717994936542\n",
      "Iteration 5149 Loss: 0.07652703850812215\n",
      "Iteration 5150 Loss: 0.07652689708661796\n",
      "Iteration 5151 Loss: 0.07652675568482346\n",
      "Iteration 5152 Loss: 0.07652661430270954\n",
      "Iteration 5153 Loss: 0.07652647294024702\n",
      "Iteration 5154 Loss: 0.07652633159740688\n",
      "Iteration 5155 Loss: 0.07652619027416013\n",
      "Iteration 5156 Loss: 0.07652604897047789\n",
      "Iteration 5157 Loss: 0.07652590768633123\n",
      "Iteration 5158 Loss: 0.07652576642169143\n",
      "Iteration 5159 Loss: 0.0765256251765297\n",
      "Iteration 5160 Loss: 0.07652548395081754\n",
      "Iteration 5161 Loss: 0.07652534274452626\n",
      "Iteration 5162 Loss: 0.07652520155762735\n",
      "Iteration 5163 Loss: 0.07652506039009249\n",
      "Iteration 5164 Loss: 0.07652491924189318\n",
      "Iteration 5165 Loss: 0.07652477811300124\n",
      "Iteration 5166 Loss: 0.07652463700338838\n",
      "Iteration 5167 Loss: 0.0765244959130264\n",
      "Iteration 5168 Loss: 0.07652435484188726\n",
      "Iteration 5169 Loss: 0.07652421378994291\n",
      "Iteration 5170 Loss: 0.07652407275716543\n",
      "Iteration 5171 Loss: 0.07652393174352685\n",
      "Iteration 5172 Loss: 0.0765237907489994\n",
      "Iteration 5173 Loss: 0.0765236497735553\n",
      "Iteration 5174 Loss: 0.07652350881716685\n",
      "Iteration 5175 Loss: 0.07652336787980643\n",
      "Iteration 5176 Loss: 0.07652322696144646\n",
      "Iteration 5177 Loss: 0.07652308606205949\n",
      "Iteration 5178 Loss: 0.07652294518161813\n",
      "Iteration 5179 Loss: 0.07652280432009483\n",
      "Iteration 5180 Loss: 0.07652266347746249\n",
      "Iteration 5181 Loss: 0.0765225226536938\n",
      "Iteration 5182 Loss: 0.07652238184876155\n",
      "Iteration 5183 Loss: 0.07652224106263872\n",
      "Iteration 5184 Loss: 0.07652210029529823\n",
      "Iteration 5185 Loss: 0.07652195954671309\n",
      "Iteration 5186 Loss: 0.07652181881685644\n",
      "Iteration 5187 Loss: 0.07652167810570139\n",
      "Iteration 5188 Loss: 0.07652153741322124\n",
      "Iteration 5189 Loss: 0.07652139673938914\n",
      "Iteration 5190 Loss: 0.07652125608417851\n",
      "Iteration 5191 Loss: 0.0765211154475628\n",
      "Iteration 5192 Loss: 0.07652097482951552\n",
      "Iteration 5193 Loss: 0.07652083423001009\n",
      "Iteration 5194 Loss: 0.07652069364902017\n",
      "Iteration 5195 Loss: 0.0765205530865194\n",
      "Iteration 5196 Loss: 0.07652041254248156\n",
      "Iteration 5197 Loss: 0.07652027201688044\n",
      "Iteration 5198 Loss: 0.07652013150968986\n",
      "Iteration 5199 Loss: 0.07651999102088379\n",
      "Iteration 5200 Loss: 0.07651985055043611\n",
      "Iteration 5201 Loss: 0.07651971009832097\n",
      "Iteration 5202 Loss: 0.07651956966451243\n",
      "Iteration 5203 Loss: 0.07651942924898465\n",
      "Iteration 5204 Loss: 0.0765192888517119\n",
      "Iteration 5205 Loss: 0.0765191484726684\n",
      "Iteration 5206 Loss: 0.07651900811182859\n",
      "Iteration 5207 Loss: 0.07651886776916678\n",
      "Iteration 5208 Loss: 0.07651872744465754\n",
      "Iteration 5209 Loss: 0.07651858713827536\n",
      "Iteration 5210 Loss: 0.07651844684999483\n",
      "Iteration 5211 Loss: 0.0765183065797907\n",
      "Iteration 5212 Loss: 0.07651816632763751\n",
      "Iteration 5213 Loss: 0.07651802609351024\n",
      "Iteration 5214 Loss: 0.07651788587738358\n",
      "Iteration 5215 Loss: 0.07651774567923249\n",
      "Iteration 5216 Loss: 0.07651760549903196\n",
      "Iteration 5217 Loss: 0.07651746533675692\n",
      "Iteration 5218 Loss: 0.07651732519238251\n",
      "Iteration 5219 Loss: 0.07651718506588388\n",
      "Iteration 5220 Loss: 0.07651704495723621\n",
      "Iteration 5221 Loss: 0.07651690486641477\n",
      "Iteration 5222 Loss: 0.07651676479339481\n",
      "Iteration 5223 Loss: 0.07651662473815178\n",
      "Iteration 5224 Loss: 0.07651648470066112\n",
      "Iteration 5225 Loss: 0.07651634468089835\n",
      "Iteration 5226 Loss: 0.07651620467883893\n",
      "Iteration 5227 Loss: 0.07651606469445851\n",
      "Iteration 5228 Loss: 0.07651592472773273\n",
      "Iteration 5229 Loss: 0.0765157847786374\n",
      "Iteration 5230 Loss: 0.07651564484714825\n",
      "Iteration 5231 Loss: 0.07651550493324111\n",
      "Iteration 5232 Loss: 0.07651536503689195\n",
      "Iteration 5233 Loss: 0.07651522515807667\n",
      "Iteration 5234 Loss: 0.07651508529677128\n",
      "Iteration 5235 Loss: 0.07651494545295193\n",
      "Iteration 5236 Loss: 0.07651480562659463\n",
      "Iteration 5237 Loss: 0.07651466581767567\n",
      "Iteration 5238 Loss: 0.07651452602617125\n",
      "Iteration 5239 Loss: 0.07651438625205766\n",
      "Iteration 5240 Loss: 0.07651424649531131\n",
      "Iteration 5241 Loss: 0.07651410675590861\n",
      "Iteration 5242 Loss: 0.07651396703382594\n",
      "Iteration 5243 Loss: 0.07651382732903994\n",
      "Iteration 5244 Loss: 0.07651368764152713\n",
      "Iteration 5245 Loss: 0.07651354797126421\n",
      "Iteration 5246 Loss: 0.0765134083182278\n",
      "Iteration 5247 Loss: 0.0765132686823947\n",
      "Iteration 5248 Loss: 0.07651312906374173\n",
      "Iteration 5249 Loss: 0.0765129894622457\n",
      "Iteration 5250 Loss: 0.07651284987788351\n",
      "Iteration 5251 Loss: 0.07651271031063225\n",
      "Iteration 5252 Loss: 0.07651257076046886\n",
      "Iteration 5253 Loss: 0.07651243122737043\n",
      "Iteration 5254 Loss: 0.0765122917113141\n",
      "Iteration 5255 Loss: 0.07651215221227707\n",
      "Iteration 5256 Loss: 0.07651201273023656\n",
      "Iteration 5257 Loss: 0.07651187326516992\n",
      "Iteration 5258 Loss: 0.07651173381705449\n",
      "Iteration 5259 Loss: 0.07651159438586771\n",
      "Iteration 5260 Loss: 0.07651145497158693\n",
      "Iteration 5261 Loss: 0.07651131557418979\n",
      "Iteration 5262 Loss: 0.07651117619365379\n",
      "Iteration 5263 Loss: 0.07651103682995662\n",
      "Iteration 5264 Loss: 0.07651089748307591\n",
      "Iteration 5265 Loss: 0.0765107581529894\n",
      "Iteration 5266 Loss: 0.0765106188396748\n",
      "Iteration 5267 Loss: 0.07651047954311016\n",
      "Iteration 5268 Loss: 0.07651034026327311\n",
      "Iteration 5269 Loss: 0.07651020100014169\n",
      "Iteration 5270 Loss: 0.07651006175369406\n",
      "Iteration 5271 Loss: 0.07650992252390801\n",
      "Iteration 5272 Loss: 0.07650978331076179\n",
      "Iteration 5273 Loss: 0.07650964411423357\n",
      "Iteration 5274 Loss: 0.07650950493430148\n",
      "Iteration 5275 Loss: 0.07650936577094375\n",
      "Iteration 5276 Loss: 0.07650922662413882\n",
      "Iteration 5277 Loss: 0.07650908749386494\n",
      "Iteration 5278 Loss: 0.07650894838010062\n",
      "Iteration 5279 Loss: 0.0765088092828242\n",
      "Iteration 5280 Loss: 0.07650867020201432\n",
      "Iteration 5281 Loss: 0.07650853113764941\n",
      "Iteration 5282 Loss: 0.07650839208970821\n",
      "Iteration 5283 Loss: 0.07650825305816931\n",
      "Iteration 5284 Loss: 0.0765081140430115\n",
      "Iteration 5285 Loss: 0.07650797504421353\n",
      "Iteration 5286 Loss: 0.07650783606175414\n",
      "Iteration 5287 Loss: 0.07650769709561234\n",
      "Iteration 5288 Loss: 0.07650755814576694\n",
      "Iteration 5289 Loss: 0.07650741921219695\n",
      "Iteration 5290 Loss: 0.07650728029488144\n",
      "Iteration 5291 Loss: 0.07650714139379938\n",
      "Iteration 5292 Loss: 0.07650700250892996\n",
      "Iteration 5293 Loss: 0.07650686364025241\n",
      "Iteration 5294 Loss: 0.07650672478774581\n",
      "Iteration 5295 Loss: 0.07650658595138958\n",
      "Iteration 5296 Loss: 0.07650644713116293\n",
      "Iteration 5297 Loss: 0.07650630832704523\n",
      "Iteration 5298 Loss: 0.07650616953901601\n",
      "Iteration 5299 Loss: 0.07650603076705464\n",
      "Iteration 5300 Loss: 0.07650589201114069\n",
      "Iteration 5301 Loss: 0.07650575327125364\n",
      "Iteration 5302 Loss: 0.07650561454737322\n",
      "Iteration 5303 Loss: 0.07650547583947902\n",
      "Iteration 5304 Loss: 0.07650533714755083\n",
      "Iteration 5305 Loss: 0.07650519847156827\n",
      "Iteration 5306 Loss: 0.07650505981151128\n",
      "Iteration 5307 Loss: 0.07650492116735966\n",
      "Iteration 5308 Loss: 0.07650478253909332\n",
      "Iteration 5309 Loss: 0.0765046439266922\n",
      "Iteration 5310 Loss: 0.07650450533013639\n",
      "Iteration 5311 Loss: 0.07650436674940582\n",
      "Iteration 5312 Loss: 0.07650422818448066\n",
      "Iteration 5313 Loss: 0.07650408963534104\n",
      "Iteration 5314 Loss: 0.07650395110196709\n",
      "Iteration 5315 Loss: 0.07650381258433911\n",
      "Iteration 5316 Loss: 0.07650367408243741\n",
      "Iteration 5317 Loss: 0.07650353559624225\n",
      "Iteration 5318 Loss: 0.07650339712573405\n",
      "Iteration 5319 Loss: 0.07650325867089325\n",
      "Iteration 5320 Loss: 0.0765031202317003\n",
      "Iteration 5321 Loss: 0.07650298180813567\n",
      "Iteration 5322 Loss: 0.07650284340017999\n",
      "Iteration 5323 Loss: 0.07650270500781385\n",
      "Iteration 5324 Loss: 0.07650256663101797\n",
      "Iteration 5325 Loss: 0.07650242826977292\n",
      "Iteration 5326 Loss: 0.07650228992405954\n",
      "Iteration 5327 Loss: 0.07650215159385865\n",
      "Iteration 5328 Loss: 0.07650201327915104\n",
      "Iteration 5329 Loss: 0.07650187497991762\n",
      "Iteration 5330 Loss: 0.07650173669613924\n",
      "Iteration 5331 Loss: 0.07650159842779704\n",
      "Iteration 5332 Loss: 0.07650146017487187\n",
      "Iteration 5333 Loss: 0.07650132193734495\n",
      "Iteration 5334 Loss: 0.07650118371519726\n",
      "Iteration 5335 Loss: 0.07650104550841007\n",
      "Iteration 5336 Loss: 0.07650090731696452\n",
      "Iteration 5337 Loss: 0.07650076914084188\n",
      "Iteration 5338 Loss: 0.07650063098002341\n",
      "Iteration 5339 Loss: 0.07650049283449051\n",
      "Iteration 5340 Loss: 0.07650035470422445\n",
      "Iteration 5341 Loss: 0.07650021658920676\n",
      "Iteration 5342 Loss: 0.0765000784894189\n",
      "Iteration 5343 Loss: 0.07649994040484231\n",
      "Iteration 5344 Loss: 0.07649980233545864\n",
      "Iteration 5345 Loss: 0.07649966428124942\n",
      "Iteration 5346 Loss: 0.07649952624219634\n",
      "Iteration 5347 Loss: 0.07649938821828106\n",
      "Iteration 5348 Loss: 0.07649925020948534\n",
      "Iteration 5349 Loss: 0.07649911221579095\n",
      "Iteration 5350 Loss: 0.07649897423717963\n",
      "Iteration 5351 Loss: 0.0764988362736334\n",
      "Iteration 5352 Loss: 0.076498698325134\n",
      "Iteration 5353 Loss: 0.07649856039166349\n",
      "Iteration 5354 Loss: 0.07649842247320388\n",
      "Iteration 5355 Loss: 0.07649828456973706\n",
      "Iteration 5356 Loss: 0.07649814668124524\n",
      "Iteration 5357 Loss: 0.07649800880771053\n",
      "Iteration 5358 Loss: 0.07649787094911498\n",
      "Iteration 5359 Loss: 0.076497733105441\n",
      "Iteration 5360 Loss: 0.07649759527667058\n",
      "Iteration 5361 Loss: 0.0764974574627862\n",
      "Iteration 5362 Loss: 0.07649731966377017\n",
      "Iteration 5363 Loss: 0.07649718187960476\n",
      "Iteration 5364 Loss: 0.07649704411027244\n",
      "Iteration 5365 Loss: 0.07649690635575572\n",
      "Iteration 5366 Loss: 0.07649676861603705\n",
      "Iteration 5367 Loss: 0.076496630891099\n",
      "Iteration 5368 Loss: 0.07649649318092404\n",
      "Iteration 5369 Loss: 0.07649635548549491\n",
      "Iteration 5370 Loss: 0.07649621780479424\n",
      "Iteration 5371 Loss: 0.0764960801388048\n",
      "Iteration 5372 Loss: 0.07649594248750921\n",
      "Iteration 5373 Loss: 0.07649580485089034\n",
      "Iteration 5374 Loss: 0.07649566722893102\n",
      "Iteration 5375 Loss: 0.07649552962161406\n",
      "Iteration 5376 Loss: 0.07649539202892244\n",
      "Iteration 5377 Loss: 0.07649525445083905\n",
      "Iteration 5378 Loss: 0.07649511688734693\n",
      "Iteration 5379 Loss: 0.07649497933842905\n",
      "Iteration 5380 Loss: 0.07649484180406854\n",
      "Iteration 5381 Loss: 0.0764947042842485\n",
      "Iteration 5382 Loss: 0.07649456677895204\n",
      "Iteration 5383 Loss: 0.07649442928816241\n",
      "Iteration 5384 Loss: 0.07649429181186285\n",
      "Iteration 5385 Loss: 0.07649415435003655\n",
      "Iteration 5386 Loss: 0.07649401690266691\n",
      "Iteration 5387 Loss: 0.0764938794697372\n",
      "Iteration 5388 Loss: 0.07649374205123083\n",
      "Iteration 5389 Loss: 0.07649360464713129\n",
      "Iteration 5390 Loss: 0.076493467257422\n",
      "Iteration 5391 Loss: 0.07649332988208644\n",
      "Iteration 5392 Loss: 0.07649319252110821\n",
      "Iteration 5393 Loss: 0.07649305517447089\n",
      "Iteration 5394 Loss: 0.0764929178421581\n",
      "Iteration 5395 Loss: 0.0764927805241534\n",
      "Iteration 5396 Loss: 0.07649264322044073\n",
      "Iteration 5397 Loss: 0.07649250593100362\n",
      "Iteration 5398 Loss: 0.07649236865582591\n",
      "Iteration 5399 Loss: 0.07649223139489143\n",
      "Iteration 5400 Loss: 0.07649209414818406\n",
      "Iteration 5401 Loss: 0.07649195691568764\n",
      "Iteration 5402 Loss: 0.07649181969738611\n",
      "Iteration 5403 Loss: 0.07649168249326346\n",
      "Iteration 5404 Loss: 0.07649154530330372\n",
      "Iteration 5405 Loss: 0.07649140812749096\n",
      "Iteration 5406 Loss: 0.07649127096580918\n",
      "Iteration 5407 Loss: 0.07649113381824253\n",
      "Iteration 5408 Loss: 0.07649099668477516\n",
      "Iteration 5409 Loss: 0.07649085956539137\n",
      "Iteration 5410 Loss: 0.07649072246007527\n",
      "Iteration 5411 Loss: 0.0764905853688112\n",
      "Iteration 5412 Loss: 0.07649044829158344\n",
      "Iteration 5413 Loss: 0.07649031122837634\n",
      "Iteration 5414 Loss: 0.07649017417917428\n",
      "Iteration 5415 Loss: 0.07649003714396167\n",
      "Iteration 5416 Loss: 0.07648990012272304\n",
      "Iteration 5417 Loss: 0.07648976311544278\n",
      "Iteration 5418 Loss: 0.07648962612210548\n",
      "Iteration 5419 Loss: 0.07648948914269571\n",
      "Iteration 5420 Loss: 0.07648935217719807\n",
      "Iteration 5421 Loss: 0.0764892152255972\n",
      "Iteration 5422 Loss: 0.07648907828787775\n",
      "Iteration 5423 Loss: 0.07648894136402445\n",
      "Iteration 5424 Loss: 0.07648880445402209\n",
      "Iteration 5425 Loss: 0.07648866755785541\n",
      "Iteration 5426 Loss: 0.07648853067550923\n",
      "Iteration 5427 Loss: 0.07648839380696842\n",
      "Iteration 5428 Loss: 0.07648825695221789\n",
      "Iteration 5429 Loss: 0.07648812011124251\n",
      "Iteration 5430 Loss: 0.0764879832840273\n",
      "Iteration 5431 Loss: 0.07648784647055724\n",
      "Iteration 5432 Loss: 0.07648770967081743\n",
      "Iteration 5433 Loss: 0.07648757288479287\n",
      "Iteration 5434 Loss: 0.07648743611246858\n",
      "Iteration 5435 Loss: 0.0764872993538299\n",
      "Iteration 5436 Loss: 0.07648716260886186\n",
      "Iteration 5437 Loss: 0.07648702587754977\n",
      "Iteration 5438 Loss: 0.07648688915987881\n",
      "Iteration 5439 Loss: 0.07648675245583429\n",
      "Iteration 5440 Loss: 0.0764866157654015\n",
      "Iteration 5441 Loss: 0.07648647908856582\n",
      "Iteration 5442 Loss: 0.07648634242531259\n",
      "Iteration 5443 Loss: 0.0764862057756273\n",
      "Iteration 5444 Loss: 0.07648606913949535\n",
      "Iteration 5445 Loss: 0.07648593251690225\n",
      "Iteration 5446 Loss: 0.07648579590783353\n",
      "Iteration 5447 Loss: 0.0764856593122747\n",
      "Iteration 5448 Loss: 0.07648552273021146\n",
      "Iteration 5449 Loss: 0.0764853861616293\n",
      "Iteration 5450 Loss: 0.07648524960651397\n",
      "Iteration 5451 Loss: 0.07648511306485112\n",
      "Iteration 5452 Loss: 0.07648497653662654\n",
      "Iteration 5453 Loss: 0.07648484002182591\n",
      "Iteration 5454 Loss: 0.07648470352043509\n",
      "Iteration 5455 Loss: 0.07648456703243986\n",
      "Iteration 5456 Loss: 0.07648443055782612\n",
      "Iteration 5457 Loss: 0.07648429409657977\n",
      "Iteration 5458 Loss: 0.07648415764868673\n",
      "Iteration 5459 Loss: 0.0764840212141329\n",
      "Iteration 5460 Loss: 0.07648388479290431\n",
      "Iteration 5461 Loss: 0.07648374838498706\n",
      "Iteration 5462 Loss: 0.07648361199036711\n",
      "Iteration 5463 Loss: 0.07648347560903061\n",
      "Iteration 5464 Loss: 0.07648333924096369\n",
      "Iteration 5465 Loss: 0.07648320288615248\n",
      "Iteration 5466 Loss: 0.07648306654458319\n",
      "Iteration 5467 Loss: 0.07648293021624201\n",
      "Iteration 5468 Loss: 0.07648279390111527\n",
      "Iteration 5469 Loss: 0.07648265759918921\n",
      "Iteration 5470 Loss: 0.07648252131045012\n",
      "Iteration 5471 Loss: 0.07648238503488441\n",
      "Iteration 5472 Loss: 0.07648224877247851\n",
      "Iteration 5473 Loss: 0.07648211252321868\n",
      "Iteration 5474 Loss: 0.07648197628709152\n",
      "Iteration 5475 Loss: 0.07648184006408344\n",
      "Iteration 5476 Loss: 0.07648170385418099\n",
      "Iteration 5477 Loss: 0.07648156765737073\n",
      "Iteration 5478 Loss: 0.07648143147363912\n",
      "Iteration 5479 Loss: 0.07648129530297297\n",
      "Iteration 5480 Loss: 0.07648115914535873\n",
      "Iteration 5481 Loss: 0.07648102300078317\n",
      "Iteration 5482 Loss: 0.07648088686923293\n",
      "Iteration 5483 Loss: 0.07648075075069491\n",
      "Iteration 5484 Loss: 0.07648061464515565\n",
      "Iteration 5485 Loss: 0.07648047855260202\n",
      "Iteration 5486 Loss: 0.07648034247302105\n",
      "Iteration 5487 Loss: 0.0764802064063993\n",
      "Iteration 5488 Loss: 0.07648007035272379\n",
      "Iteration 5489 Loss: 0.07647993431198151\n",
      "Iteration 5490 Loss: 0.07647979828415928\n",
      "Iteration 5491 Loss: 0.0764796622692442\n",
      "Iteration 5492 Loss: 0.07647952626722324\n",
      "Iteration 5493 Loss: 0.07647939027808338\n",
      "Iteration 5494 Loss: 0.07647925430181181\n",
      "Iteration 5495 Loss: 0.07647911833839553\n",
      "Iteration 5496 Loss: 0.07647898238782179\n",
      "Iteration 5497 Loss: 0.07647884645007764\n",
      "Iteration 5498 Loss: 0.07647871052515035\n",
      "Iteration 5499 Loss: 0.0764785746130271\n",
      "Iteration 5500 Loss: 0.07647843871369515\n",
      "Iteration 5501 Loss: 0.07647830282714185\n",
      "Iteration 5502 Loss: 0.0764781669533545\n",
      "Iteration 5503 Loss: 0.07647803109232032\n",
      "Iteration 5504 Loss: 0.07647789524402686\n",
      "Iteration 5505 Loss: 0.07647775940846141\n",
      "Iteration 5506 Loss: 0.07647762358561147\n",
      "Iteration 5507 Loss: 0.07647748777546445\n",
      "Iteration 5508 Loss: 0.0764773519780079\n",
      "Iteration 5509 Loss: 0.07647721619322931\n",
      "Iteration 5510 Loss: 0.0764770804211162\n",
      "Iteration 5511 Loss: 0.07647694466165623\n",
      "Iteration 5512 Loss: 0.07647680891483702\n",
      "Iteration 5513 Loss: 0.07647667318064608\n",
      "Iteration 5514 Loss: 0.0764765374590712\n",
      "Iteration 5515 Loss: 0.07647640175010005\n",
      "Iteration 5516 Loss: 0.07647626605372033\n",
      "Iteration 5517 Loss: 0.07647613036991976\n",
      "Iteration 5518 Loss: 0.07647599469868625\n",
      "Iteration 5519 Loss: 0.07647585904000759\n",
      "Iteration 5520 Loss: 0.07647572339387144\n",
      "Iteration 5521 Loss: 0.07647558776026588\n",
      "Iteration 5522 Loss: 0.07647545213917872\n",
      "Iteration 5523 Loss: 0.07647531653059791\n",
      "Iteration 5524 Loss: 0.07647518093451146\n",
      "Iteration 5525 Loss: 0.0764750453509072\n",
      "Iteration 5526 Loss: 0.07647490977977331\n",
      "Iteration 5527 Loss: 0.07647477422109772\n",
      "Iteration 5528 Loss: 0.07647463867486853\n",
      "Iteration 5529 Loss: 0.07647450314107383\n",
      "Iteration 5530 Loss: 0.07647436761970178\n",
      "Iteration 5531 Loss: 0.07647423211074053\n",
      "Iteration 5532 Loss: 0.07647409661417823\n",
      "Iteration 5533 Loss: 0.0764739611300031\n",
      "Iteration 5534 Loss: 0.07647382565820336\n",
      "Iteration 5535 Loss: 0.0764736901987673\n",
      "Iteration 5536 Loss: 0.07647355475168321\n",
      "Iteration 5537 Loss: 0.07647341931693936\n",
      "Iteration 5538 Loss: 0.07647328389452417\n",
      "Iteration 5539 Loss: 0.07647314848442599\n",
      "Iteration 5540 Loss: 0.07647301308663318\n",
      "Iteration 5541 Loss: 0.07647287770113417\n",
      "Iteration 5542 Loss: 0.07647274232791752\n",
      "Iteration 5543 Loss: 0.07647260696697153\n",
      "Iteration 5544 Loss: 0.07647247161828491\n",
      "Iteration 5545 Loss: 0.07647233628184606\n",
      "Iteration 5546 Loss: 0.07647220095764354\n",
      "Iteration 5547 Loss: 0.07647206564566604\n",
      "Iteration 5548 Loss: 0.07647193034590206\n",
      "Iteration 5549 Loss: 0.07647179505834034\n",
      "Iteration 5550 Loss: 0.07647165978296948\n",
      "Iteration 5551 Loss: 0.07647152451977828\n",
      "Iteration 5552 Loss: 0.07647138926875531\n",
      "Iteration 5553 Loss: 0.07647125402988944\n",
      "Iteration 5554 Loss: 0.07647111880316931\n",
      "Iteration 5555 Loss: 0.07647098358858391\n",
      "Iteration 5556 Loss: 0.07647084838612192\n",
      "Iteration 5557 Loss: 0.0764707131957723\n",
      "Iteration 5558 Loss: 0.07647057801752381\n",
      "Iteration 5559 Loss: 0.07647044285136545\n",
      "Iteration 5560 Loss: 0.07647030769728612\n",
      "Iteration 5561 Loss: 0.07647017255527479\n",
      "Iteration 5562 Loss: 0.07647003742532045\n",
      "Iteration 5563 Loss: 0.07646990230741205\n",
      "Iteration 5564 Loss: 0.0764697672015387\n",
      "Iteration 5565 Loss: 0.07646963210768941\n",
      "Iteration 5566 Loss: 0.07646949702585328\n",
      "Iteration 5567 Loss: 0.07646936195601947\n",
      "Iteration 5568 Loss: 0.07646922689817709\n",
      "Iteration 5569 Loss: 0.07646909185231525\n",
      "Iteration 5570 Loss: 0.07646895681842318\n",
      "Iteration 5571 Loss: 0.07646882179649006\n",
      "Iteration 5572 Loss: 0.07646868678650524\n",
      "Iteration 5573 Loss: 0.07646855178845781\n",
      "Iteration 5574 Loss: 0.07646841680233725\n",
      "Iteration 5575 Loss: 0.07646828182813271\n",
      "Iteration 5576 Loss: 0.07646814686583364\n",
      "Iteration 5577 Loss: 0.07646801191542937\n",
      "Iteration 5578 Loss: 0.0764678769769092\n",
      "Iteration 5579 Loss: 0.07646774205026267\n",
      "Iteration 5580 Loss: 0.07646760713547923\n",
      "Iteration 5581 Loss: 0.0764674722325482\n",
      "Iteration 5582 Loss: 0.07646733734145919\n",
      "Iteration 5583 Loss: 0.07646720246220166\n",
      "Iteration 5584 Loss: 0.07646706759476522\n",
      "Iteration 5585 Loss: 0.07646693273913932\n",
      "Iteration 5586 Loss: 0.07646679789531356\n",
      "Iteration 5587 Loss: 0.07646666306327768\n",
      "Iteration 5588 Loss: 0.07646652824302122\n",
      "Iteration 5589 Loss: 0.07646639343453375\n",
      "Iteration 5590 Loss: 0.0764662586378051\n",
      "Iteration 5591 Loss: 0.07646612385282495\n",
      "Iteration 5592 Loss: 0.0764659890795829\n",
      "Iteration 5593 Loss: 0.07646585431806892\n",
      "Iteration 5594 Loss: 0.07646571956827258\n",
      "Iteration 5595 Loss: 0.07646558483018386\n",
      "Iteration 5596 Loss: 0.07646545010379238\n",
      "Iteration 5597 Loss: 0.07646531538908824\n",
      "Iteration 5598 Loss: 0.07646518068606112\n",
      "Iteration 5599 Loss: 0.07646504599470098\n",
      "Iteration 5600 Loss: 0.07646491131499775\n",
      "Iteration 5601 Loss: 0.07646477664694142\n",
      "Iteration 5602 Loss: 0.07646464199052184\n",
      "Iteration 5603 Loss: 0.07646450734572909\n",
      "Iteration 5604 Loss: 0.07646437271255312\n",
      "Iteration 5605 Loss: 0.07646423809098406\n",
      "Iteration 5606 Loss: 0.0764641034810119\n",
      "Iteration 5607 Loss: 0.07646396888262676\n",
      "Iteration 5608 Loss: 0.07646383429581871\n",
      "Iteration 5609 Loss: 0.07646369972057786\n",
      "Iteration 5610 Loss: 0.07646356515689445\n",
      "Iteration 5611 Loss: 0.07646343060475858\n",
      "Iteration 5612 Loss: 0.07646329606416054\n",
      "Iteration 5613 Loss: 0.0764631615350904\n",
      "Iteration 5614 Loss: 0.07646302701753854\n",
      "Iteration 5615 Loss: 0.07646289251149517\n",
      "Iteration 5616 Loss: 0.07646275801695063\n",
      "Iteration 5617 Loss: 0.07646262353389516\n",
      "Iteration 5618 Loss: 0.0764624890623191\n",
      "Iteration 5619 Loss: 0.07646235460221285\n",
      "Iteration 5620 Loss: 0.07646222015356677\n",
      "Iteration 5621 Loss: 0.0764620857163713\n",
      "Iteration 5622 Loss: 0.07646195129061685\n",
      "Iteration 5623 Loss: 0.07646181687629378\n",
      "Iteration 5624 Loss: 0.07646168247339268\n",
      "Iteration 5625 Loss: 0.07646154808190393\n",
      "Iteration 5626 Loss: 0.07646141370181816\n",
      "Iteration 5627 Loss: 0.0764612793331258\n",
      "Iteration 5628 Loss: 0.07646114497581748\n",
      "Iteration 5629 Loss: 0.07646101062988377\n",
      "Iteration 5630 Loss: 0.07646087629531523\n",
      "Iteration 5631 Loss: 0.07646074197210252\n",
      "Iteration 5632 Loss: 0.07646060766023628\n",
      "Iteration 5633 Loss: 0.07646047335970717\n",
      "Iteration 5634 Loss: 0.07646033907050585\n",
      "Iteration 5635 Loss: 0.0764602047926231\n",
      "Iteration 5636 Loss: 0.07646007052604958\n",
      "Iteration 5637 Loss: 0.07645993627077605\n",
      "Iteration 5638 Loss: 0.07645980202679334\n",
      "Iteration 5639 Loss: 0.07645966779409222\n",
      "Iteration 5640 Loss: 0.07645953357266351\n",
      "Iteration 5641 Loss: 0.076459399362498\n",
      "Iteration 5642 Loss: 0.07645926516358661\n",
      "Iteration 5643 Loss: 0.0764591309759202\n",
      "Iteration 5644 Loss: 0.07645899679948964\n",
      "Iteration 5645 Loss: 0.07645886263428595\n",
      "Iteration 5646 Loss: 0.07645872848029997\n",
      "Iteration 5647 Loss: 0.07645859433752272\n",
      "Iteration 5648 Loss: 0.07645846020594516\n",
      "Iteration 5649 Loss: 0.0764583260855583\n",
      "Iteration 5650 Loss: 0.07645819197635323\n",
      "Iteration 5651 Loss: 0.07645805787832094\n",
      "Iteration 5652 Loss: 0.07645792379145244\n",
      "Iteration 5653 Loss: 0.07645778971573894\n",
      "Iteration 5654 Loss: 0.07645765565117149\n",
      "Iteration 5655 Loss: 0.07645752159774118\n",
      "Iteration 5656 Loss: 0.07645738755543928\n",
      "Iteration 5657 Loss: 0.07645725352425685\n",
      "Iteration 5658 Loss: 0.07645711950418516\n",
      "Iteration 5659 Loss: 0.07645698549521535\n",
      "Iteration 5660 Loss: 0.07645685149733869\n",
      "Iteration 5661 Loss: 0.07645671751054649\n",
      "Iteration 5662 Loss: 0.07645658353482998\n",
      "Iteration 5663 Loss: 0.07645644957018041\n",
      "Iteration 5664 Loss: 0.07645631561658918\n",
      "Iteration 5665 Loss: 0.07645618167404755\n",
      "Iteration 5666 Loss: 0.07645604774254688\n",
      "Iteration 5667 Loss: 0.07645591382207859\n",
      "Iteration 5668 Loss: 0.07645577991263403\n",
      "Iteration 5669 Loss: 0.07645564601420468\n",
      "Iteration 5670 Loss: 0.07645551212678187\n",
      "Iteration 5671 Loss: 0.07645537825035718\n",
      "Iteration 5672 Loss: 0.07645524438492199\n",
      "Iteration 5673 Loss: 0.07645511053046787\n",
      "Iteration 5674 Loss: 0.07645497668698625\n",
      "Iteration 5675 Loss: 0.0764548428544687\n",
      "Iteration 5676 Loss: 0.07645470903290677\n",
      "Iteration 5677 Loss: 0.07645457522229206\n",
      "Iteration 5678 Loss: 0.07645444142261615\n",
      "Iteration 5679 Loss: 0.07645430763387061\n",
      "Iteration 5680 Loss: 0.07645417385604711\n",
      "Iteration 5681 Loss: 0.07645404008913727\n",
      "Iteration 5682 Loss: 0.07645390633313282\n",
      "Iteration 5683 Loss: 0.07645377258802541\n",
      "Iteration 5684 Loss: 0.07645363885380677\n",
      "Iteration 5685 Loss: 0.07645350513046852\n",
      "Iteration 5686 Loss: 0.07645337141800254\n",
      "Iteration 5687 Loss: 0.07645323771640056\n",
      "Iteration 5688 Loss: 0.07645310402565436\n",
      "Iteration 5689 Loss: 0.07645297034575574\n",
      "Iteration 5690 Loss: 0.07645283667669643\n",
      "Iteration 5691 Loss: 0.07645270301846839\n",
      "Iteration 5692 Loss: 0.07645256937106354\n",
      "Iteration 5693 Loss: 0.07645243573447355\n",
      "Iteration 5694 Loss: 0.07645230210869049\n",
      "Iteration 5695 Loss: 0.07645216849370623\n",
      "Iteration 5696 Loss: 0.07645203488951266\n",
      "Iteration 5697 Loss: 0.07645190129610174\n",
      "Iteration 5698 Loss: 0.07645176771346548\n",
      "Iteration 5699 Loss: 0.07645163414159589\n",
      "Iteration 5700 Loss: 0.07645150058048489\n",
      "Iteration 5701 Loss: 0.07645136703012462\n",
      "Iteration 5702 Loss: 0.07645123349050706\n",
      "Iteration 5703 Loss: 0.07645109996162427\n",
      "Iteration 5704 Loss: 0.07645096644346833\n",
      "Iteration 5705 Loss: 0.07645083293603137\n",
      "Iteration 5706 Loss: 0.07645069943930548\n",
      "Iteration 5707 Loss: 0.07645056595328283\n",
      "Iteration 5708 Loss: 0.07645043247795553\n",
      "Iteration 5709 Loss: 0.07645029901331578\n",
      "Iteration 5710 Loss: 0.0764501655593558\n",
      "Iteration 5711 Loss: 0.07645003211606777\n",
      "Iteration 5712 Loss: 0.07644989868344386\n",
      "Iteration 5713 Loss: 0.07644976526147639\n",
      "Iteration 5714 Loss: 0.07644963185015759\n",
      "Iteration 5715 Loss: 0.07644949844947976\n",
      "Iteration 5716 Loss: 0.0764493650594352\n",
      "Iteration 5717 Loss: 0.07644923168001622\n",
      "Iteration 5718 Loss: 0.0764490983112151\n",
      "Iteration 5719 Loss: 0.0764489649530243\n",
      "Iteration 5720 Loss: 0.07644883160543606\n",
      "Iteration 5721 Loss: 0.07644869826844286\n",
      "Iteration 5722 Loss: 0.07644856494203711\n",
      "Iteration 5723 Loss: 0.07644843162621112\n",
      "Iteration 5724 Loss: 0.0764482983209575\n",
      "Iteration 5725 Loss: 0.07644816502626858\n",
      "Iteration 5726 Loss: 0.07644803174213685\n",
      "Iteration 5727 Loss: 0.0764478984685548\n",
      "Iteration 5728 Loss: 0.07644776520551502\n",
      "Iteration 5729 Loss: 0.07644763195300987\n",
      "Iteration 5730 Loss: 0.07644749871103206\n",
      "Iteration 5731 Loss: 0.07644736547957405\n",
      "Iteration 5732 Loss: 0.0764472322586285\n",
      "Iteration 5733 Loss: 0.07644709904818789\n",
      "Iteration 5734 Loss: 0.0764469658482449\n",
      "Iteration 5735 Loss: 0.07644683265879214\n",
      "Iteration 5736 Loss: 0.07644669947982227\n",
      "Iteration 5737 Loss: 0.07644656631132796\n",
      "Iteration 5738 Loss: 0.07644643315330187\n",
      "Iteration 5739 Loss: 0.07644630000573666\n",
      "Iteration 5740 Loss: 0.07644616686862507\n",
      "Iteration 5741 Loss: 0.0764460337419599\n",
      "Iteration 5742 Loss: 0.07644590062573378\n",
      "Iteration 5743 Loss: 0.07644576751993952\n",
      "Iteration 5744 Loss: 0.07644563442456996\n",
      "Iteration 5745 Loss: 0.07644550133961776\n",
      "Iteration 5746 Loss: 0.07644536826507581\n",
      "Iteration 5747 Loss: 0.07644523520093696\n",
      "Iteration 5748 Loss: 0.076445102147194\n",
      "Iteration 5749 Loss: 0.07644496910383984\n",
      "Iteration 5750 Loss: 0.07644483607086733\n",
      "Iteration 5751 Loss: 0.07644470304826932\n",
      "Iteration 5752 Loss: 0.07644457003603881\n",
      "Iteration 5753 Loss: 0.07644443703416866\n",
      "Iteration 5754 Loss: 0.07644430404265186\n",
      "Iteration 5755 Loss: 0.07644417106148134\n",
      "Iteration 5756 Loss: 0.07644403809065003\n",
      "Iteration 5757 Loss: 0.07644390513015101\n",
      "Iteration 5758 Loss: 0.07644377217997719\n",
      "Iteration 5759 Loss: 0.07644363924012171\n",
      "Iteration 5760 Loss: 0.07644350631057747\n",
      "Iteration 5761 Loss: 0.07644337339133761\n",
      "Iteration 5762 Loss: 0.0764432404823952\n",
      "Iteration 5763 Loss: 0.07644310758374331\n",
      "Iteration 5764 Loss: 0.07644297469537505\n",
      "Iteration 5765 Loss: 0.07644284181728347\n",
      "Iteration 5766 Loss: 0.07644270894946181\n",
      "Iteration 5767 Loss: 0.07644257609190315\n",
      "Iteration 5768 Loss: 0.0764424432446007\n",
      "Iteration 5769 Loss: 0.0764423104075476\n",
      "Iteration 5770 Loss: 0.07644217758073706\n",
      "Iteration 5771 Loss: 0.07644204476416226\n",
      "Iteration 5772 Loss: 0.07644191195781647\n",
      "Iteration 5773 Loss: 0.07644177916169288\n",
      "Iteration 5774 Loss: 0.07644164637578486\n",
      "Iteration 5775 Loss: 0.07644151360008554\n",
      "Iteration 5776 Loss: 0.07644138083458833\n",
      "Iteration 5777 Loss: 0.07644124807928646\n",
      "Iteration 5778 Loss: 0.07644111533417323\n",
      "Iteration 5779 Loss: 0.07644098259924205\n",
      "Iteration 5780 Loss: 0.07644084987448617\n",
      "Iteration 5781 Loss: 0.0764407171598991\n",
      "Iteration 5782 Loss: 0.07644058445547404\n",
      "Iteration 5783 Loss: 0.07644045176120454\n",
      "Iteration 5784 Loss: 0.07644031907708393\n",
      "Iteration 5785 Loss: 0.07644018640310567\n",
      "Iteration 5786 Loss: 0.07644005373926312\n",
      "Iteration 5787 Loss: 0.07643992108554987\n",
      "Iteration 5788 Loss: 0.07643978844195924\n",
      "Iteration 5789 Loss: 0.07643965580848484\n",
      "Iteration 5790 Loss: 0.07643952318512011\n",
      "Iteration 5791 Loss: 0.07643939057185854\n",
      "Iteration 5792 Loss: 0.07643925796869372\n",
      "Iteration 5793 Loss: 0.07643912537561913\n",
      "Iteration 5794 Loss: 0.07643899279262838\n",
      "Iteration 5795 Loss: 0.076438860219715\n",
      "Iteration 5796 Loss: 0.07643872765687261\n",
      "Iteration 5797 Loss: 0.07643859510409481\n",
      "Iteration 5798 Loss: 0.07643846256137518\n",
      "Iteration 5799 Loss: 0.0764383300287074\n",
      "Iteration 5800 Loss: 0.07643819750608508\n",
      "Iteration 5801 Loss: 0.07643806499350188\n",
      "Iteration 5802 Loss: 0.07643793249095147\n",
      "Iteration 5803 Loss: 0.07643779999842763\n",
      "Iteration 5804 Loss: 0.07643766751592392\n",
      "Iteration 5805 Loss: 0.07643753504343413\n",
      "Iteration 5806 Loss: 0.07643740258095195\n",
      "Iteration 5807 Loss: 0.07643727012847126\n",
      "Iteration 5808 Loss: 0.07643713768598566\n",
      "Iteration 5809 Loss: 0.07643700525348901\n",
      "Iteration 5810 Loss: 0.07643687283097501\n",
      "Iteration 5811 Loss: 0.07643674041843758\n",
      "Iteration 5812 Loss: 0.07643660801587045\n",
      "Iteration 5813 Loss: 0.0764364756232675\n",
      "Iteration 5814 Loss: 0.07643634324062255\n",
      "Iteration 5815 Loss: 0.07643621086792941\n",
      "Iteration 5816 Loss: 0.0764360785051821\n",
      "Iteration 5817 Loss: 0.07643594615237434\n",
      "Iteration 5818 Loss: 0.07643581380950012\n",
      "Iteration 5819 Loss: 0.0764356814765533\n",
      "Iteration 5820 Loss: 0.07643554915352786\n",
      "Iteration 5821 Loss: 0.07643541684041774\n",
      "Iteration 5822 Loss: 0.07643528453721686\n",
      "Iteration 5823 Loss: 0.07643515224391921\n",
      "Iteration 5824 Loss: 0.07643501996051877\n",
      "Iteration 5825 Loss: 0.07643488768700946\n",
      "Iteration 5826 Loss: 0.07643475542338546\n",
      "Iteration 5827 Loss: 0.07643462316964071\n",
      "Iteration 5828 Loss: 0.0764344909257691\n",
      "Iteration 5829 Loss: 0.0764343586917649\n",
      "Iteration 5830 Loss: 0.07643422646762206\n",
      "Iteration 5831 Loss: 0.0764340942533347\n",
      "Iteration 5832 Loss: 0.07643396204889687\n",
      "Iteration 5833 Loss: 0.07643382985430269\n",
      "Iteration 5834 Loss: 0.0764336976695463\n",
      "Iteration 5835 Loss: 0.07643356549462181\n",
      "Iteration 5836 Loss: 0.07643343332952332\n",
      "Iteration 5837 Loss: 0.07643330117424506\n",
      "Iteration 5838 Loss: 0.07643316902878118\n",
      "Iteration 5839 Loss: 0.07643303689312582\n",
      "Iteration 5840 Loss: 0.07643290476727323\n",
      "Iteration 5841 Loss: 0.07643277265121759\n",
      "Iteration 5842 Loss: 0.07643264054495318\n",
      "Iteration 5843 Loss: 0.0764325084484741\n",
      "Iteration 5844 Loss: 0.07643237636177473\n",
      "Iteration 5845 Loss: 0.07643224428484924\n",
      "Iteration 5846 Loss: 0.076432112217692\n",
      "Iteration 5847 Loss: 0.07643198016029723\n",
      "Iteration 5848 Loss: 0.07643184811265924\n",
      "Iteration 5849 Loss: 0.07643171607477234\n",
      "Iteration 5850 Loss: 0.07643158404663088\n",
      "Iteration 5851 Loss: 0.07643145202822917\n",
      "Iteration 5852 Loss: 0.07643132001956157\n",
      "Iteration 5853 Loss: 0.0764311880206224\n",
      "Iteration 5854 Loss: 0.07643105603140615\n",
      "Iteration 5855 Loss: 0.07643092405190716\n",
      "Iteration 5856 Loss: 0.07643079208211977\n",
      "Iteration 5857 Loss: 0.07643066012203842\n",
      "Iteration 5858 Loss: 0.07643052817165759\n",
      "Iteration 5859 Loss: 0.07643039623097164\n",
      "Iteration 5860 Loss: 0.0764302642999751\n",
      "Iteration 5861 Loss: 0.0764301323786624\n",
      "Iteration 5862 Loss: 0.07643000046702801\n",
      "Iteration 5863 Loss: 0.07642986856506642\n",
      "Iteration 5864 Loss: 0.07642973667277216\n",
      "Iteration 5865 Loss: 0.07642960479013966\n",
      "Iteration 5866 Loss: 0.0764294729171636\n",
      "Iteration 5867 Loss: 0.07642934105383835\n",
      "Iteration 5868 Loss: 0.07642920920015857\n",
      "Iteration 5869 Loss: 0.0764290773561188\n",
      "Iteration 5870 Loss: 0.07642894552171357\n",
      "Iteration 5871 Loss: 0.07642881369693755\n",
      "Iteration 5872 Loss: 0.07642868188178523\n",
      "Iteration 5873 Loss: 0.07642855007625134\n",
      "Iteration 5874 Loss: 0.07642841828033042\n",
      "Iteration 5875 Loss: 0.07642828649401714\n",
      "Iteration 5876 Loss: 0.07642815471730613\n",
      "Iteration 5877 Loss: 0.07642802295019209\n",
      "Iteration 5878 Loss: 0.07642789119266964\n",
      "Iteration 5879 Loss: 0.07642775944473351\n",
      "Iteration 5880 Loss: 0.07642762770637834\n",
      "Iteration 5881 Loss: 0.07642749597759889\n",
      "Iteration 5882 Loss: 0.07642736425838986\n",
      "Iteration 5883 Loss: 0.07642723254874595\n",
      "Iteration 5884 Loss: 0.076427100848662\n",
      "Iteration 5885 Loss: 0.07642696915813263\n",
      "Iteration 5886 Loss: 0.07642683747715272\n",
      "Iteration 5887 Loss: 0.07642670580571702\n",
      "Iteration 5888 Loss: 0.07642657414382027\n",
      "Iteration 5889 Loss: 0.07642644249145729\n",
      "Iteration 5890 Loss: 0.07642631084862293\n",
      "Iteration 5891 Loss: 0.07642617921531199\n",
      "Iteration 5892 Loss: 0.0764260475915193\n",
      "Iteration 5893 Loss: 0.07642591597723977\n",
      "Iteration 5894 Loss: 0.07642578437246816\n",
      "Iteration 5895 Loss: 0.07642565277719937\n",
      "Iteration 5896 Loss: 0.07642552119142833\n",
      "Iteration 5897 Loss: 0.0764253896151499\n",
      "Iteration 5898 Loss: 0.07642525804835897\n",
      "Iteration 5899 Loss: 0.07642512649105049\n",
      "Iteration 5900 Loss: 0.07642499494321939\n",
      "Iteration 5901 Loss: 0.07642486340486053\n",
      "Iteration 5902 Loss: 0.07642473187596895\n",
      "Iteration 5903 Loss: 0.07642460035653965\n",
      "Iteration 5904 Loss: 0.07642446884656746\n",
      "Iteration 5905 Loss: 0.07642433734604753\n",
      "Iteration 5906 Loss: 0.07642420585497468\n",
      "Iteration 5907 Loss: 0.07642407437334403\n",
      "Iteration 5908 Loss: 0.07642394290115058\n",
      "Iteration 5909 Loss: 0.07642381143838936\n",
      "Iteration 5910 Loss: 0.07642367998505535\n",
      "Iteration 5911 Loss: 0.0764235485411437\n",
      "Iteration 5912 Loss: 0.07642341710664938\n",
      "Iteration 5913 Loss: 0.07642328568156753\n",
      "Iteration 5914 Loss: 0.07642315426589319\n",
      "Iteration 5915 Loss: 0.07642302285962149\n",
      "Iteration 5916 Loss: 0.07642289146274756\n",
      "Iteration 5917 Loss: 0.07642276007526644\n",
      "Iteration 5918 Loss: 0.07642262869717326\n",
      "Iteration 5919 Loss: 0.07642249732846325\n",
      "Iteration 5920 Loss: 0.07642236596913146\n",
      "Iteration 5921 Loss: 0.07642223461917313\n",
      "Iteration 5922 Loss: 0.07642210327858334\n",
      "Iteration 5923 Loss: 0.07642197194735732\n",
      "Iteration 5924 Loss: 0.07642184062549029\n",
      "Iteration 5925 Loss: 0.07642170931297748\n",
      "Iteration 5926 Loss: 0.07642157800981397\n",
      "Iteration 5927 Loss: 0.07642144671599507\n",
      "Iteration 5928 Loss: 0.07642131543151605\n",
      "Iteration 5929 Loss: 0.07642118415637207\n",
      "Iteration 5930 Loss: 0.07642105289055848\n",
      "Iteration 5931 Loss: 0.07642092163407044\n",
      "Iteration 5932 Loss: 0.07642079038690332\n",
      "Iteration 5933 Loss: 0.07642065914905236\n",
      "Iteration 5934 Loss: 0.07642052792051288\n",
      "Iteration 5935 Loss: 0.07642039670128013\n",
      "Iteration 5936 Loss: 0.07642026549134948\n",
      "Iteration 5937 Loss: 0.07642013429071626\n",
      "Iteration 5938 Loss: 0.07642000309937583\n",
      "Iteration 5939 Loss: 0.0764198719173235\n",
      "Iteration 5940 Loss: 0.07641974074455465\n",
      "Iteration 5941 Loss: 0.07641960958106458\n",
      "Iteration 5942 Loss: 0.0764194784268488\n",
      "Iteration 5943 Loss: 0.07641934728190261\n",
      "Iteration 5944 Loss: 0.07641921614622142\n",
      "Iteration 5945 Loss: 0.07641908501980063\n",
      "Iteration 5946 Loss: 0.07641895390263573\n",
      "Iteration 5947 Loss: 0.07641882279472205\n",
      "Iteration 5948 Loss: 0.07641869169605515\n",
      "Iteration 5949 Loss: 0.07641856060663028\n",
      "Iteration 5950 Loss: 0.0764184295264432\n",
      "Iteration 5951 Loss: 0.07641829845548913\n",
      "Iteration 5952 Loss: 0.07641816739376366\n",
      "Iteration 5953 Loss: 0.07641803634126228\n",
      "Iteration 5954 Loss: 0.07641790529798048\n",
      "Iteration 5955 Loss: 0.07641777426391372\n",
      "Iteration 5956 Loss: 0.07641764323905757\n",
      "Iteration 5957 Loss: 0.07641751222340756\n",
      "Iteration 5958 Loss: 0.07641738121695922\n",
      "Iteration 5959 Loss: 0.0764172502197081\n",
      "Iteration 5960 Loss: 0.0764171192316498\n",
      "Iteration 5961 Loss: 0.07641698825277983\n",
      "Iteration 5962 Loss: 0.07641685728309376\n",
      "Iteration 5963 Loss: 0.07641672632258724\n",
      "Iteration 5964 Loss: 0.07641659537125586\n",
      "Iteration 5965 Loss: 0.07641646442909521\n",
      "Iteration 5966 Loss: 0.07641633349610093\n",
      "Iteration 5967 Loss: 0.07641620257226861\n",
      "Iteration 5968 Loss: 0.07641607165759391\n",
      "Iteration 5969 Loss: 0.07641594075207245\n",
      "Iteration 5970 Loss: 0.07641580985569997\n",
      "Iteration 5971 Loss: 0.07641567896847201\n",
      "Iteration 5972 Loss: 0.07641554809038437\n",
      "Iteration 5973 Loss: 0.07641541722143265\n",
      "Iteration 5974 Loss: 0.07641528636161259\n",
      "Iteration 5975 Loss: 0.07641515551091986\n",
      "Iteration 5976 Loss: 0.0764150246693502\n",
      "Iteration 5977 Loss: 0.07641489383689935\n",
      "Iteration 5978 Loss: 0.076414763013563\n",
      "Iteration 5979 Loss: 0.07641463219933692\n",
      "Iteration 5980 Loss: 0.07641450139421688\n",
      "Iteration 5981 Loss: 0.07641437059819857\n",
      "Iteration 5982 Loss: 0.07641423981127782\n",
      "Iteration 5983 Loss: 0.07641410903345039\n",
      "Iteration 5984 Loss: 0.07641397826471208\n",
      "Iteration 5985 Loss: 0.07641384750505865\n",
      "Iteration 5986 Loss: 0.07641371675448597\n",
      "Iteration 5987 Loss: 0.07641358601298977\n",
      "Iteration 5988 Loss: 0.07641345528056599\n",
      "Iteration 5989 Loss: 0.0764133245572104\n",
      "Iteration 5990 Loss: 0.07641319384291884\n",
      "Iteration 5991 Loss: 0.0764130631376871\n",
      "Iteration 5992 Loss: 0.07641293244151119\n",
      "Iteration 5993 Loss: 0.07641280175438687\n",
      "Iteration 5994 Loss: 0.07641267107631004\n",
      "Iteration 5995 Loss: 0.07641254040727664\n",
      "Iteration 5996 Loss: 0.07641240974728249\n",
      "Iteration 5997 Loss: 0.07641227909632356\n",
      "Iteration 5998 Loss: 0.07641214845439569\n",
      "Iteration 5999 Loss: 0.07641201782149491\n",
      "Iteration 6000 Loss: 0.07641188719761702\n",
      "Iteration 6001 Loss: 0.07641175658275813\n",
      "Iteration 6002 Loss: 0.07641162597691405\n",
      "Iteration 6003 Loss: 0.07641149538008081\n",
      "Iteration 6004 Loss: 0.07641136479225433\n",
      "Iteration 6005 Loss: 0.07641123421343061\n",
      "Iteration 6006 Loss: 0.07641110364360573\n",
      "Iteration 6007 Loss: 0.07641097308277552\n",
      "Iteration 6008 Loss: 0.07641084253093608\n",
      "Iteration 6009 Loss: 0.0764107119880834\n",
      "Iteration 6010 Loss: 0.07641058145421349\n",
      "Iteration 6011 Loss: 0.0764104509293224\n",
      "Iteration 6012 Loss: 0.07641032041340626\n",
      "Iteration 6013 Loss: 0.0764101899064609\n",
      "Iteration 6014 Loss: 0.07641005940848253\n",
      "Iteration 6015 Loss: 0.07640992891946714\n",
      "Iteration 6016 Loss: 0.07640979843941093\n",
      "Iteration 6017 Loss: 0.07640966796830985\n",
      "Iteration 6018 Loss: 0.07640953750616002\n",
      "Iteration 6019 Loss: 0.07640940705295757\n",
      "Iteration 6020 Loss: 0.07640927660869862\n",
      "Iteration 6021 Loss: 0.07640914617337917\n",
      "Iteration 6022 Loss: 0.07640901574699552\n",
      "Iteration 6023 Loss: 0.0764088853295437\n",
      "Iteration 6024 Loss: 0.07640875492101981\n",
      "Iteration 6025 Loss: 0.07640862452142008\n",
      "Iteration 6026 Loss: 0.07640849413074062\n",
      "Iteration 6027 Loss: 0.07640836374897761\n",
      "Iteration 6028 Loss: 0.07640823337612723\n",
      "Iteration 6029 Loss: 0.07640810301218569\n",
      "Iteration 6030 Loss: 0.07640797265714912\n",
      "Iteration 6031 Loss: 0.07640784231101377\n",
      "Iteration 6032 Loss: 0.0764077119737758\n",
      "Iteration 6033 Loss: 0.07640758164543143\n",
      "Iteration 6034 Loss: 0.07640745132597693\n",
      "Iteration 6035 Loss: 0.07640732101540851\n",
      "Iteration 6036 Loss: 0.07640719071372243\n",
      "Iteration 6037 Loss: 0.07640706042091487\n",
      "Iteration 6038 Loss: 0.07640693013698212\n",
      "Iteration 6039 Loss: 0.07640679986192046\n",
      "Iteration 6040 Loss: 0.07640666959572616\n",
      "Iteration 6041 Loss: 0.07640653933839546\n",
      "Iteration 6042 Loss: 0.0764064090899247\n",
      "Iteration 6043 Loss: 0.07640627885031019\n",
      "Iteration 6044 Loss: 0.07640614861954816\n",
      "Iteration 6045 Loss: 0.07640601839763499\n",
      "Iteration 6046 Loss: 0.07640588818456698\n",
      "Iteration 6047 Loss: 0.07640575798034041\n",
      "Iteration 6048 Loss: 0.0764056277849517\n",
      "Iteration 6049 Loss: 0.07640549759839713\n",
      "Iteration 6050 Loss: 0.07640536742067305\n",
      "Iteration 6051 Loss: 0.07640523725177586\n",
      "Iteration 6052 Loss: 0.07640510709170192\n",
      "Iteration 6053 Loss: 0.07640497694044761\n",
      "Iteration 6054 Loss: 0.07640484679800923\n",
      "Iteration 6055 Loss: 0.0764047166643833\n",
      "Iteration 6056 Loss: 0.07640458653956611\n",
      "Iteration 6057 Loss: 0.07640445642355413\n",
      "Iteration 6058 Loss: 0.07640432631634375\n",
      "Iteration 6059 Loss: 0.0764041962179314\n",
      "Iteration 6060 Loss: 0.0764040661283135\n",
      "Iteration 6061 Loss: 0.07640393604748646\n",
      "Iteration 6062 Loss: 0.07640380597544681\n",
      "Iteration 6063 Loss: 0.07640367591219091\n",
      "Iteration 6064 Loss: 0.0764035458577153\n",
      "Iteration 6065 Loss: 0.07640341581201641\n",
      "Iteration 6066 Loss: 0.07640328577509073\n",
      "Iteration 6067 Loss: 0.07640315574693464\n",
      "Iteration 6068 Loss: 0.07640302572754473\n",
      "Iteration 6069 Loss: 0.07640289571691757\n",
      "Iteration 6070 Loss: 0.07640276571504948\n",
      "Iteration 6071 Loss: 0.07640263572193713\n",
      "Iteration 6072 Loss: 0.07640250573757693\n",
      "Iteration 6073 Loss: 0.07640237576196547\n",
      "Iteration 6074 Loss: 0.07640224579509924\n",
      "Iteration 6075 Loss: 0.07640211583697487\n",
      "Iteration 6076 Loss: 0.07640198588758883\n",
      "Iteration 6077 Loss: 0.0764018559469377\n",
      "Iteration 6078 Loss: 0.07640172601501809\n",
      "Iteration 6079 Loss: 0.07640159609182648\n",
      "Iteration 6080 Loss: 0.0764014661773595\n",
      "Iteration 6081 Loss: 0.07640133627161375\n",
      "Iteration 6082 Loss: 0.07640120637458575\n",
      "Iteration 6083 Loss: 0.07640107648627223\n",
      "Iteration 6084 Loss: 0.07640094660666968\n",
      "Iteration 6085 Loss: 0.07640081673577477\n",
      "Iteration 6086 Loss: 0.07640068687358409\n",
      "Iteration 6087 Loss: 0.07640055702009434\n",
      "Iteration 6088 Loss: 0.07640042717530207\n",
      "Iteration 6089 Loss: 0.07640029733920396\n",
      "Iteration 6090 Loss: 0.07640016751179672\n",
      "Iteration 6091 Loss: 0.07640003769307689\n",
      "Iteration 6092 Loss: 0.07639990788304125\n",
      "Iteration 6093 Loss: 0.07639977808168637\n",
      "Iteration 6094 Loss: 0.07639964828900903\n",
      "Iteration 6095 Loss: 0.07639951850500584\n",
      "Iteration 6096 Loss: 0.0763993887296735\n",
      "Iteration 6097 Loss: 0.07639925896300875\n",
      "Iteration 6098 Loss: 0.07639912920500833\n",
      "Iteration 6099 Loss: 0.07639899945566882\n",
      "Iteration 6100 Loss: 0.07639886971498706\n",
      "Iteration 6101 Loss: 0.0763987399829598\n",
      "Iteration 6102 Loss: 0.07639861025958364\n",
      "Iteration 6103 Loss: 0.07639848054485544\n",
      "Iteration 6104 Loss: 0.07639835083877194\n",
      "Iteration 6105 Loss: 0.07639822114132987\n",
      "Iteration 6106 Loss: 0.07639809145252595\n",
      "Iteration 6107 Loss: 0.076397961772357\n",
      "Iteration 6108 Loss: 0.0763978321008198\n",
      "Iteration 6109 Loss: 0.07639770243791112\n",
      "Iteration 6110 Loss: 0.07639757278362772\n",
      "Iteration 6111 Loss: 0.07639744313796651\n",
      "Iteration 6112 Loss: 0.07639731350092416\n",
      "Iteration 6113 Loss: 0.07639718387249754\n",
      "Iteration 6114 Loss: 0.07639705425268346\n",
      "Iteration 6115 Loss: 0.07639692464147879\n",
      "Iteration 6116 Loss: 0.07639679503888026\n",
      "Iteration 6117 Loss: 0.0763966654448848\n",
      "Iteration 6118 Loss: 0.07639653585948919\n",
      "Iteration 6119 Loss: 0.0763964062826904\n",
      "Iteration 6120 Loss: 0.07639627671448512\n",
      "Iteration 6121 Loss: 0.07639614715487032\n",
      "Iteration 6122 Loss: 0.07639601760384288\n",
      "Iteration 6123 Loss: 0.0763958880613996\n",
      "Iteration 6124 Loss: 0.0763957585275374\n",
      "Iteration 6125 Loss: 0.0763956290022532\n",
      "Iteration 6126 Loss: 0.07639549948554387\n",
      "Iteration 6127 Loss: 0.0763953699774063\n",
      "Iteration 6128 Loss: 0.07639524047783745\n",
      "Iteration 6129 Loss: 0.07639511098683421\n",
      "Iteration 6130 Loss: 0.07639498150439347\n",
      "Iteration 6131 Loss: 0.0763948520305122\n",
      "Iteration 6132 Loss: 0.07639472256518737\n",
      "Iteration 6133 Loss: 0.07639459310841584\n",
      "Iteration 6134 Loss: 0.0763944636601946\n",
      "Iteration 6135 Loss: 0.07639433422052058\n",
      "Iteration 6136 Loss: 0.07639420478939077\n",
      "Iteration 6137 Loss: 0.07639407536680215\n",
      "Iteration 6138 Loss: 0.07639394595275167\n",
      "Iteration 6139 Loss: 0.07639381654723634\n",
      "Iteration 6140 Loss: 0.07639368715025313\n",
      "Iteration 6141 Loss: 0.07639355776179894\n",
      "Iteration 6142 Loss: 0.07639342838187099\n",
      "Iteration 6143 Loss: 0.07639329901046606\n",
      "Iteration 6144 Loss: 0.0763931696475813\n",
      "Iteration 6145 Loss: 0.07639304029321364\n",
      "Iteration 6146 Loss: 0.0763929109473602\n",
      "Iteration 6147 Loss: 0.07639278161001797\n",
      "Iteration 6148 Loss: 0.07639265228118393\n",
      "Iteration 6149 Loss: 0.0763925229608552\n",
      "Iteration 6150 Loss: 0.07639239364902885\n",
      "Iteration 6151 Loss: 0.07639226434570182\n",
      "Iteration 6152 Loss: 0.07639213505087125\n",
      "Iteration 6153 Loss: 0.07639200576453418\n",
      "Iteration 6154 Loss: 0.07639187648668773\n",
      "Iteration 6155 Loss: 0.07639174721732894\n",
      "Iteration 6156 Loss: 0.07639161795645494\n",
      "Iteration 6157 Loss: 0.07639148870406273\n",
      "Iteration 6158 Loss: 0.07639135946014956\n",
      "Iteration 6159 Loss: 0.07639123022471236\n",
      "Iteration 6160 Loss: 0.07639110099774837\n",
      "Iteration 6161 Loss: 0.07639097177925468\n",
      "Iteration 6162 Loss: 0.07639084256922833\n",
      "Iteration 6163 Loss: 0.07639071336766655\n",
      "Iteration 6164 Loss: 0.07639058417456644\n",
      "Iteration 6165 Loss: 0.0763904549899251\n",
      "Iteration 6166 Loss: 0.07639032581373977\n",
      "Iteration 6167 Loss: 0.07639019664600752\n",
      "Iteration 6168 Loss: 0.07639006748672555\n",
      "Iteration 6169 Loss: 0.07638993833589099\n",
      "Iteration 6170 Loss: 0.076389809193501\n",
      "Iteration 6171 Loss: 0.07638968005955285\n",
      "Iteration 6172 Loss: 0.0763895509340436\n",
      "Iteration 6173 Loss: 0.07638942181697055\n",
      "Iteration 6174 Loss: 0.07638929270833084\n",
      "Iteration 6175 Loss: 0.07638916360812163\n",
      "Iteration 6176 Loss: 0.0763890345163401\n",
      "Iteration 6177 Loss: 0.07638890543298359\n",
      "Iteration 6178 Loss: 0.07638877635804929\n",
      "Iteration 6179 Loss: 0.07638864729153431\n",
      "Iteration 6180 Loss: 0.07638851823343601\n",
      "Iteration 6181 Loss: 0.0763883891837515\n",
      "Iteration 6182 Loss: 0.07638826014247814\n",
      "Iteration 6183 Loss: 0.07638813110961312\n",
      "Iteration 6184 Loss: 0.07638800208515362\n",
      "Iteration 6185 Loss: 0.07638787306909701\n",
      "Iteration 6186 Loss: 0.07638774406144053\n",
      "Iteration 6187 Loss: 0.07638761506218139\n",
      "Iteration 6188 Loss: 0.07638748607131687\n",
      "Iteration 6189 Loss: 0.07638735708884437\n",
      "Iteration 6190 Loss: 0.07638722811476102\n",
      "Iteration 6191 Loss: 0.07638709914906416\n",
      "Iteration 6192 Loss: 0.07638697019175114\n",
      "Iteration 6193 Loss: 0.07638684124281923\n",
      "Iteration 6194 Loss: 0.07638671230226568\n",
      "Iteration 6195 Loss: 0.07638658337008787\n",
      "Iteration 6196 Loss: 0.0763864544462831\n",
      "Iteration 6197 Loss: 0.0763863255308487\n",
      "Iteration 6198 Loss: 0.07638619662378197\n",
      "Iteration 6199 Loss: 0.07638606772508032\n",
      "Iteration 6200 Loss: 0.07638593883474099\n",
      "Iteration 6201 Loss: 0.07638580995276134\n",
      "Iteration 6202 Loss: 0.07638568107913882\n",
      "Iteration 6203 Loss: 0.07638555221387067\n",
      "Iteration 6204 Loss: 0.07638542335695431\n",
      "Iteration 6205 Loss: 0.0763852945083871\n",
      "Iteration 6206 Loss: 0.07638516566816639\n",
      "Iteration 6207 Loss: 0.0763850368362896\n",
      "Iteration 6208 Loss: 0.07638490801275409\n",
      "Iteration 6209 Loss: 0.07638477919755722\n",
      "Iteration 6210 Loss: 0.07638465039069647\n",
      "Iteration 6211 Loss: 0.07638452159216916\n",
      "Iteration 6212 Loss: 0.07638439280197266\n",
      "Iteration 6213 Loss: 0.0763842640201045\n",
      "Iteration 6214 Loss: 0.07638413524656201\n",
      "Iteration 6215 Loss: 0.07638400648134269\n",
      "Iteration 6216 Loss: 0.07638387772444383\n",
      "Iteration 6217 Loss: 0.07638374897586296\n",
      "Iteration 6218 Loss: 0.07638362023559749\n",
      "Iteration 6219 Loss: 0.07638349150364485\n",
      "Iteration 6220 Loss: 0.07638336278000256\n",
      "Iteration 6221 Loss: 0.076383234064668\n",
      "Iteration 6222 Loss: 0.0763831053576386\n",
      "Iteration 6223 Loss: 0.07638297665891192\n",
      "Iteration 6224 Loss: 0.07638284796848537\n",
      "Iteration 6225 Loss: 0.07638271928635637\n",
      "Iteration 6226 Loss: 0.07638259061252245\n",
      "Iteration 6227 Loss: 0.07638246194698113\n",
      "Iteration 6228 Loss: 0.07638233328972988\n",
      "Iteration 6229 Loss: 0.07638220464076616\n",
      "Iteration 6230 Loss: 0.07638207600008744\n",
      "Iteration 6231 Loss: 0.07638194736769136\n",
      "Iteration 6232 Loss: 0.07638181874357523\n",
      "Iteration 6233 Loss: 0.07638169012773671\n",
      "Iteration 6234 Loss: 0.07638156152017325\n",
      "Iteration 6235 Loss: 0.07638143292088247\n",
      "Iteration 6236 Loss: 0.07638130432986177\n",
      "Iteration 6237 Loss: 0.07638117574710877\n",
      "Iteration 6238 Loss: 0.07638104717262094\n",
      "Iteration 6239 Loss: 0.07638091860639587\n",
      "Iteration 6240 Loss: 0.07638079004843112\n",
      "Iteration 6241 Loss: 0.07638066149872422\n",
      "Iteration 6242 Loss: 0.07638053295727276\n",
      "Iteration 6243 Loss: 0.0763804044240742\n",
      "Iteration 6244 Loss: 0.07638027589912624\n",
      "Iteration 6245 Loss: 0.07638014738242642\n",
      "Iteration 6246 Loss: 0.07638001887397221\n",
      "Iteration 6247 Loss: 0.07637989037376135\n",
      "Iteration 6248 Loss: 0.07637976188179135\n",
      "Iteration 6249 Loss: 0.07637963339805975\n",
      "Iteration 6250 Loss: 0.07637950492256426\n",
      "Iteration 6251 Loss: 0.07637937645530236\n",
      "Iteration 6252 Loss: 0.07637924799627174\n",
      "Iteration 6253 Loss: 0.07637911954547003\n",
      "Iteration 6254 Loss: 0.0763789911028948\n",
      "Iteration 6255 Loss: 0.07637886266854364\n",
      "Iteration 6256 Loss: 0.07637873424241423\n",
      "Iteration 6257 Loss: 0.07637860582450424\n",
      "Iteration 6258 Loss: 0.07637847741481119\n",
      "Iteration 6259 Loss: 0.07637834901333278\n",
      "Iteration 6260 Loss: 0.0763782206200667\n",
      "Iteration 6261 Loss: 0.07637809223501053\n",
      "Iteration 6262 Loss: 0.07637796385816199\n",
      "Iteration 6263 Loss: 0.07637783548951865\n",
      "Iteration 6264 Loss: 0.07637770712907822\n",
      "Iteration 6265 Loss: 0.07637757877683844\n",
      "Iteration 6266 Loss: 0.07637745043279687\n",
      "Iteration 6267 Loss: 0.07637732209695121\n",
      "Iteration 6268 Loss: 0.07637719376929919\n",
      "Iteration 6269 Loss: 0.07637706544983851\n",
      "Iteration 6270 Loss: 0.07637693713856679\n",
      "Iteration 6271 Loss: 0.07637680883548174\n",
      "Iteration 6272 Loss: 0.07637668054058114\n",
      "Iteration 6273 Loss: 0.07637655225386253\n",
      "Iteration 6274 Loss: 0.07637642397532379\n",
      "Iteration 6275 Loss: 0.0763762957049626\n",
      "Iteration 6276 Loss: 0.07637616744277662\n",
      "Iteration 6277 Loss: 0.07637603918876362\n",
      "Iteration 6278 Loss: 0.07637591094292127\n",
      "Iteration 6279 Loss: 0.0763757827052474\n",
      "Iteration 6280 Loss: 0.07637565447573966\n",
      "Iteration 6281 Loss: 0.07637552625439585\n",
      "Iteration 6282 Loss: 0.07637539804121361\n",
      "Iteration 6283 Loss: 0.07637526983619083\n",
      "Iteration 6284 Loss: 0.0763751416393252\n",
      "Iteration 6285 Loss: 0.0763750134506145\n",
      "Iteration 6286 Loss: 0.07637488527005645\n",
      "Iteration 6287 Loss: 0.07637475709764885\n",
      "Iteration 6288 Loss: 0.07637462893338948\n",
      "Iteration 6289 Loss: 0.07637450077727609\n",
      "Iteration 6290 Loss: 0.07637437262930652\n",
      "Iteration 6291 Loss: 0.07637424448947844\n",
      "Iteration 6292 Loss: 0.07637411635778978\n",
      "Iteration 6293 Loss: 0.07637398823423826\n",
      "Iteration 6294 Loss: 0.07637386011882166\n",
      "Iteration 6295 Loss: 0.07637373201153781\n",
      "Iteration 6296 Loss: 0.07637360391238453\n",
      "Iteration 6297 Loss: 0.07637347582135962\n",
      "Iteration 6298 Loss: 0.0763733477384609\n",
      "Iteration 6299 Loss: 0.0763732196636862\n",
      "Iteration 6300 Loss: 0.07637309159703327\n",
      "Iteration 6301 Loss: 0.07637296353850007\n",
      "Iteration 6302 Loss: 0.07637283548808434\n",
      "Iteration 6303 Loss: 0.07637270744578391\n",
      "Iteration 6304 Loss: 0.0763725794115967\n",
      "Iteration 6305 Loss: 0.07637245138552051\n",
      "Iteration 6306 Loss: 0.07637232336755317\n",
      "Iteration 6307 Loss: 0.07637219535769259\n",
      "Iteration 6308 Loss: 0.07637206735593655\n",
      "Iteration 6309 Loss: 0.07637193936228298\n",
      "Iteration 6310 Loss: 0.0763718113767297\n",
      "Iteration 6311 Loss: 0.07637168339927464\n",
      "Iteration 6312 Loss: 0.07637155542991562\n",
      "Iteration 6313 Loss: 0.07637142746865053\n",
      "Iteration 6314 Loss: 0.07637129951547732\n",
      "Iteration 6315 Loss: 0.07637117157039376\n",
      "Iteration 6316 Loss: 0.07637104363339786\n",
      "Iteration 6317 Loss: 0.07637091570448744\n",
      "Iteration 6318 Loss: 0.0763707877836604\n",
      "Iteration 6319 Loss: 0.07637065987091467\n",
      "Iteration 6320 Loss: 0.07637053196624818\n",
      "Iteration 6321 Loss: 0.07637040406965878\n",
      "Iteration 6322 Loss: 0.07637027618114446\n",
      "Iteration 6323 Loss: 0.0763701483007031\n",
      "Iteration 6324 Loss: 0.07637002042833263\n",
      "Iteration 6325 Loss: 0.07636989256403096\n",
      "Iteration 6326 Loss: 0.07636976470779602\n",
      "Iteration 6327 Loss: 0.07636963685962585\n",
      "Iteration 6328 Loss: 0.07636950901951824\n",
      "Iteration 6329 Loss: 0.0763693811874712\n",
      "Iteration 6330 Loss: 0.07636925336348269\n",
      "Iteration 6331 Loss: 0.07636912554755068\n",
      "Iteration 6332 Loss: 0.07636899773967308\n",
      "Iteration 6333 Loss: 0.07636886993984787\n",
      "Iteration 6334 Loss: 0.07636874214807306\n",
      "Iteration 6335 Loss: 0.07636861436434658\n",
      "Iteration 6336 Loss: 0.07636848658866635\n",
      "Iteration 6337 Loss: 0.07636835882103042\n",
      "Iteration 6338 Loss: 0.07636823106143678\n",
      "Iteration 6339 Loss: 0.07636810330988326\n",
      "Iteration 6340 Loss: 0.0763679755663681\n",
      "Iteration 6341 Loss: 0.07636784783088911\n",
      "Iteration 6342 Loss: 0.07636772010344432\n",
      "Iteration 6343 Loss: 0.07636759238403176\n",
      "Iteration 6344 Loss: 0.07636746467264943\n",
      "Iteration 6345 Loss: 0.07636733696929533\n",
      "Iteration 6346 Loss: 0.07636720927396745\n",
      "Iteration 6347 Loss: 0.07636708158666385\n",
      "Iteration 6348 Loss: 0.07636695390738245\n",
      "Iteration 6349 Loss: 0.07636682623612145\n",
      "Iteration 6350 Loss: 0.07636669857287873\n",
      "Iteration 6351 Loss: 0.07636657091765237\n",
      "Iteration 6352 Loss: 0.07636644327044036\n",
      "Iteration 6353 Loss: 0.07636631563124081\n",
      "Iteration 6354 Loss: 0.07636618800005167\n",
      "Iteration 6355 Loss: 0.07636606037687113\n",
      "Iteration 6356 Loss: 0.07636593276169709\n",
      "Iteration 6357 Loss: 0.07636580515452769\n",
      "Iteration 6358 Loss: 0.07636567755536099\n",
      "Iteration 6359 Loss: 0.07636554996419499\n",
      "Iteration 6360 Loss: 0.07636542238102778\n",
      "Iteration 6361 Loss: 0.07636529480585742\n",
      "Iteration 6362 Loss: 0.07636516723868204\n",
      "Iteration 6363 Loss: 0.07636503967949963\n",
      "Iteration 6364 Loss: 0.07636491212830833\n",
      "Iteration 6365 Loss: 0.0763647845851062\n",
      "Iteration 6366 Loss: 0.07636465704989132\n",
      "Iteration 6367 Loss: 0.0763645295226618\n",
      "Iteration 6368 Loss: 0.07636440200341571\n",
      "Iteration 6369 Loss: 0.07636427449215115\n",
      "Iteration 6370 Loss: 0.07636414698886627\n",
      "Iteration 6371 Loss: 0.07636401949355914\n",
      "Iteration 6372 Loss: 0.07636389200622776\n",
      "Iteration 6373 Loss: 0.07636376452687041\n",
      "Iteration 6374 Loss: 0.07636363705548517\n",
      "Iteration 6375 Loss: 0.07636350959207008\n",
      "Iteration 6376 Loss: 0.07636338213662333\n",
      "Iteration 6377 Loss: 0.07636325468914301\n",
      "Iteration 6378 Loss: 0.07636312724962728\n",
      "Iteration 6379 Loss: 0.07636299981807423\n",
      "Iteration 6380 Loss: 0.07636287239448204\n",
      "Iteration 6381 Loss: 0.07636274497884883\n",
      "Iteration 6382 Loss: 0.07636261757117274\n",
      "Iteration 6383 Loss: 0.07636249017145197\n",
      "Iteration 6384 Loss: 0.07636236277968458\n",
      "Iteration 6385 Loss: 0.07636223539586878\n",
      "Iteration 6386 Loss: 0.0763621080200027\n",
      "Iteration 6387 Loss: 0.07636198065208455\n",
      "Iteration 6388 Loss: 0.07636185329211244\n",
      "Iteration 6389 Loss: 0.07636172594008457\n",
      "Iteration 6390 Loss: 0.0763615985959991\n",
      "Iteration 6391 Loss: 0.07636147125985417\n",
      "Iteration 6392 Loss: 0.07636134393164803\n",
      "Iteration 6393 Loss: 0.07636121661137883\n",
      "Iteration 6394 Loss: 0.07636108929904474\n",
      "Iteration 6395 Loss: 0.07636096199464394\n",
      "Iteration 6396 Loss: 0.07636083469817465\n",
      "Iteration 6397 Loss: 0.07636070740963502\n",
      "Iteration 6398 Loss: 0.07636058012902339\n",
      "Iteration 6399 Loss: 0.07636045285633776\n",
      "Iteration 6400 Loss: 0.07636032559157648\n",
      "Iteration 6401 Loss: 0.07636019833473769\n",
      "Iteration 6402 Loss: 0.0763600710858196\n",
      "Iteration 6403 Loss: 0.0763599438448205\n",
      "Iteration 6404 Loss: 0.0763598166117385\n",
      "Iteration 6405 Loss: 0.07635968938657191\n",
      "Iteration 6406 Loss: 0.07635956216931888\n",
      "Iteration 6407 Loss: 0.07635943495997775\n",
      "Iteration 6408 Loss: 0.07635930775854662\n",
      "Iteration 6409 Loss: 0.07635918056502383\n",
      "Iteration 6410 Loss: 0.07635905337940754\n",
      "Iteration 6411 Loss: 0.0763589262016961\n",
      "Iteration 6412 Loss: 0.07635879903188765\n",
      "Iteration 6413 Loss: 0.07635867186998047\n",
      "Iteration 6414 Loss: 0.07635854471597281\n",
      "Iteration 6415 Loss: 0.0763584175698629\n",
      "Iteration 6416 Loss: 0.07635829043164909\n",
      "Iteration 6417 Loss: 0.07635816330132954\n",
      "Iteration 6418 Loss: 0.07635803617890255\n",
      "Iteration 6419 Loss: 0.07635790906436642\n",
      "Iteration 6420 Loss: 0.07635778195771933\n",
      "Iteration 6421 Loss: 0.0763576548589597\n",
      "Iteration 6422 Loss: 0.07635752776808569\n",
      "Iteration 6423 Loss: 0.07635740068509564\n",
      "Iteration 6424 Loss: 0.07635727360998772\n",
      "Iteration 6425 Loss: 0.07635714654276042\n",
      "Iteration 6426 Loss: 0.0763570194834118\n",
      "Iteration 6427 Loss: 0.07635689243194031\n",
      "Iteration 6428 Loss: 0.07635676538834424\n",
      "Iteration 6429 Loss: 0.07635663835262184\n",
      "Iteration 6430 Loss: 0.07635651132477142\n",
      "Iteration 6431 Loss: 0.07635638430479129\n",
      "Iteration 6432 Loss: 0.07635625729267978\n",
      "Iteration 6433 Loss: 0.07635613028843512\n",
      "Iteration 6434 Loss: 0.07635600329205576\n",
      "Iteration 6435 Loss: 0.07635587630353996\n",
      "Iteration 6436 Loss: 0.07635574932288595\n",
      "Iteration 6437 Loss: 0.0763556223500922\n",
      "Iteration 6438 Loss: 0.0763554953851569\n",
      "Iteration 6439 Loss: 0.07635536842807847\n",
      "Iteration 6440 Loss: 0.07635524147885529\n",
      "Iteration 6441 Loss: 0.07635511453748559\n",
      "Iteration 6442 Loss: 0.07635498760396771\n",
      "Iteration 6443 Loss: 0.07635486067830008\n",
      "Iteration 6444 Loss: 0.076354733760481\n",
      "Iteration 6445 Loss: 0.07635460685050881\n",
      "Iteration 6446 Loss: 0.07635447994838179\n",
      "Iteration 6447 Loss: 0.07635435305409848\n",
      "Iteration 6448 Loss: 0.0763542261676571\n",
      "Iteration 6449 Loss: 0.07635409928905601\n",
      "Iteration 6450 Loss: 0.07635397241829366\n",
      "Iteration 6451 Loss: 0.07635384555536827\n",
      "Iteration 6452 Loss: 0.07635371870027836\n",
      "Iteration 6453 Loss: 0.07635359185302223\n",
      "Iteration 6454 Loss: 0.0763534650135983\n",
      "Iteration 6455 Loss: 0.07635333818200492\n",
      "Iteration 6456 Loss: 0.07635321135824041\n",
      "Iteration 6457 Loss: 0.07635308454230326\n",
      "Iteration 6458 Loss: 0.07635295773419179\n",
      "Iteration 6459 Loss: 0.07635283093390444\n",
      "Iteration 6460 Loss: 0.07635270414143956\n",
      "Iteration 6461 Loss: 0.07635257735679554\n",
      "Iteration 6462 Loss: 0.07635245057997081\n",
      "Iteration 6463 Loss: 0.07635232381096374\n",
      "Iteration 6464 Loss: 0.07635219704977275\n",
      "Iteration 6465 Loss: 0.07635207029639635\n",
      "Iteration 6466 Loss: 0.07635194355083275\n",
      "Iteration 6467 Loss: 0.07635181681308051\n",
      "Iteration 6468 Loss: 0.07635169008313798\n",
      "Iteration 6469 Loss: 0.0763515633610036\n",
      "Iteration 6470 Loss: 0.07635143664667583\n",
      "Iteration 6471 Loss: 0.07635130994015302\n",
      "Iteration 6472 Loss: 0.07635118324143363\n",
      "Iteration 6473 Loss: 0.07635105655051612\n",
      "Iteration 6474 Loss: 0.07635092986739891\n",
      "Iteration 6475 Loss: 0.07635080319208037\n",
      "Iteration 6476 Loss: 0.07635067652455907\n",
      "Iteration 6477 Loss: 0.07635054986483332\n",
      "Iteration 6478 Loss: 0.07635042321290161\n",
      "Iteration 6479 Loss: 0.07635029656876244\n",
      "Iteration 6480 Loss: 0.0763501699324142\n",
      "Iteration 6481 Loss: 0.07635004330385536\n",
      "Iteration 6482 Loss: 0.07634991668308433\n",
      "Iteration 6483 Loss: 0.07634979007009966\n",
      "Iteration 6484 Loss: 0.0763496634648997\n",
      "Iteration 6485 Loss: 0.07634953686748307\n",
      "Iteration 6486 Loss: 0.0763494102778481\n",
      "Iteration 6487 Loss: 0.07634928369599331\n",
      "Iteration 6488 Loss: 0.07634915712191719\n",
      "Iteration 6489 Loss: 0.07634903055561808\n",
      "Iteration 6490 Loss: 0.07634890399709465\n",
      "Iteration 6491 Loss: 0.07634877744634527\n",
      "Iteration 6492 Loss: 0.07634865090336847\n",
      "Iteration 6493 Loss: 0.07634852436816274\n",
      "Iteration 6494 Loss: 0.07634839784072649\n",
      "Iteration 6495 Loss: 0.07634827132105823\n",
      "Iteration 6496 Loss: 0.07634814480915658\n",
      "Iteration 6497 Loss: 0.07634801830501989\n",
      "Iteration 6498 Loss: 0.0763478918086467\n",
      "Iteration 6499 Loss: 0.07634776532003552\n",
      "Iteration 6500 Loss: 0.0763476388391849\n",
      "Iteration 6501 Loss: 0.07634751236609326\n",
      "Iteration 6502 Loss: 0.07634738590075911\n",
      "Iteration 6503 Loss: 0.07634725944318106\n",
      "Iteration 6504 Loss: 0.07634713299335762\n",
      "Iteration 6505 Loss: 0.07634700655128718\n",
      "Iteration 6506 Loss: 0.07634688011696833\n",
      "Iteration 6507 Loss: 0.07634675369039963\n",
      "Iteration 6508 Loss: 0.07634662727157954\n",
      "Iteration 6509 Loss: 0.07634650086050661\n",
      "Iteration 6510 Loss: 0.07634637445717939\n",
      "Iteration 6511 Loss: 0.07634624806159641\n",
      "Iteration 6512 Loss: 0.07634612167375619\n",
      "Iteration 6513 Loss: 0.07634599529365729\n",
      "Iteration 6514 Loss: 0.07634586892129823\n",
      "Iteration 6515 Loss: 0.07634574255667752\n",
      "Iteration 6516 Loss: 0.07634561619979381\n",
      "Iteration 6517 Loss: 0.0763454898506455\n",
      "Iteration 6518 Loss: 0.07634536350923127\n",
      "Iteration 6519 Loss: 0.07634523717554961\n",
      "Iteration 6520 Loss: 0.07634511084959911\n",
      "Iteration 6521 Loss: 0.0763449845313783\n",
      "Iteration 6522 Loss: 0.07634485822088571\n",
      "Iteration 6523 Loss: 0.07634473191812\n",
      "Iteration 6524 Loss: 0.07634460562307964\n",
      "Iteration 6525 Loss: 0.07634447933576322\n",
      "Iteration 6526 Loss: 0.07634435305616934\n",
      "Iteration 6527 Loss: 0.07634422678429657\n",
      "Iteration 6528 Loss: 0.0763441005201434\n",
      "Iteration 6529 Loss: 0.0763439742637086\n",
      "Iteration 6530 Loss: 0.07634384801499056\n",
      "Iteration 6531 Loss: 0.07634372177398792\n",
      "Iteration 6532 Loss: 0.07634359554069929\n",
      "Iteration 6533 Loss: 0.07634346931512327\n",
      "Iteration 6534 Loss: 0.0763433430972584\n",
      "Iteration 6535 Loss: 0.0763432168871033\n",
      "Iteration 6536 Loss: 0.0763430906846566\n",
      "Iteration 6537 Loss: 0.07634296448991681\n",
      "Iteration 6538 Loss: 0.07634283830288265\n",
      "Iteration 6539 Loss: 0.07634271212355259\n",
      "Iteration 6540 Loss: 0.07634258595192532\n",
      "Iteration 6541 Loss: 0.07634245978799939\n",
      "Iteration 6542 Loss: 0.07634233363177342\n",
      "Iteration 6543 Loss: 0.07634220748324613\n",
      "Iteration 6544 Loss: 0.07634208134241606\n",
      "Iteration 6545 Loss: 0.07634195520928175\n",
      "Iteration 6546 Loss: 0.07634182908384189\n",
      "Iteration 6547 Loss: 0.07634170296609515\n",
      "Iteration 6548 Loss: 0.07634157685604002\n",
      "Iteration 6549 Loss: 0.07634145075367525\n",
      "Iteration 6550 Loss: 0.07634132465899941\n",
      "Iteration 6551 Loss: 0.07634119857201117\n",
      "Iteration 6552 Loss: 0.07634107249270915\n",
      "Iteration 6553 Loss: 0.0763409464210919\n",
      "Iteration 6554 Loss: 0.07634082035715817\n",
      "Iteration 6555 Loss: 0.07634069430090655\n",
      "Iteration 6556 Loss: 0.0763405682523357\n",
      "Iteration 6557 Loss: 0.07634044221144427\n",
      "Iteration 6558 Loss: 0.0763403161782309\n",
      "Iteration 6559 Loss: 0.07634019015269419\n",
      "Iteration 6560 Loss: 0.07634006413483288\n",
      "Iteration 6561 Loss: 0.07633993812464555\n",
      "Iteration 6562 Loss: 0.0763398121221309\n",
      "Iteration 6563 Loss: 0.07633968612728755\n",
      "Iteration 6564 Loss: 0.07633956014011418\n",
      "Iteration 6565 Loss: 0.07633943416060944\n",
      "Iteration 6566 Loss: 0.07633930818877203\n",
      "Iteration 6567 Loss: 0.07633918222460062\n",
      "Iteration 6568 Loss: 0.07633905626809379\n",
      "Iteration 6569 Loss: 0.07633893031925036\n",
      "Iteration 6570 Loss: 0.07633880437806888\n",
      "Iteration 6571 Loss: 0.07633867844454804\n",
      "Iteration 6572 Loss: 0.07633855251868661\n",
      "Iteration 6573 Loss: 0.07633842660048319\n",
      "Iteration 6574 Loss: 0.07633830068993644\n",
      "Iteration 6575 Loss: 0.07633817478704506\n",
      "Iteration 6576 Loss: 0.07633804889180784\n",
      "Iteration 6577 Loss: 0.07633792300422335\n",
      "Iteration 6578 Loss: 0.07633779712429031\n",
      "Iteration 6579 Loss: 0.07633767125200748\n",
      "Iteration 6580 Loss: 0.07633754538737342\n",
      "Iteration 6581 Loss: 0.07633741953038699\n",
      "Iteration 6582 Loss: 0.07633729368104675\n",
      "Iteration 6583 Loss: 0.07633716783935149\n",
      "Iteration 6584 Loss: 0.0763370420052999\n",
      "Iteration 6585 Loss: 0.07633691617889063\n",
      "Iteration 6586 Loss: 0.07633679036012245\n",
      "Iteration 6587 Loss: 0.07633666454899413\n",
      "Iteration 6588 Loss: 0.07633653874550421\n",
      "Iteration 6589 Loss: 0.0763364129496515\n",
      "Iteration 6590 Loss: 0.07633628716143477\n",
      "Iteration 6591 Loss: 0.07633616138085265\n",
      "Iteration 6592 Loss: 0.07633603560790392\n",
      "Iteration 6593 Loss: 0.07633590984258729\n",
      "Iteration 6594 Loss: 0.07633578408490141\n",
      "Iteration 6595 Loss: 0.07633565833484524\n",
      "Iteration 6596 Loss: 0.07633553259241724\n",
      "Iteration 6597 Loss: 0.07633540685761622\n",
      "Iteration 6598 Loss: 0.07633528113044102\n",
      "Iteration 6599 Loss: 0.07633515541089024\n",
      "Iteration 6600 Loss: 0.0763350296989627\n",
      "Iteration 6601 Loss: 0.07633490399465714\n",
      "Iteration 6602 Loss: 0.07633477829797228\n",
      "Iteration 6603 Loss: 0.07633465260890684\n",
      "Iteration 6604 Loss: 0.07633452692745958\n",
      "Iteration 6605 Loss: 0.07633440125362935\n",
      "Iteration 6606 Loss: 0.07633427558741472\n",
      "Iteration 6607 Loss: 0.07633414992881456\n",
      "Iteration 6608 Loss: 0.07633402427782764\n",
      "Iteration 6609 Loss: 0.07633389863445264\n",
      "Iteration 6610 Loss: 0.07633377299868833\n",
      "Iteration 6611 Loss: 0.07633364737053353\n",
      "Iteration 6612 Loss: 0.07633352174998696\n",
      "Iteration 6613 Loss: 0.07633339613704741\n",
      "Iteration 6614 Loss: 0.07633327053171368\n",
      "Iteration 6615 Loss: 0.0763331449339844\n",
      "Iteration 6616 Loss: 0.07633301934385847\n",
      "Iteration 6617 Loss: 0.0763328937613346\n",
      "Iteration 6618 Loss: 0.07633276818641163\n",
      "Iteration 6619 Loss: 0.07633264261908831\n",
      "Iteration 6620 Loss: 0.07633251705936338\n",
      "Iteration 6621 Loss: 0.07633239150723564\n",
      "Iteration 6622 Loss: 0.07633226596270387\n",
      "Iteration 6623 Loss: 0.07633214042576687\n",
      "Iteration 6624 Loss: 0.07633201489642341\n",
      "Iteration 6625 Loss: 0.07633188937467233\n",
      "Iteration 6626 Loss: 0.07633176386051234\n",
      "Iteration 6627 Loss: 0.07633163835394229\n",
      "Iteration 6628 Loss: 0.07633151285496097\n",
      "Iteration 6629 Loss: 0.07633138736356714\n",
      "Iteration 6630 Loss: 0.07633126187975967\n",
      "Iteration 6631 Loss: 0.07633113640353724\n",
      "Iteration 6632 Loss: 0.07633101093489875\n",
      "Iteration 6633 Loss: 0.07633088547384298\n",
      "Iteration 6634 Loss: 0.07633076002036873\n",
      "Iteration 6635 Loss: 0.07633063457447484\n",
      "Iteration 6636 Loss: 0.07633050913616002\n",
      "Iteration 6637 Loss: 0.07633038370542326\n",
      "Iteration 6638 Loss: 0.0763302582822632\n",
      "Iteration 6639 Loss: 0.07633013286667875\n",
      "Iteration 6640 Loss: 0.07633000745866868\n",
      "Iteration 6641 Loss: 0.07632988205823185\n",
      "Iteration 6642 Loss: 0.07632975666536708\n",
      "Iteration 6643 Loss: 0.07632963128007311\n",
      "Iteration 6644 Loss: 0.07632950590234887\n",
      "Iteration 6645 Loss: 0.07632938053219313\n",
      "Iteration 6646 Loss: 0.07632925516960477\n",
      "Iteration 6647 Loss: 0.07632912981458259\n",
      "Iteration 6648 Loss: 0.07632900446712537\n",
      "Iteration 6649 Loss: 0.07632887912723202\n",
      "Iteration 6650 Loss: 0.07632875379490132\n",
      "Iteration 6651 Loss: 0.07632862847013214\n",
      "Iteration 6652 Loss: 0.07632850315292333\n",
      "Iteration 6653 Loss: 0.07632837784327372\n",
      "Iteration 6654 Loss: 0.07632825254118215\n",
      "Iteration 6655 Loss: 0.07632812724664748\n",
      "Iteration 6656 Loss: 0.07632800195966852\n",
      "Iteration 6657 Loss: 0.07632787668024414\n",
      "Iteration 6658 Loss: 0.07632775140837322\n",
      "Iteration 6659 Loss: 0.07632762614405453\n",
      "Iteration 6660 Loss: 0.07632750088728706\n",
      "Iteration 6661 Loss: 0.07632737563806952\n",
      "Iteration 6662 Loss: 0.07632725039640084\n",
      "Iteration 6663 Loss: 0.07632712516227987\n",
      "Iteration 6664 Loss: 0.07632699993570549\n",
      "Iteration 6665 Loss: 0.07632687471667658\n",
      "Iteration 6666 Loss: 0.07632674950519192\n",
      "Iteration 6667 Loss: 0.07632662430125045\n",
      "Iteration 6668 Loss: 0.07632649910485101\n",
      "Iteration 6669 Loss: 0.07632637391599245\n",
      "Iteration 6670 Loss: 0.07632624873467372\n",
      "Iteration 6671 Loss: 0.07632612356089358\n",
      "Iteration 6672 Loss: 0.07632599839465098\n",
      "Iteration 6673 Loss: 0.07632587323594484\n",
      "Iteration 6674 Loss: 0.07632574808477394\n",
      "Iteration 6675 Loss: 0.07632562294113723\n",
      "Iteration 6676 Loss: 0.07632549780503355\n",
      "Iteration 6677 Loss: 0.07632537267646183\n",
      "Iteration 6678 Loss: 0.07632524755542086\n",
      "Iteration 6679 Loss: 0.07632512244190962\n",
      "Iteration 6680 Loss: 0.07632499733592699\n",
      "Iteration 6681 Loss: 0.07632487223747189\n",
      "Iteration 6682 Loss: 0.07632474714654308\n",
      "Iteration 6683 Loss: 0.07632462206313959\n",
      "Iteration 6684 Loss: 0.07632449698726024\n",
      "Iteration 6685 Loss: 0.07632437191890391\n",
      "Iteration 6686 Loss: 0.0763242468580697\n",
      "Iteration 6687 Loss: 0.0763241218047562\n",
      "Iteration 6688 Loss: 0.07632399675896255\n",
      "Iteration 6689 Loss: 0.07632387172068751\n",
      "Iteration 6690 Loss: 0.07632374668993012\n",
      "Iteration 6691 Loss: 0.07632362166668917\n",
      "Iteration 6692 Loss: 0.07632349665096365\n",
      "Iteration 6693 Loss: 0.07632337164275241\n",
      "Iteration 6694 Loss: 0.0763232466420544\n",
      "Iteration 6695 Loss: 0.07632312164886854\n",
      "Iteration 6696 Loss: 0.07632299666319366\n",
      "Iteration 6697 Loss: 0.07632287168502881\n",
      "Iteration 6698 Loss: 0.07632274671437288\n",
      "Iteration 6699 Loss: 0.07632262175122469\n",
      "Iteration 6700 Loss: 0.07632249679558326\n",
      "Iteration 6701 Loss: 0.07632237184744746\n",
      "Iteration 6702 Loss: 0.07632224690681627\n",
      "Iteration 6703 Loss: 0.0763221219736886\n",
      "Iteration 6704 Loss: 0.07632199704806336\n",
      "Iteration 6705 Loss: 0.07632187212993946\n",
      "Iteration 6706 Loss: 0.0763217472193159\n",
      "Iteration 6707 Loss: 0.07632162231619154\n",
      "Iteration 6708 Loss: 0.07632149742056535\n",
      "Iteration 6709 Loss: 0.07632137253243629\n",
      "Iteration 6710 Loss: 0.07632124765180322\n",
      "Iteration 6711 Loss: 0.0763211227786652\n",
      "Iteration 6712 Loss: 0.0763209979130211\n",
      "Iteration 6713 Loss: 0.07632087305486986\n",
      "Iteration 6714 Loss: 0.07632074820421045\n",
      "Iteration 6715 Loss: 0.07632062336104176\n",
      "Iteration 6716 Loss: 0.07632049852536284\n",
      "Iteration 6717 Loss: 0.07632037369717254\n",
      "Iteration 6718 Loss: 0.07632024887646983\n",
      "Iteration 6719 Loss: 0.07632012406325375\n",
      "Iteration 6720 Loss: 0.07631999925752317\n",
      "Iteration 6721 Loss: 0.07631987445927704\n",
      "Iteration 6722 Loss: 0.07631974966851436\n",
      "Iteration 6723 Loss: 0.07631962488523407\n",
      "Iteration 6724 Loss: 0.07631950010943514\n",
      "Iteration 6725 Loss: 0.07631937534111655\n",
      "Iteration 6726 Loss: 0.07631925058027722\n",
      "Iteration 6727 Loss: 0.07631912582691609\n",
      "Iteration 6728 Loss: 0.07631900108103222\n",
      "Iteration 6729 Loss: 0.0763188763426245\n",
      "Iteration 6730 Loss: 0.07631875161169195\n",
      "Iteration 6731 Loss: 0.07631862688823349\n",
      "Iteration 6732 Loss: 0.0763185021722482\n",
      "Iteration 6733 Loss: 0.07631837746373488\n",
      "Iteration 6734 Loss: 0.07631825276269262\n",
      "Iteration 6735 Loss: 0.07631812806912039\n",
      "Iteration 6736 Loss: 0.07631800338301722\n",
      "Iteration 6737 Loss: 0.07631787870438195\n",
      "Iteration 6738 Loss: 0.07631775403321368\n",
      "Iteration 6739 Loss: 0.07631762936951136\n",
      "Iteration 6740 Loss: 0.07631750471327395\n",
      "Iteration 6741 Loss: 0.07631738006450049\n",
      "Iteration 6742 Loss: 0.07631725542318991\n",
      "Iteration 6743 Loss: 0.07631713078934119\n",
      "Iteration 6744 Loss: 0.07631700616295334\n",
      "Iteration 6745 Loss: 0.07631688154402544\n",
      "Iteration 6746 Loss: 0.07631675693255638\n",
      "Iteration 6747 Loss: 0.07631663232854519\n",
      "Iteration 6748 Loss: 0.07631650773199085\n",
      "Iteration 6749 Loss: 0.07631638314289238\n",
      "Iteration 6750 Loss: 0.07631625856124873\n",
      "Iteration 6751 Loss: 0.07631613398705892\n",
      "Iteration 6752 Loss: 0.07631600942032205\n",
      "Iteration 6753 Loss: 0.076315884861037\n",
      "Iteration 6754 Loss: 0.07631576030920283\n",
      "Iteration 6755 Loss: 0.07631563576481852\n",
      "Iteration 6756 Loss: 0.0763155112278831\n",
      "Iteration 6757 Loss: 0.0763153866983956\n",
      "Iteration 6758 Loss: 0.07631526217635494\n",
      "Iteration 6759 Loss: 0.07631513766176032\n",
      "Iteration 6760 Loss: 0.07631501315461048\n",
      "Iteration 6761 Loss: 0.07631488865490467\n",
      "Iteration 6762 Loss: 0.07631476416264182\n",
      "Iteration 6763 Loss: 0.07631463967782094\n",
      "Iteration 6764 Loss: 0.07631451520044104\n",
      "Iteration 6765 Loss: 0.0763143907305012\n",
      "Iteration 6766 Loss: 0.07631426626800038\n",
      "Iteration 6767 Loss: 0.07631414181293762\n",
      "Iteration 6768 Loss: 0.0763140173653119\n",
      "Iteration 6769 Loss: 0.07631389292512239\n",
      "Iteration 6770 Loss: 0.07631376849236793\n",
      "Iteration 6771 Loss: 0.07631364406704769\n",
      "Iteration 6772 Loss: 0.0763135196491607\n",
      "Iteration 6773 Loss: 0.07631339523870585\n",
      "Iteration 6774 Loss: 0.07631327083568235\n",
      "Iteration 6775 Loss: 0.0763131464400891\n",
      "Iteration 6776 Loss: 0.07631302205192522\n",
      "Iteration 6777 Loss: 0.07631289767118968\n",
      "Iteration 6778 Loss: 0.07631277329788158\n",
      "Iteration 6779 Loss: 0.0763126489319999\n",
      "Iteration 6780 Loss: 0.07631252457354375\n",
      "Iteration 6781 Loss: 0.07631240022251216\n",
      "Iteration 6782 Loss: 0.07631227587890413\n",
      "Iteration 6783 Loss: 0.07631215154271868\n",
      "Iteration 6784 Loss: 0.07631202721395494\n",
      "Iteration 6785 Loss: 0.07631190289261189\n",
      "Iteration 6786 Loss: 0.07631177857868864\n",
      "Iteration 6787 Loss: 0.07631165427218424\n",
      "Iteration 6788 Loss: 0.0763115299730977\n",
      "Iteration 6789 Loss: 0.07631140568142804\n",
      "Iteration 6790 Loss: 0.07631128139717437\n",
      "Iteration 6791 Loss: 0.07631115712033577\n",
      "Iteration 6792 Loss: 0.07631103285091122\n",
      "Iteration 6793 Loss: 0.07631090858889988\n",
      "Iteration 6794 Loss: 0.07631078433430073\n",
      "Iteration 6795 Loss: 0.07631066008711286\n",
      "Iteration 6796 Loss: 0.07631053584733523\n",
      "Iteration 6797 Loss: 0.07631041161496707\n",
      "Iteration 6798 Loss: 0.07631028739000739\n",
      "Iteration 6799 Loss: 0.07631016317245522\n",
      "Iteration 6800 Loss: 0.07631003896230963\n",
      "Iteration 6801 Loss: 0.07630991475956976\n",
      "Iteration 6802 Loss: 0.07630979056423454\n",
      "Iteration 6803 Loss: 0.07630966637630317\n",
      "Iteration 6804 Loss: 0.0763095421957747\n",
      "Iteration 6805 Loss: 0.07630941802264819\n",
      "Iteration 6806 Loss: 0.0763092938569227\n",
      "Iteration 6807 Loss: 0.07630916969859729\n",
      "Iteration 6808 Loss: 0.07630904554767104\n",
      "Iteration 6809 Loss: 0.07630892140414314\n",
      "Iteration 6810 Loss: 0.0763087972680125\n",
      "Iteration 6811 Loss: 0.07630867313927828\n",
      "Iteration 6812 Loss: 0.07630854901793964\n",
      "Iteration 6813 Loss: 0.07630842490399548\n",
      "Iteration 6814 Loss: 0.07630830079744508\n",
      "Iteration 6815 Loss: 0.07630817669828742\n",
      "Iteration 6816 Loss: 0.07630805260652158\n",
      "Iteration 6817 Loss: 0.07630792852214667\n",
      "Iteration 6818 Loss: 0.07630780444516186\n",
      "Iteration 6819 Loss: 0.07630768037556611\n",
      "Iteration 6820 Loss: 0.07630755631335855\n",
      "Iteration 6821 Loss: 0.07630743225853834\n",
      "Iteration 6822 Loss: 0.07630730821110447\n",
      "Iteration 6823 Loss: 0.07630718417105614\n",
      "Iteration 6824 Loss: 0.07630706013839245\n",
      "Iteration 6825 Loss: 0.07630693611311239\n",
      "Iteration 6826 Loss: 0.07630681209521513\n",
      "Iteration 6827 Loss: 0.07630668808469976\n",
      "Iteration 6828 Loss: 0.07630656408156536\n",
      "Iteration 6829 Loss: 0.07630644008581113\n",
      "Iteration 6830 Loss: 0.07630631609743604\n",
      "Iteration 6831 Loss: 0.07630619211643927\n",
      "Iteration 6832 Loss: 0.07630606814281994\n",
      "Iteration 6833 Loss: 0.07630594417657713\n",
      "Iteration 6834 Loss: 0.07630582021770993\n",
      "Iteration 6835 Loss: 0.07630569626621748\n",
      "Iteration 6836 Loss: 0.07630557232209886\n",
      "Iteration 6837 Loss: 0.07630544838535322\n",
      "Iteration 6838 Loss: 0.07630532445597966\n",
      "Iteration 6839 Loss: 0.07630520053397735\n",
      "Iteration 6840 Loss: 0.07630507661934532\n",
      "Iteration 6841 Loss: 0.07630495271208265\n",
      "Iteration 6842 Loss: 0.07630482881218864\n",
      "Iteration 6843 Loss: 0.07630470491966221\n",
      "Iteration 6844 Loss: 0.0763045810345026\n",
      "Iteration 6845 Loss: 0.07630445715670892\n",
      "Iteration 6846 Loss: 0.07630433328628027\n",
      "Iteration 6847 Loss: 0.07630420942321575\n",
      "Iteration 6848 Loss: 0.07630408556751457\n",
      "Iteration 6849 Loss: 0.07630396171917575\n",
      "Iteration 6850 Loss: 0.07630383787819847\n",
      "Iteration 6851 Loss: 0.0763037140445819\n",
      "Iteration 6852 Loss: 0.0763035902183251\n",
      "Iteration 6853 Loss: 0.07630346639942723\n",
      "Iteration 6854 Loss: 0.07630334258788744\n",
      "Iteration 6855 Loss: 0.07630321878370482\n",
      "Iteration 6856 Loss: 0.07630309498687854\n",
      "Iteration 6857 Loss: 0.07630297119740774\n",
      "Iteration 6858 Loss: 0.07630284741529153\n",
      "Iteration 6859 Loss: 0.07630272364052909\n",
      "Iteration 6860 Loss: 0.07630259987311948\n",
      "Iteration 6861 Loss: 0.0763024761130619\n",
      "Iteration 6862 Loss: 0.07630235236035554\n",
      "Iteration 6863 Loss: 0.07630222861499943\n",
      "Iteration 6864 Loss: 0.07630210487699275\n",
      "Iteration 6865 Loss: 0.07630198114633471\n",
      "Iteration 6866 Loss: 0.07630185742302441\n",
      "Iteration 6867 Loss: 0.07630173370706093\n",
      "Iteration 6868 Loss: 0.07630160999844357\n",
      "Iteration 6869 Loss: 0.07630148629717133\n",
      "Iteration 6870 Loss: 0.07630136260324347\n",
      "Iteration 6871 Loss: 0.07630123891665906\n",
      "Iteration 6872 Loss: 0.07630111523741728\n",
      "Iteration 6873 Loss: 0.07630099156551727\n",
      "Iteration 6874 Loss: 0.07630086790095826\n",
      "Iteration 6875 Loss: 0.07630074424373931\n",
      "Iteration 6876 Loss: 0.07630062059385963\n",
      "Iteration 6877 Loss: 0.07630049695131837\n",
      "Iteration 6878 Loss: 0.07630037331611464\n",
      "Iteration 6879 Loss: 0.0763002496882477\n",
      "Iteration 6880 Loss: 0.07630012606771659\n",
      "Iteration 6881 Loss: 0.07630000245452057\n",
      "Iteration 6882 Loss: 0.07629987884865877\n",
      "Iteration 6883 Loss: 0.07629975525013032\n",
      "Iteration 6884 Loss: 0.07629963165893443\n",
      "Iteration 6885 Loss: 0.0762995080750703\n",
      "Iteration 6886 Loss: 0.07629938449853699\n",
      "Iteration 6887 Loss: 0.07629926092933376\n",
      "Iteration 6888 Loss: 0.07629913736745972\n",
      "Iteration 6889 Loss: 0.07629901381291405\n",
      "Iteration 6890 Loss: 0.07629889026569597\n",
      "Iteration 6891 Loss: 0.07629876672580463\n",
      "Iteration 6892 Loss: 0.0762986431932392\n",
      "Iteration 6893 Loss: 0.07629851966799879\n",
      "Iteration 6894 Loss: 0.07629839615008267\n",
      "Iteration 6895 Loss: 0.07629827263948993\n",
      "Iteration 6896 Loss: 0.07629814913621986\n",
      "Iteration 6897 Loss: 0.07629802564027159\n",
      "Iteration 6898 Loss: 0.0762979021516442\n",
      "Iteration 6899 Loss: 0.076297778670337\n",
      "Iteration 6900 Loss: 0.07629765519634918\n",
      "Iteration 6901 Loss: 0.07629753172967978\n",
      "Iteration 6902 Loss: 0.07629740827032813\n",
      "Iteration 6903 Loss: 0.07629728481829334\n",
      "Iteration 6904 Loss: 0.07629716137357462\n",
      "Iteration 6905 Loss: 0.07629703793617114\n",
      "Iteration 6906 Loss: 0.07629691450608206\n",
      "Iteration 6907 Loss: 0.07629679108330664\n",
      "Iteration 6908 Loss: 0.07629666766784403\n",
      "Iteration 6909 Loss: 0.07629654425969344\n",
      "Iteration 6910 Loss: 0.07629642085885402\n",
      "Iteration 6911 Loss: 0.076296297465325\n",
      "Iteration 6912 Loss: 0.07629617407910558\n",
      "Iteration 6913 Loss: 0.07629605070019489\n",
      "Iteration 6914 Loss: 0.0762959273285922\n",
      "Iteration 6915 Loss: 0.07629580396429667\n",
      "Iteration 6916 Loss: 0.07629568060730751\n",
      "Iteration 6917 Loss: 0.07629555725762392\n",
      "Iteration 6918 Loss: 0.07629543391524511\n",
      "Iteration 6919 Loss: 0.07629531058017021\n",
      "Iteration 6920 Loss: 0.07629518725239852\n",
      "Iteration 6921 Loss: 0.07629506393192916\n",
      "Iteration 6922 Loss: 0.0762949406187614\n",
      "Iteration 6923 Loss: 0.0762948173128944\n",
      "Iteration 6924 Loss: 0.07629469401432738\n",
      "Iteration 6925 Loss: 0.07629457072305956\n",
      "Iteration 6926 Loss: 0.07629444743909014\n",
      "Iteration 6927 Loss: 0.07629432416241826\n",
      "Iteration 6928 Loss: 0.07629420089304319\n",
      "Iteration 6929 Loss: 0.07629407763096419\n",
      "Iteration 6930 Loss: 0.07629395437618036\n",
      "Iteration 6931 Loss: 0.07629383112869104\n",
      "Iteration 6932 Loss: 0.07629370788849531\n",
      "Iteration 6933 Loss: 0.07629358465559247\n",
      "Iteration 6934 Loss: 0.07629346142998174\n",
      "Iteration 6935 Loss: 0.07629333821166227\n",
      "Iteration 6936 Loss: 0.0762932150006333\n",
      "Iteration 6937 Loss: 0.07629309179689407\n",
      "Iteration 6938 Loss: 0.07629296860044382\n",
      "Iteration 6939 Loss: 0.07629284541128172\n",
      "Iteration 6940 Loss: 0.076292722229407\n",
      "Iteration 6941 Loss: 0.07629259905481887\n",
      "Iteration 6942 Loss: 0.07629247588751659\n",
      "Iteration 6943 Loss: 0.07629235272749935\n",
      "Iteration 6944 Loss: 0.07629222957476639\n",
      "Iteration 6945 Loss: 0.07629210642931694\n",
      "Iteration 6946 Loss: 0.07629198329115021\n",
      "Iteration 6947 Loss: 0.07629186016026544\n",
      "Iteration 6948 Loss: 0.0762917370366618\n",
      "Iteration 6949 Loss: 0.07629161392033862\n",
      "Iteration 6950 Loss: 0.07629149081129506\n",
      "Iteration 6951 Loss: 0.07629136770953039\n",
      "Iteration 6952 Loss: 0.07629124461504377\n",
      "Iteration 6953 Loss: 0.07629112152783447\n",
      "Iteration 6954 Loss: 0.07629099844790174\n",
      "Iteration 6955 Loss: 0.07629087537524487\n",
      "Iteration 6956 Loss: 0.07629075230986299\n",
      "Iteration 6957 Loss: 0.07629062925175537\n",
      "Iteration 6958 Loss: 0.07629050620092126\n",
      "Iteration 6959 Loss: 0.07629038315735985\n",
      "Iteration 6960 Loss: 0.07629026012107046\n",
      "Iteration 6961 Loss: 0.07629013709205223\n",
      "Iteration 6962 Loss: 0.0762900140703045\n",
      "Iteration 6963 Loss: 0.07628989105582644\n",
      "Iteration 6964 Loss: 0.0762897680486174\n",
      "Iteration 6965 Loss: 0.0762896450486764\n",
      "Iteration 6966 Loss: 0.07628952205600287\n",
      "Iteration 6967 Loss: 0.07628939907059604\n",
      "Iteration 6968 Loss: 0.07628927609245509\n",
      "Iteration 6969 Loss: 0.0762891531215793\n",
      "Iteration 6970 Loss: 0.07628903015796795\n",
      "Iteration 6971 Loss: 0.07628890720162022\n",
      "Iteration 6972 Loss: 0.07628878425253537\n",
      "Iteration 6973 Loss: 0.07628866131071269\n",
      "Iteration 6974 Loss: 0.07628853837615139\n",
      "Iteration 6975 Loss: 0.07628841544885072\n",
      "Iteration 6976 Loss: 0.07628829252880999\n",
      "Iteration 6977 Loss: 0.07628816961602836\n",
      "Iteration 6978 Loss: 0.07628804671050521\n",
      "Iteration 6979 Loss: 0.07628792381223966\n",
      "Iteration 6980 Loss: 0.07628780092123102\n",
      "Iteration 6981 Loss: 0.07628767803747862\n",
      "Iteration 6982 Loss: 0.07628755516098158\n",
      "Iteration 6983 Loss: 0.07628743229173923\n",
      "Iteration 6984 Loss: 0.0762873094297508\n",
      "Iteration 6985 Loss: 0.07628718657501561\n",
      "Iteration 6986 Loss: 0.07628706372753286\n",
      "Iteration 6987 Loss: 0.07628694088730188\n",
      "Iteration 6988 Loss: 0.07628681805432186\n",
      "Iteration 6989 Loss: 0.07628669522859206\n",
      "Iteration 6990 Loss: 0.0762865724101118\n",
      "Iteration 6991 Loss: 0.07628644959888031\n",
      "Iteration 6992 Loss: 0.0762863267948968\n",
      "Iteration 6993 Loss: 0.07628620399816066\n",
      "Iteration 6994 Loss: 0.07628608120867107\n",
      "Iteration 6995 Loss: 0.07628595842642735\n",
      "Iteration 6996 Loss: 0.07628583565142867\n",
      "Iteration 6997 Loss: 0.07628571288367438\n",
      "Iteration 6998 Loss: 0.07628559012316379\n",
      "Iteration 6999 Loss: 0.07628546736989607\n",
      "Iteration 7000 Loss: 0.07628534462387053\n",
      "Iteration 7001 Loss: 0.07628522188508648\n",
      "Iteration 7002 Loss: 0.0762850991535431\n",
      "Iteration 7003 Loss: 0.0762849764292398\n",
      "Iteration 7004 Loss: 0.07628485371217576\n",
      "Iteration 7005 Loss: 0.07628473100235023\n",
      "Iteration 7006 Loss: 0.07628460829976255\n",
      "Iteration 7007 Loss: 0.07628448560441202\n",
      "Iteration 7008 Loss: 0.07628436291629784\n",
      "Iteration 7009 Loss: 0.07628424023541931\n",
      "Iteration 7010 Loss: 0.07628411756177575\n",
      "Iteration 7011 Loss: 0.07628399489536639\n",
      "Iteration 7012 Loss: 0.07628387223619054\n",
      "Iteration 7013 Loss: 0.07628374958424747\n",
      "Iteration 7014 Loss: 0.07628362693953647\n",
      "Iteration 7015 Loss: 0.07628350430205687\n",
      "Iteration 7016 Loss: 0.07628338167180783\n",
      "Iteration 7017 Loss: 0.07628325904878873\n",
      "Iteration 7018 Loss: 0.07628313643299879\n",
      "Iteration 7019 Loss: 0.07628301382443739\n",
      "Iteration 7020 Loss: 0.07628289122310378\n",
      "Iteration 7021 Loss: 0.07628276862899724\n",
      "Iteration 7022 Loss: 0.07628264604211699\n",
      "Iteration 7023 Loss: 0.07628252346246239\n",
      "Iteration 7024 Loss: 0.07628240089003277\n",
      "Iteration 7025 Loss: 0.07628227832482735\n",
      "Iteration 7026 Loss: 0.07628215576684547\n",
      "Iteration 7027 Loss: 0.07628203321608636\n",
      "Iteration 7028 Loss: 0.07628191067254934\n",
      "Iteration 7029 Loss: 0.07628178813623374\n",
      "Iteration 7030 Loss: 0.07628166560713882\n",
      "Iteration 7031 Loss: 0.07628154308526386\n",
      "Iteration 7032 Loss: 0.0762814205706082\n",
      "Iteration 7033 Loss: 0.07628129806317109\n",
      "Iteration 7034 Loss: 0.07628117556295186\n",
      "Iteration 7035 Loss: 0.0762810530699498\n",
      "Iteration 7036 Loss: 0.07628093058416424\n",
      "Iteration 7037 Loss: 0.07628080810559443\n",
      "Iteration 7038 Loss: 0.07628068563423966\n",
      "Iteration 7039 Loss: 0.0762805631700993\n",
      "Iteration 7040 Loss: 0.07628044071317254\n",
      "Iteration 7041 Loss: 0.07628031826345878\n",
      "Iteration 7042 Loss: 0.07628019582095737\n",
      "Iteration 7043 Loss: 0.0762800733856674\n",
      "Iteration 7044 Loss: 0.07627995095758841\n",
      "Iteration 7045 Loss: 0.0762798285367196\n",
      "Iteration 7046 Loss: 0.07627970612306029\n",
      "Iteration 7047 Loss: 0.07627958371660976\n",
      "Iteration 7048 Loss: 0.07627946131736732\n",
      "Iteration 7049 Loss: 0.07627933892533233\n",
      "Iteration 7050 Loss: 0.07627921654050408\n",
      "Iteration 7051 Loss: 0.0762790941628818\n",
      "Iteration 7052 Loss: 0.0762789717924649\n",
      "Iteration 7053 Loss: 0.07627884942925268\n",
      "Iteration 7054 Loss: 0.07627872707324442\n",
      "Iteration 7055 Loss: 0.07627860472443945\n",
      "Iteration 7056 Loss: 0.07627848238283703\n",
      "Iteration 7057 Loss: 0.07627836004843655\n",
      "Iteration 7058 Loss: 0.07627823772123728\n",
      "Iteration 7059 Loss: 0.07627811540123858\n",
      "Iteration 7060 Loss: 0.07627799308843967\n",
      "Iteration 7061 Loss: 0.07627787078283996\n",
      "Iteration 7062 Loss: 0.07627774848443876\n",
      "Iteration 7063 Loss: 0.07627762619323532\n",
      "Iteration 7064 Loss: 0.07627750390922905\n",
      "Iteration 7065 Loss: 0.07627738163241919\n",
      "Iteration 7066 Loss: 0.07627725936280508\n",
      "Iteration 7067 Loss: 0.07627713710038607\n",
      "Iteration 7068 Loss: 0.07627701484516147\n",
      "Iteration 7069 Loss: 0.0762768925971306\n",
      "Iteration 7070 Loss: 0.0762767703562927\n",
      "Iteration 7071 Loss: 0.07627664812264726\n",
      "Iteration 7072 Loss: 0.0762765258961935\n",
      "Iteration 7073 Loss: 0.07627640367693077\n",
      "Iteration 7074 Loss: 0.07627628146485839\n",
      "Iteration 7075 Loss: 0.07627615925997563\n",
      "Iteration 7076 Loss: 0.07627603706228189\n",
      "Iteration 7077 Loss: 0.07627591487177644\n",
      "Iteration 7078 Loss: 0.07627579268845872\n",
      "Iteration 7079 Loss: 0.07627567051232793\n",
      "Iteration 7080 Loss: 0.07627554834338351\n",
      "Iteration 7081 Loss: 0.07627542618162467\n",
      "Iteration 7082 Loss: 0.0762753040270508\n",
      "Iteration 7083 Loss: 0.07627518187966127\n",
      "Iteration 7084 Loss: 0.07627505973945536\n",
      "Iteration 7085 Loss: 0.07627493760643243\n",
      "Iteration 7086 Loss: 0.07627481548059178\n",
      "Iteration 7087 Loss: 0.07627469336193277\n",
      "Iteration 7088 Loss: 0.07627457125045471\n",
      "Iteration 7089 Loss: 0.07627444914615697\n",
      "Iteration 7090 Loss: 0.07627432704903891\n",
      "Iteration 7091 Loss: 0.0762742049590998\n",
      "Iteration 7092 Loss: 0.07627408287633894\n",
      "Iteration 7093 Loss: 0.07627396080075582\n",
      "Iteration 7094 Loss: 0.07627383873234965\n",
      "Iteration 7095 Loss: 0.07627371667111983\n",
      "Iteration 7096 Loss: 0.07627359461706562\n",
      "Iteration 7097 Loss: 0.07627347257018649\n",
      "Iteration 7098 Loss: 0.07627335053048169\n",
      "Iteration 7099 Loss: 0.07627322849795058\n",
      "Iteration 7100 Loss: 0.07627310647259247\n",
      "Iteration 7101 Loss: 0.07627298445440676\n",
      "Iteration 7102 Loss: 0.0762728624433928\n",
      "Iteration 7103 Loss: 0.07627274043954987\n",
      "Iteration 7104 Loss: 0.07627261844287733\n",
      "Iteration 7105 Loss: 0.07627249645337458\n",
      "Iteration 7106 Loss: 0.0762723744710409\n",
      "Iteration 7107 Loss: 0.07627225249587569\n",
      "Iteration 7108 Loss: 0.07627213052787823\n",
      "Iteration 7109 Loss: 0.076272008567048\n",
      "Iteration 7110 Loss: 0.07627188661338419\n",
      "Iteration 7111 Loss: 0.07627176466688623\n",
      "Iteration 7112 Loss: 0.07627164272755343\n",
      "Iteration 7113 Loss: 0.07627152079538523\n",
      "Iteration 7114 Loss: 0.07627139887038088\n",
      "Iteration 7115 Loss: 0.07627127695253982\n",
      "Iteration 7116 Loss: 0.07627115504186127\n",
      "Iteration 7117 Loss: 0.07627103313834474\n",
      "Iteration 7118 Loss: 0.07627091124198944\n",
      "Iteration 7119 Loss: 0.07627078935279485\n",
      "Iteration 7120 Loss: 0.07627066747076022\n",
      "Iteration 7121 Loss: 0.07627054559588499\n",
      "Iteration 7122 Loss: 0.07627042372816846\n",
      "Iteration 7123 Loss: 0.07627030186760998\n",
      "Iteration 7124 Loss: 0.07627018001420896\n",
      "Iteration 7125 Loss: 0.07627005816796474\n",
      "Iteration 7126 Loss: 0.07626993632887659\n",
      "Iteration 7127 Loss: 0.07626981449694405\n",
      "Iteration 7128 Loss: 0.07626969267216631\n",
      "Iteration 7129 Loss: 0.07626957085454283\n",
      "Iteration 7130 Loss: 0.07626944904407285\n",
      "Iteration 7131 Loss: 0.07626932724075593\n",
      "Iteration 7132 Loss: 0.07626920544459125\n",
      "Iteration 7133 Loss: 0.07626908365557825\n",
      "Iteration 7134 Loss: 0.07626896187371628\n",
      "Iteration 7135 Loss: 0.07626884009900473\n",
      "Iteration 7136 Loss: 0.07626871833144286\n",
      "Iteration 7137 Loss: 0.07626859657103019\n",
      "Iteration 7138 Loss: 0.076268474817766\n",
      "Iteration 7139 Loss: 0.07626835307164966\n",
      "Iteration 7140 Loss: 0.07626823133268053\n",
      "Iteration 7141 Loss: 0.076268109600858\n",
      "Iteration 7142 Loss: 0.07626798787618139\n",
      "Iteration 7143 Loss: 0.07626786615865014\n",
      "Iteration 7144 Loss: 0.07626774444826351\n",
      "Iteration 7145 Loss: 0.07626762274502101\n",
      "Iteration 7146 Loss: 0.07626750104892191\n",
      "Iteration 7147 Loss: 0.07626737935996564\n",
      "Iteration 7148 Loss: 0.07626725767815148\n",
      "Iteration 7149 Loss: 0.07626713600347887\n",
      "Iteration 7150 Loss: 0.07626701433594724\n",
      "Iteration 7151 Loss: 0.07626689267555588\n",
      "Iteration 7152 Loss: 0.07626677102230411\n",
      "Iteration 7153 Loss: 0.07626664937619142\n",
      "Iteration 7154 Loss: 0.07626652773721714\n",
      "Iteration 7155 Loss: 0.0762664061053806\n",
      "Iteration 7156 Loss: 0.07626628448068125\n",
      "Iteration 7157 Loss: 0.07626616286311844\n",
      "Iteration 7158 Loss: 0.07626604125269147\n",
      "Iteration 7159 Loss: 0.07626591964939984\n",
      "Iteration 7160 Loss: 0.07626579805324285\n",
      "Iteration 7161 Loss: 0.07626567646421993\n",
      "Iteration 7162 Loss: 0.07626555488233039\n",
      "Iteration 7163 Loss: 0.07626543330757368\n",
      "Iteration 7164 Loss: 0.07626531173994913\n",
      "Iteration 7165 Loss: 0.07626519017945613\n",
      "Iteration 7166 Loss: 0.07626506862609407\n",
      "Iteration 7167 Loss: 0.07626494707986234\n",
      "Iteration 7168 Loss: 0.07626482554076032\n",
      "Iteration 7169 Loss: 0.07626470400878732\n",
      "Iteration 7170 Loss: 0.07626458248394283\n",
      "Iteration 7171 Loss: 0.0762644609662262\n",
      "Iteration 7172 Loss: 0.0762643394556368\n",
      "Iteration 7173 Loss: 0.076264217952174\n",
      "Iteration 7174 Loss: 0.07626409645583718\n",
      "Iteration 7175 Loss: 0.07626397496662578\n",
      "Iteration 7176 Loss: 0.07626385348453912\n",
      "Iteration 7177 Loss: 0.07626373200957665\n",
      "Iteration 7178 Loss: 0.07626361054173772\n",
      "Iteration 7179 Loss: 0.07626348908102171\n",
      "Iteration 7180 Loss: 0.07626336762742802\n",
      "Iteration 7181 Loss: 0.07626324618095605\n",
      "Iteration 7182 Loss: 0.07626312474160522\n",
      "Iteration 7183 Loss: 0.0762630033093748\n",
      "Iteration 7184 Loss: 0.07626288188426429\n",
      "Iteration 7185 Loss: 0.07626276046627303\n",
      "Iteration 7186 Loss: 0.07626263905540044\n",
      "Iteration 7187 Loss: 0.07626251765164593\n",
      "Iteration 7188 Loss: 0.07626239625500887\n",
      "Iteration 7189 Loss: 0.07626227486548859\n",
      "Iteration 7190 Loss: 0.07626215348308457\n",
      "Iteration 7191 Loss: 0.07626203210779618\n",
      "Iteration 7192 Loss: 0.07626191073962284\n",
      "Iteration 7193 Loss: 0.07626178937856384\n",
      "Iteration 7194 Loss: 0.0762616680246187\n",
      "Iteration 7195 Loss: 0.07626154667778674\n",
      "Iteration 7196 Loss: 0.07626142533806737\n",
      "Iteration 7197 Loss: 0.07626130400546002\n",
      "Iteration 7198 Loss: 0.07626118267996404\n",
      "Iteration 7199 Loss: 0.07626106136157887\n",
      "Iteration 7200 Loss: 0.07626094005030389\n",
      "Iteration 7201 Loss: 0.07626081874613849\n",
      "Iteration 7202 Loss: 0.07626069744908204\n",
      "Iteration 7203 Loss: 0.07626057615913404\n",
      "Iteration 7204 Loss: 0.07626045487629379\n",
      "Iteration 7205 Loss: 0.07626033360056077\n",
      "Iteration 7206 Loss: 0.07626021233193431\n",
      "Iteration 7207 Loss: 0.07626009107041383\n",
      "Iteration 7208 Loss: 0.07625996981599871\n",
      "Iteration 7209 Loss: 0.07625984856868845\n",
      "Iteration 7210 Loss: 0.0762597273284824\n",
      "Iteration 7211 Loss: 0.07625960609537986\n",
      "Iteration 7212 Loss: 0.0762594848693804\n",
      "Iteration 7213 Loss: 0.07625936365048333\n",
      "Iteration 7214 Loss: 0.07625924243868809\n",
      "Iteration 7215 Loss: 0.07625912123399409\n",
      "Iteration 7216 Loss: 0.07625900003640065\n",
      "Iteration 7217 Loss: 0.07625887884590726\n",
      "Iteration 7218 Loss: 0.07625875766251337\n",
      "Iteration 7219 Loss: 0.07625863648621825\n",
      "Iteration 7220 Loss: 0.07625851531702145\n",
      "Iteration 7221 Loss: 0.07625839415492228\n",
      "Iteration 7222 Loss: 0.07625827299992019\n",
      "Iteration 7223 Loss: 0.07625815185201458\n",
      "Iteration 7224 Loss: 0.07625803071120486\n",
      "Iteration 7225 Loss: 0.07625790957749046\n",
      "Iteration 7226 Loss: 0.07625778845087074\n",
      "Iteration 7227 Loss: 0.07625766733134519\n",
      "Iteration 7228 Loss: 0.07625754621891316\n",
      "Iteration 7229 Loss: 0.07625742511357406\n",
      "Iteration 7230 Loss: 0.07625730401532732\n",
      "Iteration 7231 Loss: 0.07625718292417234\n",
      "Iteration 7232 Loss: 0.07625706184010864\n",
      "Iteration 7233 Loss: 0.07625694076313544\n",
      "Iteration 7234 Loss: 0.07625681969325232\n",
      "Iteration 7235 Loss: 0.07625669863045861\n",
      "Iteration 7236 Loss: 0.0762565775747537\n",
      "Iteration 7237 Loss: 0.07625645652613709\n",
      "Iteration 7238 Loss: 0.07625633548460821\n",
      "Iteration 7239 Loss: 0.07625621445016638\n",
      "Iteration 7240 Loss: 0.07625609342281103\n",
      "Iteration 7241 Loss: 0.07625597240254164\n",
      "Iteration 7242 Loss: 0.07625585138935764\n",
      "Iteration 7243 Loss: 0.07625573038325832\n",
      "Iteration 7244 Loss: 0.07625560938424322\n",
      "Iteration 7245 Loss: 0.07625548839231176\n",
      "Iteration 7246 Loss: 0.07625536740746333\n",
      "Iteration 7247 Loss: 0.07625524642969732\n",
      "Iteration 7248 Loss: 0.07625512545901315\n",
      "Iteration 7249 Loss: 0.07625500449541033\n",
      "Iteration 7250 Loss: 0.07625488353888817\n",
      "Iteration 7251 Loss: 0.07625476258944613\n",
      "Iteration 7252 Loss: 0.07625464164708368\n",
      "Iteration 7253 Loss: 0.07625452071180021\n",
      "Iteration 7254 Loss: 0.07625439978359515\n",
      "Iteration 7255 Loss: 0.07625427886246786\n",
      "Iteration 7256 Loss: 0.07625415794841785\n",
      "Iteration 7257 Loss: 0.07625403704144451\n",
      "Iteration 7258 Loss: 0.0762539161415473\n",
      "Iteration 7259 Loss: 0.07625379524872557\n",
      "Iteration 7260 Loss: 0.07625367436297883\n",
      "Iteration 7261 Loss: 0.07625355348430643\n",
      "Iteration 7262 Loss: 0.07625343261270787\n",
      "Iteration 7263 Loss: 0.07625331174818252\n",
      "Iteration 7264 Loss: 0.07625319089072981\n",
      "Iteration 7265 Loss: 0.07625307004034922\n",
      "Iteration 7266 Loss: 0.0762529491970401\n",
      "Iteration 7267 Loss: 0.07625282836080201\n",
      "Iteration 7268 Loss: 0.07625270753163424\n",
      "Iteration 7269 Loss: 0.07625258670953623\n",
      "Iteration 7270 Loss: 0.07625246589450756\n",
      "Iteration 7271 Loss: 0.07625234508654748\n",
      "Iteration 7272 Loss: 0.07625222428565553\n",
      "Iteration 7273 Loss: 0.0762521034918311\n",
      "Iteration 7274 Loss: 0.07625198270507362\n",
      "Iteration 7275 Loss: 0.07625186192538254\n",
      "Iteration 7276 Loss: 0.07625174115275729\n",
      "Iteration 7277 Loss: 0.07625162038719728\n",
      "Iteration 7278 Loss: 0.07625149962870201\n",
      "Iteration 7279 Loss: 0.07625137887727083\n",
      "Iteration 7280 Loss: 0.0762512581329032\n",
      "Iteration 7281 Loss: 0.07625113739559858\n",
      "Iteration 7282 Loss: 0.07625101666535643\n",
      "Iteration 7283 Loss: 0.0762508959421761\n",
      "Iteration 7284 Loss: 0.07625077522605707\n",
      "Iteration 7285 Loss: 0.0762506545169988\n",
      "Iteration 7286 Loss: 0.07625053381500074\n",
      "Iteration 7287 Loss: 0.07625041312006223\n",
      "Iteration 7288 Loss: 0.0762502924321828\n",
      "Iteration 7289 Loss: 0.07625017175136187\n",
      "Iteration 7290 Loss: 0.07625005107759884\n",
      "Iteration 7291 Loss: 0.07624993041089322\n",
      "Iteration 7292 Loss: 0.0762498097512444\n",
      "Iteration 7293 Loss: 0.07624968909865178\n",
      "Iteration 7294 Loss: 0.0762495684531149\n",
      "Iteration 7295 Loss: 0.07624944781463316\n",
      "Iteration 7296 Loss: 0.07624932718320591\n",
      "Iteration 7297 Loss: 0.07624920655883272\n",
      "Iteration 7298 Loss: 0.07624908594151304\n",
      "Iteration 7299 Loss: 0.07624896533124619\n",
      "Iteration 7300 Loss: 0.07624884472803169\n",
      "Iteration 7301 Loss: 0.07624872413186899\n",
      "Iteration 7302 Loss: 0.07624860354275746\n",
      "Iteration 7303 Loss: 0.07624848296069663\n",
      "Iteration 7304 Loss: 0.07624836238568591\n",
      "Iteration 7305 Loss: 0.07624824181772477\n",
      "Iteration 7306 Loss: 0.0762481212568126\n",
      "Iteration 7307 Loss: 0.07624800070294889\n",
      "Iteration 7308 Loss: 0.0762478801561331\n",
      "Iteration 7309 Loss: 0.0762477596163646\n",
      "Iteration 7310 Loss: 0.0762476390836429\n",
      "Iteration 7311 Loss: 0.07624751855796745\n",
      "Iteration 7312 Loss: 0.07624739803933762\n",
      "Iteration 7313 Loss: 0.07624727752775301\n",
      "Iteration 7314 Loss: 0.07624715702321291\n",
      "Iteration 7315 Loss: 0.07624703652571685\n",
      "Iteration 7316 Loss: 0.07624691603526428\n",
      "Iteration 7317 Loss: 0.07624679555185455\n",
      "Iteration 7318 Loss: 0.07624667507548727\n",
      "Iteration 7319 Loss: 0.07624655460616182\n",
      "Iteration 7320 Loss: 0.07624643414387765\n",
      "Iteration 7321 Loss: 0.07624631368863415\n",
      "Iteration 7322 Loss: 0.07624619324043086\n",
      "Iteration 7323 Loss: 0.07624607279926715\n",
      "Iteration 7324 Loss: 0.07624595236514259\n",
      "Iteration 7325 Loss: 0.07624583193805651\n",
      "Iteration 7326 Loss: 0.07624571151800841\n",
      "Iteration 7327 Loss: 0.07624559110499776\n",
      "Iteration 7328 Loss: 0.07624547069902399\n",
      "Iteration 7329 Loss: 0.07624535030008656\n",
      "Iteration 7330 Loss: 0.07624522990818487\n",
      "Iteration 7331 Loss: 0.07624510952331857\n",
      "Iteration 7332 Loss: 0.07624498914548684\n",
      "Iteration 7333 Loss: 0.07624486877468932\n",
      "Iteration 7334 Loss: 0.07624474841092542\n",
      "Iteration 7335 Loss: 0.07624462805419459\n",
      "Iteration 7336 Loss: 0.07624450770449628\n",
      "Iteration 7337 Loss: 0.07624438736183001\n",
      "Iteration 7338 Loss: 0.07624426702619509\n",
      "Iteration 7339 Loss: 0.0762441466975911\n",
      "Iteration 7340 Loss: 0.07624402637601746\n",
      "Iteration 7341 Loss: 0.0762439060614736\n",
      "Iteration 7342 Loss: 0.07624378575395908\n",
      "Iteration 7343 Loss: 0.0762436654534733\n",
      "Iteration 7344 Loss: 0.07624354516001568\n",
      "Iteration 7345 Loss: 0.07624342487358575\n",
      "Iteration 7346 Loss: 0.07624330459418285\n",
      "Iteration 7347 Loss: 0.07624318432180656\n",
      "Iteration 7348 Loss: 0.07624306405645631\n",
      "Iteration 7349 Loss: 0.07624294379813153\n",
      "Iteration 7350 Loss: 0.07624282354683172\n",
      "Iteration 7351 Loss: 0.07624270330255632\n",
      "Iteration 7352 Loss: 0.07624258306530479\n",
      "Iteration 7353 Loss: 0.07624246283507662\n",
      "Iteration 7354 Loss: 0.07624234261187118\n",
      "Iteration 7355 Loss: 0.0762422223956881\n",
      "Iteration 7356 Loss: 0.0762421021865267\n",
      "Iteration 7357 Loss: 0.0762419819843865\n",
      "Iteration 7358 Loss: 0.07624186178926697\n",
      "Iteration 7359 Loss: 0.07624174160116755\n",
      "Iteration 7360 Loss: 0.07624162142008767\n",
      "Iteration 7361 Loss: 0.07624150124602688\n",
      "Iteration 7362 Loss: 0.07624138107898461\n",
      "Iteration 7363 Loss: 0.07624126091896034\n",
      "Iteration 7364 Loss: 0.0762411407659535\n",
      "Iteration 7365 Loss: 0.07624102061996352\n",
      "Iteration 7366 Loss: 0.07624090048098997\n",
      "Iteration 7367 Loss: 0.07624078034903226\n",
      "Iteration 7368 Loss: 0.07624066022408986\n",
      "Iteration 7369 Loss: 0.07624054010616223\n",
      "Iteration 7370 Loss: 0.07624041999524891\n",
      "Iteration 7371 Loss: 0.07624029989134928\n",
      "Iteration 7372 Loss: 0.07624017979446285\n",
      "Iteration 7373 Loss: 0.07624005970458902\n",
      "Iteration 7374 Loss: 0.07623993962172732\n",
      "Iteration 7375 Loss: 0.07623981954587726\n",
      "Iteration 7376 Loss: 0.07623969947703825\n",
      "Iteration 7377 Loss: 0.07623957941520977\n",
      "Iteration 7378 Loss: 0.07623945936039131\n",
      "Iteration 7379 Loss: 0.07623933931258232\n",
      "Iteration 7380 Loss: 0.07623921927178232\n",
      "Iteration 7381 Loss: 0.07623909923799067\n",
      "Iteration 7382 Loss: 0.07623897921120694\n",
      "Iteration 7383 Loss: 0.07623885919143064\n",
      "Iteration 7384 Loss: 0.07623873917866114\n",
      "Iteration 7385 Loss: 0.07623861917289793\n",
      "Iteration 7386 Loss: 0.07623849917414051\n",
      "Iteration 7387 Loss: 0.07623837918238836\n",
      "Iteration 7388 Loss: 0.07623825919764093\n",
      "Iteration 7389 Loss: 0.07623813921989771\n",
      "Iteration 7390 Loss: 0.07623801924915818\n",
      "Iteration 7391 Loss: 0.07623789928542181\n",
      "Iteration 7392 Loss: 0.07623777932868808\n",
      "Iteration 7393 Loss: 0.07623765937895648\n",
      "Iteration 7394 Loss: 0.0762375394362264\n",
      "Iteration 7395 Loss: 0.07623741950049741\n",
      "Iteration 7396 Loss: 0.07623729957176899\n",
      "Iteration 7397 Loss: 0.07623717965004055\n",
      "Iteration 7398 Loss: 0.07623705973531164\n",
      "Iteration 7399 Loss: 0.07623693982758169\n",
      "Iteration 7400 Loss: 0.07623681992685012\n",
      "Iteration 7401 Loss: 0.07623670003311654\n",
      "Iteration 7402 Loss: 0.07623658014638036\n",
      "Iteration 7403 Loss: 0.07623646026664109\n",
      "Iteration 7404 Loss: 0.07623634039389814\n",
      "Iteration 7405 Loss: 0.07623622052815104\n",
      "Iteration 7406 Loss: 0.07623610066939922\n",
      "Iteration 7407 Loss: 0.07623598081764228\n",
      "Iteration 7408 Loss: 0.07623586097287954\n",
      "Iteration 7409 Loss: 0.07623574113511065\n",
      "Iteration 7410 Loss: 0.07623562130433496\n",
      "Iteration 7411 Loss: 0.07623550148055205\n",
      "Iteration 7412 Loss: 0.07623538166376127\n",
      "Iteration 7413 Loss: 0.07623526185396216\n",
      "Iteration 7414 Loss: 0.07623514205115432\n",
      "Iteration 7415 Loss: 0.07623502225533707\n",
      "Iteration 7416 Loss: 0.07623490246650996\n",
      "Iteration 7417 Loss: 0.07623478268467246\n",
      "Iteration 7418 Loss: 0.07623466290982411\n",
      "Iteration 7419 Loss: 0.07623454314196429\n",
      "Iteration 7420 Loss: 0.07623442338109257\n",
      "Iteration 7421 Loss: 0.07623430362720841\n",
      "Iteration 7422 Loss: 0.07623418388031125\n",
      "Iteration 7423 Loss: 0.07623406414040064\n",
      "Iteration 7424 Loss: 0.07623394440747609\n",
      "Iteration 7425 Loss: 0.076233824681537\n",
      "Iteration 7426 Loss: 0.07623370496258286\n",
      "Iteration 7427 Loss: 0.07623358525061319\n",
      "Iteration 7428 Loss: 0.07623346554562752\n",
      "Iteration 7429 Loss: 0.07623334584762526\n",
      "Iteration 7430 Loss: 0.0762332261566059\n",
      "Iteration 7431 Loss: 0.07623310647256899\n",
      "Iteration 7432 Loss: 0.07623298679551402\n",
      "Iteration 7433 Loss: 0.07623286712544036\n",
      "Iteration 7434 Loss: 0.07623274746234764\n",
      "Iteration 7435 Loss: 0.07623262780623526\n",
      "Iteration 7436 Loss: 0.07623250815710275\n",
      "Iteration 7437 Loss: 0.07623238851494958\n",
      "Iteration 7438 Loss: 0.07623226887977524\n",
      "Iteration 7439 Loss: 0.07623214925157919\n",
      "Iteration 7440 Loss: 0.07623202963036102\n",
      "Iteration 7441 Loss: 0.07623191001612013\n",
      "Iteration 7442 Loss: 0.07623179040885603\n",
      "Iteration 7443 Loss: 0.07623167080856817\n",
      "Iteration 7444 Loss: 0.07623155121525611\n",
      "Iteration 7445 Loss: 0.07623143162891939\n",
      "Iteration 7446 Loss: 0.07623131204955733\n",
      "Iteration 7447 Loss: 0.07623119247716957\n",
      "Iteration 7448 Loss: 0.07623107291175554\n",
      "Iteration 7449 Loss: 0.07623095335331477\n",
      "Iteration 7450 Loss: 0.07623083380184673\n",
      "Iteration 7451 Loss: 0.0762307142573509\n",
      "Iteration 7452 Loss: 0.0762305947198268\n",
      "Iteration 7453 Loss: 0.07623047518927387\n",
      "Iteration 7454 Loss: 0.07623035566569164\n",
      "Iteration 7455 Loss: 0.07623023614907964\n",
      "Iteration 7456 Loss: 0.07623011663943731\n",
      "Iteration 7457 Loss: 0.07622999713676415\n",
      "Iteration 7458 Loss: 0.07622987764105973\n",
      "Iteration 7459 Loss: 0.07622975815232341\n",
      "Iteration 7460 Loss: 0.07622963867055482\n",
      "Iteration 7461 Loss: 0.0762295191957534\n",
      "Iteration 7462 Loss: 0.07622939972791859\n",
      "Iteration 7463 Loss: 0.07622928026705\n",
      "Iteration 7464 Loss: 0.07622916081314704\n",
      "Iteration 7465 Loss: 0.07622904136620924\n",
      "Iteration 7466 Loss: 0.07622892192623605\n",
      "Iteration 7467 Loss: 0.0762288024932271\n",
      "Iteration 7468 Loss: 0.07622868306718168\n",
      "Iteration 7469 Loss: 0.0762285636480995\n",
      "Iteration 7470 Loss: 0.07622844423597988\n",
      "Iteration 7471 Loss: 0.07622832483082245\n",
      "Iteration 7472 Loss: 0.07622820543262665\n",
      "Iteration 7473 Loss: 0.07622808604139199\n",
      "Iteration 7474 Loss: 0.07622796665711797\n",
      "Iteration 7475 Loss: 0.07622784727980407\n",
      "Iteration 7476 Loss: 0.07622772790944989\n",
      "Iteration 7477 Loss: 0.07622760854605475\n",
      "Iteration 7478 Loss: 0.07622748918961825\n",
      "Iteration 7479 Loss: 0.0762273698401399\n",
      "Iteration 7480 Loss: 0.07622725049761922\n",
      "Iteration 7481 Loss: 0.07622713116205565\n",
      "Iteration 7482 Loss: 0.07622701183344874\n",
      "Iteration 7483 Loss: 0.07622689251179794\n",
      "Iteration 7484 Loss: 0.07622677319710283\n",
      "Iteration 7485 Loss: 0.07622665388936284\n",
      "Iteration 7486 Loss: 0.07622653458857753\n",
      "Iteration 7487 Loss: 0.07622641529474633\n",
      "Iteration 7488 Loss: 0.07622629600786882\n",
      "Iteration 7489 Loss: 0.07622617672794445\n",
      "Iteration 7490 Loss: 0.07622605745497274\n",
      "Iteration 7491 Loss: 0.07622593818895322\n",
      "Iteration 7492 Loss: 0.07622581892988534\n",
      "Iteration 7493 Loss: 0.07622569967776864\n",
      "Iteration 7494 Loss: 0.07622558043260268\n",
      "Iteration 7495 Loss: 0.07622546119438683\n",
      "Iteration 7496 Loss: 0.07622534196312067\n",
      "Iteration 7497 Loss: 0.07622522273880375\n",
      "Iteration 7498 Loss: 0.07622510352143547\n",
      "Iteration 7499 Loss: 0.07622498431101543\n",
      "Iteration 7500 Loss: 0.0762248651075431\n",
      "Iteration 7501 Loss: 0.076224745911018\n",
      "Iteration 7502 Loss: 0.0762246267214396\n",
      "Iteration 7503 Loss: 0.07622450753880745\n",
      "Iteration 7504 Loss: 0.07622438836312107\n",
      "Iteration 7505 Loss: 0.07622426919437986\n",
      "Iteration 7506 Loss: 0.07622415003258344\n",
      "Iteration 7507 Loss: 0.07622403087773129\n",
      "Iteration 7508 Loss: 0.07622391172982292\n",
      "Iteration 7509 Loss: 0.07622379258885775\n",
      "Iteration 7510 Loss: 0.07622367345483541\n",
      "Iteration 7511 Loss: 0.07622355432775539\n",
      "Iteration 7512 Loss: 0.07622343520761714\n",
      "Iteration 7513 Loss: 0.07622331609442023\n",
      "Iteration 7514 Loss: 0.07622319698816414\n",
      "Iteration 7515 Loss: 0.07622307788884837\n",
      "Iteration 7516 Loss: 0.07622295879647244\n",
      "Iteration 7517 Loss: 0.07622283971103587\n",
      "Iteration 7518 Loss: 0.07622272063253818\n",
      "Iteration 7519 Loss: 0.07622260156097882\n",
      "Iteration 7520 Loss: 0.07622248249635738\n",
      "Iteration 7521 Loss: 0.07622236343867331\n",
      "Iteration 7522 Loss: 0.07622224438792617\n",
      "Iteration 7523 Loss: 0.07622212534411545\n",
      "Iteration 7524 Loss: 0.07622200630724064\n",
      "Iteration 7525 Loss: 0.07622188727730125\n",
      "Iteration 7526 Loss: 0.07622176825429687\n",
      "Iteration 7527 Loss: 0.07622164923822694\n",
      "Iteration 7528 Loss: 0.07622153022909098\n",
      "Iteration 7529 Loss: 0.07622141122688847\n",
      "Iteration 7530 Loss: 0.07622129223161904\n",
      "Iteration 7531 Loss: 0.07622117324328209\n",
      "Iteration 7532 Loss: 0.0762210542618772\n",
      "Iteration 7533 Loss: 0.07622093528740384\n",
      "Iteration 7534 Loss: 0.07622081631986155\n",
      "Iteration 7535 Loss: 0.07622069735924984\n",
      "Iteration 7536 Loss: 0.07622057840556824\n",
      "Iteration 7537 Loss: 0.07622045945881623\n",
      "Iteration 7538 Loss: 0.07622034051899329\n",
      "Iteration 7539 Loss: 0.07622022158609905\n",
      "Iteration 7540 Loss: 0.07622010266013296\n",
      "Iteration 7541 Loss: 0.07621998374109448\n",
      "Iteration 7542 Loss: 0.07621986482898324\n",
      "Iteration 7543 Loss: 0.07621974592379865\n",
      "Iteration 7544 Loss: 0.07621962702554037\n",
      "Iteration 7545 Loss: 0.07621950813420775\n",
      "Iteration 7546 Loss: 0.0762193892498004\n",
      "Iteration 7547 Loss: 0.07621927037231785\n",
      "Iteration 7548 Loss: 0.07621915150175951\n",
      "Iteration 7549 Loss: 0.07621903263812509\n",
      "Iteration 7550 Loss: 0.07621891378141388\n",
      "Iteration 7551 Loss: 0.07621879493162557\n",
      "Iteration 7552 Loss: 0.07621867608875962\n",
      "Iteration 7553 Loss: 0.07621855725281552\n",
      "Iteration 7554 Loss: 0.0762184384237928\n",
      "Iteration 7555 Loss: 0.07621831960169104\n",
      "Iteration 7556 Loss: 0.07621820078650968\n",
      "Iteration 7557 Loss: 0.07621808197824827\n",
      "Iteration 7558 Loss: 0.07621796317690639\n",
      "Iteration 7559 Loss: 0.07621784438248343\n",
      "Iteration 7560 Loss: 0.07621772559497901\n",
      "Iteration 7561 Loss: 0.07621760681439266\n",
      "Iteration 7562 Loss: 0.0762174880407238\n",
      "Iteration 7563 Loss: 0.07621736927397207\n",
      "Iteration 7564 Loss: 0.07621725051413693\n",
      "Iteration 7565 Loss: 0.07621713176121787\n",
      "Iteration 7566 Loss: 0.07621701301521445\n",
      "Iteration 7567 Loss: 0.07621689427612621\n",
      "Iteration 7568 Loss: 0.0762167755439527\n",
      "Iteration 7569 Loss: 0.07621665681869333\n",
      "Iteration 7570 Loss: 0.07621653810034772\n",
      "Iteration 7571 Loss: 0.07621641938891535\n",
      "Iteration 7572 Loss: 0.07621630068439575\n",
      "Iteration 7573 Loss: 0.07621618198678844\n",
      "Iteration 7574 Loss: 0.07621606329609298\n",
      "Iteration 7575 Loss: 0.0762159446123088\n",
      "Iteration 7576 Loss: 0.07621582593543551\n",
      "Iteration 7577 Loss: 0.07621570726547261\n",
      "Iteration 7578 Loss: 0.07621558860241967\n",
      "Iteration 7579 Loss: 0.0762154699462761\n",
      "Iteration 7580 Loss: 0.07621535129704153\n",
      "Iteration 7581 Loss: 0.07621523265471543\n",
      "Iteration 7582 Loss: 0.07621511401929736\n",
      "Iteration 7583 Loss: 0.07621499539078681\n",
      "Iteration 7584 Loss: 0.07621487676918337\n",
      "Iteration 7585 Loss: 0.07621475815448642\n",
      "Iteration 7586 Loss: 0.07621463954669565\n",
      "Iteration 7587 Loss: 0.07621452094581051\n",
      "Iteration 7588 Loss: 0.07621440235183052\n",
      "Iteration 7589 Loss: 0.07621428376475525\n",
      "Iteration 7590 Loss: 0.07621416518458418\n",
      "Iteration 7591 Loss: 0.07621404661131688\n",
      "Iteration 7592 Loss: 0.0762139280449528\n",
      "Iteration 7593 Loss: 0.07621380948549157\n",
      "Iteration 7594 Loss: 0.07621369093293268\n",
      "Iteration 7595 Loss: 0.07621357238727555\n",
      "Iteration 7596 Loss: 0.07621345384851987\n",
      "Iteration 7597 Loss: 0.07621333531666509\n",
      "Iteration 7598 Loss: 0.07621321679171075\n",
      "Iteration 7599 Loss: 0.07621309827365637\n",
      "Iteration 7600 Loss: 0.07621297976250148\n",
      "Iteration 7601 Loss: 0.07621286125824563\n",
      "Iteration 7602 Loss: 0.07621274276088834\n",
      "Iteration 7603 Loss: 0.07621262427042907\n",
      "Iteration 7604 Loss: 0.07621250578686749\n",
      "Iteration 7605 Loss: 0.07621238731020301\n",
      "Iteration 7606 Loss: 0.07621226884043522\n",
      "Iteration 7607 Loss: 0.07621215037756363\n",
      "Iteration 7608 Loss: 0.07621203192158775\n",
      "Iteration 7609 Loss: 0.07621191347250714\n",
      "Iteration 7610 Loss: 0.07621179503032133\n",
      "Iteration 7611 Loss: 0.07621167659502982\n",
      "Iteration 7612 Loss: 0.07621155816663222\n",
      "Iteration 7613 Loss: 0.07621143974512787\n",
      "Iteration 7614 Loss: 0.07621132133051657\n",
      "Iteration 7615 Loss: 0.07621120292279765\n",
      "Iteration 7616 Loss: 0.07621108452197077\n",
      "Iteration 7617 Loss: 0.07621096612803538\n",
      "Iteration 7618 Loss: 0.076210847740991\n",
      "Iteration 7619 Loss: 0.07621072936083718\n",
      "Iteration 7620 Loss: 0.07621061098757352\n",
      "Iteration 7621 Loss: 0.07621049262119949\n",
      "Iteration 7622 Loss: 0.0762103742617146\n",
      "Iteration 7623 Loss: 0.07621025590911841\n",
      "Iteration 7624 Loss: 0.07621013756341054\n",
      "Iteration 7625 Loss: 0.07621001922459036\n",
      "Iteration 7626 Loss: 0.07620990089265756\n",
      "Iteration 7627 Loss: 0.07620978256761153\n",
      "Iteration 7628 Loss: 0.07620966424945194\n",
      "Iteration 7629 Loss: 0.07620954593817823\n",
      "Iteration 7630 Loss: 0.07620942763378992\n",
      "Iteration 7631 Loss: 0.07620930933628665\n",
      "Iteration 7632 Loss: 0.07620919104566787\n",
      "Iteration 7633 Loss: 0.07620907276193316\n",
      "Iteration 7634 Loss: 0.07620895448508196\n",
      "Iteration 7635 Loss: 0.07620883621511397\n",
      "Iteration 7636 Loss: 0.0762087179520286\n",
      "Iteration 7637 Loss: 0.07620859969582541\n",
      "Iteration 7638 Loss: 0.07620848144650397\n",
      "Iteration 7639 Loss: 0.0762083632040638\n",
      "Iteration 7640 Loss: 0.07620824496850437\n",
      "Iteration 7641 Loss: 0.07620812673982529\n",
      "Iteration 7642 Loss: 0.07620800851802613\n",
      "Iteration 7643 Loss: 0.07620789030310633\n",
      "Iteration 7644 Loss: 0.07620777209506553\n",
      "Iteration 7645 Loss: 0.07620765389390315\n",
      "Iteration 7646 Loss: 0.07620753569961886\n",
      "Iteration 7647 Loss: 0.07620741751221201\n",
      "Iteration 7648 Loss: 0.07620729933168235\n",
      "Iteration 7649 Loss: 0.07620718115802934\n",
      "Iteration 7650 Loss: 0.07620706299125243\n",
      "Iteration 7651 Loss: 0.07620694483135126\n",
      "Iteration 7652 Loss: 0.0762068266783253\n",
      "Iteration 7653 Loss: 0.07620670853217423\n",
      "Iteration 7654 Loss: 0.07620659039289744\n",
      "Iteration 7655 Loss: 0.0762064722604945\n",
      "Iteration 7656 Loss: 0.07620635413496499\n",
      "Iteration 7657 Loss: 0.07620623601630837\n",
      "Iteration 7658 Loss: 0.07620611790452429\n",
      "Iteration 7659 Loss: 0.07620599979961218\n",
      "Iteration 7660 Loss: 0.07620588170157165\n",
      "Iteration 7661 Loss: 0.07620576361040225\n",
      "Iteration 7662 Loss: 0.0762056455261035\n",
      "Iteration 7663 Loss: 0.07620552744867488\n",
      "Iteration 7664 Loss: 0.07620540937811598\n",
      "Iteration 7665 Loss: 0.07620529131442644\n",
      "Iteration 7666 Loss: 0.07620517325760563\n",
      "Iteration 7667 Loss: 0.07620505520765318\n",
      "Iteration 7668 Loss: 0.07620493716456864\n",
      "Iteration 7669 Loss: 0.07620481912835148\n",
      "Iteration 7670 Loss: 0.07620470109900132\n",
      "Iteration 7671 Loss: 0.07620458307651767\n",
      "Iteration 7672 Loss: 0.07620446506090012\n",
      "Iteration 7673 Loss: 0.07620434705214806\n",
      "Iteration 7674 Loss: 0.07620422905026125\n",
      "Iteration 7675 Loss: 0.0762041110552391\n",
      "Iteration 7676 Loss: 0.07620399306708117\n",
      "Iteration 7677 Loss: 0.07620387508578695\n",
      "Iteration 7678 Loss: 0.07620375711135612\n",
      "Iteration 7679 Loss: 0.07620363914378812\n",
      "Iteration 7680 Loss: 0.0762035211830825\n",
      "Iteration 7681 Loss: 0.07620340322923884\n",
      "Iteration 7682 Loss: 0.07620328528225669\n",
      "Iteration 7683 Loss: 0.07620316734213553\n",
      "Iteration 7684 Loss: 0.07620304940887493\n",
      "Iteration 7685 Loss: 0.07620293148247448\n",
      "Iteration 7686 Loss: 0.07620281356293365\n",
      "Iteration 7687 Loss: 0.07620269565025206\n",
      "Iteration 7688 Loss: 0.07620257774442923\n",
      "Iteration 7689 Loss: 0.07620245984546466\n",
      "Iteration 7690 Loss: 0.07620234195335797\n",
      "Iteration 7691 Loss: 0.07620222406810866\n",
      "Iteration 7692 Loss: 0.07620210618971626\n",
      "Iteration 7693 Loss: 0.07620198831818037\n",
      "Iteration 7694 Loss: 0.07620187045350049\n",
      "Iteration 7695 Loss: 0.07620175259567616\n",
      "Iteration 7696 Loss: 0.07620163474470695\n",
      "Iteration 7697 Loss: 0.07620151690059246\n",
      "Iteration 7698 Loss: 0.07620139906333209\n",
      "Iteration 7699 Loss: 0.07620128123292556\n",
      "Iteration 7700 Loss: 0.07620116340937226\n",
      "Iteration 7701 Loss: 0.07620104559267182\n",
      "Iteration 7702 Loss: 0.07620092778282378\n",
      "Iteration 7703 Loss: 0.07620080997982771\n",
      "Iteration 7704 Loss: 0.0762006921836831\n",
      "Iteration 7705 Loss: 0.07620057439438956\n",
      "Iteration 7706 Loss: 0.07620045661194658\n",
      "Iteration 7707 Loss: 0.07620033883635373\n",
      "Iteration 7708 Loss: 0.07620022106761053\n",
      "Iteration 7709 Loss: 0.07620010330571664\n",
      "Iteration 7710 Loss: 0.07619998555067149\n",
      "Iteration 7711 Loss: 0.07619986780247465\n",
      "Iteration 7712 Loss: 0.0761997500611257\n",
      "Iteration 7713 Loss: 0.07619963232662413\n",
      "Iteration 7714 Loss: 0.07619951459896965\n",
      "Iteration 7715 Loss: 0.07619939687816155\n",
      "Iteration 7716 Loss: 0.07619927916419961\n",
      "Iteration 7717 Loss: 0.07619916145708325\n",
      "Iteration 7718 Loss: 0.0761990437568121\n",
      "Iteration 7719 Loss: 0.07619892606338562\n",
      "Iteration 7720 Loss: 0.07619880837680341\n",
      "Iteration 7721 Loss: 0.07619869069706504\n",
      "Iteration 7722 Loss: 0.07619857302417007\n",
      "Iteration 7723 Loss: 0.07619845535811798\n",
      "Iteration 7724 Loss: 0.0761983376989084\n",
      "Iteration 7725 Loss: 0.07619822004654082\n",
      "Iteration 7726 Loss: 0.07619810240101481\n",
      "Iteration 7727 Loss: 0.0761979847623299\n",
      "Iteration 7728 Loss: 0.07619786713048568\n",
      "Iteration 7729 Loss: 0.07619774950548167\n",
      "Iteration 7730 Loss: 0.07619763188731744\n",
      "Iteration 7731 Loss: 0.07619751427599261\n",
      "Iteration 7732 Loss: 0.07619739667150659\n",
      "Iteration 7733 Loss: 0.07619727907385902\n",
      "Iteration 7734 Loss: 0.07619716148304942\n",
      "Iteration 7735 Loss: 0.07619704389907737\n",
      "Iteration 7736 Loss: 0.07619692632194244\n",
      "Iteration 7737 Loss: 0.07619680875164414\n",
      "Iteration 7738 Loss: 0.07619669118818197\n",
      "Iteration 7739 Loss: 0.0761965736315556\n",
      "Iteration 7740 Loss: 0.07619645608176451\n",
      "Iteration 7741 Loss: 0.07619633853880829\n",
      "Iteration 7742 Loss: 0.07619622100268648\n",
      "Iteration 7743 Loss: 0.07619610347339859\n",
      "Iteration 7744 Loss: 0.07619598595094429\n",
      "Iteration 7745 Loss: 0.07619586843532301\n",
      "Iteration 7746 Loss: 0.07619575092653434\n",
      "Iteration 7747 Loss: 0.07619563342457787\n",
      "Iteration 7748 Loss: 0.07619551592945309\n",
      "Iteration 7749 Loss: 0.07619539844115965\n",
      "Iteration 7750 Loss: 0.07619528095969703\n",
      "Iteration 7751 Loss: 0.07619516348506476\n",
      "Iteration 7752 Loss: 0.07619504601726249\n",
      "Iteration 7753 Loss: 0.0761949285562897\n",
      "Iteration 7754 Loss: 0.07619481110214597\n",
      "Iteration 7755 Loss: 0.07619469365483084\n",
      "Iteration 7756 Loss: 0.07619457621434389\n",
      "Iteration 7757 Loss: 0.0761944587806847\n",
      "Iteration 7758 Loss: 0.07619434135385274\n",
      "Iteration 7759 Loss: 0.07619422393384755\n",
      "Iteration 7760 Loss: 0.07619410652066891\n",
      "Iteration 7761 Loss: 0.07619398911431612\n",
      "Iteration 7762 Loss: 0.07619387171478882\n",
      "Iteration 7763 Loss: 0.07619375432208661\n",
      "Iteration 7764 Loss: 0.07619363693620902\n",
      "Iteration 7765 Loss: 0.07619351955715559\n",
      "Iteration 7766 Loss: 0.07619340218492593\n",
      "Iteration 7767 Loss: 0.0761932848195195\n",
      "Iteration 7768 Loss: 0.07619316746093595\n",
      "Iteration 7769 Loss: 0.07619305010917478\n",
      "Iteration 7770 Loss: 0.07619293276423553\n",
      "Iteration 7771 Loss: 0.07619281542611786\n",
      "Iteration 7772 Loss: 0.07619269809482124\n",
      "Iteration 7773 Loss: 0.07619258077034526\n",
      "Iteration 7774 Loss: 0.07619246345268944\n",
      "Iteration 7775 Loss: 0.07619234614185338\n",
      "Iteration 7776 Loss: 0.07619222883783663\n",
      "Iteration 7777 Loss: 0.07619211154063878\n",
      "Iteration 7778 Loss: 0.07619199425025927\n",
      "Iteration 7779 Loss: 0.07619187696669777\n",
      "Iteration 7780 Loss: 0.07619175968995384\n",
      "Iteration 7781 Loss: 0.07619164242002699\n",
      "Iteration 7782 Loss: 0.07619152515691682\n",
      "Iteration 7783 Loss: 0.07619140790062283\n",
      "Iteration 7784 Loss: 0.07619129065114465\n",
      "Iteration 7785 Loss: 0.07619117340848175\n",
      "Iteration 7786 Loss: 0.07619105617263376\n",
      "Iteration 7787 Loss: 0.07619093894360025\n",
      "Iteration 7788 Loss: 0.07619082172138074\n",
      "Iteration 7789 Loss: 0.07619070450597479\n",
      "Iteration 7790 Loss: 0.07619058729738197\n",
      "Iteration 7791 Loss: 0.07619047009560187\n",
      "Iteration 7792 Loss: 0.07619035290063403\n",
      "Iteration 7793 Loss: 0.07619023571247792\n",
      "Iteration 7794 Loss: 0.07619011853113329\n",
      "Iteration 7795 Loss: 0.07619000135659958\n",
      "Iteration 7796 Loss: 0.07618988418887637\n",
      "Iteration 7797 Loss: 0.07618976702796317\n",
      "Iteration 7798 Loss: 0.07618964987385961\n",
      "Iteration 7799 Loss: 0.07618953272656526\n",
      "Iteration 7800 Loss: 0.0761894155860796\n",
      "Iteration 7801 Loss: 0.07618929845240223\n",
      "Iteration 7802 Loss: 0.07618918132553278\n",
      "Iteration 7803 Loss: 0.07618906420547077\n",
      "Iteration 7804 Loss: 0.0761889470922157\n",
      "Iteration 7805 Loss: 0.0761888299857672\n",
      "Iteration 7806 Loss: 0.07618871288612483\n",
      "Iteration 7807 Loss: 0.07618859579328804\n",
      "Iteration 7808 Loss: 0.07618847870725663\n",
      "Iteration 7809 Loss: 0.07618836162802994\n",
      "Iteration 7810 Loss: 0.07618824455560765\n",
      "Iteration 7811 Loss: 0.07618812748998927\n",
      "Iteration 7812 Loss: 0.07618801043117437\n",
      "Iteration 7813 Loss: 0.07618789337916251\n",
      "Iteration 7814 Loss: 0.07618777633395331\n",
      "Iteration 7815 Loss: 0.07618765929554622\n",
      "Iteration 7816 Loss: 0.07618754226394091\n",
      "Iteration 7817 Loss: 0.07618742523913695\n",
      "Iteration 7818 Loss: 0.07618730822113379\n",
      "Iteration 7819 Loss: 0.07618719120993113\n",
      "Iteration 7820 Loss: 0.07618707420552842\n",
      "Iteration 7821 Loss: 0.07618695720792527\n",
      "Iteration 7822 Loss: 0.07618684021712123\n",
      "Iteration 7823 Loss: 0.07618672323311589\n",
      "Iteration 7824 Loss: 0.07618660625590885\n",
      "Iteration 7825 Loss: 0.0761864892854996\n",
      "Iteration 7826 Loss: 0.07618637232188774\n",
      "Iteration 7827 Loss: 0.07618625536507284\n",
      "Iteration 7828 Loss: 0.07618613841505446\n",
      "Iteration 7829 Loss: 0.07618602147183208\n",
      "Iteration 7830 Loss: 0.07618590453540545\n",
      "Iteration 7831 Loss: 0.07618578760577396\n",
      "Iteration 7832 Loss: 0.07618567068293727\n",
      "Iteration 7833 Loss: 0.07618555376689495\n",
      "Iteration 7834 Loss: 0.07618543685764649\n",
      "Iteration 7835 Loss: 0.07618531995519154\n",
      "Iteration 7836 Loss: 0.07618520305952962\n",
      "Iteration 7837 Loss: 0.0761850861706603\n",
      "Iteration 7838 Loss: 0.07618496928858315\n",
      "Iteration 7839 Loss: 0.07618485241329771\n",
      "Iteration 7840 Loss: 0.0761847355448036\n",
      "Iteration 7841 Loss: 0.07618461868310038\n",
      "Iteration 7842 Loss: 0.07618450182818756\n",
      "Iteration 7843 Loss: 0.07618438498006476\n",
      "Iteration 7844 Loss: 0.07618426813873157\n",
      "Iteration 7845 Loss: 0.07618415130418747\n",
      "Iteration 7846 Loss: 0.0761840344764321\n",
      "Iteration 7847 Loss: 0.076183917655465\n",
      "Iteration 7848 Loss: 0.07618380084128577\n",
      "Iteration 7849 Loss: 0.07618368403389388\n",
      "Iteration 7850 Loss: 0.07618356723328903\n",
      "Iteration 7851 Loss: 0.07618345043947068\n",
      "Iteration 7852 Loss: 0.07618333365243843\n",
      "Iteration 7853 Loss: 0.07618321687219189\n",
      "Iteration 7854 Loss: 0.07618310009873062\n",
      "Iteration 7855 Loss: 0.07618298333205412\n",
      "Iteration 7856 Loss: 0.07618286657216201\n",
      "Iteration 7857 Loss: 0.07618274981905389\n",
      "Iteration 7858 Loss: 0.07618263307272934\n",
      "Iteration 7859 Loss: 0.07618251633318776\n",
      "Iteration 7860 Loss: 0.0761823996004289\n",
      "Iteration 7861 Loss: 0.07618228287445229\n",
      "Iteration 7862 Loss: 0.0761821661552574\n",
      "Iteration 7863 Loss: 0.07618204944284397\n",
      "Iteration 7864 Loss: 0.07618193273721141\n",
      "Iteration 7865 Loss: 0.07618181603835944\n",
      "Iteration 7866 Loss: 0.07618169934628745\n",
      "Iteration 7867 Loss: 0.07618158266099515\n",
      "Iteration 7868 Loss: 0.07618146598248207\n",
      "Iteration 7869 Loss: 0.07618134931074778\n",
      "Iteration 7870 Loss: 0.07618123264579187\n",
      "Iteration 7871 Loss: 0.07618111598761379\n",
      "Iteration 7872 Loss: 0.07618099933621326\n",
      "Iteration 7873 Loss: 0.07618088269158983\n",
      "Iteration 7874 Loss: 0.07618076605374303\n",
      "Iteration 7875 Loss: 0.07618064942267243\n",
      "Iteration 7876 Loss: 0.07618053279837754\n",
      "Iteration 7877 Loss: 0.07618041618085807\n",
      "Iteration 7878 Loss: 0.0761802995701135\n",
      "Iteration 7879 Loss: 0.07618018296614346\n",
      "Iteration 7880 Loss: 0.07618006636894747\n",
      "Iteration 7881 Loss: 0.07617994977852507\n",
      "Iteration 7882 Loss: 0.07617983319487588\n",
      "Iteration 7883 Loss: 0.0761797166179995\n",
      "Iteration 7884 Loss: 0.07617960004789548\n",
      "Iteration 7885 Loss: 0.07617948348456335\n",
      "Iteration 7886 Loss: 0.07617936692800273\n",
      "Iteration 7887 Loss: 0.07617925037821315\n",
      "Iteration 7888 Loss: 0.07617913383519424\n",
      "Iteration 7889 Loss: 0.07617901729894552\n",
      "Iteration 7890 Loss: 0.0761789007694665\n",
      "Iteration 7891 Loss: 0.07617878424675699\n",
      "Iteration 7892 Loss: 0.07617866773081632\n",
      "Iteration 7893 Loss: 0.07617855122164416\n",
      "Iteration 7894 Loss: 0.07617843471924006\n",
      "Iteration 7895 Loss: 0.07617831822360364\n",
      "Iteration 7896 Loss: 0.07617820173473441\n",
      "Iteration 7897 Loss: 0.07617808525263194\n",
      "Iteration 7898 Loss: 0.07617796877729589\n",
      "Iteration 7899 Loss: 0.07617785230872572\n",
      "Iteration 7900 Loss: 0.07617773584692113\n",
      "Iteration 7901 Loss: 0.07617761939188156\n",
      "Iteration 7902 Loss: 0.07617750294360671\n",
      "Iteration 7903 Loss: 0.07617738650209602\n",
      "Iteration 7904 Loss: 0.07617727006734919\n",
      "Iteration 7905 Loss: 0.07617715363936572\n",
      "Iteration 7906 Loss: 0.07617703721814525\n",
      "Iteration 7907 Loss: 0.07617692080368724\n",
      "Iteration 7908 Loss: 0.07617680439599132\n",
      "Iteration 7909 Loss: 0.07617668799505715\n",
      "Iteration 7910 Loss: 0.07617657160088422\n",
      "Iteration 7911 Loss: 0.07617645521347205\n",
      "Iteration 7912 Loss: 0.0761763388328203\n",
      "Iteration 7913 Loss: 0.07617622245892856\n",
      "Iteration 7914 Loss: 0.07617610609179637\n",
      "Iteration 7915 Loss: 0.07617598973142328\n",
      "Iteration 7916 Loss: 0.0761758733778089\n",
      "Iteration 7917 Loss: 0.07617575703095278\n",
      "Iteration 7918 Loss: 0.07617564069085452\n",
      "Iteration 7919 Loss: 0.07617552435751371\n",
      "Iteration 7920 Loss: 0.07617540803092987\n",
      "Iteration 7921 Loss: 0.0761752917111026\n",
      "Iteration 7922 Loss: 0.07617517539803152\n",
      "Iteration 7923 Loss: 0.07617505909171617\n",
      "Iteration 7924 Loss: 0.07617494279215607\n",
      "Iteration 7925 Loss: 0.07617482649935088\n",
      "Iteration 7926 Loss: 0.07617471021330018\n",
      "Iteration 7927 Loss: 0.07617459393400348\n",
      "Iteration 7928 Loss: 0.07617447766146039\n",
      "Iteration 7929 Loss: 0.0761743613956705\n",
      "Iteration 7930 Loss: 0.07617424513663335\n",
      "Iteration 7931 Loss: 0.07617412888434855\n",
      "Iteration 7932 Loss: 0.07617401263881567\n",
      "Iteration 7933 Loss: 0.07617389640003429\n",
      "Iteration 7934 Loss: 0.07617378016800398\n",
      "Iteration 7935 Loss: 0.07617366394272437\n",
      "Iteration 7936 Loss: 0.07617354772419489\n",
      "Iteration 7937 Loss: 0.07617343151241524\n",
      "Iteration 7938 Loss: 0.07617331530738498\n",
      "Iteration 7939 Loss: 0.07617319910910368\n",
      "Iteration 7940 Loss: 0.07617308291757088\n",
      "Iteration 7941 Loss: 0.0761729667327863\n",
      "Iteration 7942 Loss: 0.07617285055474929\n",
      "Iteration 7943 Loss: 0.0761727343834596\n",
      "Iteration 7944 Loss: 0.07617261821891672\n",
      "Iteration 7945 Loss: 0.07617250206112032\n",
      "Iteration 7946 Loss: 0.07617238591006988\n",
      "Iteration 7947 Loss: 0.07617226976576505\n",
      "Iteration 7948 Loss: 0.07617215362820536\n",
      "Iteration 7949 Loss: 0.07617203749739038\n",
      "Iteration 7950 Loss: 0.07617192137331977\n",
      "Iteration 7951 Loss: 0.07617180525599303\n",
      "Iteration 7952 Loss: 0.07617168914540975\n",
      "Iteration 7953 Loss: 0.07617157304156956\n",
      "Iteration 7954 Loss: 0.07617145694447194\n",
      "Iteration 7955 Loss: 0.07617134085411659\n",
      "Iteration 7956 Loss: 0.07617122477050303\n",
      "Iteration 7957 Loss: 0.0761711086936308\n",
      "Iteration 7958 Loss: 0.07617099262349955\n",
      "Iteration 7959 Loss: 0.07617087656010878\n",
      "Iteration 7960 Loss: 0.07617076050345815\n",
      "Iteration 7961 Loss: 0.07617064445354722\n",
      "Iteration 7962 Loss: 0.07617052841037554\n",
      "Iteration 7963 Loss: 0.0761704123739427\n",
      "Iteration 7964 Loss: 0.07617029634424834\n",
      "Iteration 7965 Loss: 0.07617018032129198\n",
      "Iteration 7966 Loss: 0.0761700643050731\n",
      "Iteration 7967 Loss: 0.07616994829559148\n",
      "Iteration 7968 Loss: 0.07616983229284657\n",
      "Iteration 7969 Loss: 0.07616971629683801\n",
      "Iteration 7970 Loss: 0.0761696003075654\n",
      "Iteration 7971 Loss: 0.07616948432502822\n",
      "Iteration 7972 Loss: 0.07616936834922615\n",
      "Iteration 7973 Loss: 0.07616925238015868\n",
      "Iteration 7974 Loss: 0.07616913641782547\n",
      "Iteration 7975 Loss: 0.07616902046222608\n",
      "Iteration 7976 Loss: 0.07616890451336007\n",
      "Iteration 7977 Loss: 0.07616878857122705\n",
      "Iteration 7978 Loss: 0.07616867263582656\n",
      "Iteration 7979 Loss: 0.0761685567071582\n",
      "Iteration 7980 Loss: 0.07616844078522161\n",
      "Iteration 7981 Loss: 0.07616832487001628\n",
      "Iteration 7982 Loss: 0.07616820896154183\n",
      "Iteration 7983 Loss: 0.07616809305979784\n",
      "Iteration 7984 Loss: 0.07616797716478398\n",
      "Iteration 7985 Loss: 0.07616786127649963\n",
      "Iteration 7986 Loss: 0.07616774539494456\n",
      "Iteration 7987 Loss: 0.0761676295201183\n",
      "Iteration 7988 Loss: 0.07616751365202037\n",
      "Iteration 7989 Loss: 0.0761673977906504\n",
      "Iteration 7990 Loss: 0.07616728193600798\n",
      "Iteration 7991 Loss: 0.07616716608809267\n",
      "Iteration 7992 Loss: 0.07616705024690405\n",
      "Iteration 7993 Loss: 0.07616693441244178\n",
      "Iteration 7994 Loss: 0.07616681858470532\n",
      "Iteration 7995 Loss: 0.07616670276369435\n",
      "Iteration 7996 Loss: 0.07616658694940841\n",
      "Iteration 7997 Loss: 0.07616647114184703\n",
      "Iteration 7998 Loss: 0.07616635534100992\n",
      "Iteration 7999 Loss: 0.07616623954689658\n",
      "Iteration 8000 Loss: 0.07616612375950658\n",
      "Iteration 8001 Loss: 0.07616600797883957\n",
      "Iteration 8002 Loss: 0.07616589220489507\n",
      "Iteration 8003 Loss: 0.07616577643767274\n",
      "Iteration 8004 Loss: 0.07616566067717206\n",
      "Iteration 8005 Loss: 0.07616554492339266\n",
      "Iteration 8006 Loss: 0.07616542917633416\n",
      "Iteration 8007 Loss: 0.07616531343599607\n",
      "Iteration 8008 Loss: 0.07616519770237808\n",
      "Iteration 8009 Loss: 0.07616508197547966\n",
      "Iteration 8010 Loss: 0.07616496625530049\n",
      "Iteration 8011 Loss: 0.0761648505418401\n",
      "Iteration 8012 Loss: 0.07616473483509803\n",
      "Iteration 8013 Loss: 0.076164619135074\n",
      "Iteration 8014 Loss: 0.07616450344176746\n",
      "Iteration 8015 Loss: 0.07616438775517807\n",
      "Iteration 8016 Loss: 0.0761642720753054\n",
      "Iteration 8017 Loss: 0.07616415640214899\n",
      "Iteration 8018 Loss: 0.0761640407357085\n",
      "Iteration 8019 Loss: 0.07616392507598349\n",
      "Iteration 8020 Loss: 0.07616380942297346\n",
      "Iteration 8021 Loss: 0.07616369377667813\n",
      "Iteration 8022 Loss: 0.07616357813709701\n",
      "Iteration 8023 Loss: 0.07616346250422971\n",
      "Iteration 8024 Loss: 0.07616334687807579\n",
      "Iteration 8025 Loss: 0.07616323125863486\n",
      "Iteration 8026 Loss: 0.07616311564590653\n",
      "Iteration 8027 Loss: 0.07616300003989031\n",
      "Iteration 8028 Loss: 0.07616288444058579\n",
      "Iteration 8029 Loss: 0.07616276884799265\n",
      "Iteration 8030 Loss: 0.07616265326211036\n",
      "Iteration 8031 Loss: 0.07616253768293864\n",
      "Iteration 8032 Loss: 0.07616242211047694\n",
      "Iteration 8033 Loss: 0.07616230654472494\n",
      "Iteration 8034 Loss: 0.07616219098568217\n",
      "Iteration 8035 Loss: 0.07616207543334831\n",
      "Iteration 8036 Loss: 0.07616195988772279\n",
      "Iteration 8037 Loss: 0.07616184434880534\n",
      "Iteration 8038 Loss: 0.07616172881659543\n",
      "Iteration 8039 Loss: 0.07616161329109278\n",
      "Iteration 8040 Loss: 0.0761614977722968\n",
      "Iteration 8041 Loss: 0.07616138226020727\n",
      "Iteration 8042 Loss: 0.07616126675482367\n",
      "Iteration 8043 Loss: 0.07616115125614557\n",
      "Iteration 8044 Loss: 0.07616103576417262\n",
      "Iteration 8045 Loss: 0.07616092027890442\n",
      "Iteration 8046 Loss: 0.07616080480034042\n",
      "Iteration 8047 Loss: 0.07616068932848039\n",
      "Iteration 8048 Loss: 0.07616057386332377\n",
      "Iteration 8049 Loss: 0.07616045840487021\n",
      "Iteration 8050 Loss: 0.0761603429531193\n",
      "Iteration 8051 Loss: 0.07616022750807067\n",
      "Iteration 8052 Loss: 0.07616011206972378\n",
      "Iteration 8053 Loss: 0.07615999663807836\n",
      "Iteration 8054 Loss: 0.07615988121313391\n",
      "Iteration 8055 Loss: 0.07615976579489009\n",
      "Iteration 8056 Loss: 0.07615965038334636\n",
      "Iteration 8057 Loss: 0.07615953497850249\n",
      "Iteration 8058 Loss: 0.0761594195803579\n",
      "Iteration 8059 Loss: 0.07615930418891226\n",
      "Iteration 8060 Loss: 0.07615918880416515\n",
      "Iteration 8061 Loss: 0.07615907342611618\n",
      "Iteration 8062 Loss: 0.07615895805476484\n",
      "Iteration 8063 Loss: 0.07615884269011086\n",
      "Iteration 8064 Loss: 0.07615872733215376\n",
      "Iteration 8065 Loss: 0.0761586119808931\n",
      "Iteration 8066 Loss: 0.07615849663632848\n",
      "Iteration 8067 Loss: 0.0761583812984595\n",
      "Iteration 8068 Loss: 0.07615826596728581\n",
      "Iteration 8069 Loss: 0.07615815064280688\n",
      "Iteration 8070 Loss: 0.07615803532502242\n",
      "Iteration 8071 Loss: 0.07615792001393194\n",
      "Iteration 8072 Loss: 0.07615780470953509\n",
      "Iteration 8073 Loss: 0.07615768941183142\n",
      "Iteration 8074 Loss: 0.07615757412082048\n",
      "Iteration 8075 Loss: 0.0761574588365019\n",
      "Iteration 8076 Loss: 0.07615734355887532\n",
      "Iteration 8077 Loss: 0.07615722828794023\n",
      "Iteration 8078 Loss: 0.07615711302369632\n",
      "Iteration 8079 Loss: 0.07615699776614308\n",
      "Iteration 8080 Loss: 0.07615688251528017\n",
      "Iteration 8081 Loss: 0.07615676727110719\n",
      "Iteration 8082 Loss: 0.07615665203362365\n",
      "Iteration 8083 Loss: 0.07615653680282923\n",
      "Iteration 8084 Loss: 0.07615642157872342\n",
      "Iteration 8085 Loss: 0.07615630636130592\n",
      "Iteration 8086 Loss: 0.07615619115057634\n",
      "Iteration 8087 Loss: 0.07615607594653408\n",
      "Iteration 8088 Loss: 0.07615596074917894\n",
      "Iteration 8089 Loss: 0.07615584555851038\n",
      "Iteration 8090 Loss: 0.07615573037452801\n",
      "Iteration 8091 Loss: 0.07615561519723149\n",
      "Iteration 8092 Loss: 0.07615550002662039\n",
      "Iteration 8093 Loss: 0.07615538486269426\n",
      "Iteration 8094 Loss: 0.07615526970545272\n",
      "Iteration 8095 Loss: 0.07615515455489531\n",
      "Iteration 8096 Loss: 0.0761550394110217\n",
      "Iteration 8097 Loss: 0.0761549242738314\n",
      "Iteration 8098 Loss: 0.07615480914332404\n",
      "Iteration 8099 Loss: 0.07615469401949923\n",
      "Iteration 8100 Loss: 0.07615457890235657\n",
      "Iteration 8101 Loss: 0.07615446379189562\n",
      "Iteration 8102 Loss: 0.07615434868811599\n",
      "Iteration 8103 Loss: 0.07615423359101724\n",
      "Iteration 8104 Loss: 0.07615411850059903\n",
      "Iteration 8105 Loss: 0.07615400341686081\n",
      "Iteration 8106 Loss: 0.07615388833980236\n",
      "Iteration 8107 Loss: 0.07615377326942312\n",
      "Iteration 8108 Loss: 0.07615365820572279\n",
      "Iteration 8109 Loss: 0.07615354314870089\n",
      "Iteration 8110 Loss: 0.07615342809835704\n",
      "Iteration 8111 Loss: 0.07615331305469081\n",
      "Iteration 8112 Loss: 0.07615319801770187\n",
      "Iteration 8113 Loss: 0.07615308298738968\n",
      "Iteration 8114 Loss: 0.07615296796375395\n",
      "Iteration 8115 Loss: 0.07615285294679419\n",
      "Iteration 8116 Loss: 0.07615273793651005\n",
      "Iteration 8117 Loss: 0.0761526229329011\n",
      "Iteration 8118 Loss: 0.07615250793596699\n",
      "Iteration 8119 Loss: 0.07615239294570722\n",
      "Iteration 8120 Loss: 0.07615227796212144\n",
      "Iteration 8121 Loss: 0.07615216298520919\n",
      "Iteration 8122 Loss: 0.07615204801497014\n",
      "Iteration 8123 Loss: 0.07615193305140383\n",
      "Iteration 8124 Loss: 0.07615181809450984\n",
      "Iteration 8125 Loss: 0.0761517031442878\n",
      "Iteration 8126 Loss: 0.07615158820073734\n",
      "Iteration 8127 Loss: 0.07615147326385792\n",
      "Iteration 8128 Loss: 0.07615135833364926\n",
      "Iteration 8129 Loss: 0.07615124341011094\n",
      "Iteration 8130 Loss: 0.07615112849324253\n",
      "Iteration 8131 Loss: 0.0761510135830436\n",
      "Iteration 8132 Loss: 0.07615089867951375\n",
      "Iteration 8133 Loss: 0.07615078378265262\n",
      "Iteration 8134 Loss: 0.07615066889245974\n",
      "Iteration 8135 Loss: 0.07615055400893475\n",
      "Iteration 8136 Loss: 0.07615043913207725\n",
      "Iteration 8137 Loss: 0.0761503242618868\n",
      "Iteration 8138 Loss: 0.07615020939836298\n",
      "Iteration 8139 Loss: 0.07615009454150545\n",
      "Iteration 8140 Loss: 0.07614997969131376\n",
      "Iteration 8141 Loss: 0.07614986484778755\n",
      "Iteration 8142 Loss: 0.07614975001092633\n",
      "Iteration 8143 Loss: 0.07614963518072976\n",
      "Iteration 8144 Loss: 0.07614952035719742\n",
      "Iteration 8145 Loss: 0.07614940554032888\n",
      "Iteration 8146 Loss: 0.07614929073012376\n",
      "Iteration 8147 Loss: 0.07614917592658169\n",
      "Iteration 8148 Loss: 0.07614906112970223\n",
      "Iteration 8149 Loss: 0.0761489463394849\n",
      "Iteration 8150 Loss: 0.07614883155592937\n",
      "Iteration 8151 Loss: 0.07614871677903531\n",
      "Iteration 8152 Loss: 0.07614860200880219\n",
      "Iteration 8153 Loss: 0.0761484872452297\n",
      "Iteration 8154 Loss: 0.07614837248831732\n",
      "Iteration 8155 Loss: 0.07614825773806476\n",
      "Iteration 8156 Loss: 0.07614814299447152\n",
      "Iteration 8157 Loss: 0.07614802825753729\n",
      "Iteration 8158 Loss: 0.0761479135272616\n",
      "Iteration 8159 Loss: 0.07614779880364407\n",
      "Iteration 8160 Loss: 0.07614768408668433\n",
      "Iteration 8161 Loss: 0.07614756937638187\n",
      "Iteration 8162 Loss: 0.07614745467273637\n",
      "Iteration 8163 Loss: 0.07614733997574744\n",
      "Iteration 8164 Loss: 0.07614722528541462\n",
      "Iteration 8165 Loss: 0.07614711060173757\n",
      "Iteration 8166 Loss: 0.07614699592471581\n",
      "Iteration 8167 Loss: 0.07614688125434896\n",
      "Iteration 8168 Loss: 0.07614676659063668\n",
      "Iteration 8169 Loss: 0.07614665193357852\n",
      "Iteration 8170 Loss: 0.07614653728317401\n",
      "Iteration 8171 Loss: 0.07614642263942283\n",
      "Iteration 8172 Loss: 0.07614630800232462\n",
      "Iteration 8173 Loss: 0.07614619337187883\n",
      "Iteration 8174 Loss: 0.07614607874808522\n",
      "Iteration 8175 Loss: 0.07614596413094324\n",
      "Iteration 8176 Loss: 0.0761458495204526\n",
      "Iteration 8177 Loss: 0.07614573491661285\n",
      "Iteration 8178 Loss: 0.07614562031942355\n",
      "Iteration 8179 Loss: 0.07614550572888436\n",
      "Iteration 8180 Loss: 0.07614539114499486\n",
      "Iteration 8181 Loss: 0.07614527656775458\n",
      "Iteration 8182 Loss: 0.07614516199716326\n",
      "Iteration 8183 Loss: 0.07614504743322038\n",
      "Iteration 8184 Loss: 0.0761449328759256\n",
      "Iteration 8185 Loss: 0.0761448183252784\n",
      "Iteration 8186 Loss: 0.07614470378127856\n",
      "Iteration 8187 Loss: 0.07614458924392555\n",
      "Iteration 8188 Loss: 0.076144474713219\n",
      "Iteration 8189 Loss: 0.07614436018915849\n",
      "Iteration 8190 Loss: 0.07614424567174369\n",
      "Iteration 8191 Loss: 0.07614413116097413\n",
      "Iteration 8192 Loss: 0.0761440166568494\n",
      "Iteration 8193 Loss: 0.07614390215936911\n",
      "Iteration 8194 Loss: 0.07614378766853286\n",
      "Iteration 8195 Loss: 0.07614367318434029\n",
      "Iteration 8196 Loss: 0.076143558706791\n",
      "Iteration 8197 Loss: 0.0761434442358845\n",
      "Iteration 8198 Loss: 0.07614332977162047\n",
      "Iteration 8199 Loss: 0.0761432153139985\n",
      "Iteration 8200 Loss: 0.07614310086301809\n",
      "Iteration 8201 Loss: 0.07614298641867898\n",
      "Iteration 8202 Loss: 0.0761428719809807\n",
      "Iteration 8203 Loss: 0.07614275754992283\n",
      "Iteration 8204 Loss: 0.07614264312550505\n",
      "Iteration 8205 Loss: 0.0761425287077269\n",
      "Iteration 8206 Loss: 0.07614241429658793\n",
      "Iteration 8207 Loss: 0.0761422998920878\n",
      "Iteration 8208 Loss: 0.07614218549422608\n",
      "Iteration 8209 Loss: 0.07614207110300242\n",
      "Iteration 8210 Loss: 0.07614195671841636\n",
      "Iteration 8211 Loss: 0.07614184234046756\n",
      "Iteration 8212 Loss: 0.07614172796915558\n",
      "Iteration 8213 Loss: 0.07614161360448002\n",
      "Iteration 8214 Loss: 0.07614149924644051\n",
      "Iteration 8215 Loss: 0.07614138489503657\n",
      "Iteration 8216 Loss: 0.07614127055026787\n",
      "Iteration 8217 Loss: 0.076141156212134\n",
      "Iteration 8218 Loss: 0.07614104188063457\n",
      "Iteration 8219 Loss: 0.07614092755576912\n",
      "Iteration 8220 Loss: 0.07614081323753727\n",
      "Iteration 8221 Loss: 0.07614069892593876\n",
      "Iteration 8222 Loss: 0.07614058462097295\n",
      "Iteration 8223 Loss: 0.07614047032263963\n",
      "Iteration 8224 Loss: 0.07614035603093833\n",
      "Iteration 8225 Loss: 0.07614024174586859\n",
      "Iteration 8226 Loss: 0.07614012746743008\n",
      "Iteration 8227 Loss: 0.07614001319562244\n",
      "Iteration 8228 Loss: 0.07613989893044523\n",
      "Iteration 8229 Loss: 0.07613978467189804\n",
      "Iteration 8230 Loss: 0.07613967041998043\n",
      "Iteration 8231 Loss: 0.07613955617469204\n",
      "Iteration 8232 Loss: 0.07613944193603249\n",
      "Iteration 8233 Loss: 0.07613932770400134\n",
      "Iteration 8234 Loss: 0.07613921347859824\n",
      "Iteration 8235 Loss: 0.07613909925982276\n",
      "Iteration 8236 Loss: 0.07613898504767454\n",
      "Iteration 8237 Loss: 0.07613887084215308\n",
      "Iteration 8238 Loss: 0.07613875664325813\n",
      "Iteration 8239 Loss: 0.07613864245098914\n",
      "Iteration 8240 Loss: 0.07613852826534578\n",
      "Iteration 8241 Loss: 0.07613841408632767\n",
      "Iteration 8242 Loss: 0.07613829991393435\n",
      "Iteration 8243 Loss: 0.07613818574816549\n",
      "Iteration 8244 Loss: 0.0761380715890207\n",
      "Iteration 8245 Loss: 0.07613795743649951\n",
      "Iteration 8246 Loss: 0.07613784329060154\n",
      "Iteration 8247 Loss: 0.07613772915132644\n",
      "Iteration 8248 Loss: 0.07613761501867371\n",
      "Iteration 8249 Loss: 0.07613750089264308\n",
      "Iteration 8250 Loss: 0.0761373867732341\n",
      "Iteration 8251 Loss: 0.07613727266044634\n",
      "Iteration 8252 Loss: 0.0761371585542794\n",
      "Iteration 8253 Loss: 0.07613704445473292\n",
      "Iteration 8254 Loss: 0.07613693036180652\n",
      "Iteration 8255 Loss: 0.07613681627549974\n",
      "Iteration 8256 Loss: 0.07613670219581223\n",
      "Iteration 8257 Loss: 0.07613658812274356\n",
      "Iteration 8258 Loss: 0.07613647405629335\n",
      "Iteration 8259 Loss: 0.07613635999646118\n",
      "Iteration 8260 Loss: 0.07613624594324664\n",
      "Iteration 8261 Loss: 0.07613613189664943\n",
      "Iteration 8262 Loss: 0.07613601785666906\n",
      "Iteration 8263 Loss: 0.07613590382330514\n",
      "Iteration 8264 Loss: 0.07613578979655729\n",
      "Iteration 8265 Loss: 0.07613567577642515\n",
      "Iteration 8266 Loss: 0.07613556176290821\n",
      "Iteration 8267 Loss: 0.0761354477560062\n",
      "Iteration 8268 Loss: 0.07613533375571864\n",
      "Iteration 8269 Loss: 0.07613521976204517\n",
      "Iteration 8270 Loss: 0.0761351057749854\n",
      "Iteration 8271 Loss: 0.07613499179453892\n",
      "Iteration 8272 Loss: 0.0761348778207053\n",
      "Iteration 8273 Loss: 0.07613476385348414\n",
      "Iteration 8274 Loss: 0.07613464989287513\n",
      "Iteration 8275 Loss: 0.07613453593887781\n",
      "Iteration 8276 Loss: 0.07613442199149176\n",
      "Iteration 8277 Loss: 0.07613430805071664\n",
      "Iteration 8278 Loss: 0.07613419411655203\n",
      "Iteration 8279 Loss: 0.0761340801889975\n",
      "Iteration 8280 Loss: 0.07613396626805272\n",
      "Iteration 8281 Loss: 0.07613385235371722\n",
      "Iteration 8282 Loss: 0.07613373844599067\n",
      "Iteration 8283 Loss: 0.07613362454487263\n",
      "Iteration 8284 Loss: 0.07613351065036271\n",
      "Iteration 8285 Loss: 0.07613339676246056\n",
      "Iteration 8286 Loss: 0.07613328288116569\n",
      "Iteration 8287 Loss: 0.07613316900647776\n",
      "Iteration 8288 Loss: 0.07613305513839641\n",
      "Iteration 8289 Loss: 0.07613294127692113\n",
      "Iteration 8290 Loss: 0.07613282742205173\n",
      "Iteration 8291 Loss: 0.07613271357378756\n",
      "Iteration 8292 Loss: 0.07613259973212835\n",
      "Iteration 8293 Loss: 0.07613248589707378\n",
      "Iteration 8294 Loss: 0.07613237206862333\n",
      "Iteration 8295 Loss: 0.07613225824677664\n",
      "Iteration 8296 Loss: 0.07613214443153331\n",
      "Iteration 8297 Loss: 0.07613203062289299\n",
      "Iteration 8298 Loss: 0.07613191682085524\n",
      "Iteration 8299 Loss: 0.07613180302541968\n",
      "Iteration 8300 Loss: 0.07613168923658589\n",
      "Iteration 8301 Loss: 0.07613157545435349\n",
      "Iteration 8302 Loss: 0.0761314616787221\n",
      "Iteration 8303 Loss: 0.07613134790969135\n",
      "Iteration 8304 Loss: 0.07613123414726075\n",
      "Iteration 8305 Loss: 0.07613112039142995\n",
      "Iteration 8306 Loss: 0.07613100664219861\n",
      "Iteration 8307 Loss: 0.07613089289956627\n",
      "Iteration 8308 Loss: 0.07613077916353259\n",
      "Iteration 8309 Loss: 0.0761306654340971\n",
      "Iteration 8310 Loss: 0.07613055171125944\n",
      "Iteration 8311 Loss: 0.07613043799501926\n",
      "Iteration 8312 Loss: 0.07613032428537615\n",
      "Iteration 8313 Loss: 0.07613021058232962\n",
      "Iteration 8314 Loss: 0.07613009688587934\n",
      "Iteration 8315 Loss: 0.07612998319602494\n",
      "Iteration 8316 Loss: 0.07612986951276604\n",
      "Iteration 8317 Loss: 0.07612975583610215\n",
      "Iteration 8318 Loss: 0.07612964216603299\n",
      "Iteration 8319 Loss: 0.07612952850255814\n",
      "Iteration 8320 Loss: 0.07612941484567712\n",
      "Iteration 8321 Loss: 0.07612930119538953\n",
      "Iteration 8322 Loss: 0.07612918755169513\n",
      "Iteration 8323 Loss: 0.07612907391459346\n",
      "Iteration 8324 Loss: 0.07612896028408404\n",
      "Iteration 8325 Loss: 0.07612884666016655\n",
      "Iteration 8326 Loss: 0.07612873304284058\n",
      "Iteration 8327 Loss: 0.07612861943210573\n",
      "Iteration 8328 Loss: 0.07612850582796164\n",
      "Iteration 8329 Loss: 0.07612839223040785\n",
      "Iteration 8330 Loss: 0.07612827863944399\n",
      "Iteration 8331 Loss: 0.07612816505506974\n",
      "Iteration 8332 Loss: 0.07612805147728458\n",
      "Iteration 8333 Loss: 0.07612793790608822\n",
      "Iteration 8334 Loss: 0.07612782434148023\n",
      "Iteration 8335 Loss: 0.07612771078346026\n",
      "Iteration 8336 Loss: 0.07612759723202778\n",
      "Iteration 8337 Loss: 0.07612748368718257\n",
      "Iteration 8338 Loss: 0.07612737014892412\n",
      "Iteration 8339 Loss: 0.076127256617252\n",
      "Iteration 8340 Loss: 0.07612714309216599\n",
      "Iteration 8341 Loss: 0.07612702957366553\n",
      "Iteration 8342 Loss: 0.07612691606175032\n",
      "Iteration 8343 Loss: 0.07612680255641995\n",
      "Iteration 8344 Loss: 0.07612668905767393\n",
      "Iteration 8345 Loss: 0.076126575565512\n",
      "Iteration 8346 Loss: 0.07612646207993373\n",
      "Iteration 8347 Loss: 0.07612634860093874\n",
      "Iteration 8348 Loss: 0.07612623512852658\n",
      "Iteration 8349 Loss: 0.07612612166269687\n",
      "Iteration 8350 Loss: 0.07612600820344928\n",
      "Iteration 8351 Loss: 0.07612589475078334\n",
      "Iteration 8352 Loss: 0.07612578130469866\n",
      "Iteration 8353 Loss: 0.07612566786519488\n",
      "Iteration 8354 Loss: 0.07612555443227163\n",
      "Iteration 8355 Loss: 0.07612544100592848\n",
      "Iteration 8356 Loss: 0.07612532758616507\n",
      "Iteration 8357 Loss: 0.07612521417298092\n",
      "Iteration 8358 Loss: 0.07612510076637573\n",
      "Iteration 8359 Loss: 0.07612498736634912\n",
      "Iteration 8360 Loss: 0.07612487397290064\n",
      "Iteration 8361 Loss: 0.07612476058602989\n",
      "Iteration 8362 Loss: 0.0761246472057365\n",
      "Iteration 8363 Loss: 0.0761245338320201\n",
      "Iteration 8364 Loss: 0.07612442046488029\n",
      "Iteration 8365 Loss: 0.07612430710431664\n",
      "Iteration 8366 Loss: 0.07612419375032876\n",
      "Iteration 8367 Loss: 0.0761240804029163\n",
      "Iteration 8368 Loss: 0.07612396706207888\n",
      "Iteration 8369 Loss: 0.07612385372781605\n",
      "Iteration 8370 Loss: 0.07612374040012743\n",
      "Iteration 8371 Loss: 0.0761236270790126\n",
      "Iteration 8372 Loss: 0.0761235137644713\n",
      "Iteration 8373 Loss: 0.076123400456503\n",
      "Iteration 8374 Loss: 0.07612328715510734\n",
      "Iteration 8375 Loss: 0.07612317386028393\n",
      "Iteration 8376 Loss: 0.0761230605720325\n",
      "Iteration 8377 Loss: 0.07612294729035246\n",
      "Iteration 8378 Loss: 0.07612283401524354\n",
      "Iteration 8379 Loss: 0.07612272074670529\n",
      "Iteration 8380 Loss: 0.07612260748473734\n",
      "Iteration 8381 Loss: 0.07612249422933937\n",
      "Iteration 8382 Loss: 0.07612238098051084\n",
      "Iteration 8383 Loss: 0.07612226773825151\n",
      "Iteration 8384 Loss: 0.07612215450256091\n",
      "Iteration 8385 Loss: 0.0761220412734386\n",
      "Iteration 8386 Loss: 0.07612192805088429\n",
      "Iteration 8387 Loss: 0.07612181483489756\n",
      "Iteration 8388 Loss: 0.07612170162547796\n",
      "Iteration 8389 Loss: 0.07612158842262522\n",
      "Iteration 8390 Loss: 0.07612147522633879\n",
      "Iteration 8391 Loss: 0.07612136203661839\n",
      "Iteration 8392 Loss: 0.07612124885346362\n",
      "Iteration 8393 Loss: 0.07612113567687406\n",
      "Iteration 8394 Loss: 0.07612102250684934\n",
      "Iteration 8395 Loss: 0.07612090934338904\n",
      "Iteration 8396 Loss: 0.07612079618649284\n",
      "Iteration 8397 Loss: 0.07612068303616022\n",
      "Iteration 8398 Loss: 0.07612056989239088\n",
      "Iteration 8399 Loss: 0.07612045675518443\n",
      "Iteration 8400 Loss: 0.07612034362454051\n",
      "Iteration 8401 Loss: 0.0761202305004586\n",
      "Iteration 8402 Loss: 0.07612011738293846\n",
      "Iteration 8403 Loss: 0.07612000427197961\n",
      "Iteration 8404 Loss: 0.07611989116758168\n",
      "Iteration 8405 Loss: 0.07611977806974428\n",
      "Iteration 8406 Loss: 0.07611966497846706\n",
      "Iteration 8407 Loss: 0.07611955189374957\n",
      "Iteration 8408 Loss: 0.07611943881559141\n",
      "Iteration 8409 Loss: 0.07611932574399226\n",
      "Iteration 8410 Loss: 0.07611921267895166\n",
      "Iteration 8411 Loss: 0.07611909962046931\n",
      "Iteration 8412 Loss: 0.07611898656854472\n",
      "Iteration 8413 Loss: 0.0761188735231775\n",
      "Iteration 8414 Loss: 0.07611876048436735\n",
      "Iteration 8415 Loss: 0.07611864745211386\n",
      "Iteration 8416 Loss: 0.07611853442641656\n",
      "Iteration 8417 Loss: 0.07611842140727514\n",
      "Iteration 8418 Loss: 0.07611830839468914\n",
      "Iteration 8419 Loss: 0.0761181953886583\n",
      "Iteration 8420 Loss: 0.07611808238918209\n",
      "Iteration 8421 Loss: 0.0761179693962602\n",
      "Iteration 8422 Loss: 0.07611785640989216\n",
      "Iteration 8423 Loss: 0.07611774343007767\n",
      "Iteration 8424 Loss: 0.07611763045681628\n",
      "Iteration 8425 Loss: 0.07611751749010766\n",
      "Iteration 8426 Loss: 0.0761174045299514\n",
      "Iteration 8427 Loss: 0.07611729157634704\n",
      "Iteration 8428 Loss: 0.07611717862929429\n",
      "Iteration 8429 Loss: 0.07611706568879273\n",
      "Iteration 8430 Loss: 0.07611695275484194\n",
      "Iteration 8431 Loss: 0.07611683982744155\n",
      "Iteration 8432 Loss: 0.07611672690659115\n",
      "Iteration 8433 Loss: 0.07611661399229035\n",
      "Iteration 8434 Loss: 0.07611650108453882\n",
      "Iteration 8435 Loss: 0.07611638818333616\n",
      "Iteration 8436 Loss: 0.07611627528868194\n",
      "Iteration 8437 Loss: 0.07611616240057577\n",
      "Iteration 8438 Loss: 0.07611604951901728\n",
      "Iteration 8439 Loss: 0.07611593664400611\n",
      "Iteration 8440 Loss: 0.07611582377554178\n",
      "Iteration 8441 Loss: 0.07611571091362403\n",
      "Iteration 8442 Loss: 0.07611559805825237\n",
      "Iteration 8443 Loss: 0.07611548520942646\n",
      "Iteration 8444 Loss: 0.07611537236714583\n",
      "Iteration 8445 Loss: 0.07611525953141024\n",
      "Iteration 8446 Loss: 0.07611514670221918\n",
      "Iteration 8447 Loss: 0.07611503387957232\n",
      "Iteration 8448 Loss: 0.0761149210634692\n",
      "Iteration 8449 Loss: 0.07611480825390954\n",
      "Iteration 8450 Loss: 0.07611469545089285\n",
      "Iteration 8451 Loss: 0.07611458265441884\n",
      "Iteration 8452 Loss: 0.07611446986448706\n",
      "Iteration 8453 Loss: 0.07611435708109708\n",
      "Iteration 8454 Loss: 0.0761142443042486\n",
      "Iteration 8455 Loss: 0.07611413153394118\n",
      "Iteration 8456 Loss: 0.07611401877017447\n",
      "Iteration 8457 Loss: 0.07611390601294803\n",
      "Iteration 8458 Loss: 0.07611379326226154\n",
      "Iteration 8459 Loss: 0.07611368051811455\n",
      "Iteration 8460 Loss: 0.07611356778050667\n",
      "Iteration 8461 Loss: 0.07611345504943755\n",
      "Iteration 8462 Loss: 0.0761133423249068\n",
      "Iteration 8463 Loss: 0.07611322960691404\n",
      "Iteration 8464 Loss: 0.07611311689545881\n",
      "Iteration 8465 Loss: 0.07611300419054083\n",
      "Iteration 8466 Loss: 0.07611289149215962\n",
      "Iteration 8467 Loss: 0.07611277880031488\n",
      "Iteration 8468 Loss: 0.07611266611500611\n",
      "Iteration 8469 Loss: 0.07611255343623304\n",
      "Iteration 8470 Loss: 0.07611244076399518\n",
      "Iteration 8471 Loss: 0.07611232809829224\n",
      "Iteration 8472 Loss: 0.07611221543912376\n",
      "Iteration 8473 Loss: 0.07611210278648937\n",
      "Iteration 8474 Loss: 0.0761119901403887\n",
      "Iteration 8475 Loss: 0.07611187750082134\n",
      "Iteration 8476 Loss: 0.07611176486778694\n",
      "Iteration 8477 Loss: 0.07611165224128506\n",
      "Iteration 8478 Loss: 0.07611153962131537\n",
      "Iteration 8479 Loss: 0.07611142700787742\n",
      "Iteration 8480 Loss: 0.07611131440097088\n",
      "Iteration 8481 Loss: 0.07611120180059537\n",
      "Iteration 8482 Loss: 0.0761110892067504\n",
      "Iteration 8483 Loss: 0.07611097661943572\n",
      "Iteration 8484 Loss: 0.07611086403865083\n",
      "Iteration 8485 Loss: 0.07611075146439542\n",
      "Iteration 8486 Loss: 0.07611063889666908\n",
      "Iteration 8487 Loss: 0.07611052633547141\n",
      "Iteration 8488 Loss: 0.07611041378080202\n",
      "Iteration 8489 Loss: 0.07611030123266058\n",
      "Iteration 8490 Loss: 0.07611018869104663\n",
      "Iteration 8491 Loss: 0.07611007615595983\n",
      "Iteration 8492 Loss: 0.07610996362739969\n",
      "Iteration 8493 Loss: 0.076109851105366\n",
      "Iteration 8494 Loss: 0.07610973858985828\n",
      "Iteration 8495 Loss: 0.07610962608087615\n",
      "Iteration 8496 Loss: 0.07610951357841919\n",
      "Iteration 8497 Loss: 0.07610940108248704\n",
      "Iteration 8498 Loss: 0.07610928859307936\n",
      "Iteration 8499 Loss: 0.07610917611019571\n",
      "Iteration 8500 Loss: 0.0761090636338357\n",
      "Iteration 8501 Loss: 0.07610895116399896\n",
      "Iteration 8502 Loss: 0.0761088387006851\n",
      "Iteration 8503 Loss: 0.07610872624389377\n",
      "Iteration 8504 Loss: 0.0761086137936245\n",
      "Iteration 8505 Loss: 0.07610850134987701\n",
      "Iteration 8506 Loss: 0.07610838891265083\n",
      "Iteration 8507 Loss: 0.07610827648194561\n",
      "Iteration 8508 Loss: 0.07610816405776095\n",
      "Iteration 8509 Loss: 0.0761080516400965\n",
      "Iteration 8510 Loss: 0.07610793922895183\n",
      "Iteration 8511 Loss: 0.07610782682432657\n",
      "Iteration 8512 Loss: 0.07610771442622034\n",
      "Iteration 8513 Loss: 0.07610760203463272\n",
      "Iteration 8514 Loss: 0.0761074896495634\n",
      "Iteration 8515 Loss: 0.07610737727101193\n",
      "Iteration 8516 Loss: 0.0761072648989779\n",
      "Iteration 8517 Loss: 0.07610715253346101\n",
      "Iteration 8518 Loss: 0.0761070401744609\n",
      "Iteration 8519 Loss: 0.07610692782197703\n",
      "Iteration 8520 Loss: 0.0761068154760091\n",
      "Iteration 8521 Loss: 0.0761067031365568\n",
      "Iteration 8522 Loss: 0.07610659080361963\n",
      "Iteration 8523 Loss: 0.07610647847719722\n",
      "Iteration 8524 Loss: 0.07610636615728925\n",
      "Iteration 8525 Loss: 0.07610625384389526\n",
      "Iteration 8526 Loss: 0.07610614153701493\n",
      "Iteration 8527 Loss: 0.07610602923664783\n",
      "Iteration 8528 Loss: 0.07610591694279359\n",
      "Iteration 8529 Loss: 0.0761058046554518\n",
      "Iteration 8530 Loss: 0.0761056923746222\n",
      "Iteration 8531 Loss: 0.07610558010030423\n",
      "Iteration 8532 Loss: 0.07610546783249757\n",
      "Iteration 8533 Loss: 0.0761053555712019\n",
      "Iteration 8534 Loss: 0.07610524331641673\n",
      "Iteration 8535 Loss: 0.07610513106814178\n",
      "Iteration 8536 Loss: 0.07610501882637653\n",
      "Iteration 8537 Loss: 0.07610490659112078\n",
      "Iteration 8538 Loss: 0.076104794362374\n",
      "Iteration 8539 Loss: 0.0761046821401358\n",
      "Iteration 8540 Loss: 0.07610456992440592\n",
      "Iteration 8541 Loss: 0.07610445771518387\n",
      "Iteration 8542 Loss: 0.07610434551246927\n",
      "Iteration 8543 Loss: 0.07610423331626179\n",
      "Iteration 8544 Loss: 0.07610412112656106\n",
      "Iteration 8545 Loss: 0.07610400894336657\n",
      "Iteration 8546 Loss: 0.07610389676667806\n",
      "Iteration 8547 Loss: 0.07610378459649511\n",
      "Iteration 8548 Loss: 0.0761036724328173\n",
      "Iteration 8549 Loss: 0.0761035602756443\n",
      "Iteration 8550 Loss: 0.07610344812497569\n",
      "Iteration 8551 Loss: 0.07610333598081113\n",
      "Iteration 8552 Loss: 0.07610322384315014\n",
      "Iteration 8553 Loss: 0.07610311171199244\n",
      "Iteration 8554 Loss: 0.07610299958733761\n",
      "Iteration 8555 Loss: 0.07610288746918528\n",
      "Iteration 8556 Loss: 0.07610277535753503\n",
      "Iteration 8557 Loss: 0.07610266325238647\n",
      "Iteration 8558 Loss: 0.07610255115373925\n",
      "Iteration 8559 Loss: 0.076102439061593\n",
      "Iteration 8560 Loss: 0.07610232697594727\n",
      "Iteration 8561 Loss: 0.07610221489680177\n",
      "Iteration 8562 Loss: 0.07610210282415605\n",
      "Iteration 8563 Loss: 0.07610199075800972\n",
      "Iteration 8564 Loss: 0.07610187869836242\n",
      "Iteration 8565 Loss: 0.07610176664521379\n",
      "Iteration 8566 Loss: 0.07610165459856344\n",
      "Iteration 8567 Loss: 0.0761015425584109\n",
      "Iteration 8568 Loss: 0.07610143052475592\n",
      "Iteration 8569 Loss: 0.07610131849759803\n",
      "Iteration 8570 Loss: 0.07610120647693683\n",
      "Iteration 8571 Loss: 0.07610109446277204\n",
      "Iteration 8572 Loss: 0.07610098245510315\n",
      "Iteration 8573 Loss: 0.0761008704539299\n",
      "Iteration 8574 Loss: 0.0761007584592518\n",
      "Iteration 8575 Loss: 0.07610064647106847\n",
      "Iteration 8576 Loss: 0.07610053448937966\n",
      "Iteration 8577 Loss: 0.0761004225141849\n",
      "Iteration 8578 Loss: 0.07610031054548373\n",
      "Iteration 8579 Loss: 0.07610019858327587\n",
      "Iteration 8580 Loss: 0.07610008662756092\n",
      "Iteration 8581 Loss: 0.07609997467833847\n",
      "Iteration 8582 Loss: 0.07609986273560813\n",
      "Iteration 8583 Loss: 0.07609975079936956\n",
      "Iteration 8584 Loss: 0.07609963886962237\n",
      "Iteration 8585 Loss: 0.07609952694636614\n",
      "Iteration 8586 Loss: 0.07609941502960053\n",
      "Iteration 8587 Loss: 0.07609930311932513\n",
      "Iteration 8588 Loss: 0.07609919121553953\n",
      "Iteration 8589 Loss: 0.07609907931824342\n",
      "Iteration 8590 Loss: 0.07609896742743634\n",
      "Iteration 8591 Loss: 0.07609885554311802\n",
      "Iteration 8592 Loss: 0.07609874366528793\n",
      "Iteration 8593 Loss: 0.07609863179394583\n",
      "Iteration 8594 Loss: 0.07609851992909118\n",
      "Iteration 8595 Loss: 0.07609840807072377\n",
      "Iteration 8596 Loss: 0.0760982962188431\n",
      "Iteration 8597 Loss: 0.07609818437344887\n",
      "Iteration 8598 Loss: 0.07609807253454058\n",
      "Iteration 8599 Loss: 0.07609796070211795\n",
      "Iteration 8600 Loss: 0.07609784887618057\n",
      "Iteration 8601 Loss: 0.07609773705672804\n",
      "Iteration 8602 Loss: 0.07609762524376\n",
      "Iteration 8603 Loss: 0.07609751343727605\n",
      "Iteration 8604 Loss: 0.07609740163727584\n",
      "Iteration 8605 Loss: 0.07609728984375894\n",
      "Iteration 8606 Loss: 0.07609717805672502\n",
      "Iteration 8607 Loss: 0.07609706627617366\n",
      "Iteration 8608 Loss: 0.07609695450210449\n",
      "Iteration 8609 Loss: 0.07609684273451714\n",
      "Iteration 8610 Loss: 0.07609673097341119\n",
      "Iteration 8611 Loss: 0.07609661921878628\n",
      "Iteration 8612 Loss: 0.07609650747064206\n",
      "Iteration 8613 Loss: 0.07609639572897811\n",
      "Iteration 8614 Loss: 0.07609628399379403\n",
      "Iteration 8615 Loss: 0.07609617226508954\n",
      "Iteration 8616 Loss: 0.07609606054286411\n",
      "Iteration 8617 Loss: 0.07609594882711748\n",
      "Iteration 8618 Loss: 0.0760958371178492\n",
      "Iteration 8619 Loss: 0.07609572541505895\n",
      "Iteration 8620 Loss: 0.07609561371874625\n",
      "Iteration 8621 Loss: 0.0760955020289108\n",
      "Iteration 8622 Loss: 0.07609539034555221\n",
      "Iteration 8623 Loss: 0.07609527866867011\n",
      "Iteration 8624 Loss: 0.07609516699826403\n",
      "Iteration 8625 Loss: 0.07609505533433372\n",
      "Iteration 8626 Loss: 0.0760949436768787\n",
      "Iteration 8627 Loss: 0.07609483202589862\n",
      "Iteration 8628 Loss: 0.0760947203813931\n",
      "Iteration 8629 Loss: 0.07609460874336176\n",
      "Iteration 8630 Loss: 0.07609449711180423\n",
      "Iteration 8631 Loss: 0.0760943854867201\n",
      "Iteration 8632 Loss: 0.076094273868109\n",
      "Iteration 8633 Loss: 0.0760941622559706\n",
      "Iteration 8634 Loss: 0.0760940506503044\n",
      "Iteration 8635 Loss: 0.07609393905111016\n",
      "Iteration 8636 Loss: 0.07609382745838739\n",
      "Iteration 8637 Loss: 0.07609371587213577\n",
      "Iteration 8638 Loss: 0.0760936042923549\n",
      "Iteration 8639 Loss: 0.07609349271904439\n",
      "Iteration 8640 Loss: 0.07609338115220388\n",
      "Iteration 8641 Loss: 0.07609326959183296\n",
      "Iteration 8642 Loss: 0.07609315803793129\n",
      "Iteration 8643 Loss: 0.07609304649049847\n",
      "Iteration 8644 Loss: 0.0760929349495341\n",
      "Iteration 8645 Loss: 0.0760928234150378\n",
      "Iteration 8646 Loss: 0.0760927118870092\n",
      "Iteration 8647 Loss: 0.07609260036544796\n",
      "Iteration 8648 Loss: 0.07609248885035366\n",
      "Iteration 8649 Loss: 0.07609237734172593\n",
      "Iteration 8650 Loss: 0.07609226583956435\n",
      "Iteration 8651 Loss: 0.07609215434386858\n",
      "Iteration 8652 Loss: 0.07609204285463825\n",
      "Iteration 8653 Loss: 0.07609193137187298\n",
      "Iteration 8654 Loss: 0.07609181989557233\n",
      "Iteration 8655 Loss: 0.07609170842573597\n",
      "Iteration 8656 Loss: 0.07609159696236353\n",
      "Iteration 8657 Loss: 0.07609148550545458\n",
      "Iteration 8658 Loss: 0.07609137405500885\n",
      "Iteration 8659 Loss: 0.07609126261102579\n",
      "Iteration 8660 Loss: 0.07609115117350514\n",
      "Iteration 8661 Loss: 0.07609103974244653\n",
      "Iteration 8662 Loss: 0.07609092831784948\n",
      "Iteration 8663 Loss: 0.07609081689971373\n",
      "Iteration 8664 Loss: 0.07609070548803883\n",
      "Iteration 8665 Loss: 0.07609059408282437\n",
      "Iteration 8666 Loss: 0.07609048268407005\n",
      "Iteration 8667 Loss: 0.07609037129177544\n",
      "Iteration 8668 Loss: 0.0760902599059402\n",
      "Iteration 8669 Loss: 0.07609014852656389\n",
      "Iteration 8670 Loss: 0.07609003715364616\n",
      "Iteration 8671 Loss: 0.07608992578718668\n",
      "Iteration 8672 Loss: 0.076089814427185\n",
      "Iteration 8673 Loss: 0.07608970307364073\n",
      "Iteration 8674 Loss: 0.07608959172655357\n",
      "Iteration 8675 Loss: 0.07608948038592306\n",
      "Iteration 8676 Loss: 0.07608936905174887\n",
      "Iteration 8677 Loss: 0.07608925772403062\n",
      "Iteration 8678 Loss: 0.0760891464027679\n",
      "Iteration 8679 Loss: 0.07608903508796036\n",
      "Iteration 8680 Loss: 0.07608892377960763\n",
      "Iteration 8681 Loss: 0.07608881247770928\n",
      "Iteration 8682 Loss: 0.07608870118226493\n",
      "Iteration 8683 Loss: 0.07608858989327431\n",
      "Iteration 8684 Loss: 0.07608847861073693\n",
      "Iteration 8685 Loss: 0.07608836733465242\n",
      "Iteration 8686 Loss: 0.07608825606502041\n",
      "Iteration 8687 Loss: 0.07608814480184058\n",
      "Iteration 8688 Loss: 0.07608803354511245\n",
      "Iteration 8689 Loss: 0.07608792229483577\n",
      "Iteration 8690 Loss: 0.07608781105101003\n",
      "Iteration 8691 Loss: 0.07608769981363492\n",
      "Iteration 8692 Loss: 0.07608758858271006\n",
      "Iteration 8693 Loss: 0.0760874773582351\n",
      "Iteration 8694 Loss: 0.0760873661402095\n",
      "Iteration 8695 Loss: 0.07608725492863312\n",
      "Iteration 8696 Loss: 0.07608714372350545\n",
      "Iteration 8697 Loss: 0.07608703252482613\n",
      "Iteration 8698 Loss: 0.07608692133259472\n",
      "Iteration 8699 Loss: 0.07608681014681092\n",
      "Iteration 8700 Loss: 0.07608669896747433\n",
      "Iteration 8701 Loss: 0.07608658779458462\n",
      "Iteration 8702 Loss: 0.07608647662814126\n",
      "Iteration 8703 Loss: 0.07608636546814405\n",
      "Iteration 8704 Loss: 0.07608625431459255\n",
      "Iteration 8705 Loss: 0.07608614316748634\n",
      "Iteration 8706 Loss: 0.07608603202682503\n",
      "Iteration 8707 Loss: 0.07608592089260834\n",
      "Iteration 8708 Loss: 0.07608580976483582\n",
      "Iteration 8709 Loss: 0.07608569864350709\n",
      "Iteration 8710 Loss: 0.0760855875286218\n",
      "Iteration 8711 Loss: 0.07608547642017953\n",
      "Iteration 8712 Loss: 0.07608536531817994\n",
      "Iteration 8713 Loss: 0.07608525422262263\n",
      "Iteration 8714 Loss: 0.07608514313350721\n",
      "Iteration 8715 Loss: 0.0760850320508334\n",
      "Iteration 8716 Loss: 0.07608492097460073\n",
      "Iteration 8717 Loss: 0.07608480990480879\n",
      "Iteration 8718 Loss: 0.07608469884145729\n",
      "Iteration 8719 Loss: 0.07608458778454576\n",
      "Iteration 8720 Loss: 0.07608447673407395\n",
      "Iteration 8721 Loss: 0.07608436569004136\n",
      "Iteration 8722 Loss: 0.07608425465244767\n",
      "Iteration 8723 Loss: 0.07608414362129248\n",
      "Iteration 8724 Loss: 0.07608403259657542\n",
      "Iteration 8725 Loss: 0.07608392157829613\n",
      "Iteration 8726 Loss: 0.07608381056645422\n",
      "Iteration 8727 Loss: 0.07608369956104932\n",
      "Iteration 8728 Loss: 0.07608358856208099\n",
      "Iteration 8729 Loss: 0.07608347756954893\n",
      "Iteration 8730 Loss: 0.07608336658345272\n",
      "Iteration 8731 Loss: 0.07608325560379205\n",
      "Iteration 8732 Loss: 0.07608314463056647\n",
      "Iteration 8733 Loss: 0.07608303366377558\n",
      "Iteration 8734 Loss: 0.0760829227034191\n",
      "Iteration 8735 Loss: 0.07608281174949658\n",
      "Iteration 8736 Loss: 0.07608270080200766\n",
      "Iteration 8737 Loss: 0.07608258986095195\n",
      "Iteration 8738 Loss: 0.07608247892632908\n",
      "Iteration 8739 Loss: 0.07608236799813872\n",
      "Iteration 8740 Loss: 0.07608225707638043\n",
      "Iteration 8741 Loss: 0.07608214616105395\n",
      "Iteration 8742 Loss: 0.07608203525215866\n",
      "Iteration 8743 Loss: 0.07608192434969437\n",
      "Iteration 8744 Loss: 0.07608181345366068\n",
      "Iteration 8745 Loss: 0.07608170256405718\n",
      "Iteration 8746 Loss: 0.07608159168088348\n",
      "Iteration 8747 Loss: 0.07608148080413932\n",
      "Iteration 8748 Loss: 0.07608136993382417\n",
      "Iteration 8749 Loss: 0.07608125906993773\n",
      "Iteration 8750 Loss: 0.07608114821247963\n",
      "Iteration 8751 Loss: 0.07608103736144944\n",
      "Iteration 8752 Loss: 0.07608092651684688\n",
      "Iteration 8753 Loss: 0.07608081567867143\n",
      "Iteration 8754 Loss: 0.07608070484692284\n",
      "Iteration 8755 Loss: 0.07608059402160068\n",
      "Iteration 8756 Loss: 0.07608048320270455\n",
      "Iteration 8757 Loss: 0.07608037239023412\n",
      "Iteration 8758 Loss: 0.07608026158418904\n",
      "Iteration 8759 Loss: 0.07608015078456883\n",
      "Iteration 8760 Loss: 0.07608003999137322\n",
      "Iteration 8761 Loss: 0.0760799292046017\n",
      "Iteration 8762 Loss: 0.07607981842425404\n",
      "Iteration 8763 Loss: 0.07607970765032986\n",
      "Iteration 8764 Loss: 0.07607959688282864\n",
      "Iteration 8765 Loss: 0.07607948612175015\n",
      "Iteration 8766 Loss: 0.07607937536709387\n",
      "Iteration 8767 Loss: 0.07607926461885955\n",
      "Iteration 8768 Loss: 0.07607915387704683\n",
      "Iteration 8769 Loss: 0.07607904314165523\n",
      "Iteration 8770 Loss: 0.07607893241268439\n",
      "Iteration 8771 Loss: 0.07607882169013397\n",
      "Iteration 8772 Loss: 0.07607871097400361\n",
      "Iteration 8773 Loss: 0.0760786002642929\n",
      "Iteration 8774 Loss: 0.07607848956100148\n",
      "Iteration 8775 Loss: 0.07607837886412894\n",
      "Iteration 8776 Loss: 0.07607826817367493\n",
      "Iteration 8777 Loss: 0.0760781574896391\n",
      "Iteration 8778 Loss: 0.07607804681202102\n",
      "Iteration 8779 Loss: 0.07607793614082035\n",
      "Iteration 8780 Loss: 0.07607782547603677\n",
      "Iteration 8781 Loss: 0.07607771481766976\n",
      "Iteration 8782 Loss: 0.07607760416571901\n",
      "Iteration 8783 Loss: 0.07607749352018424\n",
      "Iteration 8784 Loss: 0.07607738288106497\n",
      "Iteration 8785 Loss: 0.0760772722483608\n",
      "Iteration 8786 Loss: 0.07607716162207144\n",
      "Iteration 8787 Loss: 0.0760770510021965\n",
      "Iteration 8788 Loss: 0.0760769403887355\n",
      "Iteration 8789 Loss: 0.07607682978168823\n",
      "Iteration 8790 Loss: 0.07607671918105414\n",
      "Iteration 8791 Loss: 0.076076608586833\n",
      "Iteration 8792 Loss: 0.07607649799902436\n",
      "Iteration 8793 Loss: 0.07607638741762787\n",
      "Iteration 8794 Loss: 0.07607627684264315\n",
      "Iteration 8795 Loss: 0.0760761662740698\n",
      "Iteration 8796 Loss: 0.07607605571190751\n",
      "Iteration 8797 Loss: 0.07607594515615576\n",
      "Iteration 8798 Loss: 0.07607583460681437\n",
      "Iteration 8799 Loss: 0.07607572406388283\n",
      "Iteration 8800 Loss: 0.07607561352736081\n",
      "Iteration 8801 Loss: 0.07607550299724791\n",
      "Iteration 8802 Loss: 0.0760753924735438\n",
      "Iteration 8803 Loss: 0.07607528195624806\n",
      "Iteration 8804 Loss: 0.07607517144536033\n",
      "Iteration 8805 Loss: 0.07607506094088023\n",
      "Iteration 8806 Loss: 0.07607495044280743\n",
      "Iteration 8807 Loss: 0.07607483995114142\n",
      "Iteration 8808 Loss: 0.07607472946588201\n",
      "Iteration 8809 Loss: 0.07607461898702873\n",
      "Iteration 8810 Loss: 0.0760745085145812\n",
      "Iteration 8811 Loss: 0.07607439804853901\n",
      "Iteration 8812 Loss: 0.07607428758890185\n",
      "Iteration 8813 Loss: 0.07607417713566936\n",
      "Iteration 8814 Loss: 0.0760740666888411\n",
      "Iteration 8815 Loss: 0.07607395624841676\n",
      "Iteration 8816 Loss: 0.0760738458143959\n",
      "Iteration 8817 Loss: 0.07607373538677818\n",
      "Iteration 8818 Loss: 0.07607362496556323\n",
      "Iteration 8819 Loss: 0.07607351455075066\n",
      "Iteration 8820 Loss: 0.07607340414234008\n",
      "Iteration 8821 Loss: 0.07607329374033119\n",
      "Iteration 8822 Loss: 0.07607318334472352\n",
      "Iteration 8823 Loss: 0.07607307295551671\n",
      "Iteration 8824 Loss: 0.07607296257271043\n",
      "Iteration 8825 Loss: 0.07607285219630434\n",
      "Iteration 8826 Loss: 0.07607274182629795\n",
      "Iteration 8827 Loss: 0.07607263146269097\n",
      "Iteration 8828 Loss: 0.076072521105483\n",
      "Iteration 8829 Loss: 0.0760724107546737\n",
      "Iteration 8830 Loss: 0.07607230041026265\n",
      "Iteration 8831 Loss: 0.0760721900722495\n",
      "Iteration 8832 Loss: 0.07607207974063385\n",
      "Iteration 8833 Loss: 0.07607196941541532\n",
      "Iteration 8834 Loss: 0.07607185909659359\n",
      "Iteration 8835 Loss: 0.07607174878416824\n",
      "Iteration 8836 Loss: 0.0760716384781389\n",
      "Iteration 8837 Loss: 0.07607152817850524\n",
      "Iteration 8838 Loss: 0.07607141788526685\n",
      "Iteration 8839 Loss: 0.07607130759842333\n",
      "Iteration 8840 Loss: 0.0760711973179743\n",
      "Iteration 8841 Loss: 0.07607108704391945\n",
      "Iteration 8842 Loss: 0.07607097677625838\n",
      "Iteration 8843 Loss: 0.07607086651499072\n",
      "Iteration 8844 Loss: 0.07607075626011606\n",
      "Iteration 8845 Loss: 0.07607064601163407\n",
      "Iteration 8846 Loss: 0.07607053576954434\n",
      "Iteration 8847 Loss: 0.07607042553384656\n",
      "Iteration 8848 Loss: 0.07607031530454025\n",
      "Iteration 8849 Loss: 0.07607020508162514\n",
      "Iteration 8850 Loss: 0.07607009486510082\n",
      "Iteration 8851 Loss: 0.07606998465496682\n",
      "Iteration 8852 Loss: 0.07606987445122293\n",
      "Iteration 8853 Loss: 0.07606976425386866\n",
      "Iteration 8854 Loss: 0.07606965406290374\n",
      "Iteration 8855 Loss: 0.0760695438783277\n",
      "Iteration 8856 Loss: 0.07606943370014017\n",
      "Iteration 8857 Loss: 0.07606932352834084\n",
      "Iteration 8858 Loss: 0.0760692133629293\n",
      "Iteration 8859 Loss: 0.07606910320390518\n",
      "Iteration 8860 Loss: 0.07606899305126812\n",
      "Iteration 8861 Loss: 0.07606888290501765\n",
      "Iteration 8862 Loss: 0.07606877276515356\n",
      "Iteration 8863 Loss: 0.07606866263167536\n",
      "Iteration 8864 Loss: 0.07606855250458276\n",
      "Iteration 8865 Loss: 0.07606844238387532\n",
      "Iteration 8866 Loss: 0.0760683322695526\n",
      "Iteration 8867 Loss: 0.07606822216161441\n",
      "Iteration 8868 Loss: 0.0760681120600602\n",
      "Iteration 8869 Loss: 0.07606800196488975\n",
      "Iteration 8870 Loss: 0.07606789187610258\n",
      "Iteration 8871 Loss: 0.07606778179369833\n",
      "Iteration 8872 Loss: 0.07606767171767666\n",
      "Iteration 8873 Loss: 0.07606756164803719\n",
      "Iteration 8874 Loss: 0.07606745158477951\n",
      "Iteration 8875 Loss: 0.0760673415279033\n",
      "Iteration 8876 Loss: 0.07606723147740815\n",
      "Iteration 8877 Loss: 0.07606712143329374\n",
      "Iteration 8878 Loss: 0.07606701139555962\n",
      "Iteration 8879 Loss: 0.07606690136420541\n",
      "Iteration 8880 Loss: 0.07606679133923083\n",
      "Iteration 8881 Loss: 0.07606668132063549\n",
      "Iteration 8882 Loss: 0.07606657130841893\n",
      "Iteration 8883 Loss: 0.07606646130258085\n",
      "Iteration 8884 Loss: 0.07606635130312087\n",
      "Iteration 8885 Loss: 0.07606624131003854\n",
      "Iteration 8886 Loss: 0.07606613132333359\n",
      "Iteration 8887 Loss: 0.07606602134300562\n",
      "Iteration 8888 Loss: 0.07606591136905429\n",
      "Iteration 8889 Loss: 0.07606580140147912\n",
      "Iteration 8890 Loss: 0.07606569144027983\n",
      "Iteration 8891 Loss: 0.07606558148545602\n",
      "Iteration 8892 Loss: 0.07606547153700727\n",
      "Iteration 8893 Loss: 0.07606536159493334\n",
      "Iteration 8894 Loss: 0.07606525165923368\n",
      "Iteration 8895 Loss: 0.07606514172990807\n",
      "Iteration 8896 Loss: 0.07606503180695605\n",
      "Iteration 8897 Loss: 0.0760649218903773\n",
      "Iteration 8898 Loss: 0.07606481198017141\n",
      "Iteration 8899 Loss: 0.07606470207633802\n",
      "Iteration 8900 Loss: 0.07606459217887673\n",
      "Iteration 8901 Loss: 0.07606448228778721\n",
      "Iteration 8902 Loss: 0.07606437240306906\n",
      "Iteration 8903 Loss: 0.07606426252472194\n",
      "Iteration 8904 Loss: 0.0760641526527455\n",
      "Iteration 8905 Loss: 0.07606404278713925\n",
      "Iteration 8906 Loss: 0.07606393292790291\n",
      "Iteration 8907 Loss: 0.07606382307503608\n",
      "Iteration 8908 Loss: 0.07606371322853846\n",
      "Iteration 8909 Loss: 0.07606360338840958\n",
      "Iteration 8910 Loss: 0.07606349355464907\n",
      "Iteration 8911 Loss: 0.07606338372725661\n",
      "Iteration 8912 Loss: 0.07606327390623184\n",
      "Iteration 8913 Loss: 0.07606316409157436\n",
      "Iteration 8914 Loss: 0.07606305428328375\n",
      "Iteration 8915 Loss: 0.07606294448135967\n",
      "Iteration 8916 Loss: 0.07606283468580183\n",
      "Iteration 8917 Loss: 0.07606272489660978\n",
      "Iteration 8918 Loss: 0.07606261511378312\n",
      "Iteration 8919 Loss: 0.0760625053373215\n",
      "Iteration 8920 Loss: 0.07606239556722462\n",
      "Iteration 8921 Loss: 0.07606228580349207\n",
      "Iteration 8922 Loss: 0.07606217604612339\n",
      "Iteration 8923 Loss: 0.07606206629511832\n",
      "Iteration 8924 Loss: 0.07606195655047641\n",
      "Iteration 8925 Loss: 0.07606184681219735\n",
      "Iteration 8926 Loss: 0.07606173708028077\n",
      "Iteration 8927 Loss: 0.0760616273547263\n",
      "Iteration 8928 Loss: 0.07606151763553343\n",
      "Iteration 8929 Loss: 0.07606140792270198\n",
      "Iteration 8930 Loss: 0.07606129821623143\n",
      "Iteration 8931 Loss: 0.07606118851612154\n",
      "Iteration 8932 Loss: 0.07606107882237187\n",
      "Iteration 8933 Loss: 0.07606096913498207\n",
      "Iteration 8934 Loss: 0.07606085945395173\n",
      "Iteration 8935 Loss: 0.07606074977928051\n",
      "Iteration 8936 Loss: 0.07606064011096801\n",
      "Iteration 8937 Loss: 0.07606053044901387\n",
      "Iteration 8938 Loss: 0.07606042079341774\n",
      "Iteration 8939 Loss: 0.07606031114417922\n",
      "Iteration 8940 Loss: 0.076060201501298\n",
      "Iteration 8941 Loss: 0.07606009186477362\n",
      "Iteration 8942 Loss: 0.07605998223460576\n",
      "Iteration 8943 Loss: 0.07605987261079405\n",
      "Iteration 8944 Loss: 0.07605976299333805\n",
      "Iteration 8945 Loss: 0.07605965338223754\n",
      "Iteration 8946 Loss: 0.07605954377749204\n",
      "Iteration 8947 Loss: 0.07605943417910117\n",
      "Iteration 8948 Loss: 0.07605932458706456\n",
      "Iteration 8949 Loss: 0.07605921500138192\n",
      "Iteration 8950 Loss: 0.07605910542205278\n",
      "Iteration 8951 Loss: 0.07605899584907681\n",
      "Iteration 8952 Loss: 0.07605888628245369\n",
      "Iteration 8953 Loss: 0.07605877672218297\n",
      "Iteration 8954 Loss: 0.07605866716826432\n",
      "Iteration 8955 Loss: 0.07605855762069733\n",
      "Iteration 8956 Loss: 0.07605844807948176\n",
      "Iteration 8957 Loss: 0.076058338544617\n",
      "Iteration 8958 Loss: 0.07605822901610289\n",
      "Iteration 8959 Loss: 0.07605811949393894\n",
      "Iteration 8960 Loss: 0.07605800997812484\n",
      "Iteration 8961 Loss: 0.0760579004686602\n",
      "Iteration 8962 Loss: 0.07605779096554471\n",
      "Iteration 8963 Loss: 0.07605768146877788\n",
      "Iteration 8964 Loss: 0.07605757197835937\n",
      "Iteration 8965 Loss: 0.07605746249428894\n",
      "Iteration 8966 Loss: 0.07605735301656609\n",
      "Iteration 8967 Loss: 0.07605724354519046\n",
      "Iteration 8968 Loss: 0.07605713408016174\n",
      "Iteration 8969 Loss: 0.07605702462147947\n",
      "Iteration 8970 Loss: 0.07605691516914334\n",
      "Iteration 8971 Loss: 0.07605680572315299\n",
      "Iteration 8972 Loss: 0.07605669628350802\n",
      "Iteration 8973 Loss: 0.07605658685020804\n",
      "Iteration 8974 Loss: 0.07605647742325275\n",
      "Iteration 8975 Loss: 0.07605636800264166\n",
      "Iteration 8976 Loss: 0.07605625858837457\n",
      "Iteration 8977 Loss: 0.07605614918045099\n",
      "Iteration 8978 Loss: 0.07605603977887054\n",
      "Iteration 8979 Loss: 0.07605593038363292\n",
      "Iteration 8980 Loss: 0.07605582099473776\n",
      "Iteration 8981 Loss: 0.07605571161218463\n",
      "Iteration 8982 Loss: 0.07605560223597316\n",
      "Iteration 8983 Loss: 0.07605549286610301\n",
      "Iteration 8984 Loss: 0.07605538350257385\n",
      "Iteration 8985 Loss: 0.07605527414538522\n",
      "Iteration 8986 Loss: 0.0760551647945368\n",
      "Iteration 8987 Loss: 0.07605505545002822\n",
      "Iteration 8988 Loss: 0.07605494611185915\n",
      "Iteration 8989 Loss: 0.07605483678002913\n",
      "Iteration 8990 Loss: 0.0760547274545378\n",
      "Iteration 8991 Loss: 0.07605461813538493\n",
      "Iteration 8992 Loss: 0.07605450882256996\n",
      "Iteration 8993 Loss: 0.07605439951609265\n",
      "Iteration 8994 Loss: 0.07605429021595257\n",
      "Iteration 8995 Loss: 0.0760541809221494\n",
      "Iteration 8996 Loss: 0.07605407163468265\n",
      "Iteration 8997 Loss: 0.07605396235355213\n",
      "Iteration 8998 Loss: 0.07605385307875742\n",
      "Iteration 8999 Loss: 0.07605374381029803\n",
      "Iteration 9000 Loss: 0.07605363454817367\n",
      "Iteration 9001 Loss: 0.07605352529238395\n",
      "Iteration 9002 Loss: 0.07605341604292859\n",
      "Iteration 9003 Loss: 0.07605330679980708\n",
      "Iteration 9004 Loss: 0.07605319756301915\n",
      "Iteration 9005 Loss: 0.0760530883325644\n",
      "Iteration 9006 Loss: 0.07605297910844248\n",
      "Iteration 9007 Loss: 0.07605286989065302\n",
      "Iteration 9008 Loss: 0.07605276067919559\n",
      "Iteration 9009 Loss: 0.0760526514740699\n",
      "Iteration 9010 Loss: 0.07605254227527547\n",
      "Iteration 9011 Loss: 0.07605243308281207\n",
      "Iteration 9012 Loss: 0.07605232389667928\n",
      "Iteration 9013 Loss: 0.07605221471687669\n",
      "Iteration 9014 Loss: 0.07605210554340397\n",
      "Iteration 9015 Loss: 0.07605199637626071\n",
      "Iteration 9016 Loss: 0.07605188721544658\n",
      "Iteration 9017 Loss: 0.07605177806096125\n",
      "Iteration 9018 Loss: 0.07605166891280424\n",
      "Iteration 9019 Loss: 0.07605155977097523\n",
      "Iteration 9020 Loss: 0.07605145063547393\n",
      "Iteration 9021 Loss: 0.07605134150629989\n",
      "Iteration 9022 Loss: 0.07605123238345277\n",
      "Iteration 9023 Loss: 0.0760511232669321\n",
      "Iteration 9024 Loss: 0.07605101415673762\n",
      "Iteration 9025 Loss: 0.076050905052869\n",
      "Iteration 9026 Loss: 0.07605079595532578\n",
      "Iteration 9027 Loss: 0.07605068686410765\n",
      "Iteration 9028 Loss: 0.07605057777921419\n",
      "Iteration 9029 Loss: 0.07605046870064502\n",
      "Iteration 9030 Loss: 0.07605035962839982\n",
      "Iteration 9031 Loss: 0.07605025056247823\n",
      "Iteration 9032 Loss: 0.07605014150287986\n",
      "Iteration 9033 Loss: 0.07605003244960429\n",
      "Iteration 9034 Loss: 0.07604992340265125\n",
      "Iteration 9035 Loss: 0.07604981436202028\n",
      "Iteration 9036 Loss: 0.07604970532771112\n",
      "Iteration 9037 Loss: 0.07604959629972324\n",
      "Iteration 9038 Loss: 0.07604948727805644\n",
      "Iteration 9039 Loss: 0.07604937826271022\n",
      "Iteration 9040 Loss: 0.07604926925368431\n",
      "Iteration 9041 Loss: 0.07604916025097828\n",
      "Iteration 9042 Loss: 0.07604905125459178\n",
      "Iteration 9043 Loss: 0.07604894226452447\n",
      "Iteration 9044 Loss: 0.07604883328077594\n",
      "Iteration 9045 Loss: 0.07604872430334582\n",
      "Iteration 9046 Loss: 0.07604861533223377\n",
      "Iteration 9047 Loss: 0.07604850636743939\n",
      "Iteration 9048 Loss: 0.07604839740896235\n",
      "Iteration 9049 Loss: 0.07604828845680223\n",
      "Iteration 9050 Loss: 0.07604817951095874\n",
      "Iteration 9051 Loss: 0.07604807057143144\n",
      "Iteration 9052 Loss: 0.07604796163822003\n",
      "Iteration 9053 Loss: 0.07604785271132403\n",
      "Iteration 9054 Loss: 0.07604774379074317\n",
      "Iteration 9055 Loss: 0.07604763487647703\n",
      "Iteration 9056 Loss: 0.07604752596852529\n",
      "Iteration 9057 Loss: 0.07604741706688761\n",
      "Iteration 9058 Loss: 0.0760473081715635\n",
      "Iteration 9059 Loss: 0.07604719928255271\n",
      "Iteration 9060 Loss: 0.07604709039985477\n",
      "Iteration 9061 Loss: 0.0760469815234694\n",
      "Iteration 9062 Loss: 0.07604687265339619\n",
      "Iteration 9063 Loss: 0.07604676378963476\n",
      "Iteration 9064 Loss: 0.07604665493218477\n",
      "Iteration 9065 Loss: 0.07604654608104588\n",
      "Iteration 9066 Loss: 0.07604643723621762\n",
      "Iteration 9067 Loss: 0.07604632839769973\n",
      "Iteration 9068 Loss: 0.0760462195654918\n",
      "Iteration 9069 Loss: 0.07604611073959346\n",
      "Iteration 9070 Loss: 0.0760460019200043\n",
      "Iteration 9071 Loss: 0.07604589310672406\n",
      "Iteration 9072 Loss: 0.07604578429975226\n",
      "Iteration 9073 Loss: 0.07604567549908861\n",
      "Iteration 9074 Loss: 0.07604556670473275\n",
      "Iteration 9075 Loss: 0.07604545791668421\n",
      "Iteration 9076 Loss: 0.07604534913494275\n",
      "Iteration 9077 Loss: 0.07604524035950791\n",
      "Iteration 9078 Loss: 0.07604513159037932\n",
      "Iteration 9079 Loss: 0.07604502282755667\n",
      "Iteration 9080 Loss: 0.07604491407103954\n",
      "Iteration 9081 Loss: 0.07604480532082766\n",
      "Iteration 9082 Loss: 0.07604469657692053\n",
      "Iteration 9083 Loss: 0.07604458783931789\n",
      "Iteration 9084 Loss: 0.07604447910801931\n",
      "Iteration 9085 Loss: 0.07604437038302446\n",
      "Iteration 9086 Loss: 0.07604426166433295\n",
      "Iteration 9087 Loss: 0.0760441529519444\n",
      "Iteration 9088 Loss: 0.07604404424585848\n",
      "Iteration 9089 Loss: 0.07604393554607476\n",
      "Iteration 9090 Loss: 0.07604382685259296\n",
      "Iteration 9091 Loss: 0.07604371816541262\n",
      "Iteration 9092 Loss: 0.07604360948453348\n",
      "Iteration 9093 Loss: 0.0760435008099551\n",
      "Iteration 9094 Loss: 0.0760433921416771\n",
      "Iteration 9095 Loss: 0.07604328347969913\n",
      "Iteration 9096 Loss: 0.07604317482402088\n",
      "Iteration 9097 Loss: 0.07604306617464188\n",
      "Iteration 9098 Loss: 0.07604295753156187\n",
      "Iteration 9099 Loss: 0.0760428488947804\n",
      "Iteration 9100 Loss: 0.07604274026429715\n",
      "Iteration 9101 Loss: 0.07604263164011174\n",
      "Iteration 9102 Loss: 0.0760425230222238\n",
      "Iteration 9103 Loss: 0.07604241441063295\n",
      "Iteration 9104 Loss: 0.07604230580533886\n",
      "Iteration 9105 Loss: 0.07604219720634112\n",
      "Iteration 9106 Loss: 0.0760420886136394\n",
      "Iteration 9107 Loss: 0.07604198002723328\n",
      "Iteration 9108 Loss: 0.07604187144712246\n",
      "Iteration 9109 Loss: 0.07604176287330652\n",
      "Iteration 9110 Loss: 0.07604165430578516\n",
      "Iteration 9111 Loss: 0.07604154574455796\n",
      "Iteration 9112 Loss: 0.07604143718962451\n",
      "Iteration 9113 Loss: 0.07604132864098455\n",
      "Iteration 9114 Loss: 0.07604122009863767\n",
      "Iteration 9115 Loss: 0.07604111156258343\n",
      "Iteration 9116 Loss: 0.07604100303282153\n",
      "Iteration 9117 Loss: 0.0760408945093517\n",
      "Iteration 9118 Loss: 0.0760407859921734\n",
      "Iteration 9119 Loss: 0.0760406774812863\n",
      "Iteration 9120 Loss: 0.07604056897669018\n",
      "Iteration 9121 Loss: 0.07604046047838443\n",
      "Iteration 9122 Loss: 0.07604035198636891\n",
      "Iteration 9123 Loss: 0.07604024350064321\n",
      "Iteration 9124 Loss: 0.07604013502120677\n",
      "Iteration 9125 Loss: 0.07604002654805944\n",
      "Iteration 9126 Loss: 0.0760399180812008\n",
      "Iteration 9127 Loss: 0.07603980962063045\n",
      "Iteration 9128 Loss: 0.07603970116634803\n",
      "Iteration 9129 Loss: 0.07603959271835321\n",
      "Iteration 9130 Loss: 0.07603948427664559\n",
      "Iteration 9131 Loss: 0.07603937584122482\n",
      "Iteration 9132 Loss: 0.07603926741209052\n",
      "Iteration 9133 Loss: 0.0760391589892423\n",
      "Iteration 9134 Loss: 0.07603905057267983\n",
      "Iteration 9135 Loss: 0.07603894216240277\n",
      "Iteration 9136 Loss: 0.07603883375841072\n",
      "Iteration 9137 Loss: 0.0760387253607033\n",
      "Iteration 9138 Loss: 0.07603861696928016\n",
      "Iteration 9139 Loss: 0.07603850858414091\n",
      "Iteration 9140 Loss: 0.07603840020528527\n",
      "Iteration 9141 Loss: 0.07603829183271277\n",
      "Iteration 9142 Loss: 0.07603818346642306\n",
      "Iteration 9143 Loss: 0.07603807510641584\n",
      "Iteration 9144 Loss: 0.07603796675269064\n",
      "Iteration 9145 Loss: 0.07603785840524728\n",
      "Iteration 9146 Loss: 0.07603775006408516\n",
      "Iteration 9147 Loss: 0.07603764172920408\n",
      "Iteration 9148 Loss: 0.07603753340060364\n",
      "Iteration 9149 Loss: 0.07603742507828341\n",
      "Iteration 9150 Loss: 0.07603731676224305\n",
      "Iteration 9151 Loss: 0.07603720845248228\n",
      "Iteration 9152 Loss: 0.07603710014900063\n",
      "Iteration 9153 Loss: 0.07603699185179778\n",
      "Iteration 9154 Loss: 0.0760368835608733\n",
      "Iteration 9155 Loss: 0.07603677527622696\n",
      "Iteration 9156 Loss: 0.0760366669978583\n",
      "Iteration 9157 Loss: 0.07603655872576695\n",
      "Iteration 9158 Loss: 0.07603645045995257\n",
      "Iteration 9159 Loss: 0.07603634220041483\n",
      "Iteration 9160 Loss: 0.07603623394715328\n",
      "Iteration 9161 Loss: 0.07603612570016761\n",
      "Iteration 9162 Loss: 0.07603601745945744\n",
      "Iteration 9163 Loss: 0.07603590922502243\n",
      "Iteration 9164 Loss: 0.07603580099686216\n",
      "Iteration 9165 Loss: 0.07603569277497631\n",
      "Iteration 9166 Loss: 0.0760355845593645\n",
      "Iteration 9167 Loss: 0.07603547635002639\n",
      "Iteration 9168 Loss: 0.07603536814696155\n",
      "Iteration 9169 Loss: 0.0760352599501697\n",
      "Iteration 9170 Loss: 0.07603515175965035\n",
      "Iteration 9171 Loss: 0.07603504357540328\n",
      "Iteration 9172 Loss: 0.07603493539742806\n",
      "Iteration 9173 Loss: 0.07603482722572433\n",
      "Iteration 9174 Loss: 0.0760347190602917\n",
      "Iteration 9175 Loss: 0.07603461090112985\n",
      "Iteration 9176 Loss: 0.07603450274823835\n",
      "Iteration 9177 Loss: 0.07603439460161693\n",
      "Iteration 9178 Loss: 0.07603428646126513\n",
      "Iteration 9179 Loss: 0.07603417832718261\n",
      "Iteration 9180 Loss: 0.07603407019936909\n",
      "Iteration 9181 Loss: 0.07603396207782404\n",
      "Iteration 9182 Loss: 0.0760338539625473\n",
      "Iteration 9183 Loss: 0.07603374585353832\n",
      "Iteration 9184 Loss: 0.07603363775079683\n",
      "Iteration 9185 Loss: 0.07603352965432247\n",
      "Iteration 9186 Loss: 0.07603342156411483\n",
      "Iteration 9187 Loss: 0.07603331348017357\n",
      "Iteration 9188 Loss: 0.07603320540249836\n",
      "Iteration 9189 Loss: 0.07603309733108869\n",
      "Iteration 9190 Loss: 0.07603298926594433\n",
      "Iteration 9191 Loss: 0.07603288120706499\n",
      "Iteration 9192 Loss: 0.07603277315445016\n",
      "Iteration 9193 Loss: 0.07603266510809949\n",
      "Iteration 9194 Loss: 0.07603255706801262\n",
      "Iteration 9195 Loss: 0.07603244903418926\n",
      "Iteration 9196 Loss: 0.076032341006629\n",
      "Iteration 9197 Loss: 0.07603223298533143\n",
      "Iteration 9198 Loss: 0.07603212497029625\n",
      "Iteration 9199 Loss: 0.07603201696152309\n",
      "Iteration 9200 Loss: 0.07603190895901152\n",
      "Iteration 9201 Loss: 0.07603180096276123\n",
      "Iteration 9202 Loss: 0.07603169297277189\n",
      "Iteration 9203 Loss: 0.07603158498904307\n",
      "Iteration 9204 Loss: 0.07603147701157441\n",
      "Iteration 9205 Loss: 0.07603136904036557\n",
      "Iteration 9206 Loss: 0.07603126107541619\n",
      "Iteration 9207 Loss: 0.07603115311672597\n",
      "Iteration 9208 Loss: 0.07603104516429435\n",
      "Iteration 9209 Loss: 0.07603093721812115\n",
      "Iteration 9210 Loss: 0.07603082927820597\n",
      "Iteration 9211 Loss: 0.07603072134454837\n",
      "Iteration 9212 Loss: 0.07603061341714808\n",
      "Iteration 9213 Loss: 0.07603050549600465\n",
      "Iteration 9214 Loss: 0.07603039758111776\n",
      "Iteration 9215 Loss: 0.0760302896724871\n",
      "Iteration 9216 Loss: 0.0760301817701122\n",
      "Iteration 9217 Loss: 0.07603007387399273\n",
      "Iteration 9218 Loss: 0.07602996598412838\n",
      "Iteration 9219 Loss: 0.07602985810051871\n",
      "Iteration 9220 Loss: 0.07602975022316341\n",
      "Iteration 9221 Loss: 0.07602964235206215\n",
      "Iteration 9222 Loss: 0.07602953448721443\n",
      "Iteration 9223 Loss: 0.07602942662862004\n",
      "Iteration 9224 Loss: 0.07602931877627853\n",
      "Iteration 9225 Loss: 0.07602921093018952\n",
      "Iteration 9226 Loss: 0.07602910309035274\n",
      "Iteration 9227 Loss: 0.07602899525676773\n",
      "Iteration 9228 Loss: 0.07602888742943413\n",
      "Iteration 9229 Loss: 0.07602877960835168\n",
      "Iteration 9230 Loss: 0.0760286717935199\n",
      "Iteration 9231 Loss: 0.07602856398493853\n",
      "Iteration 9232 Loss: 0.07602845618260706\n",
      "Iteration 9233 Loss: 0.07602834838652525\n",
      "Iteration 9234 Loss: 0.07602824059669272\n",
      "Iteration 9235 Loss: 0.07602813281310906\n",
      "Iteration 9236 Loss: 0.07602802503577394\n",
      "Iteration 9237 Loss: 0.07602791726468702\n",
      "Iteration 9238 Loss: 0.07602780949984787\n",
      "Iteration 9239 Loss: 0.07602770174125621\n",
      "Iteration 9240 Loss: 0.07602759398891154\n",
      "Iteration 9241 Loss: 0.07602748624281368\n",
      "Iteration 9242 Loss: 0.07602737850296214\n",
      "Iteration 9243 Loss: 0.07602727076935661\n",
      "Iteration 9244 Loss: 0.07602716304199666\n",
      "Iteration 9245 Loss: 0.07602705532088198\n",
      "Iteration 9246 Loss: 0.07602694760601224\n",
      "Iteration 9247 Loss: 0.076026839897387\n",
      "Iteration 9248 Loss: 0.07602673219500594\n",
      "Iteration 9249 Loss: 0.07602662449886871\n",
      "Iteration 9250 Loss: 0.0760265168089749\n",
      "Iteration 9251 Loss: 0.07602640912532418\n",
      "Iteration 9252 Loss: 0.07602630144791622\n",
      "Iteration 9253 Loss: 0.07602619377675056\n",
      "Iteration 9254 Loss: 0.07602608611182696\n",
      "Iteration 9255 Loss: 0.07602597845314492\n",
      "Iteration 9256 Loss: 0.07602587080070415\n",
      "Iteration 9257 Loss: 0.07602576315450434\n",
      "Iteration 9258 Loss: 0.07602565551454502\n",
      "Iteration 9259 Loss: 0.07602554788082591\n",
      "Iteration 9260 Loss: 0.0760254402533466\n",
      "Iteration 9261 Loss: 0.07602533263210677\n",
      "Iteration 9262 Loss: 0.07602522501710603\n",
      "Iteration 9263 Loss: 0.07602511740834397\n",
      "Iteration 9264 Loss: 0.0760250098058203\n",
      "Iteration 9265 Loss: 0.07602490220953464\n",
      "Iteration 9266 Loss: 0.07602479461948666\n",
      "Iteration 9267 Loss: 0.07602468703567586\n",
      "Iteration 9268 Loss: 0.07602457945810201\n",
      "Iteration 9269 Loss: 0.0760244718867647\n",
      "Iteration 9270 Loss: 0.07602436432166361\n",
      "Iteration 9271 Loss: 0.07602425676279838\n",
      "Iteration 9272 Loss: 0.07602414921016856\n",
      "Iteration 9273 Loss: 0.07602404166377381\n",
      "Iteration 9274 Loss: 0.0760239341236138\n",
      "Iteration 9275 Loss: 0.07602382658968827\n",
      "Iteration 9276 Loss: 0.07602371906199668\n",
      "Iteration 9277 Loss: 0.07602361154053872\n",
      "Iteration 9278 Loss: 0.07602350402531405\n",
      "Iteration 9279 Loss: 0.07602339651632231\n",
      "Iteration 9280 Loss: 0.07602328901356312\n",
      "Iteration 9281 Loss: 0.07602318151703619\n",
      "Iteration 9282 Loss: 0.07602307402674102\n",
      "Iteration 9283 Loss: 0.07602296654267737\n",
      "Iteration 9284 Loss: 0.07602285906484482\n",
      "Iteration 9285 Loss: 0.07602275159324302\n",
      "Iteration 9286 Loss: 0.0760226441278716\n",
      "Iteration 9287 Loss: 0.07602253666873018\n",
      "Iteration 9288 Loss: 0.07602242921581846\n",
      "Iteration 9289 Loss: 0.07602232176913605\n",
      "Iteration 9290 Loss: 0.07602221432868257\n",
      "Iteration 9291 Loss: 0.07602210689445763\n",
      "Iteration 9292 Loss: 0.07602199946646095\n",
      "Iteration 9293 Loss: 0.0760218920446921\n",
      "Iteration 9294 Loss: 0.0760217846291507\n",
      "Iteration 9295 Loss: 0.07602167721983648\n",
      "Iteration 9296 Loss: 0.07602156981674904\n",
      "Iteration 9297 Loss: 0.07602146241988793\n",
      "Iteration 9298 Loss: 0.07602135502925295\n",
      "Iteration 9299 Loss: 0.07602124764484358\n",
      "Iteration 9300 Loss: 0.07602114026665956\n",
      "Iteration 9301 Loss: 0.07602103289470045\n",
      "Iteration 9302 Loss: 0.076020925528966\n",
      "Iteration 9303 Loss: 0.07602081816945574\n",
      "Iteration 9304 Loss: 0.07602071081616935\n",
      "Iteration 9305 Loss: 0.07602060346910648\n",
      "Iteration 9306 Loss: 0.07602049612826672\n",
      "Iteration 9307 Loss: 0.07602038879364978\n",
      "Iteration 9308 Loss: 0.07602028146525526\n",
      "Iteration 9309 Loss: 0.07602017414308278\n",
      "Iteration 9310 Loss: 0.07602006682713199\n",
      "Iteration 9311 Loss: 0.07601995951740255\n",
      "Iteration 9312 Loss: 0.07601985221389412\n",
      "Iteration 9313 Loss: 0.07601974491660625\n",
      "Iteration 9314 Loss: 0.07601963762553864\n",
      "Iteration 9315 Loss: 0.07601953034069094\n",
      "Iteration 9316 Loss: 0.07601942306206273\n",
      "Iteration 9317 Loss: 0.07601931578965374\n",
      "Iteration 9318 Loss: 0.07601920852346353\n",
      "Iteration 9319 Loss: 0.07601910126349173\n",
      "Iteration 9320 Loss: 0.07601899400973808\n",
      "Iteration 9321 Loss: 0.07601888676220207\n",
      "Iteration 9322 Loss: 0.07601877952088348\n",
      "Iteration 9323 Loss: 0.07601867228578182\n",
      "Iteration 9324 Loss: 0.0760185650568969\n",
      "Iteration 9325 Loss: 0.07601845783422818\n",
      "Iteration 9326 Loss: 0.07601835061777536\n",
      "Iteration 9327 Loss: 0.07601824340753814\n",
      "Iteration 9328 Loss: 0.07601813620351608\n",
      "Iteration 9329 Loss: 0.07601802900570886\n",
      "Iteration 9330 Loss: 0.0760179218141161\n",
      "Iteration 9331 Loss: 0.07601781462873744\n",
      "Iteration 9332 Loss: 0.07601770744957254\n",
      "Iteration 9333 Loss: 0.07601760027662102\n",
      "Iteration 9334 Loss: 0.07601749310988247\n",
      "Iteration 9335 Loss: 0.07601738594935666\n",
      "Iteration 9336 Loss: 0.07601727879504314\n",
      "Iteration 9337 Loss: 0.07601717164694151\n",
      "Iteration 9338 Loss: 0.07601706450505151\n",
      "Iteration 9339 Loss: 0.0760169573693727\n",
      "Iteration 9340 Loss: 0.07601685023990473\n",
      "Iteration 9341 Loss: 0.07601674311664727\n",
      "Iteration 9342 Loss: 0.07601663599959993\n",
      "Iteration 9343 Loss: 0.07601652888876238\n",
      "Iteration 9344 Loss: 0.07601642178413422\n",
      "Iteration 9345 Loss: 0.07601631468571517\n",
      "Iteration 9346 Loss: 0.0760162075935047\n",
      "Iteration 9347 Loss: 0.07601610050750263\n",
      "Iteration 9348 Loss: 0.07601599342770857\n",
      "Iteration 9349 Loss: 0.07601588635412203\n",
      "Iteration 9350 Loss: 0.07601577928674273\n",
      "Iteration 9351 Loss: 0.07601567222557037\n",
      "Iteration 9352 Loss: 0.07601556517060458\n",
      "Iteration 9353 Loss: 0.07601545812184486\n",
      "Iteration 9354 Loss: 0.07601535107929099\n",
      "Iteration 9355 Loss: 0.0760152440429425\n",
      "Iteration 9356 Loss: 0.07601513701279917\n",
      "Iteration 9357 Loss: 0.07601502998886053\n",
      "Iteration 9358 Loss: 0.07601492297112623\n",
      "Iteration 9359 Loss: 0.07601481595959593\n",
      "Iteration 9360 Loss: 0.07601470895426936\n",
      "Iteration 9361 Loss: 0.07601460195514594\n",
      "Iteration 9362 Loss: 0.0760144949622255\n",
      "Iteration 9363 Loss: 0.07601438797550758\n",
      "Iteration 9364 Loss: 0.07601428099499191\n",
      "Iteration 9365 Loss: 0.07601417402067803\n",
      "Iteration 9366 Loss: 0.07601406705256568\n",
      "Iteration 9367 Loss: 0.07601396009065438\n",
      "Iteration 9368 Loss: 0.07601385313494391\n",
      "Iteration 9369 Loss: 0.07601374618543376\n",
      "Iteration 9370 Loss: 0.07601363924212369\n",
      "Iteration 9371 Loss: 0.07601353230501327\n",
      "Iteration 9372 Loss: 0.07601342537410213\n",
      "Iteration 9373 Loss: 0.07601331844939004\n",
      "Iteration 9374 Loss: 0.07601321153087649\n",
      "Iteration 9375 Loss: 0.07601310461856114\n",
      "Iteration 9376 Loss: 0.07601299771244369\n",
      "Iteration 9377 Loss: 0.07601289081252377\n",
      "Iteration 9378 Loss: 0.07601278391880101\n",
      "Iteration 9379 Loss: 0.07601267703127497\n",
      "Iteration 9380 Loss: 0.0760125701499454\n",
      "Iteration 9381 Loss: 0.07601246327481193\n",
      "Iteration 9382 Loss: 0.07601235640587412\n",
      "Iteration 9383 Loss: 0.07601224954313172\n",
      "Iteration 9384 Loss: 0.07601214268658427\n",
      "Iteration 9385 Loss: 0.07601203583623146\n",
      "Iteration 9386 Loss: 0.07601192899207292\n",
      "Iteration 9387 Loss: 0.07601182215410829\n",
      "Iteration 9388 Loss: 0.07601171532233723\n",
      "Iteration 9389 Loss: 0.0760116084967593\n",
      "Iteration 9390 Loss: 0.07601150167737426\n",
      "Iteration 9391 Loss: 0.07601139486418171\n",
      "Iteration 9392 Loss: 0.0760112880571812\n",
      "Iteration 9393 Loss: 0.07601118125637252\n",
      "Iteration 9394 Loss: 0.07601107446175517\n",
      "Iteration 9395 Loss: 0.07601096767332888\n",
      "Iteration 9396 Loss: 0.07601086089109323\n",
      "Iteration 9397 Loss: 0.07601075411504796\n",
      "Iteration 9398 Loss: 0.07601064734519256\n",
      "Iteration 9399 Loss: 0.07601054058152677\n",
      "Iteration 9400 Loss: 0.07601043382405027\n",
      "Iteration 9401 Loss: 0.07601032707276262\n",
      "Iteration 9402 Loss: 0.07601022032766347\n",
      "Iteration 9403 Loss: 0.07601011358875245\n",
      "Iteration 9404 Loss: 0.07601000685602925\n",
      "Iteration 9405 Loss: 0.07600990012949348\n",
      "Iteration 9406 Loss: 0.07600979340914481\n",
      "Iteration 9407 Loss: 0.07600968669498284\n",
      "Iteration 9408 Loss: 0.07600957998700722\n",
      "Iteration 9409 Loss: 0.0760094732852176\n",
      "Iteration 9410 Loss: 0.07600936658961363\n",
      "Iteration 9411 Loss: 0.0760092599001949\n",
      "Iteration 9412 Loss: 0.0760091532169611\n",
      "Iteration 9413 Loss: 0.07600904653991193\n",
      "Iteration 9414 Loss: 0.07600893986904693\n",
      "Iteration 9415 Loss: 0.07600883320436574\n",
      "Iteration 9416 Loss: 0.07600872654586796\n",
      "Iteration 9417 Loss: 0.07600861989355341\n",
      "Iteration 9418 Loss: 0.07600851324742161\n",
      "Iteration 9419 Loss: 0.0760084066074722\n",
      "Iteration 9420 Loss: 0.07600829997370485\n",
      "Iteration 9421 Loss: 0.07600819334611914\n",
      "Iteration 9422 Loss: 0.07600808672471478\n",
      "Iteration 9423 Loss: 0.07600798010949139\n",
      "Iteration 9424 Loss: 0.0760078735004486\n",
      "Iteration 9425 Loss: 0.0760077668975861\n",
      "Iteration 9426 Loss: 0.07600766030090346\n",
      "Iteration 9427 Loss: 0.07600755371040037\n",
      "Iteration 9428 Loss: 0.07600744712607642\n",
      "Iteration 9429 Loss: 0.07600734054793126\n",
      "Iteration 9430 Loss: 0.0760072339759646\n",
      "Iteration 9431 Loss: 0.07600712741017601\n",
      "Iteration 9432 Loss: 0.07600702085056514\n",
      "Iteration 9433 Loss: 0.07600691429713166\n",
      "Iteration 9434 Loss: 0.07600680774987521\n",
      "Iteration 9435 Loss: 0.07600670120879542\n",
      "Iteration 9436 Loss: 0.07600659467389198\n",
      "Iteration 9437 Loss: 0.07600648814516438\n",
      "Iteration 9438 Loss: 0.07600638162261246\n",
      "Iteration 9439 Loss: 0.07600627510623569\n",
      "Iteration 9440 Loss: 0.07600616859603379\n",
      "Iteration 9441 Loss: 0.07600606209200642\n",
      "Iteration 9442 Loss: 0.0760059555941532\n",
      "Iteration 9443 Loss: 0.07600584910247375\n",
      "Iteration 9444 Loss: 0.07600574261696774\n",
      "Iteration 9445 Loss: 0.0760056361376348\n",
      "Iteration 9446 Loss: 0.07600552966447455\n",
      "Iteration 9447 Loss: 0.07600542319748672\n",
      "Iteration 9448 Loss: 0.07600531673667085\n",
      "Iteration 9449 Loss: 0.0760052102820266\n",
      "Iteration 9450 Loss: 0.0760051038335536\n",
      "Iteration 9451 Loss: 0.07600499739125159\n",
      "Iteration 9452 Loss: 0.07600489095512013\n",
      "Iteration 9453 Loss: 0.07600478452515881\n",
      "Iteration 9454 Loss: 0.07600467810136739\n",
      "Iteration 9455 Loss: 0.07600457168374546\n",
      "Iteration 9456 Loss: 0.07600446527229261\n",
      "Iteration 9457 Loss: 0.07600435886700857\n",
      "Iteration 9458 Loss: 0.07600425246789295\n",
      "Iteration 9459 Loss: 0.07600414607494534\n",
      "Iteration 9460 Loss: 0.07600403968816545\n",
      "Iteration 9461 Loss: 0.07600393330755288\n",
      "Iteration 9462 Loss: 0.07600382693310731\n",
      "Iteration 9463 Loss: 0.07600372056482838\n",
      "Iteration 9464 Loss: 0.07600361420271566\n",
      "Iteration 9465 Loss: 0.07600350784676889\n",
      "Iteration 9466 Loss: 0.0760034014969876\n",
      "Iteration 9467 Loss: 0.07600329515337156\n",
      "Iteration 9468 Loss: 0.07600318881592032\n",
      "Iteration 9469 Loss: 0.07600308248463354\n",
      "Iteration 9470 Loss: 0.07600297615951089\n",
      "Iteration 9471 Loss: 0.07600286984055199\n",
      "Iteration 9472 Loss: 0.07600276352775652\n",
      "Iteration 9473 Loss: 0.07600265722112402\n",
      "Iteration 9474 Loss: 0.07600255092065422\n",
      "Iteration 9475 Loss: 0.07600244462634677\n",
      "Iteration 9476 Loss: 0.07600233833820123\n",
      "Iteration 9477 Loss: 0.07600223205621737\n",
      "Iteration 9478 Loss: 0.07600212578039471\n",
      "Iteration 9479 Loss: 0.07600201951073297\n",
      "Iteration 9480 Loss: 0.07600191324723174\n",
      "Iteration 9481 Loss: 0.07600180698989072\n",
      "Iteration 9482 Loss: 0.07600170073870949\n",
      "Iteration 9483 Loss: 0.07600159449368774\n",
      "Iteration 9484 Loss: 0.07600148825482504\n",
      "Iteration 9485 Loss: 0.07600138202212109\n",
      "Iteration 9486 Loss: 0.07600127579557556\n",
      "Iteration 9487 Loss: 0.07600116957518806\n",
      "Iteration 9488 Loss: 0.07600106336095827\n",
      "Iteration 9489 Loss: 0.07600095715288567\n",
      "Iteration 9490 Loss: 0.07600085095097012\n",
      "Iteration 9491 Loss: 0.07600074475521114\n",
      "Iteration 9492 Loss: 0.07600063856560842\n",
      "Iteration 9493 Loss: 0.07600053238216158\n",
      "Iteration 9494 Loss: 0.07600042620487024\n",
      "Iteration 9495 Loss: 0.07600032003373405\n",
      "Iteration 9496 Loss: 0.07600021386875272\n",
      "Iteration 9497 Loss: 0.0760001077099258\n",
      "Iteration 9498 Loss: 0.07600000155725303\n",
      "Iteration 9499 Loss: 0.07599989541073397\n",
      "Iteration 9500 Loss: 0.07599978927036828\n",
      "Iteration 9501 Loss: 0.07599968313615559\n",
      "Iteration 9502 Loss: 0.07599957700809562\n",
      "Iteration 9503 Loss: 0.07599947088618791\n",
      "Iteration 9504 Loss: 0.07599936477043222\n",
      "Iteration 9505 Loss: 0.07599925866082805\n",
      "Iteration 9506 Loss: 0.07599915255737512\n",
      "Iteration 9507 Loss: 0.0759990464600731\n",
      "Iteration 9508 Loss: 0.07599894036892159\n",
      "Iteration 9509 Loss: 0.07599883428392028\n",
      "Iteration 9510 Loss: 0.07599872820506871\n",
      "Iteration 9511 Loss: 0.0759986221323666\n",
      "Iteration 9512 Loss: 0.0759985160658136\n",
      "Iteration 9513 Loss: 0.07599841000540938\n",
      "Iteration 9514 Loss: 0.07599830395115353\n",
      "Iteration 9515 Loss: 0.07599819790304563\n",
      "Iteration 9516 Loss: 0.07599809186108543\n",
      "Iteration 9517 Loss: 0.07599798582527258\n",
      "Iteration 9518 Loss: 0.07599787979560668\n",
      "Iteration 9519 Loss: 0.07599777377208727\n",
      "Iteration 9520 Loss: 0.07599766775471421\n",
      "Iteration 9521 Loss: 0.07599756174348697\n",
      "Iteration 9522 Loss: 0.07599745573840527\n",
      "Iteration 9523 Loss: 0.07599734973946871\n",
      "Iteration 9524 Loss: 0.07599724374667698\n",
      "Iteration 9525 Loss: 0.07599713776002971\n",
      "Iteration 9526 Loss: 0.07599703177952653\n",
      "Iteration 9527 Loss: 0.07599692580516706\n",
      "Iteration 9528 Loss: 0.07599681983695106\n",
      "Iteration 9529 Loss: 0.07599671387487798\n",
      "Iteration 9530 Loss: 0.07599660791894762\n",
      "Iteration 9531 Loss: 0.07599650196915958\n",
      "Iteration 9532 Loss: 0.07599639602551349\n",
      "Iteration 9533 Loss: 0.07599629008800896\n",
      "Iteration 9534 Loss: 0.07599618415664572\n",
      "Iteration 9535 Loss: 0.07599607823142333\n",
      "Iteration 9536 Loss: 0.07599597231234148\n",
      "Iteration 9537 Loss: 0.0759958663993998\n",
      "Iteration 9538 Loss: 0.07599576049259796\n",
      "Iteration 9539 Loss: 0.07599565459193555\n",
      "Iteration 9540 Loss: 0.07599554869741225\n",
      "Iteration 9541 Loss: 0.07599544280902767\n",
      "Iteration 9542 Loss: 0.07599533692678151\n",
      "Iteration 9543 Loss: 0.07599523105067336\n",
      "Iteration 9544 Loss: 0.07599512518070292\n",
      "Iteration 9545 Loss: 0.07599501931686975\n",
      "Iteration 9546 Loss: 0.07599491345917361\n",
      "Iteration 9547 Loss: 0.07599480760761405\n",
      "Iteration 9548 Loss: 0.07599470176219068\n",
      "Iteration 9549 Loss: 0.07599459592290328\n",
      "Iteration 9550 Loss: 0.0759944900897514\n",
      "Iteration 9551 Loss: 0.07599438426273472\n",
      "Iteration 9552 Loss: 0.07599427844185282\n",
      "Iteration 9553 Loss: 0.07599417262710541\n",
      "Iteration 9554 Loss: 0.0759940668184921\n",
      "Iteration 9555 Loss: 0.07599396101601255\n",
      "Iteration 9556 Loss: 0.07599385521966645\n",
      "Iteration 9557 Loss: 0.07599374942945336\n",
      "Iteration 9558 Loss: 0.07599364364537294\n",
      "Iteration 9559 Loss: 0.07599353786742485\n",
      "Iteration 9560 Loss: 0.07599343209560877\n",
      "Iteration 9561 Loss: 0.07599332632992428\n",
      "Iteration 9562 Loss: 0.07599322057037104\n",
      "Iteration 9563 Loss: 0.07599311481694876\n",
      "Iteration 9564 Loss: 0.07599300906965704\n",
      "Iteration 9565 Loss: 0.07599290332849545\n",
      "Iteration 9566 Loss: 0.07599279759346374\n",
      "Iteration 9567 Loss: 0.07599269186456148\n",
      "Iteration 9568 Loss: 0.07599258614178836\n",
      "Iteration 9569 Loss: 0.07599248042514403\n",
      "Iteration 9570 Loss: 0.07599237471462815\n",
      "Iteration 9571 Loss: 0.0759922690102403\n",
      "Iteration 9572 Loss: 0.07599216331198011\n",
      "Iteration 9573 Loss: 0.07599205761984733\n",
      "Iteration 9574 Loss: 0.07599195193384153\n",
      "Iteration 9575 Loss: 0.07599184625396234\n",
      "Iteration 9576 Loss: 0.07599174058020948\n",
      "Iteration 9577 Loss: 0.07599163491258251\n",
      "Iteration 9578 Loss: 0.07599152925108112\n",
      "Iteration 9579 Loss: 0.07599142359570493\n",
      "Iteration 9580 Loss: 0.07599131794645363\n",
      "Iteration 9581 Loss: 0.07599121230332682\n",
      "Iteration 9582 Loss: 0.07599110666632415\n",
      "Iteration 9583 Loss: 0.07599100103544527\n",
      "Iteration 9584 Loss: 0.07599089541068985\n",
      "Iteration 9585 Loss: 0.07599078979205748\n",
      "Iteration 9586 Loss: 0.07599068417954782\n",
      "Iteration 9587 Loss: 0.07599057857316062\n",
      "Iteration 9588 Loss: 0.07599047297289538\n",
      "Iteration 9589 Loss: 0.07599036737875178\n",
      "Iteration 9590 Loss: 0.07599026179072949\n",
      "Iteration 9591 Loss: 0.07599015620882818\n",
      "Iteration 9592 Loss: 0.07599005063304742\n",
      "Iteration 9593 Loss: 0.07598994506338692\n",
      "Iteration 9594 Loss: 0.07598983949984635\n",
      "Iteration 9595 Loss: 0.07598973394242522\n",
      "Iteration 9596 Loss: 0.07598962839112332\n",
      "Iteration 9597 Loss: 0.07598952284594025\n",
      "Iteration 9598 Loss: 0.07598941730687557\n",
      "Iteration 9599 Loss: 0.07598931177392902\n",
      "Iteration 9600 Loss: 0.07598920624710026\n",
      "Iteration 9601 Loss: 0.07598910072638886\n",
      "Iteration 9602 Loss: 0.07598899521179454\n",
      "Iteration 9603 Loss: 0.07598888970331688\n",
      "Iteration 9604 Loss: 0.07598878420095553\n",
      "Iteration 9605 Loss: 0.07598867870471018\n",
      "Iteration 9606 Loss: 0.0759885732145805\n",
      "Iteration 9607 Loss: 0.07598846773056604\n",
      "Iteration 9608 Loss: 0.07598836225266645\n",
      "Iteration 9609 Loss: 0.07598825678088145\n",
      "Iteration 9610 Loss: 0.07598815131521067\n",
      "Iteration 9611 Loss: 0.07598804585565365\n",
      "Iteration 9612 Loss: 0.07598794040221023\n",
      "Iteration 9613 Loss: 0.0759878349548799\n",
      "Iteration 9614 Loss: 0.07598772951366235\n",
      "Iteration 9615 Loss: 0.07598762407855725\n",
      "Iteration 9616 Loss: 0.07598751864956418\n",
      "Iteration 9617 Loss: 0.07598741322668286\n",
      "Iteration 9618 Loss: 0.07598730780991288\n",
      "Iteration 9619 Loss: 0.07598720239925393\n",
      "Iteration 9620 Loss: 0.07598709699470561\n",
      "Iteration 9621 Loss: 0.07598699159626757\n",
      "Iteration 9622 Loss: 0.07598688620393951\n",
      "Iteration 9623 Loss: 0.07598678081772102\n",
      "Iteration 9624 Loss: 0.07598667543761178\n",
      "Iteration 9625 Loss: 0.07598657006361136\n",
      "Iteration 9626 Loss: 0.07598646469571951\n",
      "Iteration 9627 Loss: 0.07598635933393584\n",
      "Iteration 9628 Loss: 0.07598625397825995\n",
      "Iteration 9629 Loss: 0.07598614862869157\n",
      "Iteration 9630 Loss: 0.07598604328523029\n",
      "Iteration 9631 Loss: 0.07598593794787571\n",
      "Iteration 9632 Loss: 0.07598583261662756\n",
      "Iteration 9633 Loss: 0.07598572729148544\n",
      "Iteration 9634 Loss: 0.07598562197244903\n",
      "Iteration 9635 Loss: 0.07598551665951793\n",
      "Iteration 9636 Loss: 0.07598541135269178\n",
      "Iteration 9637 Loss: 0.07598530605197032\n",
      "Iteration 9638 Loss: 0.07598520075735309\n",
      "Iteration 9639 Loss: 0.07598509546883982\n",
      "Iteration 9640 Loss: 0.07598499018643004\n",
      "Iteration 9641 Loss: 0.07598488491012353\n",
      "Iteration 9642 Loss: 0.07598477963991983\n",
      "Iteration 9643 Loss: 0.07598467437581866\n",
      "Iteration 9644 Loss: 0.07598456911781959\n",
      "Iteration 9645 Loss: 0.07598446386592234\n",
      "Iteration 9646 Loss: 0.07598435862012652\n",
      "Iteration 9647 Loss: 0.07598425338043184\n",
      "Iteration 9648 Loss: 0.0759841481468378\n",
      "Iteration 9649 Loss: 0.07598404291934413\n",
      "Iteration 9650 Loss: 0.07598393769795052\n",
      "Iteration 9651 Loss: 0.07598383248265658\n",
      "Iteration 9652 Loss: 0.07598372727346192\n",
      "Iteration 9653 Loss: 0.07598362207036626\n",
      "Iteration 9654 Loss: 0.07598351687336917\n",
      "Iteration 9655 Loss: 0.07598341168247037\n",
      "Iteration 9656 Loss: 0.07598330649766939\n",
      "Iteration 9657 Loss: 0.07598320131896606\n",
      "Iteration 9658 Loss: 0.07598309614635981\n",
      "Iteration 9659 Loss: 0.07598299097985044\n",
      "Iteration 9660 Loss: 0.07598288581943752\n",
      "Iteration 9661 Loss: 0.07598278066512075\n",
      "Iteration 9662 Loss: 0.0759826755168998\n",
      "Iteration 9663 Loss: 0.07598257037477418\n",
      "Iteration 9664 Loss: 0.0759824652387437\n",
      "Iteration 9665 Loss: 0.07598236010880789\n",
      "Iteration 9666 Loss: 0.07598225498496643\n",
      "Iteration 9667 Loss: 0.07598214986721899\n",
      "Iteration 9668 Loss: 0.07598204475556523\n",
      "Iteration 9669 Loss: 0.0759819396500047\n",
      "Iteration 9670 Loss: 0.07598183455053711\n",
      "Iteration 9671 Loss: 0.07598172945716214\n",
      "Iteration 9672 Loss: 0.07598162436987942\n",
      "Iteration 9673 Loss: 0.07598151928868856\n",
      "Iteration 9674 Loss: 0.07598141421358921\n",
      "Iteration 9675 Loss: 0.07598130914458105\n",
      "Iteration 9676 Loss: 0.07598120408166373\n",
      "Iteration 9677 Loss: 0.07598109902483684\n",
      "Iteration 9678 Loss: 0.07598099397410012\n",
      "Iteration 9679 Loss: 0.0759808889294531\n",
      "Iteration 9680 Loss: 0.0759807838908955\n",
      "Iteration 9681 Loss: 0.07598067885842696\n",
      "Iteration 9682 Loss: 0.07598057383204714\n",
      "Iteration 9683 Loss: 0.07598046881175566\n",
      "Iteration 9684 Loss: 0.07598036379755213\n",
      "Iteration 9685 Loss: 0.07598025878943626\n",
      "Iteration 9686 Loss: 0.07598015378740768\n",
      "Iteration 9687 Loss: 0.07598004879146608\n",
      "Iteration 9688 Loss: 0.07597994380161098\n",
      "Iteration 9689 Loss: 0.07597983881784216\n",
      "Iteration 9690 Loss: 0.07597973384015917\n",
      "Iteration 9691 Loss: 0.0759796288685617\n",
      "Iteration 9692 Loss: 0.07597952390304943\n",
      "Iteration 9693 Loss: 0.07597941894362198\n",
      "Iteration 9694 Loss: 0.07597931399027898\n",
      "Iteration 9695 Loss: 0.07597920904302008\n",
      "Iteration 9696 Loss: 0.0759791041018449\n",
      "Iteration 9697 Loss: 0.07597899916675319\n",
      "Iteration 9698 Loss: 0.07597889423774445\n",
      "Iteration 9699 Loss: 0.07597878931481847\n",
      "Iteration 9700 Loss: 0.0759786843979748\n",
      "Iteration 9701 Loss: 0.07597857948721312\n",
      "Iteration 9702 Loss: 0.0759784745825331\n",
      "Iteration 9703 Loss: 0.07597836968393433\n",
      "Iteration 9704 Loss: 0.07597826479141648\n",
      "Iteration 9705 Loss: 0.07597815990497925\n",
      "Iteration 9706 Loss: 0.07597805502462222\n",
      "Iteration 9707 Loss: 0.07597795015034506\n",
      "Iteration 9708 Loss: 0.07597784528214746\n",
      "Iteration 9709 Loss: 0.07597774042002894\n",
      "Iteration 9710 Loss: 0.07597763556398929\n",
      "Iteration 9711 Loss: 0.0759775307140281\n",
      "Iteration 9712 Loss: 0.075977425870145\n",
      "Iteration 9713 Loss: 0.07597732103233967\n",
      "Iteration 9714 Loss: 0.0759772162006117\n",
      "Iteration 9715 Loss: 0.07597711137496083\n",
      "Iteration 9716 Loss: 0.07597700655538664\n",
      "Iteration 9717 Loss: 0.0759769017418888\n",
      "Iteration 9718 Loss: 0.07597679693446695\n",
      "Iteration 9719 Loss: 0.0759766921331207\n",
      "Iteration 9720 Loss: 0.07597658733784977\n",
      "Iteration 9721 Loss: 0.07597648254865376\n",
      "Iteration 9722 Loss: 0.07597637776553233\n",
      "Iteration 9723 Loss: 0.07597627298848511\n",
      "Iteration 9724 Loss: 0.07597616821751181\n",
      "Iteration 9725 Loss: 0.075976063452612\n",
      "Iteration 9726 Loss: 0.07597595869378539\n",
      "Iteration 9727 Loss: 0.07597585394103155\n",
      "Iteration 9728 Loss: 0.07597574919435021\n",
      "Iteration 9729 Loss: 0.07597564445374097\n",
      "Iteration 9730 Loss: 0.07597553971920351\n",
      "Iteration 9731 Loss: 0.07597543499073746\n",
      "Iteration 9732 Loss: 0.07597533026834247\n",
      "Iteration 9733 Loss: 0.07597522555201812\n",
      "Iteration 9734 Loss: 0.07597512084176422\n",
      "Iteration 9735 Loss: 0.0759750161375802\n",
      "Iteration 9736 Loss: 0.0759749114394659\n",
      "Iteration 9737 Loss: 0.07597480674742087\n",
      "Iteration 9738 Loss: 0.0759747020614448\n",
      "Iteration 9739 Loss: 0.07597459738153732\n",
      "Iteration 9740 Loss: 0.07597449270769811\n",
      "Iteration 9741 Loss: 0.07597438803992676\n",
      "Iteration 9742 Loss: 0.07597428337822293\n",
      "Iteration 9743 Loss: 0.07597417872258627\n",
      "Iteration 9744 Loss: 0.07597407407301647\n",
      "Iteration 9745 Loss: 0.07597396942951312\n",
      "Iteration 9746 Loss: 0.07597386479207595\n",
      "Iteration 9747 Loss: 0.07597376016070449\n",
      "Iteration 9748 Loss: 0.07597365553539848\n",
      "Iteration 9749 Loss: 0.07597355091615754\n",
      "Iteration 9750 Loss: 0.07597344630298133\n",
      "Iteration 9751 Loss: 0.07597334169586947\n",
      "Iteration 9752 Loss: 0.07597323709482161\n",
      "Iteration 9753 Loss: 0.07597313249983742\n",
      "Iteration 9754 Loss: 0.0759730279109165\n",
      "Iteration 9755 Loss: 0.07597292332805862\n",
      "Iteration 9756 Loss: 0.0759728187512633\n",
      "Iteration 9757 Loss: 0.07597271418053024\n",
      "Iteration 9758 Loss: 0.07597260961585905\n",
      "Iteration 9759 Loss: 0.07597250505724949\n",
      "Iteration 9760 Loss: 0.07597240050470105\n",
      "Iteration 9761 Loss: 0.07597229595821349\n",
      "Iteration 9762 Loss: 0.07597219141778645\n",
      "Iteration 9763 Loss: 0.07597208688341951\n",
      "Iteration 9764 Loss: 0.0759719823551124\n",
      "Iteration 9765 Loss: 0.0759718778328647\n",
      "Iteration 9766 Loss: 0.07597177331667611\n",
      "Iteration 9767 Loss: 0.07597166880654624\n",
      "Iteration 9768 Loss: 0.07597156430247477\n",
      "Iteration 9769 Loss: 0.07597145980446132\n",
      "Iteration 9770 Loss: 0.0759713553125056\n",
      "Iteration 9771 Loss: 0.07597125082660715\n",
      "Iteration 9772 Loss: 0.07597114634676569\n",
      "Iteration 9773 Loss: 0.0759710418729809\n",
      "Iteration 9774 Loss: 0.07597093740525233\n",
      "Iteration 9775 Loss: 0.07597083294357974\n",
      "Iteration 9776 Loss: 0.07597072848796267\n",
      "Iteration 9777 Loss: 0.07597062403840088\n",
      "Iteration 9778 Loss: 0.07597051959489394\n",
      "Iteration 9779 Loss: 0.07597041515744149\n",
      "Iteration 9780 Loss: 0.07597031072604322\n",
      "Iteration 9781 Loss: 0.0759702063006988\n",
      "Iteration 9782 Loss: 0.07597010188140783\n",
      "Iteration 9783 Loss: 0.07596999746816997\n",
      "Iteration 9784 Loss: 0.07596989306098487\n",
      "Iteration 9785 Loss: 0.07596978865985218\n",
      "Iteration 9786 Loss: 0.07596968426477158\n",
      "Iteration 9787 Loss: 0.07596957987574265\n",
      "Iteration 9788 Loss: 0.07596947549276507\n",
      "Iteration 9789 Loss: 0.07596937111583851\n",
      "Iteration 9790 Loss: 0.07596926674496265\n",
      "Iteration 9791 Loss: 0.07596916238013708\n",
      "Iteration 9792 Loss: 0.07596905802136142\n",
      "Iteration 9793 Loss: 0.0759689536686354\n",
      "Iteration 9794 Loss: 0.07596884932195865\n",
      "Iteration 9795 Loss: 0.07596874498133074\n",
      "Iteration 9796 Loss: 0.07596864064675145\n",
      "Iteration 9797 Loss: 0.07596853631822033\n",
      "Iteration 9798 Loss: 0.07596843199573702\n",
      "Iteration 9799 Loss: 0.07596832767930124\n",
      "Iteration 9800 Loss: 0.07596822336891261\n",
      "Iteration 9801 Loss: 0.07596811906457081\n",
      "Iteration 9802 Loss: 0.07596801476627539\n",
      "Iteration 9803 Loss: 0.07596791047402612\n",
      "Iteration 9804 Loss: 0.07596780618782258\n",
      "Iteration 9805 Loss: 0.0759677019076644\n",
      "Iteration 9806 Loss: 0.07596759763355129\n",
      "Iteration 9807 Loss: 0.07596749336548284\n",
      "Iteration 9808 Loss: 0.07596738910345875\n",
      "Iteration 9809 Loss: 0.07596728484747867\n",
      "Iteration 9810 Loss: 0.0759671805975422\n",
      "Iteration 9811 Loss: 0.07596707635364904\n",
      "Iteration 9812 Loss: 0.07596697211579881\n",
      "Iteration 9813 Loss: 0.07596686788399118\n",
      "Iteration 9814 Loss: 0.07596676365822573\n",
      "Iteration 9815 Loss: 0.07596665943850224\n",
      "Iteration 9816 Loss: 0.07596655522482024\n",
      "Iteration 9817 Loss: 0.07596645101717941\n",
      "Iteration 9818 Loss: 0.07596634681557948\n",
      "Iteration 9819 Loss: 0.07596624262002001\n",
      "Iteration 9820 Loss: 0.07596613843050062\n",
      "Iteration 9821 Loss: 0.07596603424702103\n",
      "Iteration 9822 Loss: 0.07596593006958093\n",
      "Iteration 9823 Loss: 0.07596582589817986\n",
      "Iteration 9824 Loss: 0.07596572173281756\n",
      "Iteration 9825 Loss: 0.07596561757349363\n",
      "Iteration 9826 Loss: 0.07596551342020773\n",
      "Iteration 9827 Loss: 0.07596540927295949\n",
      "Iteration 9828 Loss: 0.07596530513174858\n",
      "Iteration 9829 Loss: 0.07596520099657464\n",
      "Iteration 9830 Loss: 0.07596509686743737\n",
      "Iteration 9831 Loss: 0.07596499274433635\n",
      "Iteration 9832 Loss: 0.07596488862727128\n",
      "Iteration 9833 Loss: 0.07596478451624175\n",
      "Iteration 9834 Loss: 0.0759646804112475\n",
      "Iteration 9835 Loss: 0.07596457631228809\n",
      "Iteration 9836 Loss: 0.07596447221936323\n",
      "Iteration 9837 Loss: 0.07596436813247257\n",
      "Iteration 9838 Loss: 0.07596426405161569\n",
      "Iteration 9839 Loss: 0.07596415997679226\n",
      "Iteration 9840 Loss: 0.07596405590800201\n",
      "Iteration 9841 Loss: 0.07596395184524458\n",
      "Iteration 9842 Loss: 0.07596384778851954\n",
      "Iteration 9843 Loss: 0.07596374373782658\n",
      "Iteration 9844 Loss: 0.07596363969316534\n",
      "Iteration 9845 Loss: 0.07596353565453545\n",
      "Iteration 9846 Loss: 0.07596343162193665\n",
      "Iteration 9847 Loss: 0.07596332759536847\n",
      "Iteration 9848 Loss: 0.07596322357483067\n",
      "Iteration 9849 Loss: 0.0759631195603228\n",
      "Iteration 9850 Loss: 0.07596301555184459\n",
      "Iteration 9851 Loss: 0.07596291154939562\n",
      "Iteration 9852 Loss: 0.07596280755297563\n",
      "Iteration 9853 Loss: 0.07596270356258421\n",
      "Iteration 9854 Loss: 0.07596259957822099\n",
      "Iteration 9855 Loss: 0.07596249559988567\n",
      "Iteration 9856 Loss: 0.0759623916275779\n",
      "Iteration 9857 Loss: 0.07596228766129727\n",
      "Iteration 9858 Loss: 0.07596218370104352\n",
      "Iteration 9859 Loss: 0.07596207974681624\n",
      "Iteration 9860 Loss: 0.07596197579861506\n",
      "Iteration 9861 Loss: 0.07596187185643968\n",
      "Iteration 9862 Loss: 0.07596176792028973\n",
      "Iteration 9863 Loss: 0.07596166399016485\n",
      "Iteration 9864 Loss: 0.07596156006606473\n",
      "Iteration 9865 Loss: 0.07596145614798896\n",
      "Iteration 9866 Loss: 0.0759613522359373\n",
      "Iteration 9867 Loss: 0.0759612483299092\n",
      "Iteration 9868 Loss: 0.07596114442990455\n",
      "Iteration 9869 Loss: 0.07596104053592286\n",
      "Iteration 9870 Loss: 0.07596093664796373\n",
      "Iteration 9871 Loss: 0.07596083276602696\n",
      "Iteration 9872 Loss: 0.07596072889011206\n",
      "Iteration 9873 Loss: 0.07596062502021883\n",
      "Iteration 9874 Loss: 0.07596052115634683\n",
      "Iteration 9875 Loss: 0.0759604172984957\n",
      "Iteration 9876 Loss: 0.07596031344666511\n",
      "Iteration 9877 Loss: 0.07596020960085469\n",
      "Iteration 9878 Loss: 0.07596010576106414\n",
      "Iteration 9879 Loss: 0.07596000192729306\n",
      "Iteration 9880 Loss: 0.07595989809954118\n",
      "Iteration 9881 Loss: 0.07595979427780804\n",
      "Iteration 9882 Loss: 0.07595969046209335\n",
      "Iteration 9883 Loss: 0.07595958665239677\n",
      "Iteration 9884 Loss: 0.07595948284871794\n",
      "Iteration 9885 Loss: 0.07595937905105649\n",
      "Iteration 9886 Loss: 0.07595927525941208\n",
      "Iteration 9887 Loss: 0.07595917147378439\n",
      "Iteration 9888 Loss: 0.07595906769417306\n",
      "Iteration 9889 Loss: 0.07595896392057772\n",
      "Iteration 9890 Loss: 0.07595886015299799\n",
      "Iteration 9891 Loss: 0.0759587563914336\n",
      "Iteration 9892 Loss: 0.07595865263588421\n",
      "Iteration 9893 Loss: 0.07595854888634937\n",
      "Iteration 9894 Loss: 0.07595844514282883\n",
      "Iteration 9895 Loss: 0.07595834140532212\n",
      "Iteration 9896 Loss: 0.07595823767382903\n",
      "Iteration 9897 Loss: 0.07595813394834917\n",
      "Iteration 9898 Loss: 0.07595803022888209\n",
      "Iteration 9899 Loss: 0.07595792651542757\n",
      "Iteration 9900 Loss: 0.07595782280798524\n",
      "Iteration 9901 Loss: 0.07595771910655467\n",
      "Iteration 9902 Loss: 0.07595761541113565\n",
      "Iteration 9903 Loss: 0.07595751172172766\n",
      "Iteration 9904 Loss: 0.07595740803833045\n",
      "Iteration 9905 Loss: 0.07595730436094371\n",
      "Iteration 9906 Loss: 0.075957200689567\n",
      "Iteration 9907 Loss: 0.07595709702420006\n",
      "Iteration 9908 Loss: 0.07595699336484246\n",
      "Iteration 9909 Loss: 0.07595688971149385\n",
      "Iteration 9910 Loss: 0.07595678606415396\n",
      "Iteration 9911 Loss: 0.0759566824228224\n",
      "Iteration 9912 Loss: 0.0759565787874988\n",
      "Iteration 9913 Loss: 0.07595647515818284\n",
      "Iteration 9914 Loss: 0.07595637153487418\n",
      "Iteration 9915 Loss: 0.07595626791757241\n",
      "Iteration 9916 Loss: 0.07595616430627727\n",
      "Iteration 9917 Loss: 0.07595606070098836\n",
      "Iteration 9918 Loss: 0.07595595710170536\n",
      "Iteration 9919 Loss: 0.07595585350842785\n",
      "Iteration 9920 Loss: 0.0759557499211555\n",
      "Iteration 9921 Loss: 0.07595564633988806\n",
      "Iteration 9922 Loss: 0.07595554276462516\n",
      "Iteration 9923 Loss: 0.0759554391953663\n",
      "Iteration 9924 Loss: 0.07595533563211129\n",
      "Iteration 9925 Loss: 0.07595523207485973\n",
      "Iteration 9926 Loss: 0.07595512852361125\n",
      "Iteration 9927 Loss: 0.07595502497836558\n",
      "Iteration 9928 Loss: 0.07595492143912225\n",
      "Iteration 9929 Loss: 0.07595481790588104\n",
      "Iteration 9930 Loss: 0.07595471437864147\n",
      "Iteration 9931 Loss: 0.07595461085740329\n",
      "Iteration 9932 Loss: 0.07595450734216615\n",
      "Iteration 9933 Loss: 0.07595440383292966\n",
      "Iteration 9934 Loss: 0.07595430032969348\n",
      "Iteration 9935 Loss: 0.07595419683245727\n",
      "Iteration 9936 Loss: 0.07595409334122069\n",
      "Iteration 9937 Loss: 0.07595398985598337\n",
      "Iteration 9938 Loss: 0.075953886376745\n",
      "Iteration 9939 Loss: 0.07595378290350517\n",
      "Iteration 9940 Loss: 0.07595367943626356\n",
      "Iteration 9941 Loss: 0.0759535759750199\n",
      "Iteration 9942 Loss: 0.0759534725197737\n",
      "Iteration 9943 Loss: 0.07595336907052472\n",
      "Iteration 9944 Loss: 0.07595326562727257\n",
      "Iteration 9945 Loss: 0.07595316219001695\n",
      "Iteration 9946 Loss: 0.07595305875875741\n",
      "Iteration 9947 Loss: 0.07595295533349371\n",
      "Iteration 9948 Loss: 0.07595285191422542\n",
      "Iteration 9949 Loss: 0.07595274850095221\n",
      "Iteration 9950 Loss: 0.07595264509367385\n",
      "Iteration 9951 Loss: 0.07595254169238978\n",
      "Iteration 9952 Loss: 0.07595243829709979\n",
      "Iteration 9953 Loss: 0.07595233490780354\n",
      "Iteration 9954 Loss: 0.07595223152450066\n",
      "Iteration 9955 Loss: 0.07595212814719077\n",
      "Iteration 9956 Loss: 0.07595202477587351\n",
      "Iteration 9957 Loss: 0.07595192141054864\n",
      "Iteration 9958 Loss: 0.0759518180512157\n",
      "Iteration 9959 Loss: 0.07595171469787433\n",
      "Iteration 9960 Loss: 0.07595161135052432\n",
      "Iteration 9961 Loss: 0.07595150800916521\n",
      "Iteration 9962 Loss: 0.07595140467379667\n",
      "Iteration 9963 Loss: 0.07595130134441837\n",
      "Iteration 9964 Loss: 0.07595119802102991\n",
      "Iteration 9965 Loss: 0.07595109470363105\n",
      "Iteration 9966 Loss: 0.07595099139222133\n",
      "Iteration 9967 Loss: 0.07595088808680048\n",
      "Iteration 9968 Loss: 0.07595078478736814\n",
      "Iteration 9969 Loss: 0.07595068149392391\n",
      "Iteration 9970 Loss: 0.07595057820646751\n",
      "Iteration 9971 Loss: 0.07595047492499857\n",
      "Iteration 9972 Loss: 0.07595037164951667\n",
      "Iteration 9973 Loss: 0.07595026838002154\n",
      "Iteration 9974 Loss: 0.0759501651165129\n",
      "Iteration 9975 Loss: 0.07595006185899027\n",
      "Iteration 9976 Loss: 0.07594995860745336\n",
      "Iteration 9977 Loss: 0.07594985536190181\n",
      "Iteration 9978 Loss: 0.07594975212233529\n",
      "Iteration 9979 Loss: 0.07594964888875345\n",
      "Iteration 9980 Loss: 0.07594954566115591\n",
      "Iteration 9981 Loss: 0.07594944243954241\n",
      "Iteration 9982 Loss: 0.07594933922391249\n",
      "Iteration 9983 Loss: 0.0759492360142659\n",
      "Iteration 9984 Loss: 0.0759491328106022\n",
      "Iteration 9985 Loss: 0.07594902961292109\n",
      "Iteration 9986 Loss: 0.07594892642122228\n",
      "Iteration 9987 Loss: 0.07594882323550531\n",
      "Iteration 9988 Loss: 0.07594872005576993\n",
      "Iteration 9989 Loss: 0.07594861688201576\n",
      "Iteration 9990 Loss: 0.07594851371424244\n",
      "Iteration 9991 Loss: 0.0759484105524496\n",
      "Iteration 9992 Loss: 0.07594830739663695\n",
      "Iteration 9993 Loss: 0.07594820424680408\n",
      "Iteration 9994 Loss: 0.0759481011029507\n",
      "Iteration 9995 Loss: 0.07594799796507648\n",
      "Iteration 9996 Loss: 0.07594789483318098\n",
      "Iteration 9997 Loss: 0.07594779170726394\n",
      "Iteration 9998 Loss: 0.07594768858732498\n",
      "Iteration 9999 Loss: 0.07594758547336379\n"
     ]
    }
   ],
   "source": [
    "w, b, l = fit(X, y, degrees=[2], n_iters=10000, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aecd47",
   "metadata": {},
   "source": [
    "### Plotting the Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "74ec380e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.scatter(X, y, color='blue', marker='o', s = 10)\n",
    "plt.scatter(X, predict(X, w, b, [2]), color='red', marker='o', s = 10)\n",
    "plt.legend([\"Data\", \"Polynomial predictions\"])\n",
    "plt.xlabel('X (Input)')\n",
    "plt.ylabel('y (target)')\n",
    "plt.title('Polynomial Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6245d686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9725751232289634"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def r2_score(y, y_pred):\n",
    "    return 1 - (np.sum((np.array(y_pred)-np.array(y))**2)/\n",
    "                np.sum((np.array(y)-np.mean(np.array(y)))**2))\n",
    "r2_score(y, predict(X, w, b, [2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa20e6ac",
   "metadata": {},
   "source": [
    "**An R2 score of 0.97 indicates that 97% of the variance in our dependent variable is explained by our independent variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8e698ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(l)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Training Loss Over Iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56702b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
